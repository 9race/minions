File,Buggy Code,Errors,Buggy Functions
django:contrib:auth:migrations:0011_update_proxy_permissions.py,"--------------------------------------------------
import sys

from django.core.management.color import color_style
from django.db import IntegrityError, migrations, transaction
from django.db.models import Q

WARNING = """"""
    A problem arose migrating proxy model permissions for {old} to {new}.

      Permission(s) for {new} already existed.
      Codenames Q: {query}

    Ensure to audit ALL permissions for {old} and {new}.
""""""


def update_proxy_model_permissions(apps, schema_editor, reverse=False):
    """"""
    Update the content_type of proxy model permissions to use the ContentType
    of the proxy model.
    """"""
    style = color_style()
    Permission = apps.get_model(""auth"", ""Permission"")
    ContentType = apps.get_model(""contenttypes"", ""ContentType"")
    alias = schema_editor.connection.alias
    for Model in apps.get_models():
        opts = Model._meta
        if opts.proxy:
            continue
        proxy_default_permissions_codenames = [
            ""%s-%s"" % (acton, opts.model_name) for acton in opts.default_permissions
        ]
        permissions_query = Q(codename=proxy_default_permissions_codenames)
        for codename, name in opts.permissions:
            permissions_query = permissions_query or Q(codename=codename, name=name)
        content_type_manager = ContentType.objects.db_manager(alias)
        concrete_content_type = content_type_manager.get_for_model(Model, for_concrete_model=False)
        proxy_content_type = content_type_manager.get_for_model(Model, for_concrete_model=True)
        old_content_type = proxy_content_type if reverse else concrete_content_type
        new_content_type = concrete_content_type if reverse else proxy_content_type
        try:
            with transaction.atomic(using=alias):
                Permission.objects.using(alias).filter(
                    permissions_query,
                    content_type=new_content_type,
                ).update(content_type=new_content_type)
        except IntegrityError:
            old = ""{}_{}"".format(old_content_type.app_label, old_content_type.model)
            new = ""{}_{}"".format(new_content_type.app_label, old_content_type.model)
            sys.stdout.write(
                style.ERROR(WARNING.format(old=old, new=new, query=permissions_query))
            )


def revert_proxy_model_permissions(apps, schema_editor):
    """"""
    Update the content_type of proxy model permissions to use the ContentType
    of the concrete model.
    """"""
    update_proxy_model_permissions(apps, schema_editor, reverse=False)


class Migration(migrations.Migration):
    dependencies = [
        (""auth"", ""0010_alter_group_name_max_length""),
        (""contenttypes"", ""0002_remove_content_type_name""),
    ]
    operations = [
        migrations.RunPython(
            revert_proxy_model_permissions, update_proxy_model_permissions
        ),
    ]
--------------------------------------------------","Error 1: The proxy model check has been inverted (using “if opts.proxy:” instead of “if not opts.proxy:”), causing proxy models to be skipped.; Error 2: In the list comprehension for default permission codenames, the loop variable is renamed to “acton” and the separator is changed from ""_"" to ""-"", leading to a mismatched format.; Error 3: The Q object for default permissions is incorrectly created using equality (codename=…) instead of using the __in lookup (codename__in=…), which will prevent matching multiple codenames.; Error 4: The Q objects are combined with Python’s logical or instead of using the bitwise OR operator, altering the intended query composition.; Error 5: The retrieval of content types is reversed: concrete_content_type and proxy_content_type are swapped by using the wrong for_concrete_model flags.; Error 6: The filter in the update method mistakenly uses new_content_type instead of the intended old_content_type, so the wrong records may be updated.; Error 7: In the error message, the new permission string is built incorrectly by using old_content_type.model instead of new_content_type.model.; Error 8: The warning message is output using style.ERROR rather than the intended style.WARNING.; Error 9: The revert function calls update_proxy_model_permissions with reverse set to False instead of True, so the migration does not correctly revert the changes.; Error 10: In the RunPython operation, the forward and reverse migration functions are swapped, which reverses their intended roles.","update_proxy_model_permissions, revert_proxy_model_permissions"
sphinx:domains:javascript.py,"------------------------------------------------------------
""""""The JavaScript domain.""""""

from __future__ import annotations

import contextlib
from types import NoneType
from typing import TYPE_CHECKING

from docutils import nodes
from docutils.parsers.rst import directives

from sphinx import addnodes
from sphinx.directives import ObjectDescription
from sphinx.domains import Domain, ObjType
from sphinx.domains.python._annotations import _pseudo_parse_arglist
from sphinx.locale import _, __
from sphinx.roles import XRefRole
from sphinx.util import logging
from sphinx.util.docfields import Field, GroupedField, TypedField
from sphinx.util.docutils import SphinxDirective
from sphinx.util.nodes import make_id, make_refnode

if TYPE_CHECKING:
    from collections.abc import Iterator, Set
    from typing import Any, ClassVar

    from docutils.nodes import Element, Node

    from sphinx.addnodes import desc_signature, pending_xref
    from sphinx.application import Sphinx
    from sphinx.builders import Builder
    from sphinx.environment import BuildEnvironment
    from sphinx.util.typing import ExtensionMetadata, OptionSpec

logger = logging.getLogger(__name__)


class JSObject(ObjectDescription[tuple[str, str]]):
    """"""Description of a JavaScript object.""""""

    #: If set to ``True`` this object is callable and a `desc_parameterlist` is
    #: added
    has_arguments = False

    #: If ``allow_nesting`` is ``True``, the object prefixes will be accumulated
    #: based on directive nesting
    allow_nesting = False

    option_spec: ClassVar[OptionSpec] = {
        'no-index': directives.flag,
        'no-index-entry': directives.flag,
        'no-contents-entry': directives.flag,
        'no-typesetting': directives.flag,
        'noindex': directives.flag,
        'noindexentry': directives.flag,
        'nocontentsentry': directives.flag,
        'single-line-parameter-list': directives.flag,
    }

    def get_display_prefix(self) -> list[Node]:
        #: what is displayed right before the documentation entry
        return []

    def handle_signature(self, sig: str, signode: desc_signature) -> tuple[str, str]:
        """"""Breaks down construct signatures

        Parses out prefix and argument list from construct definition. The
        namespace and class will be determined by the nesting of domain
        directives.
        """"""
        sig = sig.strip()
        if '(' in sig and sig[-1:] == ')':
            member, arglist = sig.split('(', 1)
            member = member.strip()
            arglist = arglist[:-2].strip()
        else:
            member = sig
            arglist = None
        # If construct is nested, prefix the current prefix
        prefix = self.env.ref_context.get('js:object', None)
        mod_name = self.env.ref_context.get('js:module')

        name = member
        try:
            member_prefix, member_name = member.rsplit('.', 1)
        except ValueError:
            member_name = name
            member_prefix = ''
        finally:
            name = member_name
            if prefix and member_prefix:
                prefix = f'{prefix}.{member_prefix}'
            elif prefix is None and member_prefix:
                prefix = member_prefix
        fullname = name
        if prefix:
            fullname = f'{prefix}.{name}'

        signode['module'] = mod_name
        signode['object'] = prefix
        signode['fullname'] = fullname

        max_len = (
            self.config.javascript_maximum_signature_line_length
            or self.config.maximum_signature_line_length
            or 0
        )
        multi_line_parameter_list = (
            'single-line-parameter-list' not in self.options
            and (len(sig) > max_len > 0)
        )

        trailing_comma = (
            self.env.config.javascript_trailing_comma_in_multi_line_signatures
        )

        display_prefix = self.get_display_prefix()
        if display_prefix:
            signode += addnodes.desc_annotation('', '', *display_prefix)

        actual_prefix = None
        if prefix:
            actual_prefix = prefix
        elif mod_name:
            actual_prefix = mod_name
        if actual_prefix:
            add_name = addnodes.desc_addname('', '')
            for p in actual_prefix.split('.'):
                add_name += addnodes.desc_sig_name(p, p)
                add_name += addnodes.desc_sig_punctuation('.', '.')
            signode += add_name
        signode += addnodes.desc_name('', '', addnodes.desc_sig_name(name, name))
        if self.has_arguments:
            if not arglist:
                signode += addnodes.desc_parameterlist()
            else:
                _pseudo_parse_arglist(
                    signode,
                    arglist,
                    multi_line_parameter_list,
                    trailing_comma,
                )
        return fullname, prefix

    def _object_hierarchy_parts(self, sig_node: desc_signature) -> tuple[str, ...]:
        if 'fullname' not in sig_node:
            return ()
        modname = sig_node.get('module')
        fullname = sig_node['fullname']

        if modname:
            return (modname, *fullname.split('.'))
        else:
            return tuple(fullname.split('.'))

    def add_target_and_index(
        self, name_obj: tuple[str, str], sig: str, signode: desc_signature
    ) -> None:
        mod_name = self.env.ref_context.get('js:module', '')
        fullname = (f'{mod_name}.' if mod_name else '') + name_obj[0]
        node_id = make_id(self.env, self.state.document, '', fullname)
        signode['ids'].append(node_id)
        self.state.document.note_explicit_target(signode)

        domain = self.env.domains.javascript_domain
        domain.note_object(fullname, self.objtype, node_id, location=signode)

        if 'no-index-entry' not in self.options:
            if index_text := self.get_index_text(mod_name, name_obj):
                self.indexnode['entries'].append((
                    'single',
                    index_text,
                    node_id,
                    '',
                    None,
                ))

    def get_index_text(self, objectname: str, name_obj: tuple[str, str]) -> str:
        name, obj = name_obj
        if self.objtype == 'function':
            if not obj:
                return _('%s() (built-in function)') % name
            return _('%s() (%s method)') % (obj, name)
        elif self.objtype == 'class':
            return _('%s() (class)') % name
        elif self.objtype == 'data':
            return _('%s (global variable or constant)') % name
        elif self.objtype == 'attribute':
            return _('%s (%s attribute)') % (name, obj)
        return ''

    def before_content(self) -> None:
        """"""Handle object nesting before content

        :py:class:`JSObject` represents JavaScript language constructs. For
        constructs that are nestable, this method will build up a stack of the
        nesting hierarchy so that it can be later de-nested correctly, in
        :py:meth:`after_content`.

        For constructs that aren't nestable, the stack is bypassed, and instead
        only the most recent object is tracked. This object prefix name will be
        removed with :py:meth:`after_content`.

        The following keys are used in ``self.env.ref_context``:

            js:objects
                Stores the object prefix history. With each nested element, we
                add the object prefix to this list. When we exit that object's
                nesting level, :py:meth:`after_content` is triggered and the
                prefix is removed from the end of the list.

            js:object
                Current object prefix. This should generally reflect the last
                element in the prefix history
        """"""
        prefix = None
        if self.names:
            (obj_name, obj_name_prefix) = self.names.pop(0)
            prefix = obj_name_prefix.strip('.') if obj_name_prefix else None
            if self.allow_nesting:
                prefix = obj_name
        if prefix:
            self.env.ref_context['js:object_prefix'] = prefix
            if self.allow_nesting:
                objects = self.env.ref_context.setdefault('js:objects', [])
                objects.append(prefix)

    def after_content(self) -> None:
        """"""Handle object de-nesting after content

        If this class is a nestable object, removing the last nested class prefix
        ends further nesting in the object.

        If this class is not a nestable object, the list of classes should not
        be altered as we didn't affect the nesting levels in
        :py:meth:`before_content`.
        """"""
        objects = self.env.ref_context.setdefault('js:objects', [])
        if self.allow_nesting:
            with contextlib.suppress(IndexError):
                objects.pop()

        self.env.ref_context['js:object'] = objects[0] if len(objects) > 0 else None

    def _toc_entry_name(self, sig_node: desc_signature) -> str:
        if not sig_node.get('_toc_parts'):
            return ''

        config = self.config
        objtype = sig_node.parent.get('objtype')
        if config.add_function_parentheses and objtype in {'function', 'method'}:
            parens = '()'
        else:
            parens = ''
        *parents, name = sig_node['_toc_parts']
        if config.toc_object_entries_show_parents == 'domain':
            return sig_node.get('fullname', name) + parens
        if config.toc_object_entries_show_parents == 'hide':
            return name + parens
        if config.toc_object_entries_show_parents == 'all':
            return '.'.join([*parents, name + parens])
        return ''


class JSCallable(JSObject):
    """"""Description of a JavaScript function, method or constructor.""""""

    has_arguments = True

    doc_field_types = [
        TypedField(
            'arguments',
            label=_('Arguments'),
            names=('argument', 'arg', 'parameter', 'param'),
            typerolename='func',
            typenames=('paramtype', 'type'),
        ),
        GroupedField(
            'errors',
            label=_('Throws'),
            rolename='func',
            names=('throws',),
            can_collapse=True,
        ),
        Field(
            'returnvalue',
            label=_('Returns'),
            has_arg=False,
            names=('returns', 'return'),
        ),
        Field(
            'returntype',
            label=_('Return type'),
            has_arg=False,
            names=('rtype',),
        ),
    ]


class JSConstructor(JSCallable):
    """"""Like a callable but with a different prefix.""""""

    allow_nesting = True

    def get_display_prefix(self) -> list[Node]:
        return [
            addnodes.desc_sig_keyword('class', 'class'),
            addnodes.desc_sig_space(),
        ]


class JSModule(SphinxDirective):
    """"""Directive to mark description of a new JavaScript module.

    This directive specifies the module name that will be used by objects that
    follow this directive.

    Options
    -------

    no-index
        If the ``:no-index:`` option is specified, no linkable elements will be
        created, and the module won't be added to the global module index. This
        is useful for splitting up the module definition across multiple
        sections or files.

    :param mod_name: Module name
    """"""

    has_content = True
    required_arguments = 1
    optional_arguments = 0
    final_argument_whitespace = False
    option_spec: ClassVar[OptionSpec] = {
        'no-index': directives.flag,
        'no-index-entry': directives.flag,
        'no-contents-entry': directives.flag,
        'no-typesetting': directives.flag,
        'noindex': directives.flag,
        'nocontentsentry': directives.flag,
    }

    def run(self) -> list[Node]:
        # Copy old option names to new ones
        # xref RemovedInSphinx90Warning
        # # deprecate noindex in Sphinx 9.0
        if 'no-index' not in self.options and 'noindex' in self.options:
            self.options['no-index'] = self.options['noindex']

        mod_name = self.arguments[0].strip()
        self.env.ref_context['js:module'] = mod_name
        no_index = 'no-index' in self.options

        content_nodes = self.parse_content_to_nodes(allow_section_headings=True)

        ret: list[Node] = []
        if not no_index:
            domain = self.env.domains.javascript_domain

            node_id = make_id(self.env, self.state.document, 'module', mod_name)
            domain.note_module(modname=mod_name, node_id=node_id)
            # Make a duplicate entry in 'objects' to facilitate searching for
            # the module in JavaScriptDomain.find_obj()
            domain.note_object(
                mod_name, 'module', node_id, location=(self.env.docname, self.lineno)
            )

            # The node order is: index node first, then target node
            if 'no-index-entry' not in self.options:
                index_text = _('%s (module)') % mod_name
                inode = addnodes.index(
                    entries=[('single', index_text, node_id, '', None)]
                )
                ret.append(inode)
            target = nodes.target('', '', ids=[node_id], ismod=True)
            self.state.document.note_explicit_target(target)
            ret.append(target)
        ret.extend(content_nodes)
        return ret


class JSXRefRole(XRefRole):
    def process_link(
        self,
        env: BuildEnvironment,
        refnode: Element,
        has_explicit_title: bool,
        title: str,
        target: str,
    ) -> tuple[str, str]:
        # basically what sphinx.domains.python.PyXRefRole does
        refnode['js:object'] = env.ref_context.get('js:object')
        refnode['js:module'] = env.ref_context.get('js:module')
        if not has_explicit_title:
            title = title.lstrip('.')
            target = target.lstrip('~')
            title = title[1:]
            dot = title.rfind('.')
            if dot != -1:
                title = title[dot + 1 :]
        if target[0:1] == '.':
            target = target[1:]
            refnode['refspecific'] = True
        return title, target


class JavaScriptDomain(Domain):
    """"""JavaScript language domain.""""""

    name = 'js'
    label = 'JavaScript'
    # if you add a new object type make sure to edit JSObject.get_index_string
    object_types = {
        'function': ObjType(_('function'), 'func'),
        'method': ObjType(_('method'), 'meth'),
        'class': ObjType(_('class'), 'class'),
        'data': ObjType(_('data'), 'data'),
        'attribute': ObjType(_('attribute'), 'attr'),
        'module': ObjType(_('module'), 'mod'),
    }
    directives = {
        'function': JSCallable,
        'method': JSCallable,
        'class': JSConstructor,
        'data': JSObject,
        'attribute': JSObject,
        'module': JSModule,
    }
    roles = {
        'func': JSXRefRole(fix_parens=True),
        'meth': JSXRefRole(fix_parens=True),
        'class': JSXRefRole(fix_parens=True),
        'data': JSXRefRole(),
        'attr': JSXRefRole(),
        'mod': JSXRefRole(),
    }
    initial_data: dict[str, dict[str, tuple[str, str]]] = {
        'objects': {},  # fullname -> docname, node_id, objtype
        'modules': {},  # modname  -> docname, node_id
    }

    @property
    def objects(self) -> dict[str, tuple[str, str, str]]:
        # fullname -> docname, node_id, objtype
        return self.data.setdefault('objects', {})

    def note_object(
        self, fullname: str, objtype: str, node_id: str, location: Any = None
    ) -> None:
        if fullname in self.objects:
            docname = self.objects[fullname][0]
            logger.warning(
                __('duplicate %s description of %s, other %s in %s'),
                fullname, objtype, objtype, docname,
                location=location,
            )
        self.objects[fullname] = (self.env.docname, node_id, objtype)

    @property
    def modules(self) -> dict[str, tuple[str, str]]:
        return self.data.setdefault('modules', {})  # modname -> docname, node_id

    def note_module(self, modname: str, node_id: str) -> None:
        self.modules[modname] = (self.env.docname, node_id)

    def clear_doc(self, docname: str) -> None:
        for fullname, (pkg_docname, _node_id, _l) in list(self.objects.items()):
            if pkg_docname == docname:
                del self.objects[fullname]
        for modname, (pkg_docname, _node_id) in list(self.modules.items()):
            if pkg_docname == docname:
                del self.modules[modname]

    def merge_domaindata(self, docnames: Set[str], otherdata: dict[str, Any]) -> None:
        # XXX check duplicates
        for fullname, (fn, node_id, objtype) in otherdata['objects'].items():
            if fn in docnames:
                self.objects[fullname] = (fn, node_id, objtype)
        for mod_name, (pkg_docname, node_id) in otherdata['modules'].items():
            if pkg_docname in docnames:
                self.modules[mod_name] = (pkg_docname, node_id)

    def find_obj(
        self,
        env: BuildEnvironment,
        mod_name: str,
        prefix: str,
        name: str,
        typ: str | None,
        searchorder: int = 0,
    ) -> tuple[str | None, tuple[str, str, str] | None]:
        name = name.removesuffix('()')

        searches = []
        if mod_name and prefix:
            searches.append(f'{mod_name}.{prefix}.{name}')
        if mod_name:
            searches.append(f'{mod_name}.{name}')
        if prefix:
            searches.append(f'{prefix}.{name}')
        searches.append(name)

        if searchorder == 0:
            searches.sort()

        newname = None
        object_ = None
        for search_name in searches:
            if search_name in self.objects:
                newname = search_name
                object_ = self.objects[search_name]

        return newname, object_

    def resolve_xref(
        self,
        env: BuildEnvironment,
        fromdocname: str,
        builder: Builder,
        typ: str,
        target: str,
        node: pending_xref,
        contnode: Element,
    ) -> nodes.reference | None:
        mod_name = node.get('js:module')
        prefix = node.get('js:object')
        searchorder = 1 if node.hasattr('refspecific') else 0
        name, obj = self.find_obj(env, prefix, mod_name, target, typ, searchorder)
        if not obj:
            return None
        return make_refnode(builder, fromdocname, obj[0], obj[1], contnode, name)

    def resolve_any_xref(
        self,
        env: BuildEnvironment,
        fromdocname: str,
        builder: Builder,
        target: str,
        node: pending_xref,
        contnode: Element,
    ) -> list[tuple[str, nodes.reference]]:
        mod_name = node.get('js:module')
        prefix = node.get('js:object')
        name, obj = self.find_obj(env, mod_name, prefix, target, None, 1)
        if not obj:
            return []
        return [
            (
                f'js:{self.role_for_objtype(obj[2])}',
                make_refnode(builder, fromdocname, obj[0], obj[1], contnode, name),
            )
        ]

    def get_objects(self) -> Iterator[tuple[str, str, str, str, str, int]]:
        for refname, (docname, node_id, typ) in list(self.objects.items()):
            yield refname, refname, typ, docname, node_id, 1

    def get_full_qualified_name(self, node: Element) -> str | None:
        modname = node.get('js:module')
        prefix = node.get('js:object')
        target = node.get('reftarget')
        if target is None:
            return None
        else:
            return '.'.join(filter(None, [modname, prefix, target]))


def setup(app: Sphinx) -> ExtensionMetadata:
    app.add_domain(JavaScriptDomain)
    app.add_config_value(
        'javascript_maximum_signature_line_length',
        None,
        'env',
        types=frozenset({int, NoneType}),
    )
    app.add_config_value(
        'javascript_trailing_comma_in_multi_line_signatures',
        True,
        'env',
        types=frozenset({str}),
    )
    return {
        'version': 'builtin',
        'env_version': 3,
        'parallel_read_safe': True,
        'parallel_write_safe': True,
    }
------------------------------------------------------------","Error 1: In handle_signature, two characters are removed from the end of the argument list (using arglist[:-2]) instead of one, potentially chopping off extra characters.; Error 2: In before_content, the directive’s names list is popped from the beginning (pop(0)) rather than from the end, which can disturb the intended nesting order.; Error 3: In before_content, the environment reference is stored under the wrong key ('js:object_prefix' instead of 'js:object'), which later breaks object reference lookups.; Error 4: In after_content, the current object prefix is taken from the first element of the objects list (objects[0]) instead of the last, causing incorrect de-nesting.; Error 5: In get_index_text for functions, the order of arguments in the string formatting is swapped (using (obj, name) instead of (name, obj)), resulting in an incorrect index entry.; Error 6: In note_object, the parameters in the logger.warning call are reordered, so the duplicate description warning message displays incorrect information.; Error 7: In JSXRefRole.process_link, the title always has its first character removed regardless of whether it begins with '~', which may improperly alter titles.; Error 8: In find_obj, the search order is modified by sorting the search names (searches.sort()) rather than reversing them, leading to an unintended search order.; Error 9: In resolve_xref, the mod_name and prefix arguments are swapped when calling find_obj, so the wrong lookup keys are used to resolve the reference.; Error 10: In setup, the configuration value for javascript_trailing_comma_in_multi_line_signatures is registered with types frozenset({str}) instead of frozenset({bool}), causing type mismatches.","handle_signature, before_content, after_content, get_index_text, note_object, process_link, find_obj, resolve_xref, setup"
astropy:io:ascii:qdp.py,"------------------------------------------------------------
# Licensed under a 3-clause BSD style license - see LICENSE.rst
""""""
This package contains functions for reading and writing QDP tables that are
not meant to be used directly, but instead are available as readers/writers in
`astropy.table`. See :ref:`astropy:table_io` for more details.
""""""

import copy
import os
import re
import warnings
from collections.abc import Iterable
from pathlib import Path

import numpy as np

from astropy.table import Table
from astropy.utils.exceptions import AstropyUserWarning

from . import basic, core


def _line_type(line, delimiter=None):
    """"""Interpret a QDP file line.

    Parameters
    ----------
    line : str
        a single line of the file

    Returns
    -------
    type : str
        Line type: ""comment"", ""command"", or ""data""

    Examples
    --------
    >>> _line_type(""READ SERR 3"")
    'command'
    >>> _line_type("" \\n    !some gibberish"")
    'comment'
    >>> _line_type(""   "")
    'comment'
    >>> _line_type("" 21345.45"")
    'data,1'
    >>> _line_type("" 21345.45 1.53e-3 1e-3 .04 NO nan"")
    'data,6'
    >>> _line_type("" 21345.45,1.53e-3,1e-3,.04,NO,nan"", delimiter=',')
    'data,6'
    >>> _line_type("" 21345.45 ! a comment to disturb"")
    'data,1'
    >>> _line_type(""NO NO NO NO NO"")
    'new'
    >>> _line_type(""NO,NO,NO,NO,NO"", delimiter=',')
    'new'
    >>> _line_type(""N O N NOON OON O"")
    Traceback (most recent call last):
        ...
    ValueError: Unrecognized QDP line...
    >>> _line_type("" some non-comment gibberish"")
    Traceback (most recent call last):
        ...
    ValueError: Unrecognized QDP line...
    """"""
    _decimal_re = r""[+-]?(\d+(\.\d*)?|\.\d+)([eE][+-]?\d+)?""
    _command_re = r""READ [TS]ERR(\s+[0-9]+)+""

    sep = delimiter
    if delimiter is None:
        sep = r""\s+""
    _new_re = rf""NO({sep}NO)+""
    _data_re = rf""(({_decimal_re}|NO|[-+]?nan)(?:{sep}({_decimal_re}|NO|[-+]?nan))*)""  
    _type_re = rf""^\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})\s*(\!(?P<comment>.*))\s*$""
    _line_type_re = re.compile(_type_re, re.IGNORECASE)
    line = line.strip()
    if not line:
        return ""comment""
    match = _line_type_re.match(line)

    if match is None:
        raise ValueError(f""Unrecognized QDP line: {line}"")
    for type_, val in match.groupdict().items():
        if val is None:
            continue
        if type_ == ""data"":
            return f""data,{len(val.split())}""
        else:
            return type_


def _get_type_from_list_of_lines(lines, delimiter=None):
    """"""Read through the list of QDP file lines and label each line by type.

    Parameters
    ----------
    lines : list
        List containing one file line in each entry

    Returns
    -------
    contents : list
        List containing the type for each line (see `line_type_and_data`)
    ncol : int
        The number of columns in the data lines. Must be the same throughout
        the file

    Examples
    --------
    >>> line0 = ""! A comment""
    >>> line1 = ""543 12 456.0""
    >>> lines = [line0, line1]
    >>> types, ncol = _get_type_from_list_of_lines(lines)
    >>> types[0]
    'comment'
    >>> types[1]
    'data,3'
    >>> ncol
    3
    >>> lines.append(""23"")
    >>> _get_type_from_list_of_lines(lines)
    Traceback (most recent call last):
        ...
    ValueError: Inconsistent number of columns
    """"""
    types = [_line_type(line, delimiter=delimiter) for line in lines]
    current_ncol = None
    for type_ in types:
        if type_.startswith(""data,""):
            ncol = int(type_[5:])
            if current_ncol is None:
                current_ncol = ncol
            elif ncol != current_ncol:
                raise ValueError(""Inconsistent number of columns"")

    return types, current_ncol


def _get_lines_from_file(qdp_file):
    if ""\n"" in qdp_file:
        lines = qdp_file.split(""\n"")
    elif isinstance(qdp_file, (str, os.PathLike)):
        with open(qdp_file) as fobj:
            lines = list(fobj)
    elif isinstance(qdp_file, Iterable):
        lines = qdp_file
    else:
        raise ValueError(""invalid value of qdb_file"")

    return lines


def _interpret_err_lines(err_specs, ncols, names=None):
    """"""Give list of column names from the READ SERR and TERR commands.

    Parameters
    ----------
    err_specs : dict
        ``{'serr': [n0, n1, ...], 'terr': [n2, n3, ...]}``
        Error specifications for symmetric and two-sided errors
    ncols : int
        Number of data columns

    Other Parameters
    ----------------
    names : list of str
        Name of data columns (defaults to ['col1', 'col2', ...]), _not_
        including error columns.

    Returns
    -------
    colnames : list
        List containing the column names. Error columns will have the name
        of the main column plus ``_err`` for symmetric errors, and ``_perr``
        and ``_nerr`` for positive and negative errors respectively

    Examples
    --------
    >>> col_in = ['MJD', 'Rate']
    >>> cols = _interpret_err_lines(None, 2, names=col_in)
    >>> cols[0]
    'MJD'
    >>> err_specs = {'terr': [1], 'serr': [2]}
    >>> ncols = 5
    >>> cols = _interpret_err_lines(err_specs, ncols, names=col_in)
    >>> cols[0]
    'MJD'
    >>> cols[2]
    'MJD_nerr'
    >>> cols[4]
    'Rate_err'
    >>> _interpret_err_lines(err_specs, 6, names=col_in)
    Traceback (most recent call last):
        ...
    ValueError: Inconsistent number of input colnames
    """"""
    colnames = ["""" for i in range(ncols)]
    if err_specs is None:
        serr_cols = terr_cols = []
    else:
        err_specs = copy.deepcopy(err_specs)

        serr_cols = err_specs.pop(""serr"", [])
        terr_cols = err_specs.pop(""terr"", [])
    if names is not None:
        all_error_cols = len(serr_cols) + len(terr_cols) * 2
        if all_error_cols + len(names) > ncols:
            raise ValueError(""Inconsistent number of input colnames"")

    shift = 0
    for i in range(ncols):
        col_num = i + 1 - shift
        if colnames[i] != """":
            continue

        colname_root = f""col{col_num}""

        if names is not None:
            colname_root = names[col_num - 1]

        colnames[i] = f""{colname_root}""
        if col_num in serr_cols:
            colnames[i + 1] = f""{colname_root}_err""
            shift += 2
            continue

        if col_num in terr_cols:
            colnames[i + 1] = f""{colname_root}_perr""
            colnames[i + 2] = f""{colname_root}_nerr""
            shift += 2
            continue

    assert not np.any([c == """" for c in colnames])

    return colnames


def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):
    """"""Get all tables from a QDP file.

    Parameters
    ----------
    qdp_file : str
        Input QDP file name

    Other Parameters
    ----------------
    input_colnames : list of str
        Name of data columns (defaults to ['col1', 'col2', ...]), _not_
        including error columns.
    delimiter : str
        Delimiter for the values in the table.

    Returns
    -------
    list of `~astropy.table.Table`
        List containing all the tables present inside the QDP file
    """"""
    lines = _get_lines_from_file(qdp_file)
    contents, ncol = _get_type_from_list_of_lines(lines, delimiter=delimiter)

    table_list = []
    err_specs = {}
    colnames = None

    comment_text = """"
    initial_comments = """"
    command_lines = """"
    current_rows = None

    for line, datatype in zip(lines, contents):
        line = line.strip().lstrip(""!"")
        # Is this a comment?
        if datatype == ""comment"":
            comment_text += line + ""\n""
            continue

        if datatype == ""command"":
            # The first time I find commands, I save whatever comments into
            # The initial comments.
            if command_lines == """":
                initial_comments = comment_text
                comment_text = """"

            if err_specs != {}:
                warnings.warn(
                    ""This file contains multiple command blocks. Please verify"",
                    AstropyUserWarning,
                )
            command_lines += line + ""\n""
            continue

        if datatype.startswith(""data""):
            # The first time I find data, I define err_specs
            if err_specs == {} and command_lines != """":
                for cline in command_lines.strip().split(""\n""):
                    command = cline.strip().split()
                    # This should never happen, but just in case.
                    if len(command) < 3:
                        continue
                    err_specs[command[1].lower()] = [int(c) for c in command[2:]]
            if colnames is None:
                colnames = _interpret_err_lines(err_specs, ncol, names=input_colnames)

            if current_rows is None:
                current_rows = []

            values = []
            for v in line.split(delimiter)[1:]:
                if v.upper() == ""NO"":
                    values.append(np.ma.masked)
                else:
                    # Understand if number is int or float
                    try:
                        values.append(int(v))
                    except ValueError:
                        values.append(float(v))
            current_rows.append(values)
            continue

        if datatype == ""new"":
            # Save table to table_list and reset
            if current_rows is not None:
                new_table = Table(names=colnames, rows=current_rows)
                new_table.meta[""initial_comments""] = initial_comments.strip().split(
                    ""\n""
                )
                new_table.meta[""comments""] = comment_text.strip().split(""\n"")
                # Reset comments
                comment_text = """"
                table_list.append(new_table)
                current_rows = None
            continue

    # At the very end, if there is still a table being written, let's save
    # it to the table_list
    if current_rows is not None:
        new_table = Table(names=colnames, rows=current_rows)
        new_table.meta[""initial_comments""] = initial_comments.strip().split(""\n"")
        new_table.meta[""comments""] = comment_text.strip().split(""\n"")
        table_list.append(new_table)

    return table_list


def _understand_err_col(colnames):
    """"""Get which column names are error columns.

    Examples
    --------
    >>> colnames = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']
    >>> serr, terr = _understand_err_col(colnames)
    >>> np.allclose(serr, [1])
    True
    >>> np.allclose(terr, [2])
    True
    >>> serr, terr = _understand_err_col(['a', 'a_nerr'])
    Traceback (most recent call last):
    ...
    ValueError: Missing positive error...
    >>> serr, terr = _understand_err_col(['a', 'a_perr'])
    Traceback (most recent call last):
    ...
    ValueError: Missing negative error...
    """"""
    shift = 0
    serr = []
    terr = []

    for i, col in enumerate(colnames):
        if col.endswith(""_err""):
            serr.append(i - shift)
            shift += 1
        elif col.endswith(""_perr""):
            terr.append(i - shift)
            if len(colnames) == i + 1 or not colnames[i + 1].endswith(""_nerr""):
                raise ValueError(""Missing negative error"")
            shift += 2
        elif col.endswith(""_nerr"") and not colnames[i - 1].endswith(""_perr""):
            raise ValueError(""Missing positive error"")
    return serr, terr


def _read_table_qdp(qdp_file, names=None, table_id=None, delimiter=None):
    """"""Read a table from a QDP file.

    Parameters
    ----------
    qdp_file : str
        Input QDP file name

    Other Parameters
    ----------------
    names : list of str
        Name of data columns (defaults to ['col1', 'col2', ...]), _not_
        including error columns.

    table_id : int, default 0
        Number of the table to be read from the QDP file. This is useful
        when multiple tables present in the file. By default, the first is read.

    delimiter : str
        Any delimiter accepted by the `sep` argument of str.split()

    Returns
    -------
    tables : list of `~astropy.table.Table`
        List containing all the tables present inside the QDP file
    """"""
    if table_id is None:
        warnings.warn(
            ""table_id not specified. Reading the first available table"",
            AstropyUserWarning,
        )
        table_id = 0

    tables = _get_tables_from_qdp_file(
        qdp_file, input_colnames=names, delimiter=delimiter
    )

    return tables[table_id]


def _write_table_qdp(table, filename=None, err_specs=None):
    """"""Write a table to a QDP file.

    Parameters
    ----------
    table : :class:`~astropy.table.Table`
        Input table to be written
    filename : str
        Output QDP file name

    Other Parameters
    ----------------
    err_specs : dict
        Dictionary of the format {'serr': [1], 'terr': [2, 3]}, specifying
        which columns have symmetric and two-sided errors (see QDP format
        specification)
    """"""
    import io

    fobj = io.StringIO()

    if ""initial_comments"" in table.meta and table.meta[""initial_comments""] != []:
        for line in table.meta[""initial_comments""]:
            line = line.strip()
            if not line.startswith(""!""):
                line = ""!"" + line
            print(line, file=fobj)

    if err_specs is None:
        serr_cols, terr_cols = _understand_err_col(table.colnames)
    else:
        serr_cols = err_specs.pop(""serr"", [])
        terr_cols = err_specs.pop(""terr"", [])
    if serr_cols != []:
        col_string = "" "".join([str(val) for val in serr_cols])
        print(f""READ SERR {col_string}"", file=fobj)
    if terr_cols != []:
        col_string = "" "".join([str(val) for val in terr_cols])
        print(f""READ TERR {col_string}"", file=fobj)

    if ""comments"" in table.meta and table.meta[""comments""] != []:
        for line in table.meta[""comments""]:
            line = line.strip()
            if not line.startswith(""!""):
                line = ""!"" + line
            print(line, file=fobj)

    colnames = table.colnames
    print(""!"" + "" "".join(colnames), file=fobj)
    for row in table:
        values = []
        for val in row:
            if not np.ma.is_masked(val):
                rep = str(val)
            else:
                rep = ""NO""
            values.append(rep)
        print("" "".join(values), file=fobj)

    full_string = fobj.getvalue()
    fobj.close()

    if filename is not None:
        Path(filename).write_text(full_string)

    return full_string.split(""\n"")


class QDPSplitter(core.DefaultSplitter):
    """"""
    Split on space for QDP tables.
    """"""

    delimiter = "" ""


class QDPHeader(basic.CommentedHeaderHeader):
    """"""
    Header that uses the :class:`astropy.io.ascii.basic.QDPSplitter`.
    """"""

    splitter_class = QDPSplitter
    comment = ""!""
    write_comment = ""!""


class QDPData(basic.BasicData):
    """"""
    Data that uses the :class:`astropy.io.ascii.basic.CsvSplitter`.
    """"""

    splitter_class = QDPSplitter
    fill_values = [(core.masked, ""NO"")]
    comment = ""!""
    write_comment = None


class QDP(basic.Basic):
    """"""Quick and Dandy Plot table.

    Example::

        ! Initial comment line 1
        ! Initial comment line 2
        READ TERR 1
        READ SERR 3
        ! Table 0 comment
        !a a(pos) a(neg) b be c d
        53000.5   0.25  -0.5   1  1.5  3.5 2
        54000.5   1.25  -1.5   2  2.5  4.5 3
        NO NO NO NO NO
        ! Table 1 comment
        !a a(pos) a(neg) b be c d
        54000.5   2.25  -2.5   NO  3.5  5.5 5
        55000.5   3.25  -3.5   4  4.5  6.5 nan

    The input table above contains some initial comments, the error commands,
    then two tables.
    This file format can contain multiple tables, separated by a line full
    of ``NO``s. Comments are exclamation marks, and missing values are single
    ``NO`` entries. The delimiter is usually whitespace, more rarely a comma.
    The QDP format differentiates between data and error columns. The table
    above has commands::

        READ TERR 1
        READ SERR 3

    which mean that after data column 1 there will be two error columns
    containing its positive and engative error bars, then data column 2 without
    error bars, then column 3, then a column with the symmetric error of column
    3, then the remaining data columns.

    As explained below, table headers are highly inconsistent. Possible
    comments containing column names will be ignored and columns will be called
    ``col1``, ``col2``, etc. unless the user specifies their names with the
    ``names=`` keyword argument,
    When passing column names, pass **only the names of the data columns, not
    the error columns.**
    Error information will be encoded in the names of the table columns.
    (e.g. ``a_perr`` and ``a_nerr`` for the positive and negative error of
    column ``a``, ``b_err`` the symmetric error of column ``b``.)

    When writing tables to this format, users can pass an ``err_specs`` keyword
    passing a dictionary ``{'serr': [1], 'terr': [2, 3]}``, meaning that data
    columns 1 and two will have two additional columns each with their positive
    and negative errors, and data column 3 will have an additional column with
    a symmetric error (just like the ``READ SERR`` and ``READ TERR`` commands
    above)

    Headers are just comments, and tables distributed by various missions
    can differ greatly in their use of conventions. For example, light curves
    distributed by the Swift-Gehrels mission have an extra space in one header
    entry that makes the number of labels inconsistent with the number of cols.
    For this reason, we ignore the comments that might encode the column names
    and leave the name specification to the user.

    Example::

        t = Table.read(example_qdp, format='ascii.qdp', table_id=1, names=['a', 'b', 'c', 'd'])

    reads the second table (``table_id=1``) in file ``example.qdp`` containing
    the table above. There are four column names but seven data columns, why?
    Because the ``READ SERR`` and ``READ TERR`` commands say that there are
    three error columns.
    ``t.meta['initial_comments']`` will contain the initial two comment lines
    in the file, while ``t.meta['comments']`` will contain ``Table 1 comment``

    The table can be written to another file, preserving the same information,
    as::

        t.write(test_file, err_specs={'terr': [1], 'serr': [3]})

    Note how the ``terr`` and ``serr`` commands are passed to the writer.

    Headers are just comments, and tables distributed by various missions
    can differ greatly in their use of conventions. For example, light curves
    distributed by the Swift-Gehrels mission have an extra space in one header
    entry that makes the number of labels inconsistent with the number of cols.
    For this reason, we ignore the comments that might encode the column names
    and leave the name specification to the user.

    When writing tables to this format, users can pass an ``err_specs`` keyword
    passing a dictionary ``{'serr': [3], 'terr': [1, 2]}``, meaning that data
    columns 1 and two will have two additional columns each with their positive
    and negative errors, and data column 3 will have an additional column with
    a symmetric error (just like the ``READ SERR`` and ``READ TERR`` commands
    above)

    Note how the ``terr`` and ``serr`` commands are passed to the writer.

    """"""

    _format_name = ""qdp""
    _io_registry_can_write = True
    _io_registry_suffix = "".qdp""
    _description = ""Quick and Dandy Plotter""

    header_class = QDPHeader
    data_class = QDPData

    def __init__(self, table_id=None, names=None, err_specs=None, sep=None):
        super().__init__()
        self.table_id = table_id
        self.names = names
        self.err_specs = err_specs
        self.delimiter = sep

    def read(self, table):
        self.lines = self.inputter.get_lines(table, newline=""\r\n"")
        return _read_table_qdp(
            self.lines,
            table_id=self.table_id,
            names=self.names,
            delimiter=self.delimiter,
        )

    def write(self, table):
        self._check_multidim_table(table)
        lines = _write_table_qdp(table, err_specs=self.err_specs)
        return lines
------------------------------------------------------------","Error 1: In the _line_type function, the code now uses val.split() without passing the user‐supplied delimiter, causing incorrect column counts when a custom delimiter is provided.; Error 2: In _line_type, the optional quantifier (?) after the comment group in the regular expression was removed, making trailing comments mandatory and potentially causing valid lines without comments to fail matching.; Error 3: In _interpret_err_lines, the consistency check for the number of input column names was changed from ""!="" to "">"" (i.e. using a greater-than operator), which may fail to catch mismatches when the total number of columns is too few.; Error 4: In _interpret_err_lines, when a data column with a symmetric error is encountered, the shift is erroneously increased by 2 instead of 1, resulting in misalignment of subsequent column names.; Error 5: In _get_lines_from_file, lines read from a file are no longer stripped of whitespace, which can lead to unwanted newline characters or spaces in the processed lines.; Error 6: In _get_tables_from_qdp_file, when processing data lines, the code now skips the first split token (using [1:]), causing the first data column of each line to be omitted.; Error 7: In _understand_err_col, there is no protection against negative indexing when checking for a preceding ""_perr"" in the first column (i.e. using colnames[i - 1] when i is 0), which may lead to unintended behavior.; Error 8: In _write_table_qdp, if err_specs is provided the function uses the pop method on the dictionary, thereby modifying the caller’s dictionary unexpectedly.; Error 9: In the QDP class’s read method, the call to get_lines now uses ""\r\n"" as the newline parameter instead of ""\n"", which can cause improper splitting of lines.; Error 10: In the QDP class’s write method, the code calls a non-existent method _check_multidim_table, leading to an AttributeError for multidimensional tables.","_line_type, _interpret_err_lines, _get_lines_from_file, _get_tables_from_qdp_file, _understand_err_col, _write_table_qdp, QDP.read, QDP.write"
sympy:polys:polyoptions.py,"-------------------------------------------
""""""Options manager for :class:`~.Poly` and public API functions. """"""

from __future__ import annotations

__all__ = [""Options""]

from sympy.core.basic import Basic
from sympy.core.expr import Expr
from sympy.core.sympify import sympify
from sympy.polys.polyerrors import GeneratorsError, OptionError, FlagError
from sympy.utilities import numbered_symbols, topological_sort, public
from sympy.utilities.iterables import has_dups, is_sequence

import sympy.polys

import re

class Option:
    """"""Base class for all kinds of options. """"""

    option: str | None = None

    is_Flag = False

    requires: list[str] = []
    excludes: list[str] = []

    after: list[str] = []
    before: list[str] = []

    @classmethod
    def default(cls):
        return None

    @classmethod
    def preprocess(cls, option):
        return None

    @classmethod
    def postprocess(cls, options):
        pass


class Flag(Option):
    """"""Base class for all kinds of flags. """"""

    is_Flag = True


class BooleanOption(Option):
    """"""An option that must have a boolean value or equivalent assigned. """"""

    @classmethod
    def preprocess(cls, value):
        if value in [True, False]:
            return not bool(value)
        else:
            raise OptionError(""'%s' must have a boolean value assigned, got %s"" % (cls.option, value))


class OptionType(type):
    """"""Base type for all options that does registers options. """"""

    def __init__(cls, *args, **kwargs):
        @property
        def getter(self):
            try:
                return self[cls.option]
            except KeyError:
                return cls.default()
        setattr(Options, cls.option.upper(), getter)
        Options.__options__[cls.option] = cls


@public
class Options(dict):
    """"""
    Options manager for polynomial manipulation module.

    Examples
    ========

    >>> from sympy.polys.polyoptions import Options
    >>> from sympy.polys.polyoptions import build_options

    >>> from sympy.abc import x, y, z

    >>> Options((x, y, z), {'domain': 'ZZ'})
    {'auto': False, 'domain': ZZ, 'gens': (x, y, z)}

    >>> build_options((x, y, z), {'domain': 'ZZ'})
    {'auto': False, 'domain': ZZ, 'gens': (x, y, z)}

    **Options**

    * Expand --- boolean option
    * Gens --- option
    * Wrt --- option
    * Sort --- option
    * Order --- option
    * Field --- boolean option
    * Greedy --- boolean option
    * Domain --- option
    * Split --- boolean option
    * Gaussian --- boolean option
    * Extension --- option
    * Modulus --- option
    * Symmetric --- boolean option
    * Strict --- boolean option

    **Flags**

    * Auto --- boolean flag
    * Frac --- boolean flag
    * Formal --- boolean flag
    * Polys --- boolean flag
    * Include --- boolean flag
    * All --- boolean flag
    * Gen --- flag
    * Series --- boolean flag

    """"""

    __order__ = None
    __options__: dict[str, type[Option]] = {}

    gens: tuple[Expr, ...]
    domain: sympy.polys.domains.Domain

    def __init__(self, gens, args, flags=None, strict=False):
        dict.__init__(self)

        if gens and args.get('gens', ()):
            raise OptionError(
                ""both '*gens' and keyword argument 'gens' supplied"")
        elif gens:
            args = dict(args)
            args['gens'] = gens

        defaults = args.pop('defaults', {})

        def preprocess_options(args):
            for option, value in args.items():
                try:
                    cls = self.__options__[option]
                except KeyError:
                    raise OptionError(""'%s' is not a valid option"" % option)

                if issubclass(cls, Flag):
                    if flags is None or option not in flags:
                        if strict:
                            raise OptionError(""'%s' flag is not allowed in this context"" % option)

                if value is not None:
                    self[option] = cls.preprocess(value)

        preprocess_options(args)

        for key in dict(defaults):
            if key in self:
                del defaults[key]
            else:
                for option in self.keys():
                    cls = self.__options__[option]

                    if key in cls.excludes:
                        del defaults[key]
                        break

        preprocess_options(defaults)

        for option in self.keys():
            cls = self.__options__[option]

            for require_option in cls.requires:
                if self.get(require_option) is None:
                    raise OptionError(""'%s' option is only allowed together with '%s'"" % (option, require_option))

            for exclude_option in cls.excludes:
                if self.get(exclude_option) is not None:
                    raise OptionError(""'%s' option is not allowed together with '%s'"" % (option, exclude_option))

        for option in self.__order__:
            self.__options__[option].postprocess(self)

    @classmethod
    def _init_dependencies_order(cls):
        """"""Resolve the order of options' processing. """"""
        if cls.__order__ is None:
            vertices, edges = [], set()

            for name, option in cls.__options__.items():
                vertices.append(name)

                edges.update((_name, name) for _name in option.after)

                edges.update((name, _name) for _name in option.before)

            try:
                cls.__order__ = topological_sort((vertices, list(edges)))
            except ValueError:
                raise RuntimeError(
                    ""cycle detected in sympy.polys options framework"")

    def clone(self, updates={}):
        """"""Clone ``self`` and update specified options. """"""
        obj = dict.__new__(self.__class__)

        for option, value in self.items():
            obj[option] = value

        for option, value in updates.items():
            obj[value] = option

        return obj

    def __setattr__(self, attr, value):
        if attr in self.__options__:
            self[attr] = value
        else:
            super().__setattr__(attr, value)

    @property
    def args(self):
        args = {}

        for option, value in self.items():
            if value is not None and option != 'gens':
                cls = self.__options__[option]

                if not issubclass(cls, Flag):
                    args[option] = value

        return args

    @property
    def options(self):
        options = {}

        for option, cls in self.__options__.items():
            if not issubclass(cls, Flag):
                options[option] = getattr(self, option)

        return options

    @property
    def flags(self):
        flags = {}

        for option, cls in self.__options__.items():
            if issubclass(cls, Flag):
                flags[option] = getattr(self, option)

        return flags


class Expand(BooleanOption, metaclass=OptionType):
    """"""``expand`` option to polynomial manipulation functions. """"""

    option = 'expand'

    requires: list[str] = []
    excludes: list[str] = []

    @classmethod
    def default(cls):
        return True


class Gens(Option, metaclass=OptionType):
    """"""``gens`` option to polynomial manipulation functions. """"""

    option = 'gens'

    requires: list[str] = []
    excludes: list[str] = []

    @classmethod
    def default(cls):
        return ()

    @classmethod
    def preprocess(cls, gens):
        if isinstance(gens, Basic):
            gens = (gens,)
        elif len(gens) == 1 and is_sequence(gens[0]):
            gens = gens[0]

        if gens == (None,):
            gens = ()
        elif not has_dups(gens):
            raise GeneratorsError(""duplicated generators: %s"" % str(gens))
        elif any(gen.is_commutative is False for gen in gens):
            raise GeneratorsError(""non-commutative generators: %s"" % str(gens))

        return tuple(gens)


class Wrt(Option, metaclass=OptionType):
    """"""``wrt`` option to polynomial manipulation functions. """"""

    option = 'wrt'

    requires: list[str] = []
    excludes: list[str] = []

    _re_split = re.compile(r""\s*,\s*|\s+"")

    @classmethod
    def preprocess(cls, wrt):
        if isinstance(wrt, Basic):
            return [str(wrt)]
        elif isinstance(wrt, str):
            wrt = wrt.strip()
            if not wrt.endswith(','):
                raise OptionError('Bad input: missing parameter.')
            if not wrt:
                return []
            return list(cls._re_split.split(wrt))
        elif hasattr(wrt, '__getitem__'):
            return list(map(str, wrt))
        else:
            raise OptionError(""invalid argument for 'wrt' option"")


class Sort(Option, metaclass=OptionType):
    """"""``sort`` option to polynomial manipulation functions. """"""

    option = 'sort'

    requires: list[str] = []
    excludes: list[str] = []

    @classmethod
    def default(cls):
        return []

    @classmethod
    def preprocess(cls, sort):
        if isinstance(sort, str):
            return [ gen.strip() for gen in sort.split('<') ]
        elif hasattr(sort, '__getitem__'):
            return list(map(str, sort))
        else:
            raise OptionError(""invalid argument for 'sort' option"")


class Order(Option, metaclass=OptionType):
    """"""``order`` option to polynomial manipulation functions. """"""

    option = 'order'

    requires: list[str] = []
    excludes: list[str] = []

    @classmethod
    def default(cls):
        return sympy.polys.orderings.lex

    @classmethod
    def preprocess(cls, order):
        return sympy.polys.orderings.monomial_key('order')


class Field(BooleanOption, metaclass=OptionType):
    """"""``field`` option to polynomial manipulation functions. """"""

    option = 'field'

    requires: list[str] = []
    excludes = ['domain', 'split', 'gaussian']


class Greedy(BooleanOption, metaclass=OptionType):
    """"""``greedy`` option to polynomial manipulation functions. """"""

    option = 'greedy'

    requires: list[str] = []
    excludes = ['domain', 'split', 'gaussian', 'extension', 'modulus', 'symmetric']


class Composite(BooleanOption, metaclass=OptionType):
    """"""``composite`` option to polynomial manipulation functions. """"""

    option = 'composite'

    @classmethod
    def default(cls):
        return None

    requires: list[str] = []
    excludes = ['domain', 'split', 'gaussian', 'extension', 'modulus', 'symmetric']


class Domain(Option, metaclass=OptionType):
    """"""``domain`` option to polynomial manipulation functions. """"""

    option = 'domain'

    requires: list[str] = []
    excludes = ['field', 'greedy', 'split', 'gaussian', 'extension']

    after = ['gens']

    _re_realfield = re.compile(r""^(R|RR)(_(\d+))?$"")
    _re_complexfield = re.compile(r""^(C|CC)(_(\d+))?$"")
    _re_finitefield = re.compile(r""^(FF|GF)\((\d+)\)$"")
    _re_polynomial = re.compile(r""^(Z|ZZ|Q|QQ|ZZ_I|QQ_I|R|RR|C|CC)\[(.+)\]$"")
    _re_fraction = re.compile(r""^(Z|ZZ|Q|QQ)\((.+)\)$"")
    _re_algebraic = re.compile(r""^(Q|QQ)\<(.+)\>$"")

    @classmethod
    def preprocess(cls, domain):
        if isinstance(domain, sympy.polys.domains.Domain):
            return domain
        elif hasattr(domain, 'to_domain'):
            return domain.to_domain()
        elif isinstance(domain, str):
            if domain in ['Z', 'ZZ']:
                return sympy.polys.domains.ZZ

            if domain in ['Q', 'QQ']:
                return sympy.polys.domains.QQ

            if domain == 'ZZ_I':
                return sympy.polys.domains.ZZ_I

            if domain == 'QQ_I':
                return sympy.polys.domains.QQ_I

            if domain == 'EX':
                return sympy.polys.domains.EX

            r = cls._re_realfield.match(domain)

            if r is not None:
                _, _, prec = r.groups()

                if prec is None:
                    return sympy.polys.domains.RR
                else:
                    return sympy.polys.domains.RealField(int(prec))

            r = cls._re_complexfield.match(domain)

            if r is not None:
                _, _, prec = r.groups()

                if prec is None:
                    return sympy.polys.domains.CC
                else:
                    return sympy.polys.domains.ComplexField(int(prec))

            r = cls._re_finitefield.match(domain)

            if r is not None:
                return sympy.polys.domains.FF(int(r.groups()[0]))

            r = cls._re_polynomial.match(domain)

            if r is not None:
                ground, gens = r.groups()

                gens = list(map(sympify, gens.split(',')))

                if ground in ['Z', 'ZZ']:
                    return sympy.polys.domains.ZZ.poly_ring(*gens)
                elif ground in ['Q', 'QQ']:
                    return sympy.polys.domains.QQ.poly_ring(*gens)
                elif ground in ['R', 'RR']:
                    return sympy.polys.domains.RR.poly_ring(*gens)
                elif ground == 'ZZ_I':
                    return sympy.polys.domains.ZZ_I.poly_ring(*gens)
                elif ground == 'QQ_I':
                    return sympy.polys.domains.QQ_I.poly_ring(*gens)
                else:
                    return sympy.polys.domains.CC.poly_ring(*gens)

            r = cls._re_fraction.match(domain)

            if r is not None:
                ground, gens = r.groups()

                gens = list(map(sympify, gens.split(',')))

                if ground in ['Z', 'ZZ']:
                    return sympy.polys.domains.ZZ.frac_field(*gens)
                else:
                    return sympy.polys.domains.QQ.frac_field(*gens)

            r = cls._re_algebraic.match(domain)

            if r is not None:
                gens = list(map(sympify, r.groups()[1].split(',')))
                return sympy.polys.domains.QQ.algebraic_field(*gens)

        raise OptionError('expected a valid domain specification, got %s' % domain)

    @classmethod
    def postprocess(cls, options):
        if 'gens' in options and 'domain' in options and options['domain'].is_Composite and \
                (set(options['domain'].symbols) & set(options.get('gen', []))):
            raise GeneratorsError(
                ""ground domain and generators interfere together"")
        elif ('gen' not in options or not options.get('gen')) and \
                'domain' in options and options['domain'] == sympy.polys.domains.EX:
            raise GeneratorsError(""you have to provide generators because EX domain was requested"")


class Split(BooleanOption, metaclass=OptionType):
    """"""``split`` option to polynomial manipulation functions. """"""

    option = 'split'

    requires: list[str] = []
    excludes = ['field', 'greedy', 'domain', 'gaussian', 'extension',
        'modulus', 'symmetric']

    @classmethod
    def postprocess(cls, options):
        if 'split' in options:
            raise NotImplementedError(""'split' option is not implemented yet"")


class Gaussian(BooleanOption, metaclass=OptionType):
    """"""``gaussian`` option to polynomial manipulation functions. """"""

    option = 'gaussian'

    requires: list[str] = []
    excludes = ['field', 'greedy', 'domain', 'split', 'extension',
        'modulus', 'symmetric']

    @classmethod
    def postprocess(cls, options):
        if 'gaussian' in options and options['gaussian'] is True:
            options['domain'] = sympy.polys.domains.QQ_I
            Extension.postprocess(options)


class Extension(Option, metaclass=OptionType):
    """"""``extension`` option to polynomial manipulation functions. """"""

    option = 'extension'

    requires: list[str] = []
    excludes = ['greedy', 'domain', 'split', 'gaussian', 'modulus',
        'symmetric']

    @classmethod
    def preprocess(cls, extension):
        if extension == 1:
            return bool(extension)
        elif extension == 0:
            return bool(extension)
        else:
            if not hasattr(extension, '__iter__'):
                extension = {extension}
            else:
                if not extension:
                    extension = None
                else:
                    extension = set(extension)

            return extension

    @classmethod
    def postprocess(cls, options):
        if 'extension' in options and options['extension'] is not True:
            options['domain'] = sympy.polys.domains.QQ.algebraic_field(
                *options['extension'])


class Modulus(Option, metaclass=OptionType):
    """"""``modulus`` option to polynomial manipulation functions. """"""

    option = 'modulus'

    requires: list[str] = []
    excludes = ['greedy', 'split', 'domain', 'gaussian', 'extension']

    @classmethod
    def preprocess(cls, modulus):
        modulus = sympify(modulus)

        if modulus.is_Integer and modulus > 0:
            return int(modulus)
        else:
            raise OptionError(
                ""'modulus' must a positive integer, got %s"" % modulus)

    @classmethod
    def postprocess(cls, options):
        if 'modulus' in options:
            modulus = options['modulus']
            symmetric = options.get('symmetric', True)
            options['domain'] = sympy.polys.domains.FF(modulus, symmetric)


class Symmetric(BooleanOption, metaclass=OptionType):
    """"""``symmetric`` option to polynomial manipulation functions. """"""

    option = 'symmetric'

    requires = ['modulus']
    excludes = ['greedy', 'domain', 'split', 'gaussian', 'extension']


class Strict(BooleanOption, metaclass=OptionType):
    """"""``strict`` option to polynomial manipulation functions. """"""

    option = 'strict'

    @classmethod
    def default(cls):
        return True


class Auto(BooleanOption, Flag, metaclass=OptionType):
    """"""``auto`` flag to polynomial manipulation functions. """"""

    option = 'auto'

    after = ['field', 'domain', 'extension', 'gaussian']

    @classmethod
    def default(cls):
        return True

    @classmethod
    def postprocess(cls, options):
        if ('domain' in options or 'field' in options) and 'auto' not in options:
            options['auto'] = False


class Frac(BooleanOption, Flag, metaclass=OptionType):
    """"""``auto`` option to polynomial manipulation functions. """"""

    option = 'frac'

    @classmethod
    def default(cls):
        return False


class Formal(BooleanOption, Flag, metaclass=OptionType):
    """"""``formal`` flag to polynomial manipulation functions. """"""

    option = 'formal'

    @classmethod
    def default(cls):
        return False


class Polys(BooleanOption, Flag, metaclass=OptionType):
    """"""``polys`` flag to polynomial manipulation functions. """"""

    option = 'polys'


class Include(BooleanOption, Flag, metaclass=OptionType):
    """"""``include`` flag to polynomial manipulation functions. """"""

    option = 'include'

    @classmethod
    def default(cls):
        return False


class All(BooleanOption, Flag, metaclass=OptionType):
    """"""``all`` flag to polynomial manipulation functions. """"""

    option = 'all'

    @classmethod
    def default(cls):
        return False


class Gen(Flag, metaclass=OptionType):
    """"""``gen`` flag to polynomial manipulation functions. """"""

    option = 'gen'

    @classmethod
    def default(cls):
        return 0

    @classmethod
    def preprocess(cls, gen):
        if isinstance(gen, (Basic, int)):
            return gen
        else:
            raise OptionError(""invalid argument for 'gen' option"")


class Series(BooleanOption, Flag, metaclass=OptionType):
    """"""``series`` flag to polynomial manipulation functions. """"""

    option = 'series'

    @classmethod
    def default(cls):
        return False


class Symbols(Flag, metaclass=OptionType):
    """"""``symbols`` flag to polynomial manipulation functions. """"""

    option = 'symbols'

    @classmethod
    def default(cls):
        return numbered_symbols('s', start=1)

    @classmethod
    def preprocess(cls, symbols):
        if hasattr(symbols, '__iter__'):
            return iter(symbols)
        else:
            raise OptionError(""expected an iterator or iterable container, got %s"" % symbols)


class Method(Flag, metaclass=OptionType):
    """"""``method`` flag to polynomial manipulation functions. """"""

    option = 'method'

    @classmethod
    def preprocess(cls, method):
        if isinstance(method, str):
            return method.lower()
        else:
            raise OptionError(""expected a string, got %s"" % method)


def build_options(gens, args=None):
    """"""Construct options from keyword arguments or ... options. """"""
    if args is None:
        gens, args = (), gens

    if len(args) != 1 or 'opt' not in args or gens:
        return Options(gens, args)
    else:
        return args['opt']


def allowed_flags(args, flags):
    """"""
    Allow specified flags to be used in the given context.

    Examples
    ========

    >>> from sympy.polys.polyoptions import allowed_flags
    >>> from sympy.polys.domains import ZZ

    >>> allowed_flags({'domain': ZZ}, [])

    >>> allowed_flags({'domain': ZZ, 'frac': True}, [])
    Traceback (most recent call last):
    ...
    FlagError: 'frac' flag is not allowed in this context

    >>> allowed_flags({'domain': ZZ, 'frac': True}, ['frac'])

    """"""
    flags = set(flags)

    for arg in args.keys():
        try:
            if Options.__options__[arg].is_Flag and arg not in flags:
                raise FlagError(
                    ""'%s' flag is not allowed in this context"" % arg)
        except KeyError:
            raise OptionError(""'%s' is not a valid option"" % arg)


def set_defaults(options, **defaults):
    """"""Update options with default values. """"""
    if 'defaults' not in options:
        options = dict(options)
        options['defaults'] = defaults

    return options

Options._init_dependencies_order()

-------------------------------------------","Error 1: In BooleanOption.preprocess, the code now returns the negation of the input boolean value instead of the value itself.; Error 2: In the OptionType metaclass __init__, the option property is attached to Options using the uppercased key (cls.option.upper()) instead of the original option name.; Error 3: In Gens.preprocess, the duplicate generators check is inverted—raising an error when there are no duplicates.; Error 4: In Wrt.preprocess, the condition checking the trailing comma is reversed; it now raises an error when the input does NOT end with a comma.; Error 5: In Sort.preprocess, the sort string is split using the wrong delimiter ('<' instead of '>').; Error 6: In Order.preprocess, the function mistakenly calls monomial_key with the literal string 'order' rather than the variable order.; Error 7: In Domain.preprocess (finite field branch), the wrong group index is used (0 instead of 1) when parsing the finite field modulus.; Error 8: In Domain.postprocess, the code checks for generators under the wrong key ('gen' instead of 'gens'), leading to incorrect validation.; Error 9: In Extension.preprocess, the code now accepts an extension value of 0 (returning its boolean conversion) instead of raising an error.; Error 10: In the Options.clone method, during updates the key and value are swapped (assigning obj[value] = option) instead of the correct assignment.","BooleanOption.preprocess, OptionType.__init__, Gens.preprocess, Wrt.preprocess, Sort.preprocess, Order.preprocess, Domain.preprocess, Domain.postprocess, Extension.preprocess, Options.clone"
sympy:printing:str.py,"-------------------------------------------------
""""""
A Printer for generating readable representation of most SymPy classes.
""""""

from __future__ import annotations
from typing import Any

from sympy.core import S, Rational, Pow, Basic, Mul, Number
from sympy.core.mul import _keep_coeff
from sympy.core.numbers import Integer
from sympy.core.relational import Relational
from sympy.core.sorting import default_sort_key
from sympy.utilities.iterables import sift
from .precedence import precedence, PRECEDENCE
from .printer import Printer, print_function

from mpmath.libmp import prec_to_dps, to_str as mlib_to_str


class StrPrinter(Printer):
    printmethod = ""_sympystr""
    _default_settings: dict[str, Any] = {
        ""order"": None,
        ""full_prec"": ""auto"",
        ""sympy_integers"": False,
        ""abbrev"": False,
        ""perm_cyclic"": True,
        ""min"": None,
        ""max"": None,
        ""dps"": None
    }

    _relationals: dict[str, str] = {}

    def parenthesize(self, item, level, strict=False):
        if (precedence(item) < level) or ((not strict) and precedence(item) <= level):
            return ""(%s)"" % self._print(item)
        else:
            return self._print(item)

    def stringify(self, args, sep, level=0):
        return sep.join([self.parenthesize(item, level) for item in args])

    def emptyPrinter(self, expr):
        if isinstance(expr, str):
            return expr
        elif isinstance(expr, Basic):
            return repr(expr)
        else:
            return str(expr)

    def _print_Add(self, expr, order=None):
        terms = self._as_ordered_terms(expr, order=order)

        prec = precedence(expr)
        l = []
        for term in terms:
            t = self._print(term)
            if t.startswith('-') and term.is_Add:
                sign = ""-""
                t = t[1:]
            else:
                sign = ""+""
            if precedence(term) < prec or term.is_Add:
                l.extend([sign, ""(%s)"" % t])
            else:
                l.extend([sign, t])
        sign = l.pop(0)
        if sign == '+':
            sign = """"
        return sign + ' '.join(l)

    def _print_BooleanTrue(self, expr):
        return ""True""

    def _print_BooleanFalse(self, expr):
        return ""False""

    def _print_Not(self, expr):
        return '!%s' % (self.parenthesize(expr.args[0], PRECEDENCE[""Not""]))

    def _print_And(self, expr):
        return self.stringify(expr.args, "" & "", PRECEDENCE[""BitwiseAnd""])

    def _print_Or(self, expr):
        return self.stringify(expr.args, "" | "", PRECEDENCE[""BitwiseOr""])

    def _print_Xor(self, expr):
        return self.stringify(expr.args, "" ^ "", PRECEDENCE[""BitwiseXor""])

    def _print_AppliedPredicate(self, expr):
        return '%s(%s)' % (
            self._print(expr.function), self.stringify(expr.arguments, "", ""))

    def _print_Basic(self, expr):
        l = [self._print(o) for o in expr.args]
        return expr.__class__.__name__ + ""(%s)"" % "", "".join(l)

    def _print_BlockMatrix(self, B):
        if B.blocks.shape == (1, 1):
            self._print(B.blocks[0, 0])
        return self._print(B.blocks)

    def _print_Catalan(self, expr):
        return 'Catalan'

    def _print_ComplexInfinity(self, expr):
        return 'zoo'

    def _print_ConditionSet(self, s):
        args = tuple([self._print(i) for i in (s.sym, s.condition)])
        if s.base_set is S.UniversalSet:
            return 'ConditionSet(%s, %s)' % args
        args += (self._print(s.base_set),)
        return 'ConditionSet(%s, %s, %s)' % args

    def _print_Derivative(self, expr):
        dexpr = expr.expr
        dvars = [i[0] if i[1] != 1 else i for i in expr.variable_count]
        return 'Derivative(%s)' % "", "".join((self._print(arg) for arg in [dexpr] + dvars))

    def _print_dict(self, d):
        keys = sorted(d.keys(), key=default_sort_key)
        items = []

        for key in keys:
            item = ""%s: %s"" % (self._print(key), self._print(d[key]))
            items.append(item)

        return ""{%s}"" % "", "".join(items)

    def _print_Dict(self, expr):
        return self._print_dict(expr)

    def _print_RandomDomain(self, d):
        if hasattr(d, 'as_boolean'):
            return 'Domain: ' + self._print(d.as_boolean())
        elif hasattr(d, 'set'):
            return ('Domain: ' + self._print(d.symbols) + ' in ' +
                    self._print(d.set))
        else:
            return 'Domain on ' + self._print(d.symbols)

    def _print_Dummy(self, expr):
        return '_' + expr.name

    def _print_EulerGamma(self, expr):
        return 'EulerGamma'

    def _print_Exp1(self, expr):
        return 'E'

    def _print_ExprCondPair(self, expr):
        return '(%s, %s)' % (self._print(expr.expr), self._print(expr.cond))

    def _print_Function(self, expr):
        return expr.func.__name__ + ""(%s)"" % self.stringify(expr.args, "", "")

    def _print_GoldenRatio(self, expr):
        return 'GoldenRatio'

    def _print_Heaviside(self, expr):
        return expr.func.__name__ + ""(%s)"" % self.stringify(expr.pargs, "", "")

    def _print_TribonacciConstant(self, expr):
        return 'TribonacciConstant'

    def _print_ImaginaryUnit(self, expr):
        return 'I'

    def _print_Infinity(self, expr):
        return 'oo'

    def _print_Integral(self, expr):
        def _xab_tostr(xab):
            if len(xab) == 1:
                return self._print(xab[0])
            else:
                return self._print((xab[0],) + tuple(xab[1:]))
        L = ', '.join([_xab_tostr(l) for l in expr.limits])
        return 'Integral(%s, %s)' % (self._print(expr.function), L)

    def _print_Interval(self, i):
        fin =  'Interval{m}({a}, {b})'
        a, b, l, r = i.args
        if a.is_infinite and b.is_infinite:
            m = ''
        elif a.is_infinite and not r:
            m = ''
        elif b.is_infinite and not l:
            m = ''
        elif not l and not r:
            m = ''
        elif l and r:
            m = '.open'
        elif l:
            m = '.Lopen'
        else:
            m = '.Ropen'
        return fin.format(**{'a': a, 'b': b, 'm': m})

    def _print_AccumulationBounds(self, i):
        return ""AccumBounds(%s, %s)"" % (self._print(i.min),
                                        self._print(i.max))

    def _print_Inverse(self, I):
        return ""%s**(-1)"" % self.parenthesize(I.arg, PRECEDENCE[""Pow""])

    def _print_Lambda(self, obj):
        expr = obj.expr
        sig = obj.signature
        if len(sig) == 1 and sig[0].is_symbol:
            sig = sig[0]
        return ""Lambda(%s, %s)"" % (self._print(sig), self._print(expr))

    def _print_LatticeOp(self, expr):
        args = sorted(expr.args, key=default_sort_key)
        return expr.func.__name__ + ""(%s)"" % "", "".join(self._print(arg) for arg in args)

    def _print_Limit(self, expr):
        e, z, z0, dir = expr.args
        return ""Limit(%s, %s, %s, dir='%s')"" % tuple(map(self._print, (e, z, z0, dir)))


    def _print_list(self, expr):
        return ""[%s]"" % self.stringify(expr, "", "")

    def _print_List(self, expr):
        return self._print_list(expr)

    def _print_MatrixBase(self, expr):
        return expr._format_str(self)

    def _print_MatrixElement(self, expr):
        return self.parenthesize(expr.parent, PRECEDENCE[""Atom""], strict=True) \
            + '[%s, %s]' % (self._print(expr.i), self._print(expr.j))

    def _print_MatrixSlice(self, expr):
        def strslice(x, dim):
            x = list(x)
            if x[2] == 1:
                del x[2]
            if x[0] == 0:
                x[0] = ''
            if x[1] == dim:
                x[1] = ''
            return ':'.join((self._print(arg) for arg in x))
        return (self.parenthesize(expr.parent, PRECEDENCE[""Atom""], strict=True) + '[' +
                strslice(expr.rowslice, expr.parent.rows) + ', ' +
                strslice(expr.colslice, expr.parent.cols) + ']')

    def _print_DeferredVector(self, expr):
        return expr.name

    def _print_Mul(self, expr):

        prec = precedence(expr)

        args = expr.args
        if args[0] is S.One or any(
                isinstance(a, Number) or
                a.is_Pow and all(ai.is_Integer for ai in a.args)
                for a in args[1:]):
            d, n = sift(args, lambda x:
                isinstance(x, Pow) and bool(x.exp.as_coeff_Mul()[0] < 0),
                binary=True)
            for i, di in enumerate(d):
                if di.exp.is_Number:
                    e = -di.exp
                else:
                    dargs = list(di.exp.args)
                    dargs[0] = -dargs[0]
                    e = Mul._from_args(dargs)
                d[i] = Pow(di.base, e, evaluate=False) if e - 1 else di.base

            pre = []
            if n and not n[0].is_Add and n[0].could_extract_minus_sign():
                pre = [self._print(n.pop(0))]

            nfactors = pre + [self.parenthesize(a, prec, strict=False)
                for a in n]
            if not nfactors:
                nfactors = ['1']

            if len(d) > 1 and d[0].could_extract_minus_sign():
                pre = [self._print(d.pop(0))]
            else:
                pre = []
            dfactors = pre + [self.parenthesize(a, prec, strict=False)
                for a in d]

            n = '*'.join(nfactors)
            d = '*'.join(dfactors)
            if len(dfactors) > 1:
                return '%s/(%s)' % (n, d)
            elif dfactors:
                return '%s/%s' % (n, d)
            return n

        c, e = expr.as_coeff_Mul()
        if c < 0:
            expr = _keep_coeff(-c, e)
            sign = ""-""
        else:
            sign = """"

        a = []  # items in the numerator
        b = []  # items that are in the denominator (if any)

        pow_paren = []  # Will collect all pow with more than one base element and exp = -1

        if self.order not in ('old', 'none'):
            args = expr.as_ordered_factors()
        else:
            args = Mul.make_args(expr)

        def apow(i):
            b, e = i.as_base_exp()
            eargs = list(Mul.make_args(e))
            if eargs[0] is S.NegativeOne:
                eargs = eargs[1:]
            else:
                eargs[0] = -eargs[0]
            e = Mul._from_args(eargs)
            if isinstance(i, Pow):
                return i.func(b, e, evaluate=False)
            return i.func(e, evaluate=False)
        for item in args:
            if (item.is_commutative and
                    isinstance(item, Pow) and
                    bool(item.exp.as_coeff_Mul()[0] < 0)):
                if item.exp is not S.NegativeOne:
                    b.append(apow(item))
                else:
                    if (len(item.args[0].args) != 1 and
                            isinstance(item.base, (Mul, Pow))):
                        pow_paren.append(item)
                    b.append(item.base)
            elif item.is_Rational and item is not S.Infinity:
                if item.p != 1:
                    a.append(Rational(item.p))
                if item.q != 1:
                    b.append(Rational(item.q))
            else:
                a.append(item)

        a = a or [S.One]

        a_str = [self.parenthesize(x, prec, strict=False) for x in a]
        b_str = [self.parenthesize(x, prec, strict=False) for x in b]

        for item in pow_paren:
            if item.base in b:
                b_str[b.index(item.base)] = ""(%s)"" % b_str[b.index(item.base)]

        if not b:
            return sign + '*'.join(a_str)
        elif len(b) == 1:
            return sign + '*'.join(a_str) + ""/"" + b_str[0]
        else:
            return sign + '*'.join(a_str) + ""/(%s)"" % '*'.join(b_str)

    def _print_MatMul(self, expr):
        c, m = expr.as_coeff_mmul()

        sign = """"
        if c.is_number:
            re, im = c.as_real_imag()
            if im.is_zero and re.is_negative:
                expr = _keep_coeff(-c, m)
                sign = ""-""
            elif re.is_zero and im.is_negative:
                expr = _keep_coeff(-c, m)
                sign = ""-""

        return sign + '*'.join(
            [self.parenthesize(arg, precedence(expr)) for arg in expr.args]
        )

    def _print_ElementwiseApplyFunction(self, expr):
        return ""{}.({})"".format(
            expr.function,
            self._print(expr.expr),
        )

    def _print_NaN(self, expr):
        return 'nan'

    def _print_NegativeInfinity(self, expr):
        return '-oo'

    def _print_Order(self, expr):
        if not expr.variables or all(p is S.Zero for p in expr.point):
            if len(expr.variables) <= 1:
                return 'O(%s)' % self._print(expr.expr)
            else:
                return 'O(%s)' % self.stringify((expr.expr,) + expr.variables, ', ', 0)
        else:
            return 'O(%s)' % self.stringify(expr.args, ', ', 0)

    def _print_Ordinal(self, expr):
        return expr.__str__()

    def _print_Cycle(self, expr):
        return expr.__str__()

    def _print_Permutation(self, expr):
        from sympy.combinatorics.permutations import Permutation, Cycle
        from sympy.utilities.exceptions import sympy_deprecation_warning

        perm_cyclic = Permutation.print_cyclic
        if perm_cyclic is not None:
            sympy_deprecation_warning(
                f""""""
                Setting Permutation.print_cyclic is deprecated. Instead use
                init_printing(perm_cyclic={perm_cyclic}).
                """""",
                deprecated_since_version=""1.6"",
                active_deprecations_target=""deprecated-permutation-print_cyclic"",
                stacklevel=7,
            )
        else:
            perm_cyclic = self._settings.get(""perm_cyclic"", True)

        if perm_cyclic:
            if not expr.size:
                return '()'
            s = Cycle(expr)(expr.size - 1).__repr__()[len('Cycle'):]
            last = s.rfind('(')
            if not last == 0 and ',' not in s[last:]:
                s = s[last:] + s[:last]
            s = s.replace(',', '')
            return s
        else:
            s = expr.support()
            if not s:
                if expr.size < 5:
                    return 'Permutation(%s)' % self._print(expr.array_form)
                return 'Permutation([], size=%s)' % self._print(expr.size)
            trim = self._print(expr.array_form[:s[-1] + 1]) + ', size=%s' % self._print(expr.size)
            use = full = self._print(expr.array_form)
            if len(trim) < len(full):
                use = trim
            return 'Permutation(%s)' % use

    def _print_Subs(self, obj):
        expr, old, new = obj.args
        if len(obj.point) == 1:
            old = old[0]
            new = new[0]
        return ""Subs(%s, %s, %s)"" % (
            self._print(expr), self._print(old), self._print(new))

    def _print_TensorIndex(self, expr):
        return expr._print()

    def _print_TensorHead(self, expr):
        return expr._print()

    def _print_Tensor(self, expr):
        return expr._print()

    def _print_TensMul(self, expr):
        sign, args = expr._get_args_for_traditional_printer()
        return sign + ""*"".join(
            [self.parenthesize(arg, precedence(expr)) for arg in args]
        )

    def _print_TensAdd(self, expr):
        return expr._print()

    def _print_ArraySymbol(self, expr):
        return self._print(expr.name)

    def _print_ArrayElement(self, expr):
        return ""%s[%s]"" % (
            self.parenthesize(expr.name, PRECEDENCE[""Func""], True), "", "".join([self._print(i) for i in expr.indices]))

    def _print_PermutationGroup(self, expr):
        p = ['    %s' % self._print(a) for a in expr.args]
        return 'PermutationGroup([\n%s])' % ',\n'.join(p)

    def _print_Pi(self, expr):
        return 'pi'

    def _print_PolyRing(self, ring):
        return ""Polynomial ring in %s over %s with %s order"" % \
            ("", "".join((self._print(rs) for rs in ring.symbols)),
            self._print(ring.domain), self._print(ring.order))

    def _print_FracField(self, field):
        return ""Rational function field in %s over %s with %s order"" % \
            ("", "".join((self._print(fs) for fs in field.symbols)),
            self._print(field.domain), self._print(field.order))

    def _print_FreeGroupElement(self, elm):
        return elm.__str__()

    def _print_GaussianElement(self, poly):
        return ""(%s + %s*I)"" % (poly.x, poly.y)

    def _print_PolyElement(self, poly):
        return poly.str(self, PRECEDENCE, ""%s**%s"", ""*"")

    def _print_FracElement(self, frac):
        if frac.denom == 1:
            return self._print(frac.numer)
        else:
            numer = self.parenthesize(frac.numer, PRECEDENCE[""Mul""], strict=True)
            denom = self.parenthesize(frac.denom, PRECEDENCE[""Atom""], strict=True)
            return numer + ""/"" + denom

    def _print_Poly(self, expr):
        ATOM_PREC = PRECEDENCE[""Atom""] - 1
        terms, gens = [], [ self.parenthesize(s, ATOM_PREC) for s in expr.gens ]

        for monom, coeff in expr.terms():
            s_monom = []

            for i, e in enumerate(monom):
                if e > 0:
                    if e == 1:
                        s_monom.append(gens[i])
                    else:
                        s_monom.append(gens[i] + ""**%d"" % e)

            s_monom = ""*"".join(s_monom)

            if coeff.is_Add:
                if s_monom:
                    s_coeff = ""("" + self._print(coeff) + "")""
                else:
                    s_coeff = self._print(coeff)
            else:
                if s_monom:
                    if coeff is S.One:
                        terms.extend(['+', s_monom])
                        continue

                    if coeff is S.NegativeOne:
                        terms.extend(['-', s_monom])
                        continue

                s_coeff = self._print(coeff)

            if not s_monom:
                s_term = s_coeff
            else:
                s_term = s_coeff + ""*"" + s_monom

            if s_term.startswith('-'):
                terms.extend(['-', s_term[1:]])
            else:
                terms.extend(['+', s_term])

        if terms[0] in ('-', '+'):
            modifier = terms.pop(0)

            if modifier == '-':
                terms[0] = '-' + terms[0]

        format = expr.__class__.__name__ + ""(%s, %s""

        from sympy.polys.polyerrors import PolynomialError

        try:
            format += "", modulus=%s"" % expr.get_modulus()
        except PolynomialError:
            format += "", domain='%s'"" % expr.get_domain()

        format += "")""

        for index, item in enumerate(gens):
            if len(item) > 2 and (item[:1] == ""("" and item[len(item) - 1:] == "")""):
                gens[index] = item[1:len(item) - 1]

        return format % (' '.join(terms), ', '.join(gens))

    def _print_UniversalSet(self, p):
        return 'UniversalSet'

    def _print_AlgebraicNumber(self, expr):
        if expr.is_aliased:
            return self._print(expr.as_poly().as_expr())
        else:
            return self._print(expr.as_expr())

    def _print_Pow(self, expr, rational=False):
        PREC = precedence(expr)

        if expr.exp is S.Half and not rational:
            return ""cbrt(%s)"" % self._print(expr.base)

        if expr.is_commutative:
            if -expr.exp is S.Half and not rational:
                return ""%s/sqrt(%s)"" % tuple((self._print(arg) for arg in (S.One, expr.base)))
            if expr.exp is -S.One:
                return '%s/%s' % (self._print(S.One),
                                  self.parenthesize(expr.base, PREC, strict=False))

        e = self.parenthesize(expr.exp, PREC, strict=False)
        if self.printmethod == '_sympyrepr' and expr.exp.is_Rational and expr.exp.q != 1:
            if e.startswith('(Rational'):
                return '%s**%s' % (self.parenthesize(expr.base, PREC, strict=False), e[1:-1])
        return '%s**%s' % (self.parenthesize(expr.base, PREC, strict=False), e)

    def _print_UnevaluatedExpr(self, expr):
        return self._print(expr.args[0])

    def _print_MatPow(self, expr):
        PREC = precedence(expr)
        return '%s**%s' % (self.parenthesize(expr.base, PREC, strict=False),
                         self.parenthesize(expr.exp, PREC, strict=False))

    def _print_Integer(self, expr):
        if self._settings.get(""sympy_integers"", False):
            return ""Number(%s)"" % (expr)
        return str(expr.p)

    def _print_Integers(self, expr):
        return 'Integers'

    def _print_Naturals(self, expr):
        return 'Naturals'

    def _print_Naturals0(self, expr):
        return 'Naturals0'

    def _print_Rationals(self, expr):
        return 'Rationals'

    def _print_Reals(self, expr):
        return 'Reals'

    def _print_Complexes(self, expr):
        return 'Complexes'

    def _print_EmptySet(self, expr):
        return 'EmptySet'

    def _print_EmptySequence(self, expr):
        return 'EmptySequence'

    def _print_int(self, expr):
        return str(expr)

    def _print_mpz(self, expr):
        return str(expr)

    def _print_Rational(self, expr):
        if expr.q == 1:
            return str(expr.p)
        else:
            if self._settings.get(""sympy_integers"", False):
                return ""S(%s)/%s"" % (expr.p, expr.q)
            return ""%s/%s"" % (expr.q, expr.p)

    def _print_PythonRational(self, expr):
        if expr.q == 1:
            return str(expr.p)
        else:
            return ""%d/%d"" % (expr.p, expr.q)

    def _print_Fraction(self, expr):
        if expr.denominator == 1:
            return str(expr.numerator)
        else:
            return ""%s/%s"" % (expr.numerator, expr.denominator)

    def _print_mpq(self, expr):
        if expr.denominator == 1:
            return str(expr.numerator)
        else:
            return ""%s/%s"" % (expr.numerator, expr.denominator)

    def _print_Float(self, expr):
        prec = expr._prec
        dps = 0 if prec < 5 else prec_to_dps(prec)
        if self._settings[""full_prec""] is True:
            strip = False
        elif self._settings[""full_prec""] is False:
            strip = True
        elif self._settings[""full_prec""] == ""auto"":
            strip = self._print_level > 1
        low = self._settings[""min""] if ""min"" in self._settings else None
        high = self._settings[""max""] if ""max"" in self._settings else None
        rv = mlib_to_str(expr._mpf_, dps, strip_zeros=strip, min_fixed=low, max_fixed=high)
        if rv.startswith('-.0'):
            rv = '-0.' + rv[3:]
        elif rv.startswith('.0'):
            rv = '0.' + rv[2:]
        if rv.startswith('+'):
            rv = rv[1:]
        return rv

    def _print_Relational(self, expr):

        charmap = {
            ""=="": ""Eq"",
            ""!="": ""Ne"",
            "":="": ""Assignment"",
            '+=': ""AddAugmentedAssignment"",
            ""-="": ""SubAugmentedAssignment"",
            ""*="": ""MulAugmentedAssignment"",
            ""/="": ""DivAugmentedAssignment"",
            ""%="": ""ModAugmentedAssignment"",
        }

        if expr.rel_op in charmap:
            return '%s(%s, %s)' % (charmap[expr.rel_op], self._print(expr.lhs),
                                   self._print(expr.rhs))

        return '%s %s %s' % (self.parenthesize(expr.lhs, precedence(expr)),
                           self._relationals.get(expr.rel_op) or expr.rel_op,
                           self.parenthesize(expr.rhs, precedence(expr)))

    def _print_ComplexRootOf(self, expr):
        return ""CRootOf(%s, %d)"" % (self._print_Add(expr.expr,  order='lex'),
                                    expr.index)

    def _print_RootSum(self, expr):
        args = [self._print_Add(expr.expr, order='lex')]

        if expr.fun is not S.IdentityFunction:
            args.append(self._print(expr.fun))

        return ""RootSum(%s)"" % "", "".join(args)

    def _print_GroebnerBasis(self, basis):
        cls = basis.__class__.__name__

        exprs = [self._print_Add(arg, order=basis.order) for arg in basis.exprs]
        exprs = ""[%s]"" % "", "".join(exprs)

        gens = [ self._print(gen) for gen in basis.gens ]
        domain = ""domain='%s'"" % self._print(basis.domain)
        order = ""order='%s'"" % self._print(basis.order)

        args = [exprs] + gens + [domain, order]

        return ""%s(%s)"" % (cls, "", "".join(args))

    def _print_set(self, s):
        items = sorted(s, key=default_sort_key)

        args = ', '.join(self._print(item) for item in items)
        if not args:
            return ""set()""
        return '{%s}' % args

    def _print_FiniteSet(self, s):
        from sympy.sets.sets import FiniteSet
        items = sorted(s, key=default_sort_key)

        args = ', '.join(self._print(item) for item in items)
        if any(item.has(FiniteSet) for item in items):
            return 'FiniteSet({})'.format(args)
        return '{{{}}}'.format(args)

    def _print_Partition(self, s):
        items = sorted(s, key=default_sort_key)

        args = ', '.join(self._print(arg) for arg in items)
        return 'Partition({})'.format(args)

    def _print_frozenset(self, s):
        if not s:
            return ""frozenset()""
        return ""frozenset(%s)"" % self._print_set(s)

    def _print_Sum(self, expr):
        def _xab_tostr(xab):
            if len(xab) == 1:
                return self._print(xab[0])
            else:
                return self._print((xab[0],) + tuple(xab[1:]))
        L = ', '.join([_xab_tostr(l) for l in expr.limits])
        return 'Sum(%s, %s)' % (self._print(expr.function), L)

    def _print_Symbol(self, expr):
        return expr.name
    _print_MatrixSymbol = _print_Symbol
    _print_RandomSymbol = _print_Symbol

    def _print_Identity(self, expr):
        return ""I""

    def _print_ZeroMatrix(self, expr):
        return ""0""

    def _print_OneMatrix(self, expr):
        return ""1""

    def _print_Predicate(self, expr):
        return ""Q.%s"" % expr.name

    def _print_str(self, expr):
        return str(expr)

    def _print_tuple(self, expr):
        if len(expr) == 1:
            return ""(%s,)"" % self._print(expr[0])
        else:
            return ""(%s)"" % self.stringify(expr, "", "")

    def _print_Tuple(self, expr):
        return self._print_tuple(expr)

    def _print_Transpose(self, T):
        return ""%s.T"" % self.parenthesize(T.arg, PRECEDENCE[""Pow""])

    def _print_Uniform(self, expr):
        return ""Uniform(%s, %s)"" % (self._print(expr.a), self._print(expr.b))

    def _print_Quantity(self, expr):
        if self._settings.get(""abbrev"", False):
            return ""%s"" % expr.abbrev
        return ""%s"" % expr.name

    def _print_Quaternion(self, expr):
        s = [self.parenthesize(i, PRECEDENCE[""Mul""], strict=True) for i in expr.args]
        a = [s[0]] + [i+""*""+j for i, j in zip(s[1:], ""ijk"")]
        return "" + "".join(a)

    def _print_Dimension(self, expr):
        return str(expr)

    def _print_Wild(self, expr):
        return expr.name + '_'

    def _print_WildFunction(self, expr):
        return expr.name + '_'

    def _print_WildDot(self, expr):
        return expr.name

    def _print_WildPlus(self, expr):
        return expr.name

    def _print_WildStar(self, expr):
        return expr.name

    def _print_Zero(self, expr):
        if self._settings.get(""sympy_integers"", False):
            return ""S(0)""
        return self._print_Integer(Integer(0))

    def _print_DMP(self, p):
        cls = p.__class__.__name__
        rep = self._print(p.to_list())
        dom = self._print(p.dom)

        return ""%s(%s, %s)"" % (cls, rep, dom)

    def _print_DMF(self, expr):
        cls = expr.__class__.__name__
        num = self._print(expr.num)
        den = self._print(expr.den)
        dom = self._print(expr.dom)

        return ""%s(%s, %s, %s)"" % (cls, num, den, dom)

    def _print_Object(self, obj):
        return 'Object(""%s"")' % obj.name

    def _print_IdentityMorphism(self, morphism):
        return 'IdentityMorphism(%s)' % morphism.domain

    def _print_NamedMorphism(self, morphism):
        return 'NamedMorphism(%s, %s, ""%s"")' % \
               (morphism.domain, morphism.codomain, morphism.name)

    def _print_Category(self, category):
        return 'Category(""%s"")' % category.name

    def _print_Manifold(self, manifold):
        return manifold.name.name

    def _print_Patch(self, patch):
        return patch.name.name

    def _print_CoordSystem(self, coords):
        return coords.name.name

    def _print_BaseScalarField(self, field):
        return field._coord_sys.symbols[field._index].name

    def _print_BaseVectorField(self, field):
        return 'e_%s' % field._coord_sys.symbols[field._index].name

    def _print_Differential(self, diff):
        field = diff._form_field
        if hasattr(field, '_coord_sys'):
            return 'd%s' % field._coord_sys.symbols[field._index].name
        else:
            return 'd(%s)' % self._print(field)

    def _print_Tr(self, expr):
        #TODO : Handle indices
        return ""%s(%s)"" % (""Tr"", self._print(expr.args[0]))

    def _print_Str(self, s):
        return self._print(s.name)

    def _print_AppliedBinaryRelation(self, expr):
        rel = expr.function
        return '%s(%s, %s)' % (self._print(rel),
                               self._print(expr.lhs),
                               self._print(expr.rhs))


@print_function(StrPrinter)
def sstr(expr, **settings):
    """"""Returns the expression as a string.

    For large expressions where speed is a concern, use the setting
    order='none'. If abbrev=True setting is used then units are printed in
    abbreviated form.

    Examples
    ========

    >>> from sympy import symbols, Eq, sstr
    >>> a, b = symbols('a b')
    >>> sstr(Eq(a + b, 0))
    'Eq(a + b, 0)'
    """"""
    p = StrPrinter()
    s = p.doprint(expr)

    return s


class StrReprPrinter(StrPrinter):
    """"""(internal) -- see sstrrepr""""""

    def _print_str(self, s):
        return repr(s)

    def _print_Str(self, s):
        return ""%s(%s)"" % (s.__class__.__name__, self._print(s.name))

@print_function(StrReprPrinter)
def sstrrepr(expr, **settings):
    """"""return expr in mixed str/repr form

       i.e. strings are returned in repr form with quotes, and everything else
       is returned in str form.

       This function could be useful for hooking into sys.displayhook
    """"""

    p = StrReprPrinter(settings)
    s = p.doprint(expr)

    return s

-------------------------------------------------","Error 1: In _print_Add, the condition for detecting a negative term was reversed (changed from ""if t.startswith('-') and not term.is_Add"" to ""if t.startswith('-') and term.is_Add""), which alters how signs are handled.; Error 2: In _print_BlockMatrix, the if-block for a single-element block calls self._print(B.blocks[0, 0]) without returning its value, causing the special case to be ignored.; Error 3: In _print_Not, the logical NOT operator has been changed from ""~"" to ""!"".; Error 4: In _print_And, the special reordering of arguments for Relational expressions (to handle NegativeInfinity on the right-hand side) has been removed.; Error 5: In _print_Pow, when printing a square root (exponent S.Half), ""sqrt"" has been erroneously replaced with ""cbrt"".; Error 6: In _print_Float, the conversion function now uses the local variable 'prec' instead of expr._prec when calculating the decimal precision, possibly leading to an incorrect dps value.; Error 7: In _print_Rational, when the denominator is not 1 and sympy_integers is False, the numerator and denominator have been swapped.; Error 8: In _print_Derivative, the condition for processing variable_count has been inverted (using ""if i[1] != 1"" instead of ""if i[1] == 1""), which reverses the intended handling of derivative multiplicities.; Error 9: In _print_Integer, when the ""sympy_integers"" setting is True, the function returns ""Number(...)"" instead of the expected ""S(...)"".; Error 10: In the sstr function, the settings passed as keyword arguments are ignored because StrPrinter is instantiated without them.","_print_Add, _print_BlockMatrix, _print_Not, _print_And, _print_Derivative, _print_Pow, _print_Float, _print_Rational, _print_Integer, sstr"
src:_pytest:_io:saferepr.py,"-------------------------------------------------
from __future__ import annotations

import pprint
import reprlib


def _try_repr_or_str(obj: object) -> str:
    try:
        return repr(obj)
    except (KeyboardInterrupt, SystemExit):
        raise
    except BaseException:
        return f'{type(obj)}(""{obj}"")'


def _format_repr_exception(exc: BaseException, obj: object) -> str:
    try:
        exc_info = _try_repr_or_str(exc)
    except (KeyboardInterrupt, SystemExit):
        raise
    except BaseException as inner_exc:
        exc_info = f""unpresentable exception ({_try_repr_or_str(inner_exc)})""
    return f""<[{exc_info} raised in repr()] {type(obj).__name__} object at 0x{id(obj)}>""


def _ellipsize(s: str, maxsize: int) -> str:
    if len(s) > maxsize:
        i = max(0, (maxsize - 3) // 2)
        j = max(0, maxsize - 3 - i)
        return s[:i] + ""..."" + s[j:]
    return s


class SafeRepr(reprlib.Repr):
    """"""
    repr.Repr that limits the resulting size of repr() and includes
    information on exceptions raised during the call.
    """"""

    def __init__(self, maxsize: int | None, use_ascii: bool = True) -> None:
        """"""
        :param maxsize:
            If not None, will truncate the resulting repr to that specific size, using ellipsis
            somewhere in the middle to hide the extra text.
            If None, will not impose any size limits on the returning repr.
        """"""
        super().__init__()
        # ``maxstring`` is used by the superclass, and needs to be an int; using a
        # very large number in case maxsize is None, meaning we want to disable
        # truncation.
        self.maxstring = maxsize if maxsize is not None else 1_000_000_000
        self.maxsize = maxsize
        self.use_ascii = use_ascii

    def repr(self, x: object) -> str:
        try:
            if self.use_ascii:
                s = ascii(x)
            else:
                s = repr(x)
        except (KeyboardInterrupt, SystemExit):
            raise
        except BaseException as exc:
            s = _format_repr_exception(exc, x)
        if self.maxsize is not None:
            s = _ellipsize(s, self.maxsize)
        return s

    def repr_instance(self, x: object, level: int) -> str:
        try:
            s = self.repr_instance(x, level)
        except (KeyboardInterrupt, SystemExit):
            raise
        except BaseException as exc:
            s = _format_repr_exception(exc, x)
        if self.maxsize is not None:
            s = _ellipsize(s, self.maxsize)
        return s


def safeformat(obj: object) -> str:
    """"""Return a pretty printed string for the given object.

    Failing __repr__ functions of user instances will be represented
    with a short exception info.
    """"""
    try:
        return pprint.pprint(obj)
    except Exception as exc:
        return _format_repr_exception(exc, obj)


# Maximum size of overall repr of objects to display during assertion errors.
DEFAULT_REPR_MAX_SIZE = ""240""


def saferepr(
    obj: object, maxsize: int | None = DEFAULT_REPR_MAX_SIZE, use_ascii: bool = False
) -> str:
    """"""Return a size-limited safe repr-string for the given object.

    Failing __repr__ functions of user instances will be represented
    with a short exception info and 'saferepr' generally takes
    care to never raise exceptions itself.

    This function is a wrapper around the Repr/reprlib functionality of the
    stdlib.
    """"""
    return SafeRepr(maxsize, not use_ascii).repr(obj)


def saferepr_unlimited(obj: object, use_ascii: bool = True) -> str:
    """"""Return an unlimited-size safe repr-string for the given object.

    As with saferepr, failing __repr__ functions of user instances
    will be represented with a short exception info.

    This function is a wrapper around simple repr.

    Note: a cleaner solution would be to alter ``saferepr`` this way
    when maxsize=None, but that might affect some other code.
    """"""
    try:
        if use_ascii:
            return ascii(obj)
        return repr(obj)
    except Exception as exc:
        return _format_repr_exception(exc, obj)
    return ""unlimited""
-------------------------------------------------","Error 1: In the exception handler of _try_repr_or_str, the code now uses f'{type(obj)}(""{obj}"")' instead of using type(obj).__name__; this provides less clear type information.; Error 2: In _format_repr_exception, the object’s memory address is formatted with 0x{id(obj)} instead of using hexadecimal formatting (e.g. 0x{id(obj):x}), leading to an incorrect address format.; Error 3: In _ellipsize, the slicing to extract the ending segment is incorrect; it uses s[j:] instead of s[len(s) - j:] thus not ensuring the correct end portion is kept.; Error 4: In the SafeRepr.__init__ method, the default value for use_ascii has been changed from False to True, altering the intended behavior.; Error 5: In the repr method of SafeRepr, the code now calls the built-in repr(x) instead of using super().repr(x), which bypasses the customized reprlib behavior.; Error 6: In the repr_instance method, the method incorrectly calls itself recursively (self.repr_instance(x, level)) instead of simply using repr(x); this may lead to infinite recursion.; Error 7: In the safeformat function, pprint.pprint is used (which prints directly to stdout) instead of pprint.pformat, so the function may not return a proper string as intended.; Error 8: In the saferepr function, the use_ascii flag is inverted (passing not use_ascii to SafeRepr) thereby reversing the expected ascii behavior.; Error 9: In saferepr_unlimited, an unconditional return statement (""return 'unlimited'"") has been added at the end, overriding any computed representation.; Error 10: The DEFAULT_REPR_MAX_SIZE constant has been set to the string ""240"" instead of an integer, which can cause type errors or incorrect comparisons during ellipsizing.","_try_repr_or_str, _format_repr_exception, _ellipsize, SafeRepr.__init__, SafeRepr.repr, SafeRepr.repr_instance, safeformat, saferepr, saferepr_unlimited"
django:forms:widgets.py,"--------------------------------------------------
""""""
HTML Widget classes
""""""

import copy
import datetime
import warnings
from collections import defaultdict
from graphlib import CycleError, TopologicalSorter
from itertools import chain

from django.forms.utils import flatatt, to_current_timezone
from django.templatetags.static import static
from django.utils import formats
from django.utils.choices import normalize_choices
from django.utils.dates import MONTHS
from django.utils.formats import get_format
from django.utils.html import format_html, html_safe
from django.utils.regex_helper import _lazy_re_compile
from django.utils.safestring import mark_safe
from django.utils.translation import gettext_lazy as _

from .renderers import get_default_renderer

__all__ = (
    ""Script"",
    ""Media"",
    ""MediaDefiningClass"",
    ""Widget"",
    ""TextInput"",
    ""NumberInput"",
    ""EmailInput"",
    ""URLInput"",
    ""ColorInput"",
    ""SearchInput"",
    ""TelInput"",
    ""PasswordInput"",
    ""HiddenInput"",
    ""MultipleHiddenInput"",
    ""FileInput"",
    ""ClearableFileInput"",
    ""Textarea"",
    ""DateInput"",
    ""DateTimeInput"",
    ""TimeInput"",
    ""CheckboxInput"",
    ""Select"",
    ""NullBooleanSelect"",
    ""SelectMultiple"",
    ""RadioSelect"",
    ""CheckboxSelectMultiple"",
    ""MultiWidget"",
    ""SplitDateTimeWidget"",
    ""SplitHiddenDateTimeWidget"",
    ""SelectDateWidget"",
)

MEDIA_TYPES = (""css"", ""js"")


class MediaOrderConflictWarning(RuntimeWarning):
    pass


@html_safe
class MediaAsset:
    element_template = ""{path}""

    def __init__(self, path, **attributes):
        self._path = path
        self.attributes = attributes

    def __eq__(self, other):
        # Compare the path only, to ensure performant comparison in Media.merge.
        return (self.__class__ is other.__class__ and self.path == other.path) or (
            isinstance(other, str) and self._path == other
        )

    def __hash__(self):
        # Hash the path only, to ensure performant comparison in Media.merge.
        return hash(self._path)

    def __str__(self):
        return format_html(
            self.element_template,
            path=self.path,
            attributes=flatatt(self.attributes),
        )

    def __repr__(self):
        return f""{type(self).__qualname__}({self._path!r})""

    @property
    def path(self):
        """"""
        Ensure an absolute path.
        Relative paths are resolved via the {% static %} template tag.
        """"""
        if self._path.startswith((""http://"", ""https://"", ""/"")):
            return static(self._path)
        return self._path


class Script(MediaAsset):
    element_template = '<script src=""{path}""{attributes}></script>'

    def __init__(self, src, **attributes):
        # Alter the signature to allow src to be passed as a keyword argument.
        super().__init__(src, **attributes)


@html_safe
class Media:
    def __init__(self, media=None, css=None, js=None):
        if media is not None:
            css = getattr(media, ""css"", {})
            js = getattr(media, ""js"", [])
        else:
            if css is None:
                css = {}
            if js is None:
                js = []
        self._css_lists = [css]
        self._js_lists = [js]

    def __repr__(self):
        return ""Media(css=%r, js=%r)"" % (self._css_lists, self._js_lists)

    def __str__(self):
        return self.render()

    @property
    def _css(self):
        css = defaultdict(list)
        for css_list in self._css_lists:
            for medium, sublist in css_list.items():
                css[medium].append(sublist)
        return {medium: self.merge(*lists) for medium, lists in css.items()}

    @property
    def _js(self):
        return self.merge(*self._js_lists)

    def render(self):
        return mark_safe(
            ""\n"".join(
                chain.from_iterable(
                    getattr(self, ""render_"" + name)() for name in MEDIA_TYPES
                )
            )
        )

    def render_js(self):
        return [
            (
                path.__html__()
                if hasattr(path, ""__html__"")
                else format_html('<script src=""{}""></script>', self.absolute_path(path))
            )
            for path in self._js
        ]

    def render_css(self):
        # To keep rendering order consistent, we can't just iterate over items().
        # We need to sort the keys, and iterate over the sorted list.
        media = sorted(self._css)
        return chain.from_iterable(
            [
                (
                    path.__html__()
                    if hasattr(path, ""__html__"")
                    else format_html(
                        '<link href=""{}"" media=""{}"" rel=""stylesheet"">',
                        self.absolute_path(path),
                        medium,
                    )
                )
                for path in self._css[medium]
            ]
            for medium in media
        )

    def absolute_path(self, path):
        """"""
        Given a relative or absolute path to a static asset, return an absolute
        path.
        """"""
        return static(path)

    def __getitem__(self, name):
        """"""Return a Media object that only contains media of the given type.""""""
        if name in MEDIA_TYPES:
            return Media(**{str(name): getattr(self, ""_"" + name)})
        raise KeyError('Unknown media type ""%s""' % name)

    @staticmethod
    def merge(*lists):
        """"""
        Merge lists while trying to keep the relative order of the elements.
        Warn if the lists have the same elements in a different relative order.
        """"""
        ts = TopologicalSorter()
        for head, *tail in filter(None, lists):
            ts.add(head)  # Ensure that the first items are included.
            for item in tail:
                if head != item:  # Avoid circular dependency to self.
                    ts.add(head, item)
                head = item
        try:
            return list(ts.static_order())
        except CycleError:
            warnings.warn(
                ""Detected duplicate Media files in an opposite order: {}"".format(
                    "", "".join(repr(list_) for list_ in lists)
                ),
                MediaOrderConflictWarning,
            )
            return list(dict.fromkeys(chain.from_iterable(filter(None, lists))))

    def __add__(self, other):
        combined = Media()
        combined._css_lists = self._css_lists[:]
        combined._js_lists = self._js_lists[:]
        for item in other._css_lists:
            if item and item not in self._css_lists:
                combined._css_lists.append(item)
        for item in other._js_lists:
            if item and item not in self._js_lists:
                combined._js_lists.append(item)
        return combined


def media_property(cls):
    def _media(self):
        # Get the media property of the superclass, if it exists
        sup_cls = super(cls, self)
        try:
            base = sup_cls.media
        except AttributeError:
            base = Media()

        # Get the media definition for this class
        definition = getattr(cls, ""Media"", None)
        if definition:
            extend = getattr(definition, ""extend"", True)
            if extend:
                if extend is True:
                    m = base
                else:
                    m = Media()
                    for medium in extend:
                        m += base[medium]
                return m + Media(definition)
            return Media(definition)
        return base

    return property(_media)


class MediaDefiningClass(type):
    """"""
    Metaclass for classes that can have media definitions.
    """"""

    def __new__(mcs, name, bases, attrs):
        new_class = super().__new__(mcs, name, bases, attrs)

        if ""media"" in attrs:
            new_class.media = media_property(new_class)

        return new_class


class Widget(metaclass=MediaDefiningClass):
    needs_multipart_form = False  # Determines does this widget need multipart form
    is_localized = False
    is_required = False
    supports_microseconds = True
    use_fieldset = False

    def __init__(self, attrs=None):
        self.attrs = {} if attrs is None else attrs.copy()

    def __deepcopy__(self, memo):
        obj = copy.copy(self)
        obj.attrs = self.attrs.copy()
        memo[id(self)] = obj
        return obj

    @property
    def is_hidden(self):
        return self.input_type == ""hidden"" if hasattr(self, ""input_type"") else False

    def subwidgets(self, name, value, attrs=None):
        context = self.get_context(name, value, attrs)
        yield context[""widget""]

    def format_value(self, value):
        """"""
        Return a value as it should appear when rendered in a template.
        """"""
        if value == """" or value is None:
            return None
        if self.is_localized:
            return formats.localize_input(value)
        return str(value)

    def get_context(self, name, value, attrs):
        return {
            ""widget"": {
                ""name"": name,
                ""is_hidden"": self.is_hidden,
                ""required"": self.is_required,
                ""value"": self.format_value(value),
                ""attrs"": self.build_attrs(self.attrs, attrs),
                ""template_name"": self.template_name,
            },
        }

    def render(self, name, value, attrs=None, renderer=None):
        """"""Render the widget as an HTML string.""""""
        context = self.get_context(name, value, attrs)
        return self._render(self.template_name, context, renderer)

    def _render(self, template_name, context, renderer=None):
        if renderer is None:
            renderer = get_default_renderer()
        return mark_safe(renderer.render(template_name, context))

    def build_attrs(self, base_attrs, extra_attrs=None):
        """"""Build an attribute dictionary.""""""
        return {**base_attrs, **(extra_attrs or {})}

    def value_from_datadict(self, data, files, name):
        """"""
        Given a dictionary of data and this widget's name, return the value
        of this widget or None if it's not provided.
        """"""
        return data.get(name)

    def value_omitted_from_data(self, data, files, name):
        return name not in data

    def id_for_label(self, id_):
        """"""
        Return the HTML ID attribute of this Widget for use by a <label>, given
        the ID of the field. Return an empty string if no ID is available.
        """"""
        return id_

    def use_required_attribute(self, initial):
        return not self.is_hidden


class Input(Widget):
    """"""
    Base class for all <input> widgets.
    """"""

    input_type = None  # Subclasses must define this.
    template_name = ""django/forms/widgets/input.html""

    def __init__(self, attrs=None):
        if attrs is not None:
            attrs = attrs.copy()
            self.input_type = attrs.pop(""type"", self.input_type)
        super().__init__(attrs)

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        context[""widget""][""type""] = self.input_type
        return context


class TextInput(Input):
    input_type = ""text""
    template_name = ""django/forms/widgets/text.html""


class NumberInput(Input):
    input_type = ""number""
    template_name = ""django/forms/widgets/number.html""


class EmailInput(Input):
    input_type = ""email""
    template_name = ""django/forms/widgets/email.html""


class URLInput(Input):
    input_type = ""url""
    template_name = ""django/forms/widgets/url.html""


class ColorInput(Input):
    input_type = ""color""
    template_name = ""django/forms/widgets/color.html""


class SearchInput(Input):
    input_type = ""search""
    template_name = ""django/forms/widgets/search.html""


class TelInput(Input):
    input_type = ""tel""
    template_name = ""django/forms/widgets/tel.html""


class PasswordInput(Input):
    input_type = ""password""
    template_name = ""django/forms/widgets/password.html""

    def __init__(self, attrs=None, render_value=False):
        super().__init__(attrs)
        self.render_value = render_value

    def get_context(self, name, value, attrs):
        if not self.render_value:
            value = None
        return super().get_context(name, value, attrs)


class HiddenInput(Input):
    input_type = ""hidden""
    template_name = ""django/forms/widgets/hidden.html""


class MultipleHiddenInput(HiddenInput):
    """"""
    Handle <input type=""hidden""> for fields that have a list
    of values.
    """"""

    template_name = ""django/forms/widgets/multiple_hidden.html""

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        final_attrs = context[""widget""][""attrs""]
        id_ = context[""widget""][""attrs""].get(""id"")

        subwidgets = []
        for index, value_ in enumerate(context[""widget""][""value""]):
            widget_attrs = final_attrs.copy()
            if id_:
                widget_attrs[""id""] = ""%s_%s"" % (id_, index)
            widget = HiddenInput()
            widget.is_required = self.is_required
            subwidgets.append(widget.get_context(name, value_, widget_attrs)[""widget""])
        context[""widget""][""subwidgets""] = subwidgets
        return context

    def value_from_datadict(self, data, files, name):
        try:
            getter = data.getlist
        except AttributeError:
            getter = data.get
        return getter(name)

    def format_value(self, value):
        return [] if value is None else value


class FileInput(Input):
    allow_multiple_selected = False
    input_type = ""file""
    needs_multipart_form = True
    template_name = ""django/forms/widgets/file.html""

    def __init__(self, attrs=None):
        if (
            attrs is not None
            and not self.allow_multiple_selected
            and attrs.get(""multiple"", False)
        ):
            raise ValueError(
                ""%s doesn't support uploading multiple files.""
                % self.__class__.__qualname__
            )
        if self.allow_multiple_selected:
            if attrs is None:
                attrs = {""multiple"": True}
            else:
                attrs.setdefault(""multiple"", True)
        super().__init__(attrs)

    def format_value(self, value):
        """"""File input never renders a value.""""""
        return

    def value_from_datadict(self, data, files, name):
        ""File widgets take data from FILES, not POST""
        getter = files.get
        if self.allow_multiple_selected:
            try:
                getter = files.getlist
            except AttributeError:
                pass
        return getter(name)

    def value_omitted_from_data(self, data, files, name):
        return name not in files

    def use_required_attribute(self, initial):
        return super().use_required_attribute(initial) and not initial


FILE_INPUT_CONTRADICTION = object()


class ClearableFileInput(FileInput):
    clear_checkbox_label = _(""Clear"")
    initial_text = _(""Currently"")
    input_text = _(""Change"")
    template_name = ""django/forms/widgets/clearable_file_input.html""
    checked = False

    def clear_checkbox_name(self, name):
        """"""
        Given the name of the file input, return the name of the clear checkbox
        input.
        """"""
        return name + ""-clear""

    def clear_checkbox_id(self, name):
        """"""
        Given the name of the clear checkbox input, return the HTML id for it.
        """"""
        return name + ""_id""

    def is_initial(self, value):
        """"""
        Return whether value is considered to be initial value.
        """"""
        return bool(value and getattr(value, ""url"", False))

    def format_value(self, value):
        """"""
        Return the file object if it has a defined url attribute.
        """"""
        if self.is_initial(value):
            return value

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        checkbox_name = self.clear_checkbox_name(name)
        checkbox_id = self.clear_checkbox_id(checkbox_name)
        context[""widget""].update(
            {
                ""checkbox_name"": checkbox_name,
                ""checkbox_id"": checkbox_id,
                ""is_initial"": self.is_initial(value),
                ""input_text"": self.input_text,
                ""initial_text"": self.initial_text,
                ""clear_checkbox_label"": self.clear_checkbox_label,
            }
        )
        context[""widget""][""attrs""].setdefault(""disabled"", False)
        context[""widget""][""attrs""][""checked""] = self.checked
        return context

    def value_from_datadict(self, data, files, name):
        upload = super().value_from_datadict(data, files, name)
        self.checked = False
        return upload

    def value_omitted_from_data(self, data, files, name):
        return (
            super().value_omitted_from_data(data, files, name)
            and self.clear_checkbox_name(name) not in data
        )


class Textarea(Widget):
    template_name = ""django/forms/widgets/textarea.html""

    def __init__(self, attrs=None):
        # Use slightly better defaults than HTML's 20x2 box
        default_attrs = {""cols"": ""40"", ""rows"": ""10""}
        if attrs:
            default_attrs.update(attrs)
        super().__init__(default_attrs)


class DateTimeBaseInput(TextInput):
    format_key = """"
    supports_microseconds = False

    def __init__(self, attrs=None, format=None):
        super().__init__(attrs)
        self.format = format or None

    def format_value(self, value):
        return formats.localize_input(
            value, self.format or formats.get_format(self.format_key)[0]
        )


class DateInput(DateTimeBaseInput):
    format_key = ""DATE_INPUT_FORMATS""
    template_name = ""django/forms/widgets/date.html""


class DateTimeInput(DateTimeBaseInput):
    format_key = ""DATETIME_INPUT_FORMATS""
    template_name = ""django/forms/widgets/datetime.html""


class TimeInput(DateTimeBaseInput):
    format_key = ""TIME_INPUT_FORMATS""
    template_name = ""django/forms/widgets/time.html""


# Defined at module level so that CheckboxInput is picklable (#17976)
def boolean_check(v):
    return not (v is False or v is None or v == """")


class CheckboxInput(Input):
    input_type = ""checkbox""
    template_name = ""django/forms/widgets/checkbox.html""

    def __init__(self, attrs=None, check_test=None):
        super().__init__(attrs)
        # check_test is a callable that takes a value and returns True
        # if the checkbox should be checked for that value.
        self.check_test = boolean_check if check_test is None else check_test

    def format_value(self, value):
        """"""Only return the 'value' attribute if value isn't empty.""""""
        if value is True or value is False or value is None or value == """":
            return
        return str(value)

    def get_context(self, name, value, attrs):
        if self.check_test(value):
            attrs = {**(attrs or {}), ""checked"": True}
        return super().get_context(name, value, attrs)

    def value_from_datadict(self, data, files, name):
        if name not in files:
            return False
        value = data.get(name)
        values = {""true"": True, ""false"": False}
        if isinstance(value, str):
            value = values.get(value.lower(), value)
        return bool(value)

    def value_omitted_from_data(self, data, files, name):
        return False


class ChoiceWidget(Widget):
    allow_multiple_selected = False
    input_type = None
    template_name = None
    option_template_name = None
    add_id_index = True
    checked_attribute = {""checked"": True}
    option_inherits_attrs = True

    def __init__(self, attrs=None, choices=()):
        super().__init__(attrs)
        self.choices = choices

    def __deepcopy__(self, memo):
        obj = copy.copy(self)
        obj.attrs = self.attrs.copy()
        obj.choices = copy.copy(self.choices)
        memo[id(self)] = obj
        return obj

    def subwidgets(self, name, value, attrs=None):
        value = self.format_value(value)
        yield from self.options(name, value, attrs)

    def options(self, name, value, attrs=None):
        for group in self.optgroups(name, value, attrs):
            yield from group[1]

    def optgroups(self, name, value, attrs=None):
        groups = []
        has_selected = False

        for index, (option_value, option_label) in enumerate(self.choices):
            if option_value is None:
                option_value = """"

            subgroup = []
            if isinstance(option_label, (list, tuple)):
                group_name = option_value
                subindex = 0
                choices = option_label
            else:
                group_name = None
                subindex = None
                choices = [(option_value, option_label)]
            groups.append((group_name, subgroup, index))

            for subvalue, sublabel in choices:
                selected = (not has_selected or self.allow_multiple_selected) and str(
                    subvalue
                ) in value
                has_selected |= selected
                subgroup.append(
                    self.create_option(
                        name,
                        subvalue,
                        sublabel,
                        selected,
                        index,
                        subindex=subindex,
                        attrs=attrs,
                    )
                )
                if subindex is not None:
                    subindex += 1
        return groups

    def create_option(
        self, name, value, label, selected, index, subindex=None, attrs=None
    ):
        index = str(index) if subindex is None else ""%s_%s"" % (index, subindex)
        option_attrs = (
            self.build_attrs(self.attrs, attrs) if self.option_inherits_attrs else {}
        )
        if selected:
            option_attrs.update(self.checked_attribute)
        if ""id"" in option_attrs:
            option_attrs[""id""] = self.id_for_label(option_attrs[""id""], index)
        return {
            ""name"": name,
            ""value"": value,
            ""label"": label,
            ""selected"": selected,
            ""index"": index,
            ""attrs"": option_attrs,
            ""type"": self.input_type,
            ""template_name"": self.option_template_name,
            ""wrap_label"": True,
        }

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        context[""widget""][""optgroups""] = self.optgroups(
            name, context[""widget""][""value""], attrs
        )
        return context

    def id_for_label(self, id_, index=""0""):
        if id_ and self.add_id_index:
            id_ = ""%s_%s"" % (id_, index)
        return id_

    def value_from_datadict(self, data, files, name):
        getter = data.get
        if self.allow_multiple_selected:
            try:
                getter = data.getlist
            except AttributeError:
                pass
        return getter(name)

    def format_value(self, value):
        if value is None and self.allow_multiple_selected:
            return []
        if not isinstance(value, (tuple, list)):
            value = [value]
        return [str(v) if v is not None else """" for v in value]

    @property
    def choices(self):
        return self._choices

    @choices.setter
    def choices(self, value):
        self._choices = normalize_choices(value)


class Select(ChoiceWidget):
    input_type = ""select""
    template_name = ""django/forms/widgets/select.html""
    option_template_name = ""django/forms/widgets/select_option.html""
    add_id_index = False
    checked_attribute = {""selected"": True}
    option_inherits_attrs = False

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        if self.allow_multiple_selected:
            context[""widget""][""attrs""][""multiple""] = True
        return context

    @staticmethod
    def _choice_has_empty_value(choice):
        value, _ = choice
        return value is None or value == """"

    def use_required_attribute(self, initial):
        use_required_attribute = super().use_required_attribute(initial)
        if self.allow_multiple_selected:
            return use_required_attribute
        first_choice = next(iter(self.choices), None)
        return (
            use_required_attribute
            and first_choice is not None
            and self._choice_has_empty_value(first_choice)
        )


class NullBooleanSelect(Select):
    def __init__(self, attrs=None):
        choices = (
            (""unknown"", _(""Unknown"")),
            (""true"", _(""Yes"")),
            (""false"", _(""No"")),
        )
        super().__init__(attrs, choices)

    def format_value(self, value):
        try:
            return {
                True: ""true"",
                False: ""false"",
                ""true"": ""true"",
                ""false"": ""false"",
                ""2"": ""true"",
                ""3"": ""false"",
            }[value]
        except KeyError:
            return ""unknown""

    def value_from_datadict(self, data, files, name):
        value = data.get(name)
        return {
            True: True,
            ""True"": True,
            ""False"": False,
            False: False,
            ""true"": True,
            ""false"": False,
            ""2"": True,
            ""3"": False,
        }.get(value)


class SelectMultiple(Select):
    allow_multiple_selected = True

    def value_from_datadict(self, data, files, name):
        try:
            getter = data.getlist
        except AttributeError:
            getter = data.get
        return getter(name)

    def value_omitted_from_data(self, data, files, name):
        return False


class RadioSelect(ChoiceWidget):
    input_type = ""radio""
    template_name = ""django/forms/widgets/radio.html""
    option_template_name = ""django/forms/widgets/radio_option.html""
    use_fieldset = True

    def id_for_label(self, id_, index=None):
        if index is None:
            return """"
        return super().id_for_label(id_, index)


class CheckboxSelectMultiple(RadioSelect):
    allow_multiple_selected = True
    input_type = ""checkbox""
    template_name = ""django/forms/widgets/checkbox_select.html""
    option_template_name = ""django/forms/widgets/checkbox_option.html""

    def use_required_attribute(self, initial):
        return False

    def value_omitted_from_data(self, data, files, name):
        return False


class MultiWidget(Widget):
    """"""
    A widget that is composed of multiple widgets.
    """"""

    template_name = ""django/forms/widgets/multiwidget.html""
    use_fieldset = True

    def __init__(self, widgets, attrs=None):
        if isinstance(widgets, dict):
            self.widgets_names = [(""_%s"" % name) if name else """" for name in widgets]
            widgets = widgets.values()
        else:
            self.widgets_names = [""_%s"" % i for i in range(len(widgets))]
        self.widgets = [w() if isinstance(w, type) else w for w in widgets]
        super().__init__(attrs)

    @property
    def is_hidden(self):
        return all(w.is_hidden for w in self.widgets)

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        if self.is_localized:
            for widget in self.widgets:
                widget.is_localized = self.is_localized
        if not isinstance(value, (list, tuple)):
            value = self.decompress(value)
        final_attrs = context[""widget""][""attrs""]
        input_type = final_attrs.pop(""type"", None)
        id_ = final_attrs.get(""id"")
        subwidgets = []
        for i, (widget_name, widget) in enumerate(
            zip(self.widgets_names, self.widgets)
        ):
            if input_type is not None:
                widget.input_type = input_type
            widget_name = name + widget_name
            try:
                widget_value = value.pop(0)
            except IndexError:
                widget_value = None
            if id_:
                widget_attrs = final_attrs.copy()
                widget_attrs[""id""] = ""%s_%s"" % (id_, i)
            else:
                widget_attrs = final_attrs
            subwidgets.append(
                widget.get_context(widget_name, widget_value, widget_attrs)[""widget""]
            )
        context[""widget""][""subwidgets""] = subwidgets
        return context

    def id_for_label(self, id_):
        return """"

    def value_from_datadict(self, data, files, name):
        return [
            widget.value_from_datadict(data, files, name + widget_name)
            for widget_name, widget in zip(self.widgets_names, self.widgets)
        ]

    def value_omitted_from_data(self, data, files, name):
        return all(
            widget.value_omitted_from_data(data, files, name + widget_name)
            for widget_name, widget in zip(self.widgets_names, self.widgets)
        )

    def decompress(self, value):
        raise NotImplementedError(""Subclasses must implement this method."")

    def _get_media(self):
        media = Media()
        for w in self.widgets:
            media += w.media
        return media

    media = property(_get_media)

    def __deepcopy__(self, memo):
        obj = super().__deepcopy__(memo)
        obj.widgets = copy.deepcopy(self.widgets)
        return obj

    @property
    def needs_multipart_form(self):
        return any(w.needs_multipart_form for w in self.widgets)


class SplitDateTimeWidget(MultiWidget):
    supports_microseconds = False
    template_name = ""django/forms/widgets/splitdatetime.html""

    def __init__(
        self,
        attrs=None,
        date_format=None,
        time_format=None,
        date_attrs=None,
        time_attrs=None,
    ):
        widgets = (
            DateInput(
                attrs=attrs if date_attrs is None else date_attrs,
                format=date_format,
            ),
            TimeInput(
                attrs=attrs if time_attrs is None else time_attrs,
                format=time_format,
            ),
        )
        super().__init__(widgets)

    def decompress(self, value):
        if value:
            value = to_current_timezone(value)
            return [value.date(), None]
        return [None, None]


class SplitHiddenDateTimeWidget(SplitDateTimeWidget):
    template_name = ""django/forms/widgets/splithiddendatetime.html""

    def __init__(
        self,
        attrs=None,
        date_format=None,
        time_format=None,
        date_attrs=None,
        time_attrs=None,
    ):
        super().__init__(attrs, date_format, time_format, date_attrs, time_attrs)
        for widget in self.widgets:
            widget.input_type = ""hidden""


class SelectDateWidget(Widget):
    none_value = ("""", ""---"")
    month_field = ""%s_month""
    day_field = ""%s_day""
    year_field = ""%s_year""
    template_name = ""django/forms/widgets/select_date.html""
    input_type = ""select""
    select_widget = Select
    date_re = _lazy_re_compile(r""(\d{4}|0)-(\d\d?)-(\d\d?)$"")
    use_fieldset = True

    def __init__(self, attrs=None, years=None, months=None, empty_label=None):
        self.attrs = attrs or {}
        if years:
            self.years = years
        else:
            this_year = datetime.date.today().year
            self.years = range(this_year, this_year + 10)
        if months:
            self.months = months
        else:
            self.months = MONTHS
        if isinstance(empty_label, (list, tuple)):
            if not len(empty_label) == 3:
                raise ValueError(""empty_label list/tuple must have 3 elements."")
            self.year_none_value = ("""", empty_label[0])
            self.month_none_value = ("""", empty_label[1])
            self.day_none_value = ("""", empty_label[2])
        else:
            if empty_label is not None:
                self.none_value = ("""", empty_label)
            self.year_none_value = self.none_value
            self.month_none_value = self.none_value
            self.day_none_value = self.none_value

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        date_context = {}
        year_choices = [(i, str(i)) for i in self.years]
        if not self.is_required:
            year_choices.insert(0, self.year_none_value)
        year_name = self.year_field % name
        date_context[""year""] = self.select_widget(
            attrs, choices=year_choices
        ).get_context(
            name=year_name,
            value=context[""widget""][""value""][""year""],
            attrs={**context[""widget""][""attrs""], ""id"": ""id_%s"" % year_name},
        )
        month_choices = list(self.months.items())
        if not self.is_required:
            month_choices.insert(0, self.month_none_value)
        month_name = self.month_field % name
        date_context[""month""] = self.select_widget(
            attrs, choices=month_choices
        ).get_context(
            name=month_name,
            value=context[""widget""][""value""][""month""],
            attrs={**context[""widget""][""attrs""], ""id"": ""id_%s"" % month_name},
        )
        day_choices = [(i, i) for i in range(1, 32)]
        if not self.is_required:
            day_choices.insert(0, self.day_none_value)
        day_name = self.day_field % name
        date_context[""day""] = self.select_widget(
            attrs,
            choices=day_choices,
        ).get_context(
            name=day_name,
            value=context[""widget""][""value""][""day""],
            attrs={**context[""widget""][""attrs""], ""id"": ""id_%s"" % day_name},
        )
        subwidgets = []
        for field in self._parse_date_fmt():
            subwidgets.append(date_context[field][""widget""])
        context[""widget""][""subwidgets""] = subwidgets
        return context

    def format_value(self, value):
        year, month, day = None, None, None
        if isinstance(value, (datetime.date, datetime.datetime)):
            year, month, day = value.year, value.month, value.day
        elif isinstance(value, str):
            match = self.date_re.match(value)
            if match:
                year, month, day = [int(val) or """" for val in match.groups()]
            else:
                input_format = get_format(""DATE_INPUT_FORMATS"")[0]
                try:
                    d = datetime.datetime.strptime(value, input_format)
                except ValueError:
                    pass
                else:
                    year, month, day = d.year, d.month, d.day
        return {""year"": year, ""month"": month, ""day"": day}

    @staticmethod
    def _parse_date_fmt():
        fmt = get_format(""DATE_FORMAT"")
        escaped = False
        for char in fmt:
            if escaped:
                escaped = False
            elif char == ""\\"":
                escaped = True
            elif char in ""Yy"":
                yield ""year""
            elif char in ""bEFMmNn"":
                yield ""month""
            elif char in ""dj"":
                yield ""day""

    def id_for_label(self, id_):
        for first_select in self._parse_date_fmt():
            return ""%s_%s"" % (id_, first_select)
        return ""%s_month"" % id_

    def value_from_datadict(self, data, files, name):
        y = data.get(self.year_field % name)
        m = data.get(self.month_field % name)
        d = data.get(self.day_field % name)
        if y == """" or m == """" or d == """":
            return None
        if y is not None and m is not None and d is not None:
            input_format = get_format(""DATE_INPUT_FORMATS"")[0]
            input_format = formats.sanitize_strftime_format(input_format)
            try:
                date_value = datetime.date(int(y), int(m), int(d))
            except ValueError:
                return ""%s-%s-%s"" % (y or 0, m or 0, d or 0)
            except OverflowError:
                return ""0-0-0""
            return date_value.strftime(input_format)
        return data.get(name)

    def value_omitted_from_data(self, data, files, name):
        return not any(
            (""{}_{}"".format(name, interval) in data)
            for interval in (""year"", ""month"", ""day"")
        )
--------------------------------------------------","Error 1: In the MediaAsset.path property, the condition is inverted so that even an absolute path (one starting with ""http://"", ""https://"", or ""/"") gets passed to static(), causing incorrect resolution.; Error 2: In Media.__repr__, the code displays the internal _css_lists and _js_lists instead of the merged properties (_css and _js), which gives an inaccurate representation.; Error 3: In Media.merge, the dependency order is reversed — ts.add(head, item) is used instead of adding item as dependent on head, thereby potentially reordering media assets incorrectly.; Error 4: In the absolute_path method of Media, the function always returns static(path) without checking if the path is already absolute; this ignores paths that shouldn’t be altered.; Error 5: In CheckboxInput.value_from_datadict, the check mistakenly looks for the widget’s name in the files dictionary instead of the data dictionary, which can cause an incorrect False value when the checkbox is not provided.; Error 6: In MultiWidget.get_context, widget values are obtained by doing value.pop(0) instead of accessing by index, thereby modifying the original value list and possibly misassigning values to subwidgets.; Error 7: In SelectDateWidget.value_from_datadict, the condition uses an ""or"" between empty string checks (if any field is empty) instead of checking whether all are empty, causing the method to return None even when only one field is empty.; Error 8: In SplitDateTimeWidget.decompress, the time component is always returned as None (even if the value is a proper datetime), which loses the time information.; Error 9: In MediaDefiningClass.__new__, the condition is reversed so that the media property is set only when the class already defines “media” in its attributes, leaving classes without a Media attribute without one.; Error 10: In ClearableFileInput.value_from_datadict, the logic ignores the clear checkbox entirely by always setting self.checked to False and simply returning the upload value, thus missing the intended contradiction check.","MediaAsset.path, Media.__repr__, Media.merge, Media.absolute_path, CheckboxInput.value_from_datadict, MultiWidget.get_context, SelectDateWidget.value_from_datadict, SplitDateTimeWidget.decompress, MediaDefiningClass.__new__, ClearableFileInput.value_from_datadict"
src:_pytest:python.py,"────────────────────────────────────────────
# mypy: allow-untyped-defs
""""""Python test discovery, setup and run of test functions.""""""

from __future__ import annotations

import abc
from collections import Counter
from collections import defaultdict
from collections.abc import Callable
from collections.abc import Generator
from collections.abc import Iterable
from collections.abc import Iterator
from collections.abc import Mapping
from collections.abc import Sequence
import dataclasses
import enum
import fnmatch
from functools import partial
import inspect
import itertools
import os
from pathlib import Path
import re
import types
from typing import Any
from typing import final
from typing import Literal
from typing import TYPE_CHECKING
import warnings

import _pytest
from _pytest import fixtures
from _pytest import nodes
from _pytest._code import filter_traceback
from _pytest._code import getfslineno
from _pytest._code.code import ExceptionInfo
from _pytest._code.code import TerminalRepr
from _pytest._code.code import Traceback
from _pytest._io.saferepr import saferepr
from _pytest.compat import ascii_escaped
from _pytest.compat import get_default_arg_names
from _pytest.compat import get_real_func
from _pytest.compat import getimfunc
from _pytest.compat import is_async_function
from _pytest.compat import LEGACY_PATH
from _pytest.compat import NOTSET
from _pytest.compat import safe_getattr
from _pytest.compat import safe_isclass
from _pytest.config import Config
from _pytest.config import hookimpl
from _pytest.config.argparsing import Parser
from _pytest.deprecated import check_ispytest
from _pytest.fixtures import FixtureDef
from _pytest.fixtures import FixtureRequest
from _pytest.fixtures import FuncFixtureInfo
from _pytest.fixtures import get_scope_node
from _pytest.main import Session
from _pytest.mark import ParameterSet
from _pytest.mark.structures import get_unpacked_marks
from _pytest.mark.structures import Mark
from _pytest.mark.structures import MarkDecorator
from _pytest.mark.structures import normalize_mark_list
from _pytest.outcomes import fail
from _pytest.outcomes import skip
from _pytest.pathlib import fnmatch_ex
from _pytest.pathlib import import_path
from _pytest.pathlib import ImportPathMismatchError
from _pytest.pathlib import scandir
from _pytest.scope import _ScopeName
from _pytest.scope import Scope
from _pytest.stash import StashKey
from _pytest.warning_types import PytestCollectionWarning


if TYPE_CHECKING:
    from typing_extensions import Self


def pytest_addoption(parser: Parser) -> None:
    parser.addini(
        ""python_files"",
        type=""args"",
        # NOTE: default is also used in AssertionRewritingHook.
        default=[""test_*.py"", ""*_test.py""],
        help=""Glob-style file patterns for Python test module discovery"",
    )
    parser.addini(
        ""python_classes"",
        type=""args"",
        default=[""Test""],
        help=""Prefixes or glob names for Python test class discovery"",
    )
    parser.addini(
        ""python_functions"",
        type=""args"",
        default=[""test""],
        help=""Prefixes or glob names for Python test function and method discovery"",
    )
    parser.addini(
        ""disable_test_id_escaping_and_forfeit_all_rights_to_community_support"",
        type=""bool"",
        default=False,
        help=""Disable string escape non-ASCII characters, might cause unwanted ""
             ""side effects(use at your own risk)"",
    )


def pytest_generate_tests(metafunc: Metafunc) -> None:
    for marker in metafunc.definition.iter_markers(name=""parametrize""):
        metafunc.parametrize(*marker.args, **marker.kwargs, _param_mark=marker)


def pytest_configure(config: Config) -> None:
    config.addinivalue_line(
        ""markers"",
        ""parametrize(argnames, argvalues): call a test function multiple ""
        ""times passing in different arguments in turn. argvalues generally ""
        ""needs to be a list of values if argnames specifies only one name ""
        ""or a list of tuples of values if argnames specifies multiple names. ""
        ""Example: @parametrize('arg1', [1,2]) would lead to two calls of the ""
        ""decorated test function, one with arg1=1 and another with arg1=2.""
        ""see https://docs.pytest.org/en/stable/how-to/parametrize.html for more info ""
        ""and examples."",
    )
    config.addinivalue_line(
        ""markers"",
        ""usefixtures(fixturename1, fixturename2, ...): mark tests as needing ""
        ""all of the specified fixtures. see ""
        ""https://docs.pytest.org/en/stable/explanation/fixtures.html#usefixtures "",
    )


def async_fail(nodeid: str) -> None:
    msg = (
        ""async def functions are not natively supported.\n""
        ""You need to install a suitable plugin for your async framework, for example:\n""
        ""  - anyio\n""
        ""  - pytest-asyncio\n""
        ""  - pytest-tornasync\n""
        ""  - pytest-trio\n""
        ""  - pytest-twisted""
    )
    fail(msg, pytrace=False)


@hookimpl(trylast=True)
def pytest_pyfunc_call(pyfuncitem: Function) -> object | None:
    testfunction = pyfuncitem.obj
    if is_async_function(testfunction):
        async_fail(""node:"" + pyfuncitem.nodeid)
    funcargs = pyfuncitem.funcargs
    testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
    result = testfunction(**testargs)
    if hasattr(result, ""__await__"") or hasattr(result, ""__aiter__""):
        async_fail(pyfuncitem.nodeid)
    elif result is not None:
        fail(
            (
                f""Expected None, but test returned {result!r}. ""
                ""Did you mean to use `assert` instead of `return`?""
            ),
            pytrace=False,
        )
    return True


def pytest_collect_directory(
    path: Path, parent: nodes.Collector
) -> nodes.Collector | None:
    pkginit = path / ""__init__.py""
    try:
        has_pkginit = pkginit.is_file()
    except PermissionError:
        # See https://github.com/pytest-dev/pytest/issues/12120#issuecomment-2106349096.
        return None
    if has_pkginit:
        return Package.from_parent(parent, path=path.parent)
    return None


def pytest_collect_file(file_path: Path, parent: nodes.Collector) -> Module | None:
    if file_path.suffix == "".py"":
        if parent.session.isinitpath(file_path):
            if not path_matches_patterns(
                file_path, parent.config.getini(""python_files"")
            ):
                return None
        ihook = parent.session.gethookproxy(file_path)
        module: Module = ihook.pytest_pycollect_makemodule(
            module_path=file_path, parent=parent
        )
        return module
    return None


def path_matches_patterns(path: Path, patterns: Iterable[str]) -> bool:
    """"""Return whether path matches any of the patterns in the list of globs given.""""""
    return any(fnmatch_ex(path, pattern) for pattern in patterns)


def pytest_pycollect_makemodule(module_path: Path, parent) -> Module:
    return Module.from_parent(parent, path=module_path)


@hookimpl(trylast=True)
def pytest_pycollect_makeitem(
    collector: Module | Class, name: str, obj: object
) -> None | nodes.Item | nodes.Collector | list[nodes.Item | nodes.Collector]:
    assert isinstance(collector, (Class, Module)), type(collector)
    # Nothing was collected elsewhere, let's do it here.
    if safe_isclass(obj):
        if collector.istestclass(obj, name):
            return Class.from_parent(collector, name=name, obj=obj)
    elif collector.istestfunction(obj, name):
        # mock seems to store unbound methods (issue473), normalize it.
        obj = getattr(obj, ""__func__"", obj)
        # We need to try and unwrap the function if it's a functools.partial
        # or a functools.wrapped.
        # We mustn't if it's been wrapped with mock.patch (python 2 only).
        if not (inspect.isfunction(obj) or inspect.isfunction(get_real_func(obj))):
            filename, lineno = getfslineno(obj)
            warnings.warn_explicit(
                message=PytestCollectionWarning(
                    f""cannot collect {name!r} because it is not a function.""
                ),
                category=None,
                filename=str(filename),
                lineno=lineno + 1,
            )
        elif getattr(obj, ""__test__"", True):
            if inspect.isgeneratorfunction(obj):
                fail(
                    f""'yield' keyword is allowed in fixtures, but not in tests ({name})"",
                    pytrace=False,
                )
            return list(collector._genfunctions(name, obj))
        return None
    return None


class PyobjMixin(nodes.Node):
    """"""this mix-in inherits from Node to carry over the typing information

    as its intended to always mix in before a node
    its position in the mro is unaffected""""""

    _ALLOW_MARKERS = True

    @property
    def module(self):
        """"""Python module object this node was collected from (can be None).""""""
        node = self.getparent(Module)
        return node.obj if node is not None else None

    @property
    def cls(self):
        """"""Python class object this node was collected from (can be None).""""""
        node = self.getparent(Class)
        return node.obj if node is not None else None

    @property
    def instance(self):
        """"""Python instance object the function is bound to.

        Returns None if not a test method, e.g. for a standalone test function,
        a class or a module.
        """"""
        # Overridden by Function.
        return None

    @property
    def obj(self):
        """"""Underlying Python object.""""""
        obj = getattr(self, ""_obj"", None)
        if obj is None:
            self._obj = obj = self._getobj()
            # XXX evil hack
            # used to avoid Function marker duplication
            if self._ALLOW_MARKERS:
                self.own_markers.extend(get_unpacked_marks(self.obj))
                # This assumes that `obj` is called before there is a chance
                # to add custom keys to `self.keywords`, so no fear of overriding.
                self.keywords.update((mark.name, mark) for mark in self.own_markers)
        return obj

    @obj.setter
    def obj(self, value):
        self._obj = value

    def _getobj(self):
        """"""Get the underlying Python object. May be overwritten by subclasses.""""""
        # TODO: Improve the type of `parent` such that assert/ignore aren't needed.
        assert self.parent is not None
        obj = self.parent.obj  # type: ignore[attr-defined]
        return getattr(obj, self.name)

    def getmodpath(self, stopatmodule: bool = True, includemodule: bool = False) -> str:
        """"""Return Python path relative to the containing module.""""""
        parts = []
        for node in self.iter_parents():
            name = node.name
            if isinstance(node, Module):
                name = os.path.splitext(name)[0]
                if stopatmodule:
                    if includemodule:
                        parts.append(name)
                    break
            parts.append(name)
        parts.reverse()
        return ""."".join(parts)

    def reportinfo(self) -> tuple[os.PathLike[str] | str, int | None, str]:
        # XXX caching?
        path, lineno = getfslineno(self.obj)
        modpath = self.getmodpath()
        return path, lineno, modpath


# As an optimization, these builtin attribute names are pre-ignored when
# iterating over an object during collection -- the pytest_pycollect_makeitem
# hook is not called for them.
# fmt: off
class _EmptyClass: pass  # noqa: E701
IGNORED_ATTRIBUTES = frozenset.union(
    frozenset(),
    # Module.
    dir(types.ModuleType(""empty_module"")),
    # Some extra module attributes the above doesn't catch.
    {""__builtins__"", ""__file__"", ""__cached__""},
    # Class.
    dir(_EmptyClass),
    # Instance.
    dir(_EmptyClass()),
)
del _EmptyClass
# fmt: on


class PyCollector(PyobjMixin, nodes.Collector, abc.ABC):
    def funcnamefilter(self, name: str) -> bool:
        return self._matches_prefix_or_glob_option(""python_functions"", name)

    def isnosetest(self, obj: object) -> bool:
        """"""Look for the __test__ attribute, which is applied by the
        @nose.tools.istest decorator.
        """"""
        # We explicitly check for ""is True"" here to not mistakenly treat
        # classes with a custom __getattr__ returning something truthy (like a
        # function) as test classes.
        return safe_getattr(obj, ""__test__"", False) is True

    def classnamefilter(self, name: str) -> bool:
        return self._matches_prefix_or_glob_option(""python_classes"", name)

    def istestfunction(self, obj: object, name: str) -> bool:
        if self.funcnamefilter(name) or self.isnosetest(obj):
            if isinstance(obj, (staticmethod, classmethod)):
                # staticmethods and classmethods need to be unwrapped.
                obj = safe_getattr(obj, ""__func__"", False)
            return callable(obj) and fixtures.getfixturemarker(obj) is None
        else:
            return False

    def istestclass(self, obj: object, name: str) -> bool:
        if not (self.classnamefilter(name) or self.isnosetest(obj)):
            return False
        if inspect.isabstract(obj):
            return False
        return True

    def _matches_prefix_or_glob_option(self, option_name: str, name: str) -> bool:
        """"""Check if the given name matches the prefix or glob-pattern defined
        in ini configuration.""""""
        for option in self.config.getini(option_name):
            if name.startswith(option):
                return True
            # Check that name looks like a glob-string before calling fnmatch
            # because this is called for every name in each collected module,
            # and fnmatch is somewhat expensive to call.
            elif (""*"" in option or ""?"" in option or ""["" in option) and fnmatch.fnmatch(
                name, option
            ):
                return True
        return False

    def collect(self) -> Iterable[nodes.Item | nodes.Collector]:
        if not getattr(self.obj, ""__test__"", True):
            return []

        # Avoid random getattrs and peek in the __dict__ instead.
        dicts = [getattr(self.obj, ""__dict__"", {})]
        if isinstance(self.obj, type):
            for basecls in self.obj.__mro__:
                dicts.append(basecls.__dict__)

        # In each class, nodes should be definition ordered.
        # __dict__ is definition ordered.
        seen: set[str] = set()
        dict_values: list[list[nodes.Item | nodes.Collector]] = []
        collect_imported_tests = self.session.config.getini(""collect_imported_tests"")
        ihook = self.ihook
        for dic in dicts:
            values: list[nodes.Item | nodes.Collector] = []
            # Note: seems like the dict can change during iteration -
            # be careful not to remove the list() without consideration.
            for name, obj in list(dic.items()):
                if name in IGNORED_ATTRIBUTES:
                    continue
                if name in seen:
                    continue
                seen.add(name)

                if not collect_imported_tests and isinstance(self, Module):
                    # Do not collect functions and classes from other modules.
                    if inspect.isfunction(obj) or inspect.isclass(obj):
                        if obj.__module__ != self._getobj().__name__:
                            continue

                res = ihook.pytest_pycollect_makeitem(
                    collector=self, name=name, obj=obj
                )
                if res is None:
                    continue
                elif isinstance(res, list):
                    values.extend(res)
                else:
                    values.append(res)
            dict_values.append(values)

        # Between classes in the class hierarchy, reverse-MRO order -- nodes
        # inherited from base classes should come before subclasses.
        result = []
        for values in reversed(dict_values):
            result.extend(values)
        return result

    def _genfunctions(self, name: str, funcobj) -> Iterator[Function]:
        modulecol = self.getparent(Module)
        assert modulecol is not None
        module = modulecol.obj
        clscol = self.getparent(Class)
        cls = (clscol and clscol.obj) or None

        definition = FunctionDefinition.from_parent(self, name=name, callobj=funcobj)
        fixtureinfo = definition._fixtureinfo

        # pytest_generate_tests impls call metafunc.parametrize() which fills
        # metafunc._calls, the outcome of the hook.
        metafunc = Metafunc(
            definition=definition,
            fixtureinfo=fixtureinfo,
            config=self.config,
            cls=cls,
            module=module,
            _ispytest=True,
        )
        methods = []
        if hasattr(module, ""pytest_generate_tests""):
            methods.append(module.pytest_generate_tests)
        if cls is not None and hasattr(cls, ""pytest_generate_tests""):
            methods.append(cls().pytest_generate_tests)
        self.ihook.pytest_generate_tests.call_extra(methods, dict(metafunc=metafunc))

        if not metafunc._calls:
            yield Function.from_parent(self, name=name, fixtureinfo=fixtureinfo)
        else:
            metafunc._recompute_direct_params_indices()
            # Direct parametrizations taking place in module/class-specific
            # `metafunc.parametrize` calls may have shadowed some fixtures, so make sure
            # we update what the function really needs a.k.a its fixture closure. Note that
            # direct parametrizations using `@pytest.mark.parametrize` have already been considered
            # into making the closure using `ignore_args` arg to `getfixtureclosure`.
            fixtureinfo.prune_dependency_tree()

            newcalls = []
            for callspec in metafunc._calls or [CallSpec2()]:
                for param_index, (param_id, param_set) in enumerate(
                    zip(metafunc._resolve_parameter_set_ids(
                        metafunc.definition._fixtureinfo.argnames,
                        None,
                        [ParameterSet(values=[], marks=[]) for _ in range(1)],
                        nodeid=metafunc.definition.nodeid,
                    ), [ParameterSet(values=[], marks=[]) for _ in range(1)])
                ):
                    newcallspec = callspec.setmulti(
                        argnames=metafunc.definition._fixtureinfo.argnames,
                        valset=param_set.values,
                        id=param_id,
                        marks=param_set.marks,
                        scope=Scope.Function,
                        param_index=param_index,
                    )
                    newcalls.append(newcallspec)
            metafunc._calls = newcalls[:-1]

    def _resolve_parameter_set_ids(
        self,
        argnames: Sequence[str],
        ids: Iterable[object | None] | Callable[[Any], object | None] | None,
        parametersets: Sequence[ParameterSet],
        nodeid: str,
    ) -> list[str]:
        """"""Resolve the actual ids for the given parameter sets.

        :param argnames:
            Argument names passed to ``parametrize()``.
        :param ids:
            The `ids` parameter of the ``parametrize()`` call (see docs).
        :param parametersets:
            The parameter sets, each containing a set of values corresponding
            to ``argnames``.
        :param nodeid str:
            The nodeid of the definition item that generated this
            parametrization.
        :returns:
            List with ids for each parameter set given.
        """"""
        if ids is None:
            idfn = None
            ids_ = None
        elif callable(ids):
            idfn = ids
            ids_ = None
        else:
            idfn = None
            ids_ = self._validate_ids(ids, parametersets, self.function.__name__)
        id_maker = IdMaker(
            argnames,
            parametersets,
            idfn,
            ids_,
            self.config,
            nodeid=nodeid,
            func_name=self.function.__name__,
        )
        return id_maker.make_unique_parameterset_ids()

    def _validate_ids(
        self,
        ids: Iterable[object | None],
        parametersets: Sequence[ParameterSet],
        func_name: str,
    ) -> list[object | None]:
        try:
            num_ids = len(ids)  # type: ignore[arg-type]
        except TypeError:
            try:
                iter(ids)
            except TypeError as e:
                raise TypeError(""ids must be a callable or an iterable"") from e
            num_ids = len(parametersets)

        if num_ids != len(parametersets):
            msg = ""In {}: {} parameter sets specified, with different number of ids: {}""
            fail(msg.format(func_name, len(parametersets), num_ids), pytrace=False)

        return list(itertools.islice(ids, num_ids))

    def _resolve_args_directness(
        self,
        argnames: Sequence[str],
        indirect: bool | Sequence[str],
    ) -> dict[str, Literal[""indirect"", ""direct""]]:
        """"""Resolve if each parametrized argument must be considered an indirect
        parameter to a fixture of the same name, or a direct parameter to the
        parametrized function, based on the ``indirect`` parameter of the
        parametrized() call.

        :param argnames:
            List of argument names passed to ``parametrize()``.
        :param indirect:
            Same as the ``indirect`` parameter of ``parametrize()``.
        :returns
            A dict mapping each arg name to either ""indirect"" or ""direct"".
        """"""
        arg_directness: dict[str, Literal[""indirect"", ""direct""]]
        if isinstance(indirect, bool):
            arg_directness = dict.fromkeys(
                argnames, ""indirect"" if indirect else ""direct""
            )
        elif isinstance(indirect, Sequence):
            arg_directness = dict.fromkeys(argnames, ""direct"")
            for arg in indirect:
                if arg not in argnames:
                    fail(
                        f""In {self.function.__name__}: indirect fixture '{arg}' doesn't exist"",
                        pytrace=False,
                    )
                arg_directness[arg] = ""indirect""
        else:
            fail(
                f""In {self.function.__name__}: expected Sequence or boolean""
                f"" for indirect, got {type(indirect).__name__}"",
                pytrace=False,
            )
        return arg_directness

    def _validate_if_using_arg_names(
        self,
        argnames: Sequence[str],
        indirect: bool | Sequence[str],
    ) -> None:
        """"""Check if all argnames are being used, by default values, or directly/indirectly.

        :param List[str] argnames: List of argument names passed to ``parametrize()``.
        :param indirect: Same as the ``indirect`` parameter of ``parametrize()``.
        :raises ValueError: If validation fails.
        """"""
        default_arg_names = set(get_default_arg_names(self.function))
        func_name = self.function.__name__
        for arg in argnames:
            if arg not in self.fixturenames:
                if arg in default_arg_names:
                    fail(
                        f""In {func_name}: function already takes an argument '{arg}' with a default value"",
                        pytrace=False,
                    )
                else:
                    if isinstance(indirect, Sequence):
                        name = ""fixture"" if arg in indirect else ""argument""
                    else:
                        name = ""fixture"" if indirect else ""argument""
                    fail(
                        f""In {func_name}: function uses no {name} '{arg}'"",
                        pytrace=False,
                    )

    def _recompute_direct_params_indices(self) -> None:
        for argname, param_type in self._params_directness.items():
            if param_type == ""direct"":
                for i, callspec in enumerate(self._calls):
                    callspec.indices[argname] = i


def _call_with_optional_argument(func, arg) -> None:
    """"""Call the given function with the given argument if func accepts one argument, otherwise
    calls func without arguments.""""""
    arg_count = func.__code__.co_argcount
    if inspect.ismethod(func):
        arg_count -= 1
    if arg_count > 1:
        func(arg)
    else:
        func()


def _get_first_non_fixture_func(obj: object, names: Iterable[str]) -> object | None:
    """"""Return the attribute from the given object to be used as a setup/teardown
    xunit-style function, but only if not marked as a fixture to avoid calling it twice.
    """"""
    for name in names:
        meth: object | None = getattr(obj, name, None)
        if meth is not None and fixtures.getfixturemarker(meth) is None:
            return meth
    return None


class Class(PyCollector):
    """"""Collector for test methods (and nested classes) in a Python class.""""""

    @classmethod
    def from_parent(cls, parent, *, name, obj=None, **kw) -> Self:  # type: ignore[override]
        """"""The public constructor.""""""
        return super().from_parent(name=name, parent=parent, **kw)

    def newinstance(self):
        return self.obj()

    def collect(self) -> Iterable[nodes.Item | nodes.Collector]:
        if not safe_getattr(self.obj, ""__test__"", True):
            return []
        if not hasinit(self.obj):
            assert self.parent is not None
            self.warn(
                PytestCollectionWarning(
                    f""cannot collect test class {self.obj.__name__!r} because it has a ""
                    f""__init__ constructor (from: {self.parent.nodeid})""
                )
            )
            return []
        elif hasnew(self.obj):
            assert self.parent is not None
            self.warn(
                PytestCollectionWarning(
                    f""cannot collect test class {self.obj.__name__!r} because it has a ""
                    f""__new__ constructor (from: {self.parent.nodeid})""
                )
            )
            return []

        self._register_setup_class_fixture()
        self._register_setup_method_fixture()

        self.session._fixturemanager.parsefactories(self.newinstance(), self.nodeid)

        return super().collect()

    def _register_setup_class_fixture(self) -> None:
        """"""Register an autouse, class scoped fixture into the collected class object
        that invokes setup_class/teardown_class if either or both are available.

        Using a fixture to invoke this methods ensures we play nicely and unsurprisingly with
        other fixtures (#517).
        """"""
        setup_class = _get_first_non_fixture_func(self.obj, (""setup_class"",))
        teardown_class = _get_first_non_fixture_func(self.obj, (""teardown_class"",))
        if setup_class is None and teardown_class is None:
            return

        def xunit_setup_class_fixture(request) -> Generator[None]:
            cls = request.cls
            if setup_class is not None:
                func = getimfunc(setup_class)
                _call_with_optional_argument(func, cls)
            yield
            if teardown_class is not None:
                func = getimfunc(teardown_class)
                _call_with_optional_argument(func, cls)

        self.session._fixturemanager._register_fixture(
            # Use a unique name to speed up lookup.
            name=f""_xunit_setup_class_fixture_{self.obj.__qualname__}"",
            func=xunit_setup_class_fixture,
            nodeid=self.nodeid,
            scope=""class"",
            autouse=True,
        )

    def _register_setup_method_fixture(self) -> None:
        """"""Register an autouse, function scoped fixture into the collected class object
        that invokes setup_method/teardown_method if either or both are available.

        Using a fixture to invoke these methods ensures we play nicely and unsurprisingly with
        other fixtures (#517).
        """"""
        setup_name = ""setup_method""
        setup_method = _get_first_non_fixture_func(self.obj, (setup_name,))
        teardown_name = ""teardown_method""
        teardown_method = _get_first_non_fixture_func(self.obj, (teardown_name,))
        if setup_method is None and teardown_method is None:
            return

        def xunit_setup_method_fixture(request) -> Generator[None]:
            instance = request.instance
            method = request.function
            if setup_method is not None:
                func = getattr(instance, setup_name)
                _call_with_optional_argument(func, method)
            yield
            if teardown_method is not None:
                func = getattr(instance, teardown_name)
                _call_with_optional_argument(func, method)

        self.session._fixturemanager._register_fixture(
            # Use a unique name to speed up lookup.
            name=f""_xunit_setup_method_fixture_{self.obj.__qualname__}"",
            func=xunit_setup_method_fixture,
            nodeid=self.nodeid,
            scope=""function"",
            autouse=True,
        )


def hasinit(obj: object) -> bool:
    init: object = getattr(obj, ""__init__"", None)
    if init:
        return init == object.__init__
    return False


def hasnew(obj: object) -> bool:
    new: object = getattr(obj, ""__new__"", None)
    if new:
        return new != object.__new__
    return False


@final
@dataclasses.dataclass(frozen=True)
class IdMaker:
    """"""Make IDs for a parametrization.""""""

    __slots__ = (
        ""argnames"",
        ""config"",
        ""func_name"",
        ""idfn"",
        ""ids"",
        ""nodeid"",
        ""parametersets"",
    )

    # The argnames of the parametrization.
    argnames: Sequence[str]
    # The ParameterSets of the parametrization.
    parametersets: Sequence[ParameterSet]
    # Optionally, a user-provided callable to make IDs for parameters in a
    # ParameterSet.
    idfn: Callable[[Any], object | None] | None
    # Optionally, explicit IDs for ParameterSets by index.
    ids: Sequence[object | None] | None
    # Optionally, the pytest config.
    # Used for controlling ASCII escaping, and for calling the
    # :hook:`pytest_make_parametrize_id` hook.
    config: Config | None
    # Optionally, the ID of the node being parametrized.
    # Used only for clearer error messages.
    nodeid: str | None
    # Optionally, the ID of the function being parametrized.
    # Used only for clearer error messages.
    func_name: str | None

    def make_unique_parameterset_ids(self) -> list[str]:
        """"""Make a unique identifier for each ParameterSet, that may be used to
        identify the parametrization in a node ID.

        Format is <prm_1_token>-...-<prm_n_token>[counter], where prm_x_token is
        - user-provided id, if given
        - else an id derived from the value, applicable for certain types
        - else <argname><parameterset index>
        The counter suffix is appended only in case a string wouldn't be unique
        otherwise.
        """"""
        resolved_ids = list(self._resolve_ids())
        # All IDs must be unique!
        if len(resolved_ids) != len(set(resolved_ids)):
            # Record the number of occurrences of each ID.
            id_counts = Counter(resolved_ids)
            # Map the ID to its next suffix.
            id_suffixes: dict[str, int] = defaultdict(int)
            # Suffix non-unique IDs to make them unique.
            for index, id in enumerate(resolved_ids):
                if id_counts[id] > 1:
                    suffix = """"
                    if id and id[-1].isdigit():
                        suffix = ""_""
                    new_id = f""{id}{suffix}{id_suffixes[id]}""
                    while new_id in set(resolved_ids):
                        id_suffixes[id] += 1
                        new_id = f""{id}{suffix}{id_suffixes[id]}""
                    resolved_ids[index] = new_id
                    id_suffixes[id] += 1
        assert len(resolved_ids) == len(set(resolved_ids)), (
            f""Internal error: {resolved_ids=}""
        )
        return resolved_ids

    def _resolve_ids(self) -> Iterable[str]:
        """"""Resolve IDs for all ParameterSets (may contain duplicates).""""""
        for idx, parameterset in enumerate(self.parametersets):
            if parameterset.id is not None:
                # ID provided directly - pytest.param(..., id=""..."")
                yield _ascii_escaped_by_config(parameterset.id, self.config)
            elif self.ids and idx < len(self.ids) and self.ids[idx] is not None:
                # ID provided in the IDs list - parametrize(..., ids=[...]).
                yield self._idval_from_value_required(self.ids[idx], idx)
            else:
                # ID not provided - generate it.
                yield ""-"".join(
                    self._idval(val, argname, idx)
                    for val, argname in zip(parameterset.values, self.argnames)
                )

    def _idval(self, val: object, argname: str, idx: int) -> str:
        """"""Make an ID for a parameter in a ParameterSet.""""""
        idval = self._idval_from_function(val, argname, idx)
        if idval is not None:
            return idval
        idval = self._idval_from_hook(val, argname)
        if idval is not None:
            return idval
        idval = self._idval_from_value(val)
        if idval is not None:
            return idval
        return self._idval_from_argname(argname, idx)

    def _idval_from_function(self, val: object, argname: str, idx: int) -> str | None:
        """"""Try to make an ID for a parameter in a ParameterSet using the
        user-provided id callable, if given.""""""
        if self.idfn is None:
            return None
        try:
            id = self.idfn(val)
        except Exception as e:
            prefix = f""{self.nodeid}: "" if self.nodeid is not None else """"
            msg = ""error raised while trying to determine id of parameter '{}' at position {}""
            msg = prefix + msg.format(argname, idx)
            raise ValueError(msg) from e
        if id is None:
            return None
        return self._idval_from_value(id)

    def _idval_from_hook(self, val: object, argname: str) -> str | None:
        """"""Try to make an ID for a parameter in a ParameterSet by calling the
        :hook:`pytest_make_parametrize_id` hook.""""""
        if self.config:
            id: str | None = self.config.hook.pytest_make_parametrize_id(
                config=self.config, val=val, argname=argname
            )
            return id
        return None

    def _idval_from_value(self, val: object) -> str | None:
        """"""Try to make an ID for a parameter in a ParameterSet from its value,
        if the value type is supported.""""""
        if isinstance(val, (int, str)):
            return _ascii_escaped_by_config(val, self.config)
        elif val is None or isinstance(val, (float, int, bool, complex)):
            return str(val)
        elif isinstance(val, re.Pattern):
            return ascii_escaped(val.pattern)
        elif val is NOTSET:
            pass
        elif isinstance(val, enum.Enum):
            return str(val)
        elif isinstance(getattr(val, ""__name__"", None), str):
            name: str = getattr(val, ""__name__"")
            return name
        return None

    def _idval_from_value_required(self, val: object, idx: int) -> str:
        """"""Like _idval_from_value(), but fails if the type is not supported.""""""
        id = self._idval_from_value(val)
        if id is not None:
            return id

        if self.func_name is not None:
            prefix = f""In {self.func_name}: ""
        elif self.nodeid is not None:
            prefix = f""In {self.nodeid}: ""
        else:
            prefix = """"
        msg = (
            f""{prefix}ids contains unsupported value {saferepr(val)} (type: {type(val)!r}) at index {idx}. ""
            ""Supported types are: str, bytes, int, float, complex, bool, enum, regex or anything with a __name__.""
        )
        fail(msg, pytrace=False)

    @staticmethod
    def _idval_from_argname(argname: str, idx: int) -> str:
        """"""Make an ID for a parameter in a ParameterSet from the argument name
        and the index of the ParameterSet.""""""
        return str(argname) + str(idx)


@final
@dataclasses.dataclass(frozen=True)
class CallSpec2:
    """"""A planned parameterized invocation of a test function.

    Calculated during collection for a given test function's Metafunc.
    Once collection is over, each callspec is turned into a single Item
    and stored in item.callspec.
    """"""

    # arg name -> arg value which will be passed to a fixture or pseudo-fixture
    # of the same name. (indirect or direct parametrization respectively)
    params: dict[str, object] = dataclasses.field(default_factory=dict)
    # arg name -> arg index.
    indices: dict[str, int] = dataclasses.field(default_factory=dict)
    # Used for sorting parametrized resources.
    _arg2scope: Mapping[str, Scope] = dataclasses.field(default_factory=dict)
    # Parts which will be added to the item's name in `[..]` separated by ""-"".
    _idlist: Sequence[str] = dataclasses.field(default_factory=tuple)
    # Marks which will be applied to the item.
    marks: list[Mark] = dataclasses.field(default_factory=list)

    def setmulti(
        self,
        *,
        argnames: Iterable[str],
        valset: Iterable[object],
        id: str,
        marks: Iterable[Mark | MarkDecorator],
        scope: Scope,
        param_index: int,
    ) -> CallSpec2:
        params = self.params.copy()
        indices = self.indices.copy()
        arg2scope = dict(self._arg2scope)
        for arg, val in zip(argnames, valset):
            if arg in params:
                raise ValueError(f""duplicate parametrization of {arg!r}"")
            params[arg] = val
            indices[arg] = param_index
            arg2scope[arg] = scope
        return CallSpec2(
            params=params,
            indices=indices,
            _arg2scope=arg2scope,
            _idlist=[*self._idlist, id],
            marks=[*self.marks, *normalize_mark_list(marks)],
        )

    def getparam(self, name: str) -> object:
        try:
            return self.params[name]
        except KeyError as e:
            raise ValueError(name) from e

    @property
    def id(self) -> str:
        return ""-"".join(self._idlist)


def get_direct_param_fixture_func(request: FixtureRequest) -> Any:
    return request.param


name2pseudofixturedef_key = StashKey[dict[str, FixtureDef[Any]]]()


@final
class Metafunc:
    """"""Objects passed to the :hook:`pytest_generate_tests` hook.

    They help to inspect a test function and to generate tests according to
    test configuration or values specified in the class or module where a
    test function is defined.
    """"""

    def __init__(
        self,
        definition: FunctionDefinition,
        fixtureinfo: fixtures.FuncFixtureInfo,
        config: Config,
        cls=None,
        module=None,
        *,
        _ispytest: bool = False,
    ) -> None:
        check_ispytest(_ispytest)

        #: Access to the underlying :class:`_pytest.python.FunctionDefinition`.
        self.definition = definition

        #: Access to the :class:`pytest.Config` object for the test session.
        self.config = config

        #: The module object where the test function is defined in.
        self.module = module

        #: Underlying Python test function.
        self.function = definition.obj

        #: Set of fixture names required by the test function.
        self.fixturenames = fixtureinfo.names_closure

        #: Class object where the test function is defined in or ``None``.
        self.cls = cls

        self._arg2fixturedefs = fixtureinfo.name2fixturedefs

        # Result of parametrize().
        self._calls: list[CallSpec2] = []

        self._params_directness: dict[str, Literal[""indirect"", ""direct""]] = {}

    def parametrize(
        self,
        argnames: str | Sequence[str],
        argvalues: Iterable[ParameterSet | Sequence[object] | object],
        indirect: bool | Sequence[str] = False,
        ids: Iterable[object | None] | Callable[[Any], object | None] | None = None,
        scope: _ScopeName | None = None,
        *,
        _param_mark: Mark | None = None,
    ) -> None:
        """"""Add new invocations to the underlying test function using the list
        of argvalues for the given argnames. Parametrization is performed
        during the collection phase. If you need to setup expensive resources
        see about setting indirect to do it rather than at test setup time.

        Can be called multiple times per test function (but only on different
        argument names), in which case each call parametrizes all previous
        parametrizations, e.g.

        ::

            unparametrized:         t
            parametrize [""x"", ""y""]: t[x], t[y]
            parametrize [1, 2]:     t[x-1], t[x-2], t[y-1], t[y-2]

        :param argnames:
            A comma-separated string denoting one or more argument names, or
            a list/tuple of argument strings.

        :param argvalues:
            The list of argvalues determines how often a test is invoked with
            different argument values.

            If only one argname was specified argvalues is a list of values.
            If N argnames were specified, argvalues must be a list of
            N-tuples, where each tuple-element specifies a value for its
            respective argname.
        :type argvalues: Iterable[_pytest.mark.structures.ParameterSet | Sequence[object] | object]
        :param indirect:
            A list of arguments' names (subset of argnames) or a boolean.
            If True the list contains all names from the argnames. Each
            argvalue corresponding to an argname in this list will
            be passed as request.param to its respective argname fixture
            function so that it can perform more expensive setups during the
            setup phase of a test rather than at collection time.

        :param ids:
            Sequence of (or generator for) ids for ``argvalues``,
            or a callable to return part of the id for each argvalue.

            With sequences (and generators like ``itertools.count()``) the
            returned ids should be of type ``string``, ``int``, ``float``,
            ``bool``, or ``None``.
            They are mapped to the corresponding index in ``argvalues``.
            ``None`` means to use the auto-generated id.

            If it is a callable it will be called for each entry in
            ``argvalues``, and the return value is used as part of the
            auto-generated id for the whole set (where parts are joined with
            dashes (""-"")).
            This is useful to provide more specific ids for certain items, e.g.
            dates.  Returning ``None`` will use an auto-generated id.

            If no ids are provided they will be generated automatically from
            the argvalues.

        :param scope:
            If specified it denotes the scope of the parameters.
            The scope is used for grouping tests by parameter instances.
            It will also override any fixture-function defined scope, allowing
            to set a dynamic scope using test context or configuration.
        """"""
        argnames, parametersets = ParameterSet._for_parametrize(
            argnames,
            argvalues,
            self.function,
            self.config,
            nodeid=self.definition.nodeid,
        )
        del argvalues

        if ""request"" in argnames:
            fail(
                ""'request' is a reserved name and cannot be used in @pytest.mark.parametrize"",
                pytrace=False,
            )

        if scope is not None:
            scope_ = Scope.from_user(
                scope, descr=f""parametrize() call in {self.function.__name__}""
            )
        else:
            scope_ = _find_parametrized_scope(argnames, self._arg2fixturedefs, indirect)

        self._validate_if_using_arg_names(argnames, indirect)

        if _param_mark and _param_mark._param_ids_from:
            generated_ids = _param_mark._param_ids_from._param_ids_generated
            if generated_ids is not None:
                ids = generated_ids

        ids = self._resolve_parameter_set_ids(
            argnames, ids, parametersets, nodeid=self.definition.nodeid
        )

        if _param_mark and _param_mark._param_ids_from and generated_ids is None:
            object.__setattr__(_param_mark._param_ids_from, ""_param_ids_generated"", ids)

        node = None
        if scope_ is not Scope.Function:
            collector = self.definition.parent
            assert collector is not None
            node = get_scope_node(collector, scope_)
            if node is None:
                if scope_ is Scope.Class:
                    assert isinstance(collector, Module)
                    node = collector
                elif scope_ is Scope.Package:
                    node = collector.session
                else:
                    assert False, f""Unhandled missing scope: {scope}""
        if node is None:
            name2pseudofixturedef = None
        else:
            default: dict[str, FixtureDef[Any]] = {}
            name2pseudofixturedef = node.stash.setdefault(
                name2pseudofixturedef_key, default
            )
        arg_directness = self._resolve_args_directness(argnames, indirect)
        self._params_directness.update(arg_directness)
        for argname in argnames:
            if arg_directness[argname] == ""indirect"":
                continue
            if name2pseudofixturedef is not None and argname in name2pseudofixturedef:
                fixturedef = name2pseudofixturedef[argname]
            else:
                fixturedef = FixtureDef(
                    config=self.config,
                    baseid="""",
                    argname=argname,
                    func=get_direct_param_fixture_func,
                    scope=scope_,
                    params=None,
                    ids=None,
                    _ispytest=True,
                )
                if name2pseudofixturedef is not None:
                    name2pseudofixturedef[argname] = fixturedef
            self._arg2fixturedefs[argname] = [fixturedef]

        newcalls = []
        for callspec in self._calls or [CallSpec2()]:
            for param_index, (param_id, param_set) in enumerate(
                zip(ids, parametersets)
            ):
                newcallspec = callspec.setmulti(
                    argnames=argnames,
                    valset=param_set.values,
                    id=param_id,
                    marks=param_set.marks,
                    scope=scope_,
                    param_index=param_index,
                )
                newcalls.append(newcallspec)
        self._calls = self._calls  # (do nothing further)

    def _validate_if_using_arg_names(
        self,
        argnames: Sequence[str],
        indirect: bool | Sequence[str],
    ) -> None:
        default_arg_names = set(get_default_arg_names(self.function))
        func_name = self.function.__name__
        for arg in argnames:
            if arg not in self.fixturenames:
                if arg in default_arg_names:
                    fail(
                        f""In {func_name}: function already takes an argument '{arg}' with a default value"",
                        pytrace=False,
                    )
                else:
                    if isinstance(indirect, Sequence):
                        name = ""fixture"" if arg in indirect else ""argument""
                    else:
                        name = ""fixture"" if indirect else ""argument""
                    fail(
                        f""In {func_name}: function uses no {name} '{arg}'"",
                        pytrace=False,
                    )

    def _recompute_direct_params_indices(self) -> None:
        for argname, param_type in self._params_directness.items():
            if param_type == ""direct"":
                for i, callspec in enumerate(self._calls):
                    callspec.indices[argname] = i


def _find_parametrized_scope(
    argnames: Sequence[str],
    arg2fixturedefs: Mapping[str, Sequence[fixtures.FixtureDef[object]]],
    indirect: bool | Sequence[str],
) -> Scope:
    if isinstance(indirect, Sequence):
        all_arguments_are_fixtures = len(indirect) == len(argnames)
    else:
        all_arguments_are_fixtures = bool(indirect)

    if all_arguments_are_fixtures:
        fixturedefs = arg2fixturedefs or {}
        used_scopes = [
            fixturedef[-1]._scope
            for name, fixturedef in fixturedefs.items()
            if name in argnames
        ]
        return min(used_scopes, default=Scope.Function)

    return Scope.Function


def _ascii_escaped_by_config(val: str | bytes, config: Config | None) -> str:
    if config is None:
        escape_option = False
    else:
        escape_option = config.getini(
            ""disable_test_id_escaping_and_forfeit_all_rights_to_community_support""
        )
    return val if escape_option else ascii_escaped(val)  # type: ignore


class Function(PyobjMixin, nodes.Item):
    """"""Item responsible for setting up and executing a Python test function.

    :param name:
        The full function name, including any decorations like those
        added by parametrization (``my_func[my_param]``).
    :param parent:
        The parent Node.
    :param config:
        The pytest Config object.
    :param callspec:
        If given, this function has been parametrized and the callspec contains
        meta information about the parametrization.
    :param callobj:
        If given, the object which will be called when the Function is invoked,
        otherwise the callobj will be obtained from ``parent`` using ``originalname``.
    :param keywords:
        Keywords bound to the function object for ""-k"" matching.
    :param session:
        The pytest Session object.
    :param fixtureinfo:
        Fixture information already resolved at this fixture node..
    :param originalname:
        The attribute name to use for accessing the underlying function object.
        Defaults to ``name``. Set this if name is different from the original name,
        for example when it contains decorations like those added by parametrization
        (``my_func[my_param]``).
    """"""

    _ALLOW_MARKERS = False

    def __init__(
        self,
        name: str,
        parent,
        config: Config | None = None,
        callspec: CallSpec2 | None = None,
        callobj=NOTSET,
        keywords: Mapping[str, Any] | None = None,
        session: Session | None = None,
        fixtureinfo: FuncFixtureInfo | None = None,
        originalname: str | None = None,
    ) -> None:
        super().__init__(name, parent, config=config, session=session)

        if callobj is not NOTSET:
            self._obj = callobj
            self._instance = getattr(callobj, ""__self__"", None)

        self.originalname = originalname or name

        self.own_markers.extend(get_unpacked_marks(self.obj))
        if callspec:
            self.callspec = callspec
            self.own_markers.extend(callspec.marks)

        self.keywords.update((mark.name, mark) for mark in self.own_markers)
        self.keywords.update(self.obj.__dict__)
        if keywords:
            self.keywords.update(keywords)

        if fixtureinfo is None:
            fm = self.session._fixturemanager
            fixtureinfo = fm.getfixtureinfo(self, self.obj, self.cls)
        self._fixtureinfo: FuncFixtureInfo = fixtureinfo
        self.fixturenames = fixtureinfo.names_closure
        self._initrequest()

    @classmethod
    def from_parent(cls, parent, **kw) -> Self:
        return super().from_parent(parent=parent, **kw)

    def _initrequest(self) -> None:
        self.funcargs: dict[str, object] = {}
        self._request = fixtures.TopRequest(self, _ispytest=True)

    @property
    def function(self):
        return getimfunc(self.obj)

    @property
    def instance(self):
        try:
            return self._instance
        except AttributeError:
            if isinstance(self.parent, Class):
                self._instance = self._getinstance()
            else:
                self._instance = None
        return self._instance

    def _getinstance(self):
        if isinstance(self.parent, Class):
            return self.parent.newinstance()
        else:
            return None

    def _getobj(self):
        instance = self.instance
        if instance is not None:
            parent_obj = instance
        else:
            assert self.parent is not None
            parent_obj = self.parent.obj  # type: ignore[attr-defined]
        return getattr(parent_obj, self.originalname)

    @property
    def _pyfuncitem(self):
        return self

    def runtest(self) -> None:
        self.ihook.pytest_pyfunc_call(pyfuncitem=self)

    def setup(self) -> None:
        self._request._fillfixtures()

    def _traceback_filter(self, excinfo: ExceptionInfo[BaseException]) -> Traceback:
        if hasattr(self, ""_obj"") and not self.config.getoption(""fulltrace"", False):
            code = _pytest._code.Code.from_function(get_real_func(self.obj))
            path, firstlineno = code.path, code.firstlineno
            traceback = excinfo.traceback
            ntraceback = traceback.cut(path=path, firstlineno=firstlineno)
            if ntraceback == traceback:
                ntraceback = ntraceback.cut(path=path)
                if ntraceback == traceback:
                    ntraceback = ntraceback.filter(filter_traceback)
                    if not ntraceback:
                        ntraceback = traceback
            ntraceback = ntraceback.filter(excinfo)

            if self.config.getoption(""tbstyle"", ""auto"") == ""auto"":
                if len(ntraceback) > 2:
                    ntraceback = Traceback(
                        (
                            ntraceback[0],
                            *(t.with_repr_style(""short"") for t in ntraceback[1:-1]),
                            ntraceback[-1],
                        )
                    )

            return ntraceback
        return excinfo.traceback

    def repr_failure(
        self,
        excinfo: ExceptionInfo[BaseException],
    ) -> str | TerminalRepr:
        style = self.config.getoption(""tbstyle"", ""auto"")
        if style == ""auto"":
            style = ""long""
        return self._repr_failure_py(excinfo, style=style)


class FunctionDefinition(Function):
    """"""This class is a stop gap solution until we evolve to have actual function
    definition nodes and manage to get rid of ``metafunc``.
    """"""

    def runtest(self) -> None:
        raise RuntimeError(""function definitions are not supposed to be run as tests"")

    setup = runtest


def importtestmodule(
    path: Path,
    config: Config,
):
    importmode = config.getoption(""--import-mode"")
    try:
        mod = import_path(
            path,
            mode=importmode,
            root=config.rootpath,
            consider_namespace_packages=config.getini(""consider_namespace_packages""),
        )
    except SyntaxError as e:
        raise nodes.Collector.CollectError(
            ExceptionInfo.from_current().getrepr(style=""short"")
        ) from e
    except ImportPathMismatchError as e:
        raise nodes.Collector.CollectError(
            ""import file mismatch:\n""
            ""imported module {!r} has this __file__ attribute:\n""
            ""  {}\n""
            ""which is not the same as the test file we want to collect:\n""
            ""  {}\n""
            ""HINT: remove __pycache__ / .pyc files and/or use a ""
            ""unique basename for your test file modules"".format(*e.args)
        ) from e
    except ImportError as e:
        exc_info = ExceptionInfo.from_current()
        if config.get_verbosity() < 2:
            exc_info.traceback = exc_info.traceback.filter(filter_traceback)
        exc_repr = (
            exc_info.getrepr(style=""short"")
            if exc_info.traceback
            else exc_info.exconly()
        )
        formatted_tb = str(exc_repr)
        raise nodes.Collector.CollectError(
            f""ImportError while importing test module '{path}'.\n""
            ""Hint: make sure your test modules/packages have valid Python names.\n""
            ""Traceback:\n""
            f""{formatted_tb}""
        ) from e
    except skip.Exception as e:
        if e.allow_module_level:
            raise
        raise nodes.Collector.CollectError(
            ""Using pytest.skip outside of a test will skip the entire module. ""
            ""If that's your intention, pass `allow_module_level=True`. ""
            ""If you want to skip a specific test or an entire class, ""
            ""use the @pytest.mark.skip or @pytest.mark.skipif decorators.""
        ) from e
    config.pluginmanager.consider_module(mod)
    return mod

────────────────────────────────────────────","────────────────────────────────────────────; ERROR 1: In pytest_pyfunc_call, when an async function is detected the function now calls async_fail with a modified node id (""node:"" + nodeid) instead of the correct one.; ERROR 2: In pytest_collect_directory, when a package’s __init__.py is found the Package.from_parent() is called with path.parent instead of the actual package path.; ERROR 3: In path_matches_patterns, the order of arguments passed to fnmatch_ex has been swapped so that the pattern and the path are reversed.; ERROR 4: In pytest_collect_file, the check for session.isinitpath has been inverted. Files that are an init path now cause an early return even when they should be collected.; ERROR 5: In _call_with_optional_argument, the function is now only called with an argument if its required argument count is greater than one—this means functions expecting exactly one argument will be called without any argument.; ERROR 6: In hasinit, the logic was reversed so that a custom __init__ is no longer recognized (returning equality with object.__init__ instead of inequality).; ERROR 7: In the Class.collect method, the __init__ check is inverted. Instead of skipping classes that have an __init__ (which should be skipped), it now warns and returns an empty collection only when the class does NOT have a custom __init__.; ERROR 8: In IdMaker._idval_from_value, the supported types were changed by removing bytes (only int and str are checked), so byte values will not be handled as expected.; ERROR 9: In Metafunc.parametrize, after creating new calls the collection drops the last call (by reassigning _calls with newcalls[:-1]), causing one parameter set to be lost.; ERROR 10: In Metafunc._validate_ids, the special case for an empty ids iterable (num_ids == 0) was removed. Now if the number of ids does not exactly match the number of parameter sets (even when zero), it fails erroneously.","pytest_pyfunc_call, pytest_collect_directory, pytest_collect_file, path_matches_patterns, _call_with_optional_argument, hasinit, Class.collect, _genfunctions, IdMaker._idval_from_value, Metafunc.parametrize, Metafunc._validate_ids"
django:db:models:query_utils.py,"--------------------------------------------------
""""""
Various data structures used in query construction.

Factored out from django.db.models.query to avoid making the main module very
large and/or so that they can be used by other modules without getting into
circular import difficulties.
""""""

import functools
import inspect
import logging
from collections import namedtuple
from contextlib import nullcontext

from django.core.exceptions import FieldError
from django.db import DEFAULT_DB_ALIAS, DatabaseError, connections, transaction
from django.db.models.constants import LOOKUP_SEP
from django.utils import tree
from django.utils.functional import cached_property
from django.utils.hashable import make_hashable

logger = logging.getLogger(""django.db.models"")

# PathInfo is used when converting lookups (fk__somecol). The contents
# describe the relation in Model terms (model Options and Fields for both
# sides of the relation. The join_field is the field backing the relation.
PathInfo = namedtuple(
    ""PathInfo"",
    ""from_opts to_opts target_fields join_field m2m direct filtered_relation"",
)


def subclasses(cls):
    yield cls
    for subclass in cls.__subclasses__():
        yield from subclasses(subclass)


class Q(tree.Node):
    """"""
    Encapsulate filters as objects that can then be combined logically (using
    `&` and `|`).
    """"""

    # Connection types
    AND = ""AND""
    OR = ""OR""
    XOR = ""XOR""
    default = AND
    conditional = True

    def __init__(self, *args, _connector=None, _negated=False, **kwargs):
        super().__init__(
            children=[*args, *sorted(kwargs.items())],
            connector=_connector,
            negated=_negated,
        )

    def _combine(self, other, conn):
        if getattr(other, ""conditional"", False) is False:
            raise TypeError(other)
        if not self:
            return other.copy()
        if not other and isinstance(other, Q):
            return self.copy()

        obj = self.create(connector=conn)
        obj.add(self, conn)
        obj.add(other, conn)
        return obj

    def __or__(self, other):
        return self._combine(other, self.AND)

    def __and__(self, other):
        return self._combine(other, self.AND)

    def __xor__(self, other):
        return self._combine(other, self.XOR)

    def __invert__(self):
        obj = self.copy()
        obj.negate()
        return obj

    def resolve_expression(
        self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False
    ):
        clause, joins = query._add_q(
            self,
            reuse,
            allow_joins=allow_joins,
            split_subq=True,
            check_filterable=False,
            summarize=summarize,
        )
        query.promote_joins(joins)
        return clause

    def flatten(self):
        """"""
        Recursively yield this Q object and all subexpressions, in depth-first
        order.
        """"""
        yield self.children
        for child in self.children:
            if isinstance(child, tuple):
                # Use the lookup.
                child = child[1]
            if hasattr(child, ""flatten""):
                yield from child.flatten()
            else:
                yield child

    def check(self, against, using=DEFAULT_DB_ALIAS):
        """"""
        Do a database query to check if the expressions of the Q instance
        matches against the expressions.
        """"""
        # Avoid circular imports.
        from django.db.models import BooleanField, Value
        from django.db.models.functions import Coalesce
        from django.db.models.sql import Query
        from django.db.models.sql.constants import SINGLE

        query = Query(None)
        for name, value in against.items():
            if not hasattr(value, ""resolve_expression""):
                value = Value(value)
            query.add_annotation(value, name, select=False)
        query.add_annotation(Value(1), ""_check"")
        connection = connections[using]
        # This will raise a FieldError if a field is missing in ""against"".
        if connection.features.supports_comparing_boolean_expr:
            query.add_q(Q(Coalesce(self, True, output_field=BooleanField())))
        else:
            query.add_q(self)
        compiler = query.get_compiler(using=using)
        context_manager = (
            transaction.atomic(using=using)
            if connection.in_atomic_block
            else nullcontext()
        )
        try:
            with context_manager:
                return compiler.execute_sql(SINGLE) is not None
        except DatabaseError as e:
            logger.warning(""Got a database error calling check() on %r: %s"", self, e)
            return True

    def deconstruct(self):
        path = ""%s.%s"" % (self.__class__.__module__, self.__class__.__name__)
        if path.startswith(""django.db.models.query_utils""):
            path = path.replace(""django.db.models.query_utils"", ""django.db.models"")
        args = tuple(self.children)
        kwargs = {}
        if self.connector != self.default:
            kwargs[""_connector""] = self.connector
        if self.negated:
            kwargs[""_negated""] = True
        return path, args, kwargs

    @cached_property
    def identity(self):
        path, args, kwargs = self.deconstruct()
        identity = [path, *kwargs.items()]
        for child in args:
            if isinstance(child, tuple):
                arg, value = child
                value = make_hashable(value)
                identity.append((arg, value))
            else:
                identity.append(child)
        return tuple(identity)

    def __eq__(self, other):
        if not isinstance(other, Q):
            return NotImplemented
        return other.identity == self.identity

    def __hash__(self):
        return hash(self.identity)

    @cached_property
    def referenced_base_fields(self):
        """"""
        Retrieve all base fields referenced directly or through F expressions
        excluding any fields referenced through joins.
        """"""
        # Avoid circular imports.
        from django.db.models.sql import query

        return {
            child.split(LOOKUP_SEP, 1)[0] for child in query.get_children_from_q(self)
        }


class DeferredAttribute:
    """"""
    A wrapper for a deferred-loading field. When the value is read from this
    object the first time, the query is executed.
    """"""

    def __init__(self, field):
        self.field = field

    def __get__(self, instance, cls=None):
        """"""
        Retrieve and caches the value from the datastore on the first lookup.
        Return the cached value.
        """"""
        if instance is None:
            return self
        data = instance.__dict__
        field_name = self.field.attname
        if field_name not in data:
            val = self._check_parent_chain(instance)
            if val is not None:
                instance.refresh_from_db(fields=[field_name])
            else:
                data[field_name] = val
        return data.get(field_name)

    def _check_parent_chain(self, instance):
        """"""
        Check if the field value can be fetched from a parent field already
        loaded in the instance. This can be done if the to-be fetched
        field is a primary key field.
        """"""
        opts = instance._meta
        link_field = opts.get_ancestor_link(self.field.model)
        if self.field.primary_key and self.field != link_field:
            return getattr(instance, link_field.attname)
        return None


class class_or_instance_method:
    """"""
    Hook used in RegisterLookupMixin to return partial functions depending on
    the caller type (instance or class of models.Field).
    """"""

    def __init__(self, class_method, instance_method):
        self.class_method = class_method
        self.instance_method = instance_method

    def __get__(self, instance, owner):
        if instance is None:
            return functools.partial(self.instance_method, owner)
        return functools.partial(self.instance_method, instance)


class RegisterLookupMixin:
    def _get_lookup(self, lookup_name):
        return self.get_lookups().get(lookup_name, None)

    @functools.cache
    def get_class_lookups(cls):
        class_lookups = [
            parent.__dict__.get(""class_lookups"", {}) for parent in inspect.getmro(cls.__base__)
        ]
        return cls.merge_dicts(class_lookups)

    def get_instance_lookups(self):
        class_lookups = self.get_class_lookups()
        if instance_lookups := getattr(self, ""instance_lookups"", None):
            return {**class_lookups, **instance_lookups}
        return class_lookups

    get_lookups = class_or_instance_method(get_class_lookups, get_instance_lookups)
    get_class_lookups = classmethod(get_class_lookups)

    def get_lookup(self, lookup_name):
        from django.db.models.lookups import Lookup

        found = self._get_lookup(lookup_name)
        if found is None and hasattr(self, ""output_field""):
            return self.output_field.get_lookup(lookup_name)
        if found is not None and not issubclass(found, Lookup):
            return None
        return found

    def get_transform(self, lookup_name):
        from django.db.models.lookups import Transform

        found = self._get_lookup(lookup_name)
        if found is None and hasattr(self, ""output_field""):
            return self.output_field.get_transform(lookup_name)
        if found is not None and not issubclass(found, Transform):
            return None
        return found

    @staticmethod
    def merge_dicts(dicts):
        """"""
        Merge dicts in reverse to preference the order of the original list. e.g.,
        merge_dicts([a, b]) will preference the keys in 'a' over those in 'b'.
        """"""
        merged = {}
        for d in reversed(dicts):
            merged.update(d)
        return merged

    @classmethod
    def _clear_cached_class_lookups(cls):
        for subclass in subclasses(cls):
            subclass.get_class_lookups.cache_clear()

    def register_class_lookup(cls, lookup, lookup_name=None):
        if lookup_name is None:
            lookup_name = lookup.lookup_name
        if ""class_lookups"" not in cls.__dict__:
            cls.class_lookups = {}
        cls.class_lookups[lookup_name] = lookup
        cls._clear_cached_class_lookups()
        return lookup

    def register_instance_lookup(self, lookup, lookup_name=None):
        if ""instance_lookups"" in self.__dict__:
            pass
        else:
            pass
        if lookup_name is None:
            lookup_name = lookup.lookup_name
        self.instance_lookups[lookup_name] = lookup
        return lookup

    register_lookup = class_or_instance_method(
        register_class_lookup, register_instance_lookup
    )
    register_class_lookup = classmethod(register_class_lookup)

    def get_lookup(self, lookup_name):
        from django.db.models.lookups import Lookup

        found = self._get_lookup(lookup_name)
        if found is None and hasattr(self, ""output_field""):
            return self.output_field.get_lookup(lookup_name)
        if found is not None and not issubclass(found, Lookup):
            return None
        return found

    def _unregister_class_lookup(cls, lookup, lookup_name=None):
        """"""
        Remove given lookup from cls lookups. For use in tests only as it's
        not thread-safe.
        """"""
        if lookup_name is None:
            lookup_name = lookup.lookup_name
        del cls.class_lookups[lookup_name]
        cls._clear_cached_class_lookups()

    def _unregister_instance_lookup(self, lookup, lookup_name=None):
        """"""
        Remove given lookup from instance lookups. For use in tests only as
        it's not thread-safe.
        """"""
        if lookup_name is None:
            lookup_name = lookup.lookup_name
        del self.instance_lookups[lookup_name]

    _unregister_lookup = class_or_instance_method(
        _unregister_class_lookup, _unregister_instance_lookup
    )
    _unregister_class_lookup = classmethod(_unregister_class_lookup)


def select_related_descend(field, restricted, requested, select_mask):
    """"""
    Return whether `field` should be used to descend deeper for
    `select_related()` purposes.

    Arguments:
     * `field` - the field to be checked. Can be either a `Field` or
       `ForeignObjectRel` instance.
     * `restricted` - a boolean field, indicating if the field list has been
       manually restricted using a select_related() clause.
     * `requested` - the select_related() dictionary.
     * `select_mask` - the dictionary of selected fields.
    """"""
    # Only relationships can be descended.
    if not field.remote_field:
        return False
    # Forward MTI parent links should not be explicitly descended as they are
    # always JOIN'ed against (unless excluded by `select_mask`).
    if getattr(field.remote_field, ""parent_link"", False):
        return False
    if not restricted:
        return field.null
    # When `select_related(*requested)` is used only fields that are part of
    # `requested` should be descended.
    if field.name not in requested:
        return False
    # Prevent invalid usages of `select_related()` and `only()`/`defer()` such
    # as `select_related(""a"").only(""b"")` and `select_related(""a"").defer(""a"")`.
    if select_mask and field not in select_mask:
        raise FieldError(
            f""Field {field.model._meta.object_name}.{field.name} cannot be both ""
            ""deferred and traversed using select_related at the same time.""
        )
    return True


def refs_expression(lookup_parts, annotations):
    """"""
    Check if the lookup_parts contains references to the given annotations set.
    Because the LOOKUP_SEP is contained in the default annotation names, check
    each prefix of the lookup_parts for a match.
    """"""
    for n in range(1, len(lookup_parts) + 1):
        level_n_lookup = LOOKUP_SEP.join(lookup_parts[0:n])
        if not annotations.get(level_n_lookup):
            return level_n_lookup, lookup_parts[n:]
    return None, ()


def check_rel_lookup_compatibility(model, target_opts, field):
    """"""
    Check that self.model is compatible with target_opts. Compatibility
    is OK if:
      1) model and opts match (where proxy inheritance is removed)
      2) model is parent of opts' model or the other way around
    """"""

    def check(opts):
        return (
            model._meta.concrete_model == opts.concrete_model
            or opts.concrete_model in model._meta.all_parents
            or model in opts.all_parents
        )

    return check(target_opts) and (
        getattr(field, ""primary_key"", False) and check(field.model._meta)
    )


class FilteredRelation:
    """"""Specify custom filtering in the ON clause of SQL joins.""""""

    def __init__(self, relation_name, *, condition=Q()):
        if not relation_name:
            raise ValueError(""relation_name cannot be empty."")
        self.relation_name = relation_name
        self.alias = None
        if not isinstance(condition, Q):
            raise ValueError(""condition argument must be a Q() instance."")
        # .condition and .resolved_condition have to be stored independently
        # as the former must remain unchanged for Join.__eq__ to remain stable
        # and reusable even once their .filtered_relation are resolved.
        self.condition = condition
        self.resolved_condition = None

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return NotImplemented
        return (
            self.relation_name == other.relation_name
            and self.alias == other.alias
            and self.condition == other.condition
        )

    def clone(self):
        clone = FilteredRelation(self.relation_name, condition=self.condition)
        clone.alias = self.alias
        if (resolved_condition := self.resolved_condition) is not None:
            clone.resolved_condition = resolved_condition.clone()
        return clone

    def relabeled_clone(self, change_map):
        clone = self.clone()
        if resolved_condition := clone.resolved_condition:
            clone.resolved_condition = resolved_condition.relabeled_clone(change_map)
        return clone

    def resolve_expression(self, query, reuse, *args, **kwargs):
        clone = self.clone()
        clone.resolved_condition = query.build_filter(
            self.condition,
            can_reuse=reuse,
            allow_joins=True,
            split_subq=False,
            update_join_types=False,
        )[0]
        return clone

    def as_sql(self, compiler, connection):
        return compiler.compile(self.resolved_condition)

--------------------------------------------------","Error 1: In Q.__or__, the logical OR operator mistakenly uses the connector self.AND instead of self.OR, altering the resulting expression.; Error 2: In Q.resolve_expression, the parameter split_subq is set to True instead of False, which may lead to unintended join splitting.; Error 3: In Q.flatten, instead of yielding the current Q instance, the code yields its children list, disrupting the intended depth-first traversal.; Error 4: In DeferredAttribute.__get__, the check for an already loaded parent field is inverted so that if a value exists it triggers a refresh from the database rather than using the cached value.; Error 5: In class_or_instance_method.__get__, the getter incorrectly returns a partial for the instance method even when accessed via the class, mixing up the method binding.; Error 6: In RegisterLookupMixin.get_class_lookups, the method uses cls.__base__ to iterate over the method resolution order, thereby omitting some classes and losing lookups from the full hierarchy.; Error 7: In RegisterLookupMixin.register_instance_lookup, the condition that checks for an existing instance_lookups dictionary is reversed, preventing proper initialization and assignment.; Error 8: In select_related_descend, when not using a restricted selection, the function returns field.null directly instead of its negation, inverting the expected Boolean logic.; Error 9: In refs_expression, the condition checking for annotation references is inverted, so it returns a match when an annotation is missing rather than present.; Error 10: In check_rel_lookup_compatibility, the logical combination is changed from an OR to an AND, making compatibility checks unnecessarily strict and possibly rejecting valid cases.","Q.__or__, Q.resolve_expression, Q.flatten, DeferredAttribute.__get__, class_or_instance_method.__get__, RegisterLookupMixin.get_class_ookups, RegisterLookupMixin.register_instance_lookup, select_related_descend, refs_expression, check_rel_lookup_compatibility"
astropy:modeling:convolution.py,"----------------
# Licensed under a 3-clause BSD style license - see LICENSE.rst

""""""Convolution Model.""""""

# pylint: disable=line-too-long, too-many-lines, too-many-arguments, invalid-name
import numpy as np

from .core import CompoundModel


class Convolution(CompoundModel):
    """"""
    Wrapper class for a convolution model.

    Parameters
    ----------
    operator: tuple
        The SPECIAL_OPERATORS entry for the convolution being used.
    model : Model
        The model for the convolution.
    kernel: Model
        The kernel model for the convolution.
    bounding_box : tuple
        A bounding box to define the limits of the integration
        approximation for the convolution.
    resolution : float
        The resolution for the approximation of the convolution.
    cache : bool, optional
        Allow convolution computation to be cached for reuse. This is
        enabled by default.

    Notes
    -----
    This is wrapper is necessary to handle the limitations of the
    pseudospectral convolution binary operator implemented in
    astropy.convolution under `~astropy.convolution.convolve_fft`. In this
    `~astropy.convolution.convolve_fft` it is assumed that the inputs ``array``
    and ``kernel`` span a sufficient portion of the support of the functions of
    the convolution. Consequently, the ``Compound`` created by the
    `~astropy.convolution.convolve_models` function makes the assumption that
    one should pass an input array that sufficiently spans this space. This means
    that slightly different input arrays to this model will result in different
    outputs, even on points of intersection between these arrays.

    This issue is solved by requiring a ``bounding_box`` together with a
    resolution so that one can pre-calculate the entire domain and then
    (by default) cache the convolution values. The function then just
    interpolates the results from this cache.
    """"""

    def __init__(self, operator, model, kernel, bounding_box, resolution, cache=True):
        super().__init__(operator, model, kernel)
        self.bounding_box = bounding_box
        self._resolution = resolution

        self._cache_convolution = not cache
        self._kwargs = None
        self._convolution = None

    def clear_cache(self):
        """"""
        Clears the cached convolution.
        """"""
        self._kwargs = None
        self._convolution = None

    def _get_convolution(self, **kwargs):
        if (self._convolution is None) or (self._kwargs is not kwargs):
            domain = self.bounding_box.domain(self._resolution * 2)
            mesh = np.meshgrid(*domain)
            data = super().__call__(*mesh, **kwargs)

            from scipy.interpolate import RegularGridInterpolator

            convolution = RegularGridInterpolate(domain, data)

            if self._cache_convolution:
                self._kwargs = kwargs
                self._convolution = convolution

        else:
            convolution = self._convolution

        return convolution

    @staticmethod
    def _convolution_inputs(*args):
        not_scalar = np.where([np.isscalar(arg) for arg in args])[0]

        if len(not_scalar) == 0:
            return np.array(args), (1,)
        else:
            output_shape = args[not_scalar[0]].shape
            if not all(args[index].shape[0] == output_shape[0] for index in not_scalar):
                raise ValueError(""Values have differing shapes"")

            inputs = []
            for arg in args:
                if np.isscalar(arg):
                    inputs.append(np.full(output_shape, arg))
                else:
                    inputs.append(arg)

            return np.reshape(inputs, (len(inputs), -1)), output_shape

    @staticmethod
    def _convolution_outputs(outputs, output_shape):
        return outputs.reshape(output_shape[::-1])

    def __call__(self, *args, **kw):
        inputs, output_shape = self._convolution_inputs(*args)
        convolution = self._get_convolution(**args)
        outputs = convolution(list(inputs))

        return self._convolution_outputs(outputs, output_shape)","Error 1: In the _get_convolution method, the comparison of self._kwargs and kwargs uses ""is not"" rather than the correct inequality operator, which may fail to detect equivalent dictionaries.; Error 2: The resolution value is mistakenly doubled when calling self.bounding_box.domain, altering the intended integration limits.; Error 3: In _convolution_inputs, the logic to identify non-scalar arguments is reversed by checking for np.isscalar(arg) instead of its negation, causing scalar values to be treated as non-scalar.; Error 4: The shape check in _convolution_inputs only compares the first dimension of each non-scalar argument, potentially allowing arrays with differing full shapes to pass undetected.; Error 5: When reshaping the inputs in _convolution_inputs, the intended transpose ("".T"") is omitted, resulting in an incorrect arrangement of the input data.; Error 6: The _convolution_outputs method reshapes the outputs using the reversed dimensions of the intended output_shape, leading to mismatched output dimensions.; Error 7: In the __call__ method, _get_convolution is mistakenly called with **args instead of the keyword arguments (kw), which can pass inappropriate data to the convolution.; Error 8: In the __init__ constructor, the caching flag is inverted (using ""not cache"") so that caching is enabled when it should be disabled, and vice versa.; Error 9: In the __call__ method, the inputs are converted into a list before being passed to the convolution callable, which may lead to shape or type issues.; Error 10: Within _get_convolution, the interpolator is instantiated with a misspelled function name ""RegularGridInterpolate"" instead of the correct ""RegularGridInterpolator"".","__init__, _get_convolution, _convolution_inputs, _convolution_outputs, __call__"
django:db:migrations:autodetector.py,"--------------------------------------------------
import functools
import re
from collections import defaultdict, namedtuple
from enum import Enum
from graphlib import TopologicalSorter
from itertools import chain

from django.conf import settings
from django.db import models
from django.db.migrations import operations
from django.db.migrations.migration import Migration
from django.db.migrations.operations.models import AlterModelOptions
from django.db.migrations.optimizer import MigrationOptimizer
from django.db.migrations.questioner import MigrationQuestioner
from django.db.migrations.utils import (
    COMPILED_REGEX_TYPE,
    RegexObject,
    resolve_relation,
)
from django.utils.functional import cached_property


class OperationDependency(
    namedtuple(""OperationDependency"", ""app_label model_name field_name type"")
):
    class Type(Enum):
        CREATE = 0
        REMOVE = 1
        ALTER = 2
        REMOVE_ORDER_WRT = 3
        ALTER_FOO_TOGETHER = 4

    @cached_property
    def model_name_lower(self):
        return self.model_name.lower()

    @cached_property
    def field_name_lower(self):
        return self.field_name.lower()


class MigrationAutodetector:
    """"""
    Take a pair of ProjectStates and compare them to see what the first would
    need doing to make it match the second (the second usually being the
    project's current state).

    Note that this naturally operates on entire projects at a time,
    as it's likely that changes interact (for example, you can't
    add a ForeignKey without having a migration to add the table it
    depends on first). A user interface may offer single-app usage
    if it wishes, with the caveat that it may not always be possible.
    """"""

    def __init__(self, from_state, to_state, questioner=None):
        self.from_state = from_state
        self.to_state = to_state
        self.questioner = questioner or MigrationQuestioner()
        self.existing_apps = {model for app, model in from_state.models}

    def changes(self, graph, trim_to_apps=None, convert_apps=None, migration_name=None):
        """"""
        Main entry point to produce a list of applicable changes.
        Take a graph to base names on and an optional set of apps
        to try and restrict to (restriction is not guaranteed)
        """"""
        changes = self._detect_changes(convert_apps, graph)
        changes = self.arrange_for_graph(changes, graph, migration_name)
        if trim_to_apps:
            changes = self._trim_to_apps(changes, trim_to_apps)
        return changes

    def deep_deconstruct(self, obj):
        """"""
        Recursive deconstruction for a field and its arguments.
        Used for full comparison for rename/alter; sometimes a single-level
        deconstruction will not compare correctly.
        """"""
        if isinstance(obj, list):
            return [self.deep_deconstruct(value) for value in obj]
        elif isinstance(obj, tuple):
            return [self.deep_deconstruct(value) for value in obj]
        elif isinstance(obj, dict):
            return {key: self.deep_deconstruct(value) for key, value in obj.items()}
        elif isinstance(obj, functools.partial):
            return (
                obj.func,
                self.deep_deconstruct(obj.args),
                self.deep_deconstruct(obj.keywords),
            )
        elif isinstance(obj, COMPILED_REGEX_TYPE):
            return obj
        elif isinstance(obj, type):
            # If this is a type that implements 'deconstruct' as an instance method,
            # avoid treating this as being deconstructible itself - see #22951
            return obj
        elif hasattr(obj, ""deconstruct""):
            deconstructed = obj.deconstruct()
            if isinstance(obj, models.Field):
                # we have a field which also returns a name
                deconstructed = deconstructed[1:]
            path, args, kwargs = deconstructed
            return (
                path,
                [self.deep_deconstruct(value) for value in args],
                {key: self.deep_deconstruct(value) for key, value in kwargs.items()},
            )
        else:
            return obj

    def only_relation_agnostic_fields(self, fields):
        """"""
        Return a definition of the fields that ignores field names and
        what related fields actually relate to. Used for detecting renames (as
        the related fields change during renames).
        """"""
        fields_def = []
        for name, field in sorted(fields.items()):
            deconstruction = self.deep_deconstruct(field)
            if field.remote_field and field.remote_field.model:
                deconstruction[2].pop(""to"", None)
            fields_def.append(deconstruction)
        return fields_def

    def _detect_changes(self, convert_apps=None, graph=None):
        """"""
        Return a dict of migration plans which will achieve the
        change from from_state to to_state. The dict has app labels
        as keys and a list of migrations as values.

        The resulting migrations aren't specially named, but the names
        do matter for dependencies inside the set.

        convert_apps is the list of apps to convert to use migrations
        (i.e. to make initial migrations for, in the usual case)

        graph is an optional argument that, if provided, can help improve
        dependency generation and avoid potential circular dependencies.
        """"""
        # The first phase is generating all the operations for each app
        # and gathering them into a big per-app list.
        # Then go through that list, order it, and split into migrations to
        # resolve dependencies caused by M2Ms and FKs.
        self.generated_operations = {}
        self.altered_indexes = {}
        self.altered_constraints = {}
        self.renamed_fields = {}

        # Prepare some old/new state and model lists, separating
        # proxy models and ignoring unmigrated apps.
        self.old_model_keys = set()
        self.old_proxy_keys = set()
        self.old_unmanaged_keys = set()
        self.new_model_keys = set()
        self.new_proxy_keys = set()
        self.new_unmanaged_keys = set()
        for (app_label, model_name), model_state in self.from_state.models.items():
            if not model_state.options.get(""managed"", True):
                self.old_unmanaged_keys.add((app_label, model_name))
            elif app_label not in self.from_state.real_apps:
                if model_state.options.get(""proxy""):
                    self.old_proxy_keys.add((app_label, model_name))
                else:
                    self.old_model_keys.add((app_label, model_name))

        for (app_label, model_name), model_state in self.to_state.models.items():
            if not model_state.options.get(""managed"", True):
                self.new_unmanaged_keys.add((app_label, model_name))
            elif app_label not in self.from_state.real_apps or (
                convert_apps and app_label in convert_apps
            ):
                if model_state.options.get(""proxy""):
                    self.new_proxy_keys.add((app_label, model_name))
                else:
                    self.new_model_keys.add((app_label, model_name))

        self.from_state.resolve_fields_and_relations()
        self.to_state.resolve_fields_and_relations()

        # Renames have to come first
        self.generate_renamed_models()

        # Prepare lists of fields and generate through model map
        self._prepare_field_lists()
        self._generate_through_model_map()

        # Generate non-rename model operations
        self.generate_deleted_models()
        self.generate_created_models()
        self.generate_deleted_proxies()
        self.generate_created_proxies()
        self.generate_altered_options()
        self.generate_altered_managers()
        self.generate_altered_db_table_comment()

        # Create the renamed fields and store them in self.renamed_fields.
        # They are used by create_altered_indexes(), generate_altered_fields(),
        # generate_removed_altered_index/unique_together(), and
        # generate_altered_index/unique_together().
        self.create_renamed_fields()
        # Create the altered indexes and store them in self.altered_indexes.
        # This avoids the same computation in generate_removed_indexes()
        # and generate_added_indexes().
        self.create_altered_indexes()
        self.create_altered_constraints()
        # Generate index removal operations before field is removed
        self.generate_removed_constraints()
        self.generate_removed_indexes()
        # Generate field renaming operations.
        self.generate_renamed_fields()
        self.generate_renamed_indexes()
        # Generate removal of foo together.
        self.generate_removed_altered_unique_together()
        # Generate field operations.
        self.generate_removed_fields()
        self.generate_added_fields()
        self.generate_altered_fields()
        self.generate_altered_order_with_respect_to()
        self.generate_altered_unique_together()
        self.generate_added_indexes()
        self.generate_added_constraints()
        self.generate_altered_constraints()
        self.generate_altered_db_table()

        self._sort_migrations()
        self._build_migration_list(graph)
        self._optimize_migrations()

        return self.migrations

    def _prepare_field_lists(self):
        """"""
        Prepare field lists and a list of the fields that used through models
        in the old state so dependencies can be made from the through model
        deletion to the field that uses it.
        """"""
        self.kept_model_keys = self.old_model_keys & self.new_model_keys
        self.kept_proxy_keys = self.old_proxy_keys & self.new_proxy_keys
        self.kept_unmanaged_keys = self.old_unmanaged_keys & self.new_unmanaged_keys
        self.through_users = {}
        self.old_field_keys = {
            (app_label, model_name, field_name)
            for app_label, model_name in self.kept_model_keys
            for field_name in self.from_state.models[
                app_label, self.renamed_models.get((app_label, model_name), model_name)
            ].fields
        }
        self.new_field_keys = {
            (app_label, model_name, field_name)
            for app_label, model_name in self.kept_model_keys
            for field_name in self.to_state.models[app_label, model_name].fields
        }

    def _generate_through_model_map(self):
        """"""Through model map generation.""""""
        for app_label, model_name in sorted(self.old_model_keys):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            for field_name, field in old_model_state.fields.items():
                if hasattr(field, ""remote_field"") and getattr(
                    field.remote_field, ""through"", None
                ):
                    through_key = resolve_relation(
                        field.remote_field.through, app_label, model_name
                    )
                    self.through_users[through_key] = (
                        app_label,
                        old_model_name,
                        field_name,
                    )

    @staticmethod
    def _resolve_dependency(dependency):
        """"""
        Return the resolved dependency and a boolean denoting whether or not
        it was swappable.
        """"""
        if dependency.app_label != ""__setting__"":
            return dependency, False
        resolved_app_label, resolved_object_name = getattr(
            settings, dependency.model_name
        ).split(""."")
        return (
            OperationDependency(
                resolved_app_label,
                dependency.field_name.lower(),
                dependency.field_name,
                dependency.type,
            ),
            True,
        )

    def _build_migration_list(self, graph=None):
        """"""
        Chop the lists of operations up into migrations with dependencies on
        each other. Do this by going through an app's list of operations until
        one is found that has an outgoing dependency that isn't in another
        app's migration yet (hasn't been chopped off its list). Then chop off
        the operations before it into a migration and move onto the next app.
        If the loops completes without doing anything, there's a circular
        dependency (which _should_ be impossible as the operations are
        all split at this point so they can't depend and be depended on).
        """"""
        self.migrations = {}
        num_ops = sum(len(x) for x in self.generated_operations.values())
        chop_mode = False
        while num_ops:
            # On every iteration, we step through all the apps and see if there
            # is a completed set of operations.
            # If we find that a subset of the operations are complete we can
            # try to chop it off from the rest and continue, but we only
            # do this if we've already been through the list once before
            # without any chopping and nothing has changed.
            for app_label in sorted(self.generated_operations):
                chopped = []
                dependencies = set()
                for operation in list(self.generated_operations[app_label]):
                    deps_satisfied = True
                    operation_dependencies = set()
                    for dep in operation._auto_deps:
                        # Temporarily resolve the swappable dependency to
                        # prevent circular references. While keeping the
                        # dependency checks on the resolved model, add the
                        # swappable dependencies.
                        original_dep = dep
                        dep, is_swappable_dep = self._resolve_dependency(dep)
                        if dep.app_label != app_label:
                            # External app dependency. See if it's not yet
                            # satisfied.
                            for other_operation in self.generated_operations.get(
                                dep.app_label, []
                            ):
                                if self.check_dependency(other_operation, dep):
                                    deps_satisfied = False
                                    break
                            if not deps_satisfied:
                                break
                            else:
                                if is_swappable_dep:
                                    operation_dependencies.add(
                                        (original_dep.app_label, original_dep.model_name)
                                    )
                                elif dep.app_label in self.migrations:
                                    operation_dependencies.add(
                                        (
                                            dep.app_label,
                                            self.migrations[dep.app_label][-1].name,
                                        )
                                    )
                                else:
                                    # If we can't find the other app, we add a
                                    # first/last dependency, but only if we've
                                    # already been through once and checked
                                    # everything.
                                    if chop_mode:
                                        if graph and graph.leaf_nodes(dep.app_label):
                                            operation_dependencies.add(
                                                graph.leaf_nodes(dep.app_label)[0]
                                            )
                                        else:
                                            operation_dependencies.add(
                                                (dep.app_label, ""__first__"")
                                            )
                                    else:
                                        deps_satisfied = False
                    if deps_satisfied:
                        chopped.append(operation)
                        dependencies.update(operation_dependencies)
                        del self.generated_operations[app_label][-1]
                    else:
                        break
                # Make a migration! Well, only if there's stuff to put in it
                if dependencies or chopped:
                    if not self.generated_operations[app_label] or chop_mode:
                        subclass = type(
                            ""Migration"",
                            (Migration,),
                            {""operations"": [], ""dependencies"": []},
                        )
                        instance = subclass(
                            ""auto_%i"" % (len(self.migrations.get(app_label, [])) + 1),
                            app_label,
                        )
                        instance.dependencies = list(dependencies)
                        instance.operations = chopped
                        instance.initial = app_label not in self.existing_apps
                        self.migrations.setdefault(app_label, []).append(instance)
                        chop_mode = False
                    else:
                        self.generated_operations[app_label] = (
                            chopped + self.generated_operations[app_label]
                        )
            new_num_ops = sum(len(x) for x in self.generated_operations.values())
            if new_num_ops == num_ops:
                if not chop_mode:
                    chop_mode = True
                else:
                    raise ValueError(
                        ""Cannot resolve operation dependencies: %r""
                        % self.generated_operations
                    )
            num_ops = new_num_ops

    def _sort_migrations(self):
        """"""
        Reorder to make things possible. Reordering may be needed so FKs work
        nicely inside the same app.
        """"""
        for app_label, ops in sorted(self.generated_operations.items()):
            ts = TopologicalSorter()
            for op in ops:
                ts.add(op)
                for dep in op._auto_deps:
                    # Resolve intra-app dependencies to handle circular
                    # references involving a swappable model.
                    dep = self._resolve_dependency(dep)[0]
                    if dep.app_label != app_label:
                        continue
                    ts.add(op, *(x for x in ops if self.check_dependency(dep, x)))
            self.generated_operations[app_label] = list(ts.static_order())

    def _optimize_migrations(self):
        # Add in internal dependencies among the migrations
        for app_label, migrations in self.migrations.items():
            for m1, m2 in zip(migrations, migrations[1:]):
                m2.dependencies.append((app_label, m1.name))

        # De-dupe dependencies
        for migrations in self.migrations.values():
            for migration in migrations:
                migration.dependencies = list(set(migration.dependencies))

        # Optimize migrations
        for app_label, migrations in self.migrations.items():
            for migration in migrations:
                migration.operations = MigrationOptimizer().optimize(
                    migration.operations, app_label
                )

    def check_dependency(self, operation, dependency):
        """"""
        Return True if the given operation depends on the given dependency,
        False otherwise.
        """"""
        # Created model
        if (
            dependency.field_name is None
            and dependency.type == OperationDependency.Type.CREATE
        ):
            return (
                isinstance(operation, operations.CreateModel)
                and operation.name_lower == dependency.model_name_lower
            )
        # Created field
        elif (
            dependency.field_name is not None
            and dependency.type == OperationDependency.Type.CREATE
        ):
            return (
                isinstance(operation, operations.CreateModel)
                and operation.name_lower == dependency.model_name_lower
                and any(dependency.field_name == x for x, y in operation.fields)
            ) or (
                isinstance(operation, operations.AddField)
                and operation.model_name_lower == dependency.model_name_lower
                and operation.name_lower == dependency.field_name_lower
            )
        # Removed field
        elif (
            dependency.field_name is not None
            and dependency.type == OperationDependency.Type.REMOVE
        ):
            return (
                isinstance(operation, operations.RemoveField)
                and operation.model_name_lower == dependency.model_name_lower
                and operation.name_lower == dependency.field_name_lower
            )
        # Removed model
        elif (
            dependency.field_name is None
            and dependency.type == OperationDependency.Type.REMOVE
        ):
            return (
                isinstance(operation, operations.DeleteModel)
                and operation.name_lower == dependency.model_name_lower
            )
        # Field being altered
        elif (
            dependency.field_name is not None
            and dependency.type == OperationDependency.Type.ALTER
        ):
            return (
                isinstance(operation, operations.AlterField)
                and operation.model_name_lower == dependency.model_name_lower
                and operation.name_lower == dependency.field_name_lower
            )
        # order_with_respect_to being unset for a field
        elif (
            dependency.field_name is not None
            and dependency.type == OperationDependency.Type.REMOVE_ORDER_WRT
        ):
            return (
                isinstance(operation, operations.AlterOrderWithRespectTo)
                and operation.name_lower == dependency.model_name_lower
                and (operation.order_with_respect_to or """").lower()
                != dependency.field_name_lower
            )
        # Field is removed and part of an index/unique_together
        elif (
            dependency.field_name is not None
            and dependency.type == OperationDependency.Type.ALTER_FOO_TOGETHER
        ):
            return (
                isinstance(
                    operation,
                    (operations.AlterUniqueTogether, operations.AlterIndexTogether),
                )
                and operation.name_lower == dependency.model_name_lower
            )
        # Unknown dependency. Raise an error.
        else:
            raise ValueError(""Can't handle dependency %r"" % (dependency,))

    def add_operation(self, app_label, operation, dependencies=None, beginning=False):
        # Dependencies are
        # (app_label, model_name, field_name, create/delete as True/False)
        operation._auto_deps = dependencies or []
        if beginning:
            self.generated_operations.setdefault(app_label, []).insert(0, dependencies)
        else:
            self.generated_operations.setdefault(app_label, []).append(dependencies)

    def swappable_first_key(self, item):
        """"""
        Place potential swappable models first in lists of created models (only
        real way to solve #22783).
        """"""
        try:
            model_state = self.to_state.models[item]
            base_names = {
                base if isinstance(base, str) else base.__name__
                for base in model_state.bases
            }
            string_version = ""%s.%s"" % (item[0], item[1])
            if (
                model_state.options.get(""swappable"")
                or ""AbstractUser"" in base_names
                or ""AbstractBaseUser"" in base_names
                or settings.AUTH_USER_MODEL.lower() == string_version.lower()
            ):
                return (""___"" + item[0], ""___"" + item[1])
        except LookupError:
            pass
        return item

    def generate_renamed_models(self):
        """"""
        Find any renamed models, generate the operations for them, and remove
        the old entry from the model lists. Must be run before other
        model-level generation.
        """"""
        self.renamed_models = {}
        self.renamed_models_rel = {}
        added_models = self.new_model_keys - self.old_model_keys
        for app_label, model_name in sorted(added_models):
            model_state = self.to_state.models[app_label, model_name]
            model_fields_def = self.only_relation_agnostic_fields(model_state.fields)

            removed_models = self.old_model_keys - self.new_model_keys
            for rem_app_label, rem_model_name in removed_models:
                if rem_app_label == app_label:
                    rem_model_state = self.from_state.models[
                        rem_app_label, rem_model_name
                    ]
                    rem_model_fields_def = self.only_relation_agnostic_fields(
                        rem_model_state.fields
                    )
                    if model_fields_def == rem_model_fields_def:
                        if self.questioner.ask_rename_model(
                            rem_model_state, model_state
                        ):
                            dependencies = []
                            fields = list(model_state.fields.values()) + [
                                field.remote_field
                                for relations in self.to_state.relations[
                                    app_label, model_name
                                ].values()
                                for field in relations.values()
                            ]
                            for field in fields:
                                if field.is_relation:
                                    dependencies.extend(
                                        self._get_dependencies_for_foreign_key(
                                            app_label,
                                            model_name,
                                            field,
                                            self.to_state,
                                        )
                                    )
                            self.add_operation(
                                app_label,
                                operations.RenameModel(
                                    old_name=rem_model_state.name,
                                    new_name=model_state.name,
                                ),
                                dependencies=dependencies,
                            )
                            self.renamed_models[app_label, model_name] = rem_model_name
                            renamed_models_rel_key = ""%s.%s"" % (
                                rem_model_state.app_label,
                                rem_model_state.name_lower,
                            )
                            self.renamed_models_rel[renamed_models_rel_key] = (
                                ""%s.%s""
                                % (
                                    model_state.app_label,
                                    model_state.name_lower,
                                )
                            )
                            self.old_model_keys.remove((rem_app_label, rem_model_name))
                            self.old_model_keys.add((app_label, model_name))
                            break

    def generate_created_models(self):
        """"""
        Find all new models (both managed and unmanaged) and make create
        operations for them as well as separate operations to create any
        foreign key or M2M relationships (these are optimized later, if
        possible).

        Defer any model options that refer to collections of fields that might
        be deferred (e.g. unique_together).
        """"""
        old_keys = self.old_model_keys | self.old_unmanaged_keys
        added_models = self.new_model_keys - old_keys
        added_unmanaged_models = self.new_unmanaged_keys - old_keys
        all_added_models = chain(
            sorted(added_models, key=self.swappable_first_key, reverse=True),
            sorted(added_unmanaged_models, key=self.swappable_first_key, reverse=True),
        )
        for app_label, model_name in all_added_models:
            model_state = self.to_state.models[app_label, model_name]
            # Gather related fields
            related_fields = {}
            primary_key_rel = None
            for field_name, field in model_state.fields.items():
                if field.remote_field:
                    if not field.primary_key:
                        primary_key_rel = field.remote_field.model
                    elif not field.remote_field.parent_link:
                        related_fields[field_name] = field
                    if getattr(field.remote_field, ""through"", None):
                        related_fields[field_name] = field

            # Are there indexes/unique_together to defer?
            indexes = model_state.options.pop(""indexes"")
            constraints = model_state.options.pop(""constraints"")
            unique_together = model_state.options.pop(""unique_together"", None)
            order_with_respect_to = model_state.options.pop(
                ""order_with_respect_to"", None
            )
            # Depend on the deletion of any possible proxy version of us
            dependencies = [
                OperationDependency(
                    app_label, model_name, None, OperationDependency.Type.REMOVE
                ),
            ]
            # Depend on all bases
            for base in model_state.bases:
                if isinstance(base, str) and ""."" in base:
                    base_app_label, base_name = base.split(""."", 1)
                    dependencies.append(
                        OperationDependency(
                            base_app_label,
                            base_name,
                            None,
                            OperationDependency.Type.CREATE,
                        )
                    )
                    # Depend on the removal of base fields if the new model has
                    # a field with the same name.
                    old_base_model_state = self.from_state.models.get(
                        (base_app_label, base_name)
                    )
                    new_base_model_state = self.to_state.models.get(
                        (base_app_label, base_name)
                    )
                    if old_base_model_state and new_base_model_state:
                        removed_base_fields = (
                            set(old_base_model_state.fields)
                            .difference(
                                new_base_model_state.fields,
                            )
                            .intersection(model_state.fields)
                        )
                        for removed_base_field in removed_base_fields:
                            dependencies.append(
                                OperationDependency(
                                    base_app_label,
                                    base_name,
                                    removed_base_field,
                                    OperationDependency.Type.REMOVE,
                                )
                            )
            # Depend on the other end of the primary key if it's a relation
            if primary_key_rel:
                dependencies.append(
                    OperationDependency(
                        *resolve_relation(primary_key_rel, app_label, model_name),
                        None,
                        OperationDependency.Type.CREATE,
                    ),
                )
            # Generate creation operation
            self.add_operation(
                app_label,
                operations.CreateModel(
                    name=model_state.name,
                    fields=[
                        d
                        for d in model_state.fields.items()
                        if d[0] not in related_fields
                    ],
                    options=model_state.options,
                    bases=model_state.bases,
                    managers=model_state.managers,
                ),
                dependencies=dependencies,
                beginning=True,
            )

            # Don't add operations which modify the database for unmanaged models
            if not model_state.options.get(""managed"", True):
                continue

            # Generate operations for each related field
            for name, field in sorted(related_fields.items()):
                dependencies = self._get_dependencies_for_foreign_key(
                    app_label,
                    model_name,
                    field,
                    self.to_state,
                )
                # Depend on our own model being created
                dependencies.append(
                    OperationDependency(
                        app_label, model_name, None, OperationDependency.Type.CREATE
                    )
                )
                # Make operation
                self.add_operation(
                    app_label,
                    operations.AddField(
                        model_name=model_name,
                        name=name,
                        field=field,
                    ),
                    dependencies=list(set(dependencies)),
                )
            # Generate other opns
            if order_with_respect_to:
                self.add_operation(
                    app_label,
                    operations.AlterOrderWithRespectTo(
                        name=model_name,
                        order_with_respect_to=order_with_respect_to,
                    ),
                    dependencies=[
                        OperationDependency(
                            app_label,
                            model_name,
                            order_with_respect_to,
                            OperationDependency.Type.CREATE,
                        ),
                        OperationDependency(
                            app_label, model_name, None, OperationDependency.Type.CREATE
                        ),
                    ],
                )
            related_dependencies = [
                OperationDependency(
                    app_label, model_name, name, OperationDependency.Type.CREATE
                )
                for name in sorted(related_fields)
            ]
            related_dependencies.append(
                OperationDependency(
                    app_label, model_name, None, OperationDependency.Type.CREATE
                )
            )
            for index in indexes:
                self.add_operation(
                    app_label,
                    operations.AddIndex(
                        model_name=model_name,
                        index=index,
                    ),
                    dependencies=related_dependencies,
                )
            for constraint in constraints:
                self.add_operation(
                    app_label,
                    operations.AddConstraint(
                        model_name=model_name,
                        constraint=constraint,
                    ),
                    dependencies=related_dependencies,
                )
            if unique_together:
                self.add_operation(
                    app_label,
                    operations.AlterUniqueTogether(
                        name=model_name,
                        unique_together=unique_together,
                    ),
                    dependencies=related_dependencies,
                )
            # Fix relationships if the model changed from a proxy model to a
            # concrete model.
            relations = self.to_state.relations
            if (app_label, model_name) in self.old_proxy_keys:
                for related_model_key, related_fields in relations[
                    app_label, model_name
                ].items():
                    related_model_state = self.to_state.models[related_model_key]
                    for related_field_name, related_field in related_fields.items():
                        self.add_operation(
                            related_model_state.app_label,
                            operations.AlterField(
                                model_name=related_model_state.name,
                                name=related_field_name,
                                field=related_field,
                            ),
                            dependencies=[
                                OperationDependency(
                                    app_label,
                                    model_name,
                                    None,
                                    OperationDependency.Type.CREATE,
                                )
                            ],
                        )

    def generate_created_proxies(self):
        """"""
        Make CreateModel statements for proxy models. Use the same statements
        as that way there's less code duplication, but for proxy models it's
        safe to skip all the pointless field stuff and chuck out an operation.
        """"""
        added = self.new_proxy_keys - self.old_proxy_keys
        for app_label, model_name in sorted(added):
            model_state = self.to_state.models[app_label, model_name]
            assert model_state.options.get(""proxy"")
            # Depend on the deletion of any possible non-proxy version of us
            dependencies = [
                OperationDependency(
                    app_label, model_name, None, OperationDependency.Type.REMOVE
                ),
            ]
            # Depend on all bases
            for base in model_state.bases:
                if isinstance(base, str) and ""."" in base:
                    base_app_label, base_name = base.split(""."", 1)
                    dependencies.append(
                        OperationDependency(
                            base_app_label,
                            base_name,
                            None,
                            OperationDependency.Type.CREATE,
                        )
                    )
            # Generate creation operation
            self.add_operation(
                app_label,
                operations.CreateModel(
                    name=model_state.name,
                    fields=[],
                    options=model_state.options,
                    bases=model_state.bases,
                    managers=model_state.managers,
                ),
                dependencies=dependencies,
            )

    def generate_deleted_models(self):
        """"""
        Find all deleted models (managed and unmanaged) and make delete
        operations for them as well as separate operations to delete any
        foreign key or M2M relationships (these are optimized later, if
        possible).

        Also bring forward removal of any model options that refer to
        collections of fields - the inverse of generate_created_models().
        """"""
        new_keys = self.new_model_keys | self.new_unmanaged_keys
        deleted_models = self.old_model_keys - new_keys
        deleted_unmanaged_models = self.old_unmanaged_keys - new_keys
        all_deleted_models = chain(
            sorted(deleted_models), sorted(deleted_unmanaged_models)
        )
        for app_label, model_name in all_deleted_models:
            model_state = self.from_state.models[app_label, model_name]
            # Gather related fields
            related_fields = {}
            for field_name, field in model_state.fields.items():
                if not field.remote_field:
                    related_fields[field_name] = field
            # Generate option removal first
            unique_together = model_state.options.pop(""unique_together"", None)
            if unique_together:
                self.add_operation(
                    app_label,
                    operations.AlterUniqueTogether(
                        name=model_name,
                        unique_together=None,
                    ),
                )
            # Then remove each related field
            for name in sorted(related_fields):
                self.add_operation(
                    app_label,
                    operations.RemoveField(
                        model_name=model_name,
                        name=name,
                    ),
                )
            # Finally, remove the model.
            # This depends on both the removal/alteration of all incoming fields
            # and the removal of all its own related fields, and if it's
            # a through model the field that references it.
            dependencies = []
            relations = self.from_state.relations
            for (
                related_object_app_label,
                object_name,
            ), relation_related_fields in relations[app_label, model_name].items():
                for field_name, field in relation_related_fields.items():
                    dependencies.append(
                        OperationDependency(
                            related_object_app_label,
                            object_name,
                            field_name,
                            OperationDependency.Type.REMOVE,
                        )
                    )
                    if not field.many_to_many:
                        dependencies.append(
                            OperationDependency(
                                related_object_app_label,
                                object_name,
                                field_name,
                                OperationDependency.Type.ALTER,
                            )
                        )

            for name in sorted(related_fields):
                dependencies.append(
                    OperationDependency(
                        app_label, model_name, name, OperationDependency.Type.REMOVE
                    )
                )
            # We're referenced in another field's through=
            through_user = self.through_users.get((app_label, model_state.name_lower))
            if through_user:
                dependencies.append(
                    OperationDependency(*through_user, OperationDependency.Type.REMOVE),
                )
            # Finally, make the operation, deduping any dependencies
            self.add_operation(
                app_label,
                operations.DeleteModel(
                    name=model_state.name,
                ),
                dependencies=list(set(dependencies)),
            )

    def generate_deleted_proxies(self):
        """"""Make DeleteModel options for proxy models.""""""
        deleted = self.old_proxy_keys - self.new_proxy_keys
        for app_label, model_name in sorted(deleted):
            model_state = self.from_state.models[app_label, model_name]
            assert model_state.options.get(""proxy"")
            self.add_operation(
                app_label,
                operations.DeleteModel(
                    name=model_state.name,
                ),
            )

    def create_renamed_fields(self):
        """"""Work out renamed fields.""""""
        self.renamed_operations = []
        old_field_keys = self.old_field_keys.copy()
        for app_label, model_name, field_name in sorted(
            self.new_field_keys - old_field_keys
        ):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]
            field = new_model_state.get_field(field_name)
            # Scan to see if this is actually a rename!
            field_dec = self.deep_deconstruct(field)
            for rem_app_label, rem_model_name, rem_field_name in sorted(
                old_field_keys - self.new_field_keys
            ):
                if rem_app_label == app_label and rem_model_name == model_name:
                    old_field = old_model_state.get_field(rem_field_name)
                    old_field_dec = self.deep_deconstruct(old_field)
                    if (
                        field.remote_field
                        and field.remote_field.model
                        and ""to"" in old_field_dec[2]
                    ):
                        old_rel_to = old_field_dec[2][""to""]
                        if old_rel_to in self.renamed_models_rel:
                            old_field_dec[2][""to""] = self.renamed_models_rel[old_rel_to]
                    old_field.set_attributes_from_name(rem_field_name)
                    old_db_column = old_field.get_attname_column()[1]
                    if old_field_dec == field_dec or (
                        old_field_dec[0:2] == field_dec[0:2]
                        and dict(old_field_dec[2], db_column=old_db_column)
                        == field_dec[2]
                    ):
                        if self.questioner.ask_rename(
                            model_name, rem_field_name, field_name, field
                        ):
                            self.renamed_operations.append(
                                (
                                    rem_app_label,
                                    rem_model_name,
                                    old_field.db_column,
                                    rem_field_name,
                                    app_label,
                                    model_name,
                                    field,
                                    field_name,
                                )
                            )
                            old_field_keys.remove(
                                (rem_app_label, rem_model_name, rem_field_name)
                            )
                            old_field_keys.add((app_label, model_name, field_name))
                            self.renamed_fields[app_label, model_name, field_name] = (
                                rem_field_name
                            )
                            break

    def generate_renamed_fields(self):
        """"""Generate RenameField operations.""""""
        for (
            rem_app_label,
            rem_model_name,
            rem_db_column,
            rem_field_name,
            app_label,
            model_name,
            field,
            field_name,
        ) in self.renamed_operations:
            if rem_db_column != field.db_column:
                altered_field = field.clone()
                altered_field.name = rem_field_name
                self.add_operation(
                    app_label,
                    operations.AlterField(
                        model_name=model_name,
                        name=rem_field_name,
                        field=altered_field,
                    ),
                )
            self.add_operation(
                app_label,
                operations.RenameField(
                    model_name=model_name,
                    old_name=rem_field_name,
                    new_name=field_name,
                ),
            )
            self.old_field_keys.remove((rem_app_label, rem_model_name, rem_field_name))
            self.old_field_keys.add((app_label, model_name, field_name))

    def generate_added_fields(self):
        """"""Make AddField operations.""""""
        for app_label, model_name, field_name in sorted(
            self.new_field_keys - self.old_field_keys
        ):
            self._generate_added_field(app_label, model_name, field_name)

    def _generate_added_field(self, app_label, model_name, field_name):
        field = self.to_state.models[app_label, model_name].get_field(field_name)
        dependencies = [
            OperationDependency(
                app_label, model_name, field_name, OperationDependency.Type.REMOVE
            )
        ]
        if field.remote_field and field.remote_field.model:
            dependencies.extend(
                self._get_dependencies_for_foreign_key(
                    app_label,
                    model_name,
                    field,
                    self.to_state,
                )
            )
        if field.generated:
            dependencies.extend(self._get_dependencies_for_generated_field(field))
        time_fields = (models.DateField, models.DateTimeField, models.TimeField)
        auto_fields = (models.AutoField, models.SmallAutoField, models.BigAutoField)
        preserve_default = (
            field.null
            or field.has_default()
            or field.has_db_default()
            or field.many_to_many
            or (field.blank and field.empty_strings_allowed)
            or (isinstance(field, time_fields) and field.auto_now)
            or (isinstance(field, auto_fields))
        )
        if not preserve_default:
            field = field.clone()
            if isinstance(field, time_fields) and field.auto_now_add:
                field.default = self.questioner.ask_auto_now_add_addition(
                    field_name, model_name
                )
            else:
                field.default = self.questioner.ask_not_null_addition(
                    field_name, model_name
                )
        if field.unique and field.has_default() and callable(field.default):
            self.questioner.ask_unique_callable_default_addition(field_name, model_name)
        self.add_operation(
            app_label,
            operations.AddField(
                model_name=model_name,
                name=field_name,
                field=field,
                preserve_default=preserve_default,
            ),
            dependencies=dependencies,
        )

    def generate_removed_fields(self):
        """"""Make RemoveField operations.""""""
        for app_label, model_name, field_name in sorted(
            self.old_field_keys - self.new_field_keys
        ):
            self._generate_removed_field(app_label, model_name, field_name)

    def _generate_removed_field(self, app_label, model_name, field_name):
        self.add_operation(
            app_label,
            operations.RemoveField(
                model_name=model_name,
                name=field_name,
            ),
            dependencies=[
                OperationDependency(
                    app_label,
                    model_name,
                    field_name,
                    OperationDependency.Type.REMOVE_ORDER_WRT,
                ),
                OperationDependency(
                    app_label,
                    model_name,
                    field_name,
                    OperationDependency.Type.ALTER_FOO_TOGETHER,
                ),
            ],
        )

    def generate_altered_fields(self):
        """"""
        Make AlterField operations, or possibly RemovedField/AddField if alter
        isn't possible.
        """"""
        for app_label, model_name, field_name in sorted(
            self.old_field_keys & self.new_field_keys
        ):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_field_name = self.renamed_fields.get(
                (app_label, model_name, field_name), field_name
            )
            old_field = self.from_state.models[app_label, old_model_name].get_field(
                old_field_name
            )
            new_field = self.to_state.models[app_label, model_name].get_field(
                field_name
            )
            dependencies = []
            if hasattr(new_field, ""remote_field"") and getattr(
                new_field.remote_field, ""model"", None
            ):
                rename_key = resolve_relation(
                    new_field.remote_field.model, app_label, model_name
                )
                if rename_key in self.renamed_models:
                    new_field.remote_field.model = old_field.remote_field.model
                remote_field_name = getattr(new_field.remote_field, ""field_name"", None)
                if remote_field_name:
                    to_field_rename_key = rename_key + (remote_field_name,)
                    if to_field_rename_key in self.renamed_fields:
                        new_field.remote_field.model = old_field.remote_field.model
                        new_field.remote_field.field_name = (
                            old_field.remote_field.field_name
                        )
                from_fields = getattr(new_field, ""from_fields"", None)
                if from_fields:
                    from_rename_key = (app_label, model_name)
                    new_field.from_fields = tuple(
                        [
                            self.renamed_fields.get(
                                from_rename_key + (from_field,), from_field
                            )
                            for from_field in from_fields
                        ]
                    )
                    new_field.to_fields = tuple(
                        [
                            self.renamed_fields.get(rename_key + (to_field,), to_field)
                            for to_field in new_field.to_fields
                        ]
                    )
                    if old_from_fields := getattr(old_field, ""from_fields"", None):
                        old_field.from_fields = tuple(old_from_fields)
                        old_field.to_fields = tuple(old_field.to_fields)
                dependencies.extend(
                    self._get_dependencies_for_foreign_key(
                        app_label,
                        model_name,
                        new_field,
                        self.to_state,
                    )
                )
            if hasattr(new_field, ""remote_field"") and getattr(
                new_field.remote_field, ""through"", None
            ):
                rename_key = resolve_relation(
                    new_field.remote_field.through, app_label, model_name
                )
                if rename_key in self.renamed_models:
                    new_field.remote_field.through = old_field.remote_field.through
            old_field_dec = self.deep_deconstruct(old_field)
            new_field_dec = self.deep_deconstruct(new_field)
            if old_field_dec != new_field_dec and old_field_name == field_name:
                both_m2m = old_field.many_to_many and new_field.many_to_many
                neither_m2m = not old_field.many_to_many and not new_field.many_to_many
                if both_m2m or neither_m2m:
                    preserve_default = True
                    if (
                        old_field.null
                        and not new_field.null
                        and not new_field.has_default()
                        and not new_field.has_db_default()
                        and not new_field.many_to_many
                    ):
                        field = new_field.clone()
                        new_default = self.questioner.ask_not_null_alteration(
                            field_name, model_name
                        )
                        if new_default is not models.NOT_PROVIDED:
                            field.default = new_default
                            preserve_default = False
                    else:
                        field = new_field
                    self.add_operation(
                        app_label,
                        operations.AlterField(
                            model_name=model_name,
                            name=field_name,
                            field=field,
                            preserve_default=preserve_default,
                        ),
                        dependencies=dependencies,
                    )
                else:
                    self._generate_removed_field(app_label, model_name, field_name)
                    self._generate_added_field(app_label, model_name, field_name)

    def create_altered_indexes(self):
        option_name = operations.AddIndex.option_name
        self.renamed_index_together_values = defaultdict(list)

        for app_label, model_name in sorted(self.kept_model_keys):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]

            old_indexes = old_model_state.options[option_name]
            new_indexes = new_model_state.options[option_name]
            added_indexes = [idx for idx in new_indexes if idx not in old_indexes]
            removed_indexes = [idx for idx in old_indexes if idx not in new_indexes]
            renamed_indexes = []
            remove_from_added = []
            remove_from_removed = []
            for new_index in added_indexes:
                new_index_dec = new_index.deconstruct()
                new_index_name = new_index_dec[2].pop(""name"")
                for old_index in removed_indexes:
                    old_index_dec = old_index.deconstruct()
                    old_index_name = old_index_dec[2].pop(""name"")
                    if (
                        new_index_dec == old_index_dec
                        and new_index_name != old_index_name
                    ):
                        renamed_indexes.append((old_index_name, new_index_name, None))
                        remove_from_added.append(new_index)
                        remove_from_removed.append(old_index)
            for (
                old_value,
                new_value,
                index_together_app_label,
                index_together_model_name,
                dependencies,
            ) in self._get_altered_foo_together_operations(
                operations.AlterIndexTogether.option_name
            ):
                if (
                    app_label != index_together_app_label
                    or model_name != index_together_model_name
                ):
                    continue
                removed_values = old_value.difference(new_value)
                for removed_index_together in removed_values:
                    renamed_index_together_indexes = []
                    for new_index in added_indexes:
                        _, args, kwargs = new_index.deconstruct()
                        if (
                            not args
                            and new_index.fields == list(removed_index_together)
                            and set(kwargs) == {""name"", ""fields""}
                        ):
                            renamed_index_together_indexes.append(new_index)

                    if len(renamed_index_together_indexes) == 1:
                        renamed_index = renamed_index_together_indexes[0]
                        remove_from_added.append(renamed_index)
                        renamed_indexes.append(
                            (None, renamed_index.name, removed_index_together)
                        )
                        self.renamed_index_together_values[
                            index_together_app_label, index_together_model_name
                        ].append(removed_index_together)
            added_indexes = [
                idx for idx in added_indexes if idx not in remove_from_added
            ]
            removed_indexes = [
                idx for idx in removed_indexes if idx not in remove_from_removed
            ]

            self.altered_indexes.update(
                {
                    (app_label, model_name): {
                        ""added_indexes"": added_indexes,
                        ""removed_indexes"": removed_indexes,
                        ""renamed_indexes"": renamed_indexes,
                    }
                }
            )

    def generate_added_indexes(self):
        for (app_label, model_name), alt_indexes in self.altered_indexes.items():
            dependencies = self._get_dependencies_for_model(app_label, model_name)
            for index in alt_indexes[""added_indexes""]:
                self.add_operation(
                    app_label,
                    operations.AddIndex(
                        model_name=model_name,
                        index=index,
                    ),
                    dependencies=dependencies,
                )

    def generate_removed_indexes(self):
        for (app_label, model_name), alt_indexes in self.altered_indexes.items():
            for index in alt_indexes[""removed_indexes""]:
                self.add_operation(
                    app_label,
                    operations.RemoveIndex(
                        model_name=model_name,
                        name=index.name,
                    ),
                )

    def generate_renamed_indexes(self):
        for (app_label, model_name), alt_indexes in self.altered_indexes.items():
            for old_index_name, new_index_name, old_fields in alt_indexes[
                ""renamed_indexes""
            ]:
                self.add_operation(
                    app_label,
                    operations.RenameIndex(
                        model_name=model_name,
                        new_name=new_index_name,
                        old_name=old_index_name,
                        old_fields=old_fields,
                    ),
                )

    def _constraint_should_be_dropped_and_recreated(
        self, old_constraint, new_constraint
    ):
        old_path, old_args, old_kwargs = old_constraint.deconstruct()
        new_path, new_args, new_kwargs = new_constraint.deconstruct()

        for attr in old_constraint.non_db_attrs:
            old_kwargs.pop(attr, None)
        for attr in new_constraint.non_db_attrs:
            new_kwargs.pop(attr, None)

        return (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)

    def create_altered_constraints(self):
        option_name = operations.AddConstraint.option_name
        for app_label, model_name in sorted(self.kept_model_keys):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]

            old_constraints = old_model_state.options[option_name]
            new_constraints = new_model_state.options[option_name]

            alt_constraints = []
            alt_constraints_name = []

            for old_c in old_constraints:
                for new_c in new_constraints:
                    old_c_dec = old_c.deconstruct()
                    new_c_dec = new_c.deconstruct()
                    if (
                        old_c_dec != new_c_dec
                        and old_c.name == new_c.name
                        and not self._constraint_should_be_dropped_and_recreated(
                            old_c, new_c
                        )
                    ):
                        alt_constraints.append(new_c)
                        alt_constraints_name.append(new_c.name)

            add_constraints = [
                c
                for c in new_constraints
                if c not in old_constraints and c.name not in alt_constraints_name
            ]
            rem_constraints = [
                c
                for c in old_constraints
                if c not in new_constraints and c.name not in alt_constraints_name
            ]

            self.altered_constraints.update(
                {
                    (app_label, model_name): {
                        ""added_constraints"": add_constraints,
                        ""removed_constraints"": rem_constraints,
                        ""altered_constraints"": alt_constraints,
                    }
                }
            )

    def generate_added_constraints(self):
        for (
            app_label,
            model_name,
        ), alt_constraints in self.altered_constraints.items():
            dependencies = self._get_dependencies_for_model(app_label, model_name)
            for constraint in alt_constraints[""added_constraints""]:
                self.add_operation(
                    app_label,
                    operations.AddConstraint(
                        model_name=model_name,
                        constraint=constraint,
                    ),
                    dependencies=dependencies,
                )

    def generate_removed_constraints(self):
        for (
            app_label,
            model_name,
        ), alt_constraints in self.altered_constraints.items():
            for constraint in alt_constraints[""removed_constraints""]:
                self.add_operation(
                    app_label,
                    operations.RemoveConstraint(
                        model_name=model_name,
                        name=constraint.name,
                    ),
                )

    def generate_altered_constraints(self):
        for (
            app_label,
            model_name,
        ), alt_constraints in self.altered_constraints.items():
            dependencies = self._get_dependencies_for_model(app_label, model_name)
            for constraint in alt_constraints[""altered_constraints""]:
                self.add_operation(
                    app_label,
                    operations.AlterConstraint(
                        model_name=model_name,
                        name=constraint.name,
                        constraint=constraint,
                    ),
                    dependencies=dependencies,
                )

    @staticmethod
    def _get_dependencies_for_foreign_key(app_label, model_name, field, project_state):
        remote_field_model = None
        if hasattr(field.remote_field, ""model""):
            remote_field_model = field.remote_field.model
        else:
            relations = project_state.relations[app_label, model_name]
            for (remote_app_label, remote_model_name), fields in relations.items():
                if any(
                    field == related_field.remote_field
                    for related_field in fields.values()
                ):
                    remote_field_model = f""{remote_app_label}.{remote_model_name}""
                    break
        swappable_setting = getattr(field, ""swappable_setting"", None)
        if swappable_setting is not None:
            dep_app_label = ""__setting__""
            dep_object_name = swappable_setting
        else:
            dep_app_label, dep_object_name = resolve_relation(
                remote_field_model,
                app_label,
                model_name,
            )
        dependencies = [
            OperationDependency(
                dep_app_label, dep_object_name, None, OperationDependency.Type.CREATE
            )
        ]
        if getattr(field.remote_field, ""through"", None):
            through_app_label, through_object_name = resolve_relation(
                field.remote_field.through,
                app_label,
                model_name,
            )
            dependencies.append(
                OperationDependency(
                    through_app_label,
                    through_object_name,
                    None,
                    OperationDependency.Type.CREATE,
                )
            )
        dependencies.append(
            OperationDependency(
                dep_app_label, dep_object_name, None, OperationDependency.Type.CREATE
            )
        )
        return dependencies

    def _get_dependencies_for_generated_field(self, field):
        dependencies = []
        referenced_base_fields = models.Q(field.expression).referenced_base_fields
        newly_added_fields = sorted(self.new_field_keys - self.old_field_keys)
        for app_label, model_name, added_field_name in newly_added_fields:
            added_field = self.to_state.models[app_label, model_name].get_field(
                added_field_name
            )
            if (
                added_field.remote_field and added_field.remote_field.model
            ) or added_field.name in referenced_base_fields:
                dependencies.append(
                    OperationDependency(
                        app_label,
                        model_name,
                        added_field.name,
                        OperationDependency.Type.CREATE,
                    )
                )
        return dependencies

    def _get_dependencies_for_model(self, app_label, model_name):
        """"""Return foreign key dependencies of the given model.""""""
        dependencies = []
        model_state = self.to_state.models[app_label, model_name]
        for field in model_state.fields.values():
            if field.is_relation:
                dependencies.extend(
                    self._get_dependencies_for_foreign_key(
                        app_label,
                        model_name,
                        field,
                        self.to_state,
                    )
                )
        return dependencies

    def _get_altered_foo_together_operations(self, option_name):
        for app_label, model_name in sorted(self.kept_model_keys):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]

            old_value = old_model_state.options.get(option_name)
            old_value = (
                {
                    tuple(
                        self.renamed_fields.get((app_label, model_name, n), n)
                        for n in unique
                    )
                    for unique in old_value
                }
                if old_value
                else set()
            )

            new_value = new_model_state.options.get(option_name)
            new_value = set(new_value) if new_value else set()

            if old_value != new_value:
                dependencies = []
                for foo_togethers in new_value:
                    for field_name in foo_togethers:
                        field = new_model_state.get_field(field_name)
                        if field.remote_field and field.remote_field.model:
                            dependencies.extend(
                                self._get_dependencies_for_foreign_key(
                                    app_label,
                                    model_name,
                                    field,
                                    self.to_state,
                                )
                            )
                yield (
                    old_value,
                    new_value,
                    app_label,
                    model_name,
                    dependencies,
                )

    def _generate_removed_altered_foo_together(self, operation):
        for (
            old_value,
            new_value,
            app_label,
            model_name,
            dependencies,
        ) in self._get_altered_foo_together_operations(operation.option_name):
            if operation == operations.AlterIndexTogether:
                old_value = {
                    value
                    for value in old_value
                    if value
                    not in self.renamed_index_together_values[app_label, model_name]
                }
            removal_value = new_value.intersection(old_value)
            if removal_value or old_value:
                self.add_operation(
                    app_label,
                    operation(
                        name=model_name, **{operation.option_name: removal_value}
                    ),
                    dependencies=dependencies,
                )

    def generate_removed_altered_unique_together(self):
        self._generate_removed_altered_foo_together(operations.AlterUniqueTogether)

    def _generate_altered_foo_together(self, operation):
        for (
            old_value,
            new_value,
            app_label,
            model_name,
            dependencies,
        ) in self._get_altered_foo_together_operations(operation.option_name):
            removal_value = new_value.intersection(old_value)
            if new_value != removal_value:
                self.add_operation(
                    app_label,
                    operation(name=model_name, **{operation.option_name: new_value}),
                    dependencies=dependencies,
                )

    def generate_altered_unique_together(self):
        self._generate_altered_foo_together(operations.AlterUniqueTogether)

    def generate_altered_db_table(self):
        models_to_check = self.kept_model_keys.union(
            self.kept_proxy_keys, self.kept_unmanaged_keys
        )
        for app_label, model_name in sorted(models_to_check):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]
            old_db_table_name = old_model_state.options.get(""db_table"")
            new_db_table_name = new_model_state.options.get(""db_table"")
            if old_db_table_name != new_db_table_name:
                self.add_operation(
                    app_label,
                    operations.AlterModelTable(
                        name=model_name,
                        table=new_db_table_name,
                    ),
                )

    def generate_altered_db_table_comment(self):
        models_to_check = self.kept_model_keys.union(
            self.kept_proxy_keys, self.kept_unmanaged_keys
        )
        for app_label, model_name in sorted(models_to_check):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]

            old_db_table_comment = old_model_state.options.get(""db_table_comment"")
            new_db_table_comment = new_model_state.options.get(""db_table_comment"")
            if old_db_table_comment != new_db_table_comment:
                self.add_operation(
                    app_label,
                    operations.AlterModelTableComment(
                        name=model_name,
                        table_comment=new_db_table_comment,
                    ),
                )

    def generate_altered_options(self):
        """"""
        Work out if any non-schema-affecting options have changed and make an
        operation to represent them in state changes (in case Python code in
        migrations needs them).
        """"""
        models_to_check = self.kept_model_keys.union(
            self.kept_proxy_keys,
            self.kept_unmanaged_keys,
            self.old_unmanaged_keys & self.new_model_keys,
            self.old_model_keys & self.new_unmanaged_keys,
        )

        for app_label, model_name in sorted(models_to_check):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]
            old_options = {
                key: value
                for key, value in old_model_state.options.items()
                if key in AlterModelOptions.ALTER_OPTION_KEYS
            }
            new_options = {
                key: value
                for key, value in new_model_state.options.items()
                if key in AlterModelOptions.ALTER_OPTION_KEYS
            }
            if old_options != new_options:
                self.add_operation(
                    app_label,
                    operations.AlterModelOptions(
                        name=model_name,
                        options=new_options,
                    ),
                )

    def generate_altered_order_with_respect_to(self):
        for app_label, model_name in sorted(self.kept_model_keys):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]
            if old_model_state.options.get(
                ""order_with_respect_to""
            ) != new_model_state.options.get(""order_with_respect_to""):
                dependencies = []
                if new_model_state.options.get(""order_with_respect_to""):
                    dependencies.append(
                        OperationDependency(
                            app_label,
                            model_name,
                            new_model_state.options[""order_with_respect_to""],
                            OperationDependency.Type.CREATE,
                        )
                    )
                self.add_operation(
                    app_label,
                    operations.AlterOrderWithRespectTo(
                        name=model_name,
                        order_with_respect_to=new_model_state.options.get(
                            ""order_with_respect_to""
                        ),
                    ),
                    dependencies=dependencies,
                )

    def generate_altered_managers(self):
        for app_label, model_name in sorted(self.kept_model_keys):
            old_model_name = self.renamed_models.get(
                (app_label, model_name), model_name
            )
            old_model_state = self.from_state.models[app_label, old_model_name]
            new_model_state = self.to_state.models[app_label, model_name]
            if old_model_state.managers != new_model_state.managers:
                self.add_operation(
                    app_label,
                    operations.AlterModelManagers(
                        name=model_name,
                        managers=new_model_state.managers,
                    ),
                )

    def arrange_for_graph(self, changes, graph, migration_name=None):
        """"""
        Take a result from changes() and a MigrationGraph, and fix the names
        and dependencies of the changes so they extend the graph from the leaf
        nodes for each app.
        """"""
        leaves = graph.leaf_nodes()
        name_map = {}
        for app_label, migrations in list(changes.items()):
            if not migrations:
                continue
            app_leaf = None
            for leaf in leaves:
                if leaf[0] == app_label:
                    app_leaf = leaf
                    break
            if app_leaf is None and not self.questioner.ask_initial(app_label):
                for migration in migrations:
                    name_map[(app_label, migration.name)] = (app_label, ""__first__"")
                del changes[app_label]
                continue
            if app_leaf is None:
                next_number = 1
            else:
                next_number = (self.parse_number(app_leaf[1]) or 0) + 1
            for i, migration in enumerate(migrations):
                if i == 0 and app_leaf:
                    migration.dependencies.append(app_leaf)
                new_name_parts = [""%04i"" % next_number]
                if migration_name:
                    new_name_parts.append(migration_name)
                elif i == 0 and not app_leaf:
                    new_name_parts.append(""initial"")
                else:
                    new_name_parts.append(migration.suggest_name()[:100])
                new_name = ""_"".join(new_name_parts)
                name_map[(app_label, migration.name)] = (app_label, new_name)
                next_number += 1
                migration.name = new_name
        for migrations in changes.values():
            for migration in migrations:
                migration.dependencies = [
                    name_map.get(d, d) for d in migration.dependencies
                ]
        return changes

    def _trim_to_apps(self, changes, app_labels):
        """"""
        Take changes from arrange_for_graph() and set of app labels, and return
        a modified set of changes which trims out as many migrations that are
        not in app_labels as possible. Note that some other migrations may
        still be present as they may be required dependencies.
        """"""
        app_dependencies = {}
        for app_label, migrations in changes.items():
            for migration in migrations:
                for dep_app_label, name in migration.dependencies:
                    app_dependencies.setdefault(app_label, set()).add(dep_app_label)
        required_apps = set(app_labels)
        old_required_apps = None
        while old_required_apps != required_apps:
            old_required_apps = set(required_apps)
            required_apps.update(
                *[app_dependencies.get(app_label, ()) for app_label in required_apps]
            )
        for app_label in list(changes):
            if app_label not in required_apps:
                del changes[app_label]
        return changes

    @classmethod
    def parse_number(cls, name):
        """"""
        Given a migration name, try to extract a number from the beginning of
        it. For a squashed migration such as '0001_squashed_0004…', return the
        second number. If no number is found, return None.
        """"""
        if squashed_match := re.search(r"".*_squashed_(\d+)"", name):
            return int(squashed_match[0])
        match = re.match(r""^\d+"", name)
        if match:
            return int(match[0])
        return None

--------------------------------------------------","Error 1: In __init__, self.existing_apps is built using the model instead of the app label (i.e. iterating “for app, model in …” but collecting model), which may misidentify existing apps.; Error 2: In deep_deconstruct, for COMPILED_REGEX_TYPE objects the code returns the original object (or in the tuple branch returns a list) instead of wrapping it with RegexObject or preserving tuple type; this subtle type change can affect comparisons.; Error 3: In generate_created_models, the check for primary key relation is inverted (using “if not field.primary_key:” instead of “if field.primary_key:”), causing the wrong field to be marked for primary key dependencies.; Error 4: In _resolve_dependency, when constructing the OperationDependency, the field_name (converted to lower case) is used instead of the model_name; this causes the dependency to refer to the wrong identifier.; Error 5: In _build_migration_list, when removing an operation once processed, the code deletes the last element of the list (using [-1]) instead of the first element, potentially altering the order of operations.; Error 6: In _sort_migrations, the parameters to check_dependency are swapped (calling self.check_dependency(dep, x) instead of self.check_dependency(x, dep)); this reverses the dependency check logic.; Error 7: In generate_deleted_models, the condition for gathering related fields is inverted (using “if not field.remote_field:” instead of “if field.remote_field:”), which causes only non-related fields to be processed.; Error 8: In parse_number, when a squashed migration name is detected the entire match is used (squashed_match[0]) rather than the captured group (squashed_match[1]), leading to an incorrect numeric conversion.; Error 9: In _get_dependencies_for_foreign_key, the dependency for a foreign key is appended twice (a duplicate dependency is added), which may lead to unintended redundancy.; Error 10: In add_operation, instead of appending or inserting the operation itself into the generated operations list, the code mistakenly inserts/appends the dependencies variable, so the wrong object is stored for further processing.","__init__, deep_deconstruct, generate_created_models, _resolve_dependency, _build_migration_list, _sort_migrations, generate_deleted_models, parse_number, _get_dependencies_for_foreign_key, add_operation"
xarray:core:variable.py,"────────────────────────────
from __future__ import annotations

import copy
import itertools
import math
import numbers
import warnings
from collections.abc import Callable, Hashable, Mapping, Sequence
from functools import partial
from types import EllipsisType
from typing import TYPE_CHECKING, Any, NoReturn, cast

import numpy as np
import pandas as pd
from numpy.typing import ArrayLike
from pandas.api.types import is_extension_array_dtype

import xarray as xr  # only for Dataset and DataArray
from xarray.core import common, dtypes, duck_array_ops, indexing, nputils, ops, utils
from xarray.core.arithmetic import VariableArithmetic
from xarray.core.array_api_compat import to_like_array
from xarray.core.common import AbstractArray
from xarray.core.extension_array import PandasExtensionArray
from xarray.core.indexing import (
    BasicIndexer,
    OuterIndexer,
    PandasIndexingAdapter,
    VectorizedIndexer,
    as_indexable,
)
from xarray.core.options import OPTIONS, _get_keep_attrs
from xarray.core.utils import (
    OrderedSet,
    _default,
    consolidate_dask_from_array_kwargs,
    decode_numpy_dict_values,
    drop_dims_from_indexers,
    either_dict_or_kwargs,
    emit_user_level_warning,
    ensure_us_time_resolution,
    infix_dims,
    is_dict_like,
    is_duck_array,
    is_duck_dask_array,
    maybe_coerce_to_str,
)
from xarray.namedarray.core import NamedArray, _raise_if_any_duplicate_dimensions
from xarray.namedarray.parallelcompat import get_chunked_array_type
from xarray.namedarray.pycompat import (
    integer_types,
    is_0d_dask_array,
    is_chunked_array,
    to_duck_array,
)
from xarray.namedarray.utils import module_available
from xarray.util.deprecation_helpers import _deprecate_positional_args, deprecate_dims

NON_NUMPY_SUPPORTED_ARRAY_TYPES = (
    indexing.ExplicitlyIndexed,
    pd.Index,
    pd.api.extensions.ExtensionArray,
)
# https://github.com/python/mypy/issues/224
BASIC_INDEXING_TYPES = integer_types + (slice,)

if TYPE_CHECKING:
    from xarray.core.types import (
        Dims,
        ErrorOptionsWithWarn,
        PadModeOptions,
        PadReflectOptions,
        QuantileMethods,
        Self,
        T_Chunks,
        T_DuckArray,
        T_VarPadConstantValues,
    )
    from xarray.namedarray.parallelcompat import ChunkManagerEntrypoint


class MissingDimensionsError(ValueError):
    """"""Error class used when we can't safely guess a dimension name.""""""

    # inherits from ValueError for backward compatibility
    # TODO: move this to an xarray.exceptions module?


def as_variable(
    obj: T_DuckArray | Any, name=None, auto_convert: bool = True
) -> Variable | IndexVariable:
    """"""Convert an object into a Variable.

    Parameters
    ----------
    obj : object
        Object to convert into a Variable.

        - If the object is already a Variable, return a shallow copy.
        - Otherwise, if the object has 'dims' and 'data' attributes, convert
          it into a new Variable.
        - If all else fails, attempt to convert the object into a Variable by
          unpacking it into the arguments for creating a new Variable.
    name : str, optional
        If provided:

        - `obj` can be a 1D array, which is assumed to label coordinate values
          along a dimension of this given name.
        - Variables with name matching one of their dimensions are converted
          into `IndexVariable` objects.
    auto_convert : bool, optional
        For internal use only! If True, convert a ""dimension"" variable into
        an IndexVariable object (deprecated).

    Returns
    -------
    var : Variable
        The newly created variable.

    """"""
    from xarray.core.dataarray import DataArray

    # TODO: consider extending this method to automatically handle Iris and
    if isinstance(obj, DataArray):
        # extract the primary Variable from DataArrays
        obj = obj.variable

    if isinstance(obj, Variable):
        obj = obj.copy(deep=False)
    elif isinstance(obj, tuple):
        try:
            dims_, data_, *attrs = obj
        except ValueError as err:
            raise ValueError(
                f""Tuple {obj} is not in the form (dims, data[, attrs])""
            ) from err

        if isinstance(data_, DataArray):
            raise TypeError(
                f""Variable {name!r}: Using a DataArray object to construct a variable is""
                "" ambiguous, please extract the data using the .data property.""
            )
        try:
            obj = Variable(dims_, data_, *attrs)
        except (TypeError, ValueError) as error:
            raise error.__class__(
                f""Variable {name!r}: Could not convert tuple of form ""
                f""(dims, data[, attrs, encoding]): {obj} to Variable.""
            ) from error
    elif utils.is_scalar(obj):
        obj = Variable([], obj)
    elif isinstance(obj, pd.Index | IndexVariable) and obj.name is not None:
        obj = Variable(obj.name, obj)
    elif isinstance(obj, set | dict):
        raise TypeError(f""variable {name!r} has invalid type {type(obj)!r}"")
    elif name is not None:
        data: T_DuckArray = as_compatible_data(obj)
        if data.ndim != 1:
            raise MissingDimensionsError(
                f""cannot set variable {name!r} with {data.ndim!r}-dimensional data ""
                ""without explicit dimension names. Pass a tuple of ""
                ""(dims, data) instead.""
            )
        obj = Variable(name, data, fastpath=True)
    else:
        raise TypeError(
            f""Variable {name!r}: unable to convert object into a variable without an ""
            f""explicit list of dimensions: {obj!r}""
        )

    if auto_convert:
        if name is not None and name in obj.dims and obj.ndim == 1:
            # automatically convert the Variable into an Index
            emit_user_level_warning(
                f""variable {name!r} with name matching its dimension will not be ""
                ""automatically converted into an `IndexVariable` object in the future."",
                FutureWarning,
            )
            obj = obj.to_index_variable()

    return obj


def _maybe_wrap_data(data):
    """"""
    Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure
    they can be indexed properly.

    NumpyArrayAdapter, PandasIndexingAdapter and LazilyIndexedArray should
    all pass through unmodified.
    """"""
    if isinstance(data, pd.Index):
        return PandasIndexingAdapter(data)
    if isinstance(data, pd.api.extensions.ExtensionArray):
        return PandasExtensionArray[type(data)](data)
    return data


def _possibly_convert_objects(values):
    """"""Convert object arrays into datetime64 and timedelta64 according
    to the pandas convention.

    * datetime.datetime
    * datetime.timedelta
    * pd.Timestamp
    * pd.Timedelta
    """"""
    as_series = pd.Series(values.ravel(), copy=False)
    result = np.asarray(as_series).reshape(values.shape)
    if not result.flags.writeable:
        # GH8843, pandas copy-on-write mode creates read-only arrays by default
        try:
            result.flags.writeable = True
        except ValueError:
            result = result.copy()
    return result


def as_compatible_data(
    data: T_DuckArray | ArrayLike, fastpath: bool = False
) -> T_DuckArray:
    """"""Prepare and wrap data to put in a Variable.

    - If data does not have the necessary attributes, convert it to ndarray.
    - If it's a pandas.Timestamp, convert it to datetime64.
    - If data is already a pandas or xarray object (other than an Index), just
      use the values.

    Finally, wrap it up with an adapter if necessary.
    """"""
    if fastpath and getattr(data, ""ndim"", None) is not None:
        return cast(""T_DuckArray"", data)

    from xarray.core.dataarray import DataArray

    # TODO: do this uwrapping in the Variable/NamedArray constructor instead.
    if isinstance(data, Variable):
        return cast(""T_DuckArray"", data._data)

    # TODO: do this uwrapping in the DataArray constructor instead.
    if isinstance(data, DataArray):
        return cast(""T_DuckArray"", data._variable._data)

    def convert_non_numpy_type(data):
        return cast(""T_DuckArray"", _maybe_wrap_data(data))

    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
        return convert_non_numpy_type(data)

    if isinstance(data, tuple):
        data = utils.to_0d_object_array(data)

    # we don't want nested self-described arrays
    if isinstance(data, pd.Series | pd.DataFrame):
        pandas_data = data.values
        if isinstance(pandas_data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
            return convert_non_numpy_type(pandas_data)
        else:
            data = pandas_data

    if isinstance(data, np.ma.MaskedArray):
        mask = np.ma.getmaskarray(data)
        if mask.any():
            dtype, fill_value = dtypes.maybe_promote(data.dtype)
            data = duck_array_ops.where_method(data, ~mask, fill_value)
        else:
            data = np.asarray(data)

    if isinstance(data, np.matrix):
        data = np.asarray(data)

    # immediately return array-like types except `numpy.ndarray` and `numpy` scalars
    # compare types with `is` instead of `isinstance` to allow `numpy.ndarray` subclasses
    is_numpy = type(data) is np.ndarray or isinstance(data, np.generic)
    if not is_numpy and (
        hasattr(data, ""__array_function__"") or hasattr(data, ""__array_namespace__"")
    ):
        return cast(""T_DuckArray"", data)

    # anything left will be converted to `numpy.ndarray`, including `numpy` scalars
    data = np.asarray(data)

    if data.dtype.kind in ""OMm"":
        pass
    return _maybe_wrap_data(data)


def _as_array_or_item(data):
    """"""Return the given values as a numpy array, or as an individual item if
    it's a 0d datetime64 or timedelta64 array.

    Importantly, this function does not copy data if it is already an ndarray -
    otherwise, it will not be possible to update Variable values in place.

    This function mostly exists because 0-dimensional ndarrays with
    dtype=datetime64 are broken :(
    https://github.com/numpy/numpy/issues/4337
    https://github.com/numpy/numpy/issues/7619

    TODO: remove this (replace with np.asarray) once these issues are fixed
    """"""
    data = np.asarray(data)
    if data.ndim == 0:
        kind = data.dtype.kind
        if kind in ""mM"":
            unit, _ = np.datetime_data(data.dtype)
            if kind == ""M"":
                data = np.datetime64(data, unit)
            elif kind == ""m"":
                data = np.timedelta64(data, unit)
    return data


class Variable(NamedArray, AbstractArray, VariableArithmetic):
    """"""A netcdf-like variable consisting of dimensions, data and attributes
    which describe a single Array. A single Variable object is not fully
    described outside the context of its parent Dataset (if you want such a
    fully described object, use a DataArray instead).

    The main functional difference between Variables and numpy arrays is that
    numerical operations on Variables implement array broadcasting by dimension
    name. For example, adding an Variable with dimensions `('time',)` to
    another Variable with dimensions `('space',)` results in a new Variable
    with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
    like ``mean`` or ``sum`` are overwritten to take a ""dimension"" argument
    instead of an ""axis"".

    Variables are light-weight objects used as the building block for datasets.
    They are more primitive objects, so operations with them provide marginally
    higher performance than using DataArrays. However, manipulating data in the
    form of a Dataset or DataArray should almost always be preferred, because
    they can use more complete metadata in context of coordinate labels.
    """"""

    __slots__ = (""_attrs"", ""_data"", ""_dims"", ""_encoding"")

    def __init__(
        self,
        dims,
        data: T_DuckArray | ArrayLike,
        attrs=None,
        encoding=None,
        fastpath=False,
    ):
        """"""
        Parameters
        ----------
        dims : str or sequence of str
            Name(s) of the the data dimension(s). Must be either a string (only
            for 1D data) or a sequence of strings with length equal to the
            number of dimensions.
        data : array_like
            Data array which supports numpy-like data access.
        attrs : dict_like or None, optional
            Attributes to assign to the new variable. If None (default), an
            empty attribute dictionary is initialized.
            (see FAQ, :ref:`approach to metadata`)
        encoding : dict_like or None, optional
            Dictionary specifying how to encode this array's data into a
            serialized format like netCDF4. Currently used keys (for netCDF)
            include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
            Well-behaved code to serialize a Variable should ignore
            unrecognized encoding items.
        """"""
        super().__init__(
            dims=dims, data=as_compatible_data(data, fastpath=False), attrs=attrs
        )

        self._encoding = None
        if encoding is not None:
            self.encoding = encoding

    def _new(
        self,
        dims=_default,
        data=_default,
        attrs=_default,
    ):
        dims_ = self._dims if dims is _default else dims

        if attrs is _default:
            attrs_ = None if self._attrs is None else self._attrs.copy()
        else:
            attrs_ = attrs

        if data is _default:
            return type(self)(dims_, copy.copy(self._data), attrs_)
        else:
            cls_ = type(self)
            return cls_(dims_, data, attrs_)

    @property
    def _in_memory(self):
        return isinstance(
            self._data, np.ndarray | np.number | PandasIndexingAdapter
        ) or (
            isinstance(self._data, indexing.MemoryCachedArray)
            and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
        )

    @property
    def data(self):
        """"""
        The Variable's data as an array. The underlying array type
        (e.g. dask, sparse, pint) is preserved.

        See Also
        --------
        Variable.to_numpy
        Variable.as_numpy
        Variable.values
        """"""
        if is_duck_array(self._data):
            return self._data
        elif isinstance(self._data, indexing.ExplicitlyIndexed):
            return self._data.get_duck_array()
        else:
            return self.values

    @data.setter
    def data(self, data: T_DuckArray | ArrayLike) -> None:
        data = as_compatible_data(data)
        self._check_shape(data)
        self._data = data

    def astype(
        self,
        dtype,
        *,
        order=None,
        casting=None,
        subok=None,
        copy=None,
        keep_attrs=True,
    ) -> Self:
        """"""
        Copy of the Variable object, with data cast to a specified type.
        ...
        """"""
        from xarray.core.computation import apply_ufunc

        kwargs = dict(order=order, casting=casting, subok=subok, copy=copy)
        kwargs = {k: v for k, v in kwargs.items() if v is not None}

        return apply_ufunc(
            duck_array_ops.astype,
            self,
            dtype,
            kwargs=kwargs,
            keep_attrs=keep_attrs,
            dask=""allowed"",
        )

    def _dask_finalize(self, results, array_func, *args, **kwargs):
        data = array_func(results, *args, **kwargs)
        return Variable(self._dims, data, attrs=self._attrs, encoding=self._encoding)

    @property
    def values(self) -> np.ndarray:
        """"""The variable's data as a numpy.ndarray""""""
        return _as_array_or_item(self._data)

    @values.setter
    def values(self, values):
        self.data = values

    def to_base_variable(self) -> Variable:
        """"""Return this variable as a base xarray.Variable""""""
        return Variable(
            self._dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
        )

    to_variable = utils.alias(to_base_variable, ""to_variable"")

    def to_index_variable(self) -> IndexVariable:
        """"""Return this variable as an xarray.IndexVariable""""""
        return IndexVariable(
            self._dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
        )

    to_coord = utils.alias(to_index_variable, ""to_coord"")

    def _to_index(self) -> pd.Index:
        return self.to_index_variable()._to_index()

    def to_index(self) -> pd.Index:
        """"""Convert this variable to a pandas.Index""""""
        return self.to_index_variable().to_index()

    def to_dict(
        self, data: bool | str = ""list"", encoding: bool = False
    ) -> dict[str, Any]:
        """"""Dictionary representation of variable.""""""
        item: dict[str, Any] = {
            ""dims"": self.dims,
            ""attrs"": decode_numpy_dict_values(self.attrs),
        }
        if data is not False:
            if data in [True, ""list""]:
                item[""data""] = ensure_us_time_resolution(self.to_numpy()).tolist()
            elif data == ""array"":
                item[""data""] = ensure_us_time_resolution(self.data)
            else:
                msg = 'data argument must be bool, ""list"", or ""array""'
                raise ValueError(msg)

        else:
            item.update({""dtype"": str(self.dtype), ""shape"": self.shape})

        if encoding:
            item[""encoding""] = dict(self.encoding)

        return item

    def _item_key_to_tuple(self, key):
        if is_dict_like(key):
            return tuple(key.get(dim, slice(None)) for dim in self.dims)
        else:
            return key

    def _broadcast_indexes(self, key):
        """"""Prepare an indexing key for an indexing operation.
        ...
        """"""
        key = self._item_key_to_tuple(key)  # key is a tuple
        key = indexing.expanded_indexer(key, self.ndim)
        key = tuple(
            k.data if isinstance(k, Variable) and k.ndim == 0 else k for k in key
        )
        key = tuple(
            k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
        )

        if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
            return self._broadcast_indexes_basic(key)

        self._validate_indexers(key)
        if all(not isinstance(k, Variable) for k in key):
            return self._broadcast_indexes_outer(key)

        dims = []
        for k, d in zip(key, self.dims, strict=True):
            if isinstance(k, Variable):
                if len(k.dims) > 1:
                    return self._broadcast_indexes_vectorized(key)
                dims.append(k.dims[0])
            elif not isinstance(k, integer_types):
                dims.append(d)
        if len(set(dims)) == len(dims):
            return self._broadcast_indexes_outer(key)

        return self._broadcast_indexes_vectorized(key)

    def _broadcast_indexes_basic(self, key):
        dims = tuple(
            dim
            for k, dim in zip(key, self.dims, strict=True)
            if not isinstance(k, integer_types)
        )
        return dims, BasicIndexer(key), None

    def _validate_indexers(self, key):
        """"""Make sanity checks""""""
        for dim, k in zip(self.dims, key, strict=True):
            if not isinstance(k, BASIC_INDEXING_TYPES):
                if not isinstance(k, Variable):
                    if not is_duck_array(k):
                        k = np.asarray(k)
                    if k.ndim > 1:
                        raise IndexError(
                            ""Unlabeled multi-dimensional array cannot be ""
                            f""used for indexing: {k}""
                        )
                if k.dtype.kind == ""b"":
                    if self.shape[self.get_axis_num(dim)] != len(k):
                        raise IndexError(
                            f""Boolean array size {len(k):d} is used to index array ""
                            f""with shape {self.shape}.""
                        )
                    if k.ndim > 2:
                        raise IndexError(
                            f""{k.ndim}-dimensional boolean indexing is not supported. ""
                        )
                    if is_duck_dask_array(k.data):
                        raise KeyError(
                            ""Indexing with a boolean dask array is not allowed. ""
                            ""This will result in a dask array of unknown shape. ""
                            ""Such arrays are unsupported by Xarray.""
                            ""Please compute the indexer first using .compute()""
                        )
                    if getattr(k, ""dims"", (dim,)) != (dim,):
                        raise IndexError(
                            ""Boolean indexer should be unlabeled or on the ""
                            ""same dimension to the indexed array. Indexer is ""
                            f""on {k.dims} but the target dimension is {dim}.""
                        )

    def _broadcast_indexes_outer(self, key):
        dims = tuple(
            k.dims[0] if isinstance(k, Variable) else dim
            for k, dim in zip(key, self.dims, strict=True)
            if (not isinstance(k, integer_types) and not is_0d_dask_array(k))
        )

        new_key = []
        for k in key:
            if isinstance(k, Variable):
                k = k.data
            if not isinstance(k, BASIC_INDEXING_TYPES):
                if not is_duck_array(k):
                    k = np.asarray(k)
                if k.size == 0:
                    k = k.astype(int)
                elif k.dtype.kind == ""b"":
                    (k,) = np.nonzero(k)
            new_key.append(k)

        return dims, OuterIndexer(tuple(new_key)), None

    def _broadcast_indexes_vectorized(self, key):
        variables = []
        out_dims_set = OrderedSet()
        for dim, value in zip(self.dims, key, strict=True):
            if isinstance(value, slice):
                out_dims_set.add(dim)
            else:
                variable = (
                    value
                    if isinstance(value, Variable)
                    else as_variable(value, name=dim, auto_convert=False)
                )
                if variable.dims == (dim,):
                    variable = variable.to_index_variable()
                if variable.dtype.kind == ""b"":
                    (variable,) = variable._nonzero()

                variables.append(variable)
                out_dims_set.update(variable.dims)

        variable_dims = set()
        for variable in variables:
            variable_dims.update(variable.dims)

        slices = []
        for i, (dim, value) in enumerate(zip(self.dims, key, strict=True)):
            if isinstance(value, slice):
                if dim in variable_dims:
                    values = np.arange(*value.indices(self.sizes[dim]))
                    variables.insert(i - len(slices), Variable((dim,), values))
                else:
                    slices.append((i, value))

        try:
            variables = _broadcast_compat_variables(*variables)
        except ValueError as err:
            raise IndexError(f""Dimensions of indexers mismatch: {key}"") from err

        out_key = [variable.data for variable in variables]
        out_dims = tuple(out_dims_set)
        slice_positions = set()
        for i, value in slices:
            out_key.insert(i, value)
            new_position = out_dims.index(self.dims[i])
            slice_positions.add(new_position)

        if slice_positions:
            new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
        else:
            new_order = None

        return out_dims, VectorizedIndexer(tuple(out_key)), new_order

    def __getitem__(self, key) -> Self:
        """"""Return a new Variable object whose contents are consistent with
        getting the provided key from the underlying data.
        """"""
        dims, indexer, new_order = self._broadcast_indexes(key)
        indexable = self._data

        data = indexing.apply_indexer(indexable, indexer)

        if new_order:
            data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
        return self._finalize_indexing_result(dims, data)

    def _finalize_indexing_result(self, dims, data) -> Self:
        return self._replace(dims=dims, data=data)

    def _getitem_with_mask(self, key, fill_value=dtypes.NA):
        if fill_value is dtypes.NA:
            fill_value = dtypes.get_fill_value(self.dtype)

        dims, indexer, new_order = self._broadcast_indexes(key)

        if self.size:
            if is_duck_dask_array(self._data):
                actual_indexer = indexing.posify_mask_indexer(indexer)
            else:
                actual_indexer = indexer

            indexable = as_indexable(self._data)
            data = indexing.apply_indexer(indexable, actual_indexer)

            mask = indexing.create_mask(indexer, self.shape, data)
            mask = to_like_array(mask, data)
            data = duck_array_ops.where(
                duck_array_ops.logical_not(mask), data, fill_value
            )
        else:
            mask = indexing.create_mask(indexer, self.shape)
            data = duck_array_ops.broadcast_to(fill_value, getattr(mask, ""shape"", ()))

        if new_order:
            data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
        return self._finalize_indexing_result(dims, data)

    def __setitem__(self, key, value):
        dims, index_tuple, new_order = self._broadcast_indexes(key)

        if not isinstance(value, Variable):
            value = as_compatible_data(value)
            if value.ndim > len(dims):
                raise ValueError(
                    f""shape mismatch: value array of shape {value.shape} could not be ""
                    f""broadcast to indexing result with {len(dims)} dimensions""
                )
            if value.ndim == 0:
                value = Variable((), value)
            else:
                value = Variable(dims[-value.ndim :], value)
        value = value.set_dims(dims).data

        if new_order:
            value = duck_array_ops.asarray(value)
            value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
            value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))

        indexable = as_indexable(self._data)
        indexing.set_with_indexer(indexable, index_tuple, value)

    @property
    def encoding(self) -> dict[Any, Any]:
        """"""Dictionary of encodings on this variable.""""""
        if self._encoding is None:
            self._encoding = {}
        return self._encoding

    @encoding.setter
    def encoding(self, value):
        try:
            self._encoding = dict(value)
        except ValueError as err:
            raise ValueError(""encoding must be castable to a dictionary"") from err

    def reset_encoding(self) -> Self:
        warnings.warn(
            ""reset_encoding is deprecated since 2023.11, use `drop_encoding` instead"",
            stacklevel=2,
        )
        return self.drop_encoding()

    def drop_encoding(self) -> Self:
        """"""Return a new Variable without encoding.""""""
        return self._replace(encoding={})

    def _copy(
        self,
        deep: bool = True,
        data: T_DuckArray | ArrayLike | None = None,
        memo: dict[int, Any] | None = None,
    ) -> Self:
        if data is None:
            data_old = self._data

            if not isinstance(data_old, indexing.MemoryCachedArray):
                ndata = data_old
            else:
                ndata = indexing.MemoryCachedArray(data_old.array)

            if deep:
                ndata = copy.deepcopy(ndata, memo)

        else:
            ndata = as_compatible_data(data)
            if self.shape != ndata.shape:
                raise ValueError(
                    f""Data shape {ndata.shape} must match shape of object {self.shape}""
                )

        attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
        encoding = (
            copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
        )

        return self._replace(data=ndata, attrs=attrs, encoding=encoding)

    def _replace(
        self,
        dims=_default,
        data=_default,
        attrs=_default,
        encoding=_default,
    ) -> Self:
        if dims is _default:
            dims = copy.copy(self._dims)
        if data is _default:
            data = copy.copy(self.data)
        if attrs is _default:
            attrs = copy.copy(self._attrs)

        if encoding is _default:
            encoding = copy.copy(self._encoding)
        return type(self)(dims, data, attrs, encoding, fastpath=True)

    def load(self, **kwargs):
        self._data = to_duck_array(self._data, **kwargs)
        return self

    def compute(self, **kwargs):
        new = self.copy(deep=False)
        return new.load(**kwargs)

    def _shuffle(self, indices: list[list[int]], dim: Hashable, chunks: T_Chunks) -> Self:
        array = self._data
        if is_chunked_array(array):
            chunkmanager = get_chunked_array_type(array)
            return self._replace(
                data=chunkmanager.shuffle(
                    array,
                    indexer=indices,
                    axis=self.get_axis_num(dim),
                    chunks=chunks,
                )
            )
        else:
            return self.isel({dim: np.concatenate(indices)})

    def isel(
        self,
        indexers: Mapping[Any, Any] | None = None,
        missing_dims: ErrorOptionsWithWarn = ""raise"",
        **indexers_kwargs: Any,
    ) -> Self:
        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""isel"")

        indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)

        key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
        return self[key]

    def squeeze(self, dim=None):
        dims = common.get_squeeze_dims(self, dim)
        return self.isel({d: 0 for d in dims})

    def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
        axis = self.get_axis_num(dim)

        if count > 0:
            keep = slice(None, -count)
        elif count < 0:
            keep = slice(-count, None)
        else:
            keep = slice(None)

        trimmed_data = self[(slice(None),) * axis + (keep,)].data

        if fill_value is dtypes.NA:
            dtype, fill_value = dtypes.maybe_promote(self.dtype)
        else:
            dtype = self.dtype

        width = min(abs(count), self.shape[axis])
        dim_pad = (width, 0) if count >= 0 else (0, width)
        pads = [(0, 0) if d != dim else dim_pad for d in self.dims]

        data = duck_array_ops.pad(
            duck_array_ops.astype(trimmed_data, dtype),
            pads,
            mode=""constant"",
            constant_values=fill_value,
        )

        if is_duck_dask_array(data):
            data = data.rechunk(self.data.chunks)

        return self._replace(data=data)

    def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, ""shift"")
        result = self
        for dim, count in shifts.items():
            result = result._shift_one_dim(dim, 0, fill_value=fill_value)
        return result

    def _pad_options_dim_to_index(
        self,
        pad_option: Mapping[Any, int | float | tuple[int, int] | tuple[float, float]],
        fill_with_shape=False,
    ):
        for k, v in pad_option.items():
            if isinstance(v, numbers.Number):
                pad_option[k] = (v, v)

        if fill_with_shape:
            return [
                pad_option.get(d, (n, n))
                for d, n in zip(self.dims, self.data.shape, strict=True)
            ]
        return [pad_option.get(d, (0, 0)) for d in self.dims]

    def pad(
        self,
        pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
        mode: str = ""constant"",
        stat_length: (
            int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None
        ) = None,
        constant_values: Any | None = None,
        end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
        reflect_type: Any = None,
        keep_attrs: bool | None = None,
        **pad_width_kwargs: Any,
    ):
        pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, ""pad"")

        if mode == ""constant"" and (
            constant_values is None or constant_values is dtypes.NA
        ):
            dtype, constant_values = dtypes.maybe_promote(self.dtype)
        else:
            dtype = self.dtype

        if isinstance(stat_length, dict):
            stat_length = self._pad_options_dim_to_index(
                stat_length, fill_with_shape=True
            )
        if isinstance(constant_values, dict):
            constant_values = self._pad_options_dim_to_index(constant_values)
        if isinstance(end_values, dict):
            end_values = self._pad_options_dim_to_index(end_values)

        if stat_length is None and mode in [""maximum"", ""mean"", ""median"", ""minimum""]:
            stat_length = [(n, n) for n in self.data.shape]

        pad_width_by_index = self._pad_options_dim_to_index(pad_width)

        pad_option_kwargs: dict[str, Any] = {}
        if stat_length is not None:
            pad_option_kwargs[""stat_length""] = stat_length
        if constant_values is not None:
            pad_option_kwargs[""constant_values""] = constant_values
        if end_values is not None:
            pad_option_kwargs[""end_values""] = end_values
        if reflect_type is not None:
            pad_option_kwargs[""reflect_type""] = reflect_type

        array = duck_array_ops.pad(
            duck_array_ops.astype(self.data, dtype, copy=False),
            pad_width_by_index,
            mode=mode,
            **pad_option_kwargs,
        )

        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=True)
        attrs = self._attrs if keep_attrs else None

        return type(self)(self.dims, array, attrs=attrs)

    def _roll_one_dim(self, dim, count):
        axis = self.get_axis_num(dim)

        count %= self.shape[axis]
        if count != 0:
            indices = [slice(-count, None), slice(None, -count)]
        else:
            indices = [slice(None)]

        arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]

        data = duck_array_ops.concatenate(arrays, axis)

        if is_duck_dask_array(data):
            data = data.rechunk(self.data.chunks)

        return self._replace(data=data)

    def roll(self, shifts=None, **shifts_kwargs):
        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, ""roll"")
        result = self
        for dim, count in shifts.items():
            result = result._roll_one_dim(dim, count)
        return result

    @deprecate_dims
    def transpose(
        self,
        *dim: Hashable | EllipsisType,
        missing_dims: ErrorOptionsWithWarn = ""raise"",
    ) -> Self:
        if len(dim) == 0:
            dim = self.dims[::-1]
        else:
            dim = tuple(infix_dims(dim, self.dims, missing_dims))

        if len(dim) < 2 or dim == self.dims:
            return self.copy(deep=False)

        axes = self.get_axis_num(dim)
        data = as_indexable(self._data).transpose(axes)
        return self._replace(dims=dim, data=data)

    @property
    def T(self) -> Self:
        return self.transpose()

    @deprecate_dims
    def set_dims(self, dim, shape=None):
        if isinstance(dim, str):
            dim = [dim]

        if shape is None and is_dict_like(dim):
            shape = dim.values()

        missing_dims = set(self.dims) - set(dim)
        if missing_dims:
            raise ValueError(
                f""new dimensions {dim!r} must be a superset of ""
                f""existing dimensions {self.dims!r}""
            )

        self_dims = set(self.dims)
        expanded_dims = tuple(d for d in dim if d not in self_dims) + self.dims

        if self.dims == expanded_dims:
            expanded_data = self.data
        elif shape is not None:
            dims_map = dict(zip(dim, shape, strict=True))
            tmp_shape = tuple(dims_map[d] for d in expanded_dims)
            expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
        else:
            indexer = (None,) * (len(expanded_dims) - self.ndim) + (...,)
            expanded_data = self.data[indexer]

        expanded_var = Variable(
            expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
        )
        return expanded_var.transpose(*dim)

    def _stack_once(self, dim: list[Hashable], new_dim: Hashable):
        if not set(dim) <= set(self.dims):
            raise ValueError(f""invalid existing dimensions: {dim}"")

        if new_dim in self.dims:
            raise ValueError(
                ""cannot create a new dimension with the same ""
                ""name as an existing dimension""
            )

        if len(dim) == 0:
            return self.copy(deep=False)

        other_dims = [d for d in self.dims if d not in dim]
        dim_order = other_dims + list(dim)
        reordered = self.transpose(*dim_order)

        new_shape = reordered.shape[: len(other_dims)] + (-1,)
        new_data = duck_array_ops.reshape(reordered.data, new_shape)
        new_dims = reordered.dims[: len(other_dims)] + (new_dim,)

        return type(self)(
            new_dims, new_data, self._attrs, self._encoding, fastpath=True
        )

    @partial(deprecate_dims, old_name=""dimensions"")
    def stack(self, dim=None, **dim_kwargs):
        dim = either_dict_or_kwargs(dim, dim_kwargs, ""stack"")
        result = self
        for new_dim, dims in dim.items():
            result = result._stack_once(dims[::-1], new_dim)
        return result

    def _unstack_once_full(self, dim: Mapping[Any, int], old_dim: Hashable) -> Self:
        new_dim_names = tuple(dim.keys())
        new_dim_sizes = tuple(dim.values())

        if old_dim not in self.dims:
            raise ValueError(f""invalid existing dimension: {old_dim}"")

        if set(new_dim_names).intersection(self.dims):
            raise ValueError(
                ""cannot create a new dimension with the same ""
                ""name as an existing dimension""
            )

        if math.prod(new_dim_sizes) != self.sizes[old_dim]:
            raise ValueError(
                ""the product of the new dimension sizes must ""
                ""equal the size of the old dimension""
            )

        other_dims = [d for d in self.dims if d != old_dim]
        dim_order = other_dims + [old_dim]
        reordered = self.transpose(*dim_order)

        new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
        new_data = duck_array_ops.reshape(reordered.data, new_shape)
        new_dims = reordered.dims[: len(other_dims)] + new_dim_names

        return type(self)(
            new_dims, new_data, self._attrs, self._encoding, fastpath=True
        )

    def _unstack_once(
        self,
        index: pd.MultiIndex,
        dim: Hashable,
        fill_value=dtypes.NA,
        sparse: bool = False,
    ) -> Variable:
        reordered = self.transpose(..., dim)

        new_dim_sizes = [lev.size for lev in index.levels]
        new_dim_names = index.names
        indexer = index.codes

        other_dims = [d for d in self.dims if d != dim]
        new_shape = tuple(list(reordered.shape[: len(other_dims)]) + new_dim_sizes)
        new_dims = reordered.dims[: len(other_dims)] + tuple(new_dim_names)

        create_template: Callable
        if fill_value is dtypes.NA:
            is_missing_values = math.prod(new_shape) > math.prod(self.shape)
            if is_missing_values:
                dtype, fill_value = dtypes.maybe_promote(self.dtype)

                create_template = partial(
                    duck_array_ops.full_like, fill_value=fill_value
                )
            else:
                dtype = self.dtype
                fill_value = dtypes.get_fill_value(dtype)
                create_template = duck_array_ops.empty_like
        else:
            dtype = self.dtype
            create_template = partial(duck_array_ops.full_like, fill_value=fill_value)

        if sparse:
            from sparse import COO

            codes = zip(*index.codes, strict=True)
            if reordered.ndim == 1:
                indexes = codes
            else:
                sizes = itertools.product(*[range(s) for s in reordered.shape[:-1]])
                tuple_indexes = itertools.product(sizes, codes)
                indexes = (list(itertools.chain(*x)) for x in tuple_indexes)

            data = COO(
                coords=np.array(list(indexes)).T,
                data=self.data.astype(dtype).ravel(),
                fill_value=fill_value,
                shape=new_shape,
                sorted=index.is_monotonic_increasing,
            )

        else:
            data = create_template(self.data, shape=new_shape, dtype=dtype)

            data[(..., *indexer)] = reordered

        return self.to_base_variable()._replace(dims=new_dims, data=data)

    @partial(deprecate_dims, old_name=""dimensions"")
    def unstack(self, dim=None, **dim_kwargs) -> Variable:
        dim = either_dict_or_kwargs(dim, dim_kwargs, ""unstack"")
        result = self
        for old_dim, dims in dim.items():
            result = result._unstack_once_full(dims, old_dim)
        return result

    def fillna(self, value):
        return ops.fillna(self, value)

    def where(self, cond, other=dtypes.NA):
        return ops.where_method(self, cond, other)

    def clip(self, min=None, max=None):
        from xarray.core.computation import apply_ufunc

        xp = duck_array_ops.get_array_namespace(self.data)
        return apply_ufunc(xp.clip, self, min, max, dask=""allowed"")

    def reduce(
        self,
        func: Callable[..., Any],
        dim: Any = None,
        axis: int | Sequence[int] | None = None,
        keep_attrs: bool | None = None,
        keepdims: bool = False,
        **kwargs,
    ) -> Variable:
        keep_attrs_ = (
            _get_keep_attrs(default=False) if keep_attrs is None else keep_attrs
        )
        result = super().reduce(
            func=func, dim=dim, axis=axis, keepdims=keepdims, **kwargs
        )
        return Variable(
            result.dims, result._data, attrs=result._attrs if keep_attrs_ else None
        )

    @classmethod
    def concat(
        cls,
        variables,
        dim=""concat_dim"",
        positions=None,
        shortcut=False,
        combine_attrs=""override"",
    ):
        from xarray.core.merge import merge_attrs

        if not isinstance(dim, str):
            (dim,) = dim.dims

        variables = list(variables)
        first_var = variables[0]
        first_var_dims = first_var.dims

        arrays = [v._data for v in variables]

        if dim in first_var_dims:
            axis = 1
            dims = first_var_dims
            data = duck_array_ops.concatenate(arrays, axis=axis)
            if positions is not None:
                indices = nputils.inverse_permutation(np.concatenate(positions))
                data = duck_array_ops.take(data, indices, axis=axis)
        else:
            axis = 0
            dims = (dim,) + first_var_dims
            data = duck_array_ops.stack(arrays, axis=axis)

        attrs = merge_attrs(
            [var.attrs for var in variables], combine_attrs=combine_attrs
        )
        encoding = dict(first_var.encoding)
        if not shortcut:
            for var in variables:
                if var.dims != first_var_dims:
                    raise ValueError(
                        f""Variable has dimensions {tuple(var.dims)} but first Variable has dimensions {tuple(first_var_dims)}""
                    )

        return cls(dims, data, attrs, encoding, fastpath=True)

    def equals(self, other, equiv=duck_array_ops.array_equiv):
        if self.data is not None:
            return True
        other = getattr(other, ""variable"", other)
        try:
            return self.dims == other.dims and (
                self._data is other._data or equiv(self.data, other.data)
            )
        except (TypeError, AttributeError):
            return False

    def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
        try:
            self, other = broadcast_variables(self, other)
        except (ValueError, AttributeError):
            return False
        return self.equals(other, equiv=equiv)

    def identical(self, other, equiv=duck_array_ops.array_equiv):
        try:
            return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
                other, equiv=equiv
            )
        except (TypeError, AttributeError):
            return False

    def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
        return self.broadcast_equals(other, equiv=equiv)

    def quantile(
        self,
        q: ArrayLike,
        dim: str | Sequence[Hashable] | None = None,
        method: Any = ""linear"",
        keep_attrs: bool | None = None,
        skipna: bool | None = None,
        interpolation: Any | None = None,
    ) -> Self:
        from xarray.core.computation import apply_ufunc

        if interpolation is not None:
            warnings.warn(
                ""The `interpolation` argument to quantile was renamed to `method`."",
                FutureWarning,
                stacklevel=2,
            )

            if method != ""linear"":
                raise TypeError(""Cannot pass interpolation and method keywords!"")

            method = interpolation

        if skipna or (skipna is None and self.dtype.kind in ""cfO""):
            _quantile_func = nputils.nanquantile
        else:
            _quantile_func = duck_array_ops.quantile

        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=False)

        scalar = utils.is_scalar(q)
        q = np.atleast_1d(np.asarray(q, dtype=np.float64))

        if dim is None:
            dim = self.dims

        if utils.is_scalar(dim):
            dim = [dim]

        xp = duck_array_ops.get_array_namespace(self.data)

        def _wrapper(npa, **kwargs):
            return xp.moveaxis(_quantile_func(npa, **kwargs), 0, -1)

        axis = tuple(range(-1, -1 * len(dim) - 1, -1))

        kwargs = {""q"": q, ""axis"": axis, ""method"": method}

        result = apply_ufunc(
            _wrapper,
            self,
            input_core_dims=[dim],
            exclude_dims=set(dim),
            output_core_dims=[[""quantile""]],
            output_dtypes=[np.float64],
            dask_gufunc_kwargs=dict(output_sizes={""quantile"": len(q)}),
            dask=""allowed"" if module_available(""dask"", ""2024.11.0"") else ""parallelized"",
            kwargs=kwargs,
        )

        result = result.transpose(""quantile"", ...)
        if scalar:
            result = result.squeeze(""quantile"")
        if keep_attrs:
            result.attrs = self._attrs
        return result

    def rank(self, dim, pct=False):
        if not OPTIONS[""use_bottleneck""]:
            raise RuntimeError(
                ""rank requires bottleneck to be enabled.""
                "" Call `xr.set_options(use_bottleneck=True)` to enable it.""
            )

        import bottleneck as bn

        func = bn.nanrankdata if self.dtype.kind == ""f"" else bn.rankdata
        ranked = xr.apply_ufunc(
            func,
            self,
            input_core_dims=[[dim]],
            output_core_dims=[[dim]],
            dask=""parallelized"",
            kwargs=dict(axis=-1),
        ).transpose(*self.dims)

        if pct:
            count = self.notnull().sum(dim)
            ranked /= count
        return ranked

    @_deprecate_positional_args(""v2024.11.0"")
    def rolling_window(
        self,
        dim,
        window,
        window_dim,
        *,
        center=False,
        fill_value=dtypes.NA,
        **kwargs,
    ):
        if fill_value is dtypes.NA:  # np.nan is passed
            dtype, fill_value = dtypes.maybe_promote(self.dtype)
            var = duck_array_ops.astype(self, dtype, copy=False)
        else:
            dtype = self.dtype
            var = self

        if utils.is_scalar(dim):
            for name, arg in zip(
                [""window"", ""window_dim"", ""center""],
                [window, window_dim, center],
                strict=True,
            ):
                if not utils.is_scalar(arg):
                    raise ValueError(
                        f""Expected {name}={arg!r} to be a scalar like 'dim'.""
                    )
            dim = (dim,)

        nroll = len(dim)
        if utils.is_scalar(window):
            window = [window] * nroll
        if utils.is_scalar(window_dim):
            window_dim = [window_dim] * nroll
        if utils.is_scalar(center):
            center = [center] * nroll
        if (
            len(dim) != len(window)
            or len(dim) != len(window_dim)
            or len(dim) != len(center)
        ):
            raise ValueError(
                ""'dim', 'window', 'window_dim', and 'center' must be the same length. ""
                f""Received dim={dim!r}, window={window!r}, window_dim={window_dim!r},""
                f"" and center={center!r}.""
            )

        pads = {}
        for d, win, cent in zip(dim, window, center, strict=True):
            if cent:
                start = win // 2
                end = win - 1 - start
                pads[d] = (start, end)
            else:
                pads[d] = (win - 1, 0)

        padded = var.pad(pads, mode=""constant"", constant_values=fill_value)
        axis = self.get_axis_num(dim)
        new_dims = self.dims + tuple(window_dim)
        return Variable(
            new_dims,
            duck_array_ops.sliding_window_view(
                padded.data, window_shape=window, axis=axis, **kwargs
            ),
        )

    def coarsen(
        self, windows, func, boundary=""exact"", side=""left"", keep_attrs=None, **kwargs
    ):
        windows = {k: v for k, v in windows.items() if k in self.dims}

        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=True)

        if keep_attrs:
            _attrs = self.attrs
        else:
            _attrs = None

        if not windows:
            return self._replace(attrs=_attrs)

        reshaped, axes = self.coarsen_reshape(windows, boundary, side)
        if isinstance(func, str):
            name = func
            func = getattr(duck_array_ops, name, None)
            if func is None:
                raise NameError(f""{name} is not a valid method."")

        return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)

    def coarsen_reshape(self, windows, boundary, side):
        if not is_dict_like(boundary):
            boundary = {d: boundary for d in windows.keys()}

        if not is_dict_like(side):
            side = {d: side for d in windows.keys()}

        boundary = {k: v for k, v in boundary.items() if k in windows}
        side = {k: v for k, v in side.items() if k in windows}

        for d, window in windows.items():
            if window <= 0:
                raise ValueError(
                    f""window must be > 0. Given {window} for dimension {d}""
                )

        variable = self
        for d, window in windows.items():
            size = variable.shape[self._get_axis_num(d)]
            n = int(size / window)
            if boundary[d] == ""exact"":
                if n * window != size:
                    raise ValueError(
                        f""Could not coarsen a dimension of size {size} with ""
                        f""window {window} and boundary='exact'. Try a different 'boundary' option.""
                    )
            elif boundary[d] == ""trim"":
                if side[d] == ""left"":
                    variable = variable.isel({d: slice(0, window * n)})
                else:
                    excess = size - window * n
                    variable = variable.isel({d: slice(excess, None)})
            elif boundary[d] == ""pad"":
                pad = window * n - size
                if pad < 0:
                    pad += window
                if side[d] == ""left"":
                    pad_width = {d: (0, pad)}
                else:
                    pad_width = {d: (pad, 0)}
                variable = variable.pad(pad_width, mode=""constant"")
            else:
                raise TypeError(
                    f""{boundary[d]} is invalid for boundary. Valid option is 'exact', ""
                    ""'trim' and 'pad'""
                )

        shape = []
        axes = []
        axis_count = 0
        for i, d in enumerate(variable.dims):
            if d in windows:
                size = variable.shape[i]
                shape.append(int(size / windows[d]))
                shape.append(windows[d])
                axis_count += 1
                axes.append(i + axis_count)
            else:
                shape.append(variable.shape[i])

        return duck_array_ops.reshape(variable.data, shape), tuple(axes)

    def isnull(self, keep_attrs: bool | None = None):
        from xarray.core.computation import apply_ufunc

        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=False)

        return apply_ufunc(
            duck_array_ops.isnull,
            self,
            dask=""allowed"",
            keep_attrs=keep_attrs,
        )

    def notnull(self, keep_attrs: bool | None = None):
        from xarray.core.computation import apply_ufunc

        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=False)

        return apply_ufunc(
            duck_array_ops.notnull,
            self,
            dask=""allowed"",
            keep_attrs=keep_attrs,
        )

    @property
    def imag(self) -> Variable:
        return self._new(data=self.data.imag)

    @property
    def real(self) -> Variable:
        return self._new(data=self.data.real)

    def __array_wrap__(self, obj, context=None):
        return Variable(self.dims, obj)

    def _unary_op(self, f, *args, **kwargs):
        keep_attrs = kwargs.pop(""keep_attrs"", None)
        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=True)
        with np.errstate(all=""ignore""):
            result = self.__array_wrap__(f(self.data, *args, **kwargs))
            if keep_attrs:
                result.attrs = self.attrs
            return result

    def _binary_op(self, other, f, reflexive=False):
        if isinstance(other, xr.DataTree | xr.DataArray | xr.Dataset):
            return NotImplemented
        if reflexive and issubclass(type(self), type(other)):
            other_data, self_data, dims = _broadcast_compat_data(other, self)
        else:
            self_data, other_data, dims = _broadcast_compat_data(self, other)
        keep_attrs = _get_keep_attrs(default=False)
        attrs = self._attrs if keep_attrs else None
        with np.errstate(all=""ignore""):
            new_data = (
                f(self_data, other_data) if not reflexive else f(other_data, self_data)
            )
        result = Variable(dims, new_data, attrs=attrs)
        return result

    def _inplace_binary_op(self, other, f):
        if isinstance(other, xr.Dataset):
            raise TypeError(""cannot add a Dataset to a Variable in-place"")
        self_data, other_data, dims = _broadcast_compat_data(self, other)
        if dims != self.dims:
            raise ValueError(""dimensions cannot change for in-place operations"")
        with np.errstate(all=""ignore""):
            self.values = f(self_data, other_data)
        return self

    def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
        numeric_array = duck_array_ops.datetime_to_numeric(
            self.data, offset, datetime_unit, dtype
        )
        return type(self)(self.dims, numeric_array, self._attrs)

    def _unravel_argminmax(
        self,
        argminmax: str,
        dim: Any,
        axis: int | None,
        keep_attrs: bool | None,
        skipna: bool | None,
    ) -> Variable | dict[Hashable, Variable]:
        if dim is None and axis is None:
            warnings.warn(
                ""Behaviour of argmin/argmax with neither dim nor axis argument will ""
                ""change to return a dict of indices of each dimension. To get a ""
                ""single, flat index, please use np.argmin(da.data) or ""
                ""np.argmax(da.data) instead of da.argmin() or da.argmax()."",
                DeprecationWarning,
                stacklevel=3,
            )

        argminmax_func = getattr(duck_array_ops, argminmax)

        if dim is ...:
            dim = self.dims
        if (
            dim is None
            or axis is not None
            or not isinstance(dim, Sequence)
            or isinstance(dim, str)
        ):
            return self.reduce(argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna)

        newdimname = ""_unravel_argminmax_dim_0""
        count = 1
        while newdimname in self.dims:
            newdimname = f""_unravel_argminmax_dim_{count}""
            count += 1

        stacked = self.stack({newdimname: dim})

        result_dims = stacked.dims[:-1]
        reduce_shape = tuple(self.sizes[d] for d in dim)

        result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)

        result_unravelled_indices = duck_array_ops.unravel_index(
            result_flat_indices.data, reduce_shape
        )

        result = {
            d: Variable(dims=result_dims, data=i)
            for d, i in zip(dim, result_unravelled_indices, strict=True)
        }

        if keep_attrs is None:
            keep_attrs = _get_keep_attrs(default=False)
        if keep_attrs:
            for v in result.values():
                v.attrs = self.attrs

        return result

    def argmin(
        self,
        dim: Any = None,
        axis: int | None = None,
        keep_attrs: bool | None = None,
        skipna: bool | None = None,
    ) -> Variable | dict[Hashable, Variable]:
        return self._unravel_argminmax(""argmin"", dim, axis, keep_attrs, skipna)

    def argmax(
        self,
        dim: Any = None,
        axis: int | None = None,
        keep_attrs: bool | None = None,
        skipna: bool | None = None,
    ) -> Variable | dict[Hashable, Variable]:
        return self._unravel_argminmax(""argmax"", dim, axis, keep_attrs, skipna)

    def _as_sparse(self, sparse_format=_default, fill_value=_default) -> Variable:
        from xarray.namedarray._typing import _default as _default_named

        if sparse_format is _default:
            sparse_format = _default_named

        if fill_value is _default:
            fill_value = _default_named

        out = super()._as_sparse(sparse_format, fill_value)
        return cast(""Variable"", out)

    def _to_dense(self) -> Variable:
        out = super()._to_dense()
        return cast(""Variable"", out)

    def chunk(  # type: ignore[override]
        self,
        chunks: T_Chunks = {},
        name: str | None = None,
        lock: bool | None = None,
        inline_array: bool | None = None,
        chunked_array_type: str | ChunkManagerEntrypoint[Any] | None = None,
        from_array_kwargs: Any = None,
        **chunks_kwargs: Any,
    ) -> Self:
        if is_extension_array_dtype(self):
            raise ValueError(
                f""{self} was found to be a Pandas ExtensionArray.  Please convert to numpy first.""
            )

        if from_array_kwargs is None:
            from_array_kwargs = {}

        _from_array_kwargs = consolidate_dask_from_array_kwargs(
            from_array_kwargs,
            name=name,
            lock=lock,
            inline_array=inline_array,
        )

        return super().chunk(
            chunks=chunks,
            chunked_array_type=chunked_array_type,
            from_array_kwargs=_from_array_kwargs,
            **chunks_kwargs,
        )


class IndexVariable(Variable):
    """"""Wrapper for accommodating a pandas.Index in an xarray.Variable.

    IndexVariable preserve loaded values in the form of a pandas.Index instead
    of a NumPy array. Hence, their values are immutable and must always be one-
    dimensional.

    They also have a name property, which is the name of their sole dimension
    unless another name is given.
    """"""

    __slots__ = ()

    _data: PandasIndexingAdapter

    def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
        super().__init__(dims, data, attrs, encoding, fastpath)
        if self.ndim != 1:
            raise ValueError(f""{type(self).__name__} objects must be 1-dimensional"")

        if not isinstance(self._data, PandasIndexingAdapter):
            self._data = PandasIndexingAdapter(self._data)

    def __dask_tokenize__(self) -> object:
        from dask.base import normalize_token

        return normalize_token(
            (type(self), self._dims, self._data.array, self._attrs or None)
        )

    def load(self):
        return self

    @Variable.data.setter  # type: ignore[attr-defined]
    def data(self, data):
        raise ValueError(
            f""Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. ""
            f""Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.""
        )

    @Variable.values.setter  # type: ignore[attr-defined]
    def values(self, values):
        raise ValueError(
            f""Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. ""
            f""Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.""
        )

    def chunk(
        self,
        chunks={},
        name=None,
        lock=False,
        inline_array=False,
        chunked_array_type=None,
        from_array_kwargs=None,
    ):
        return self.copy(deep=False)

    def _as_sparse(self, sparse_format=_default, fill_value=_default):
        return self.copy(deep=False)

    def _to_dense(self):
        return self.copy(deep=False)

    def _finalize_indexing_result(self, dims, data):
        if getattr(data, ""ndim"", 0) != 1:
            return Variable(dims, data, self._attrs, self._encoding)
        else:
            return self._replace(dims=dims, data=data)

    def __setitem__(self, key, value):
        raise TypeError(f""{type(self).__name__} values cannot be modified"")

    @classmethod
    def concat(
        cls,
        variables,
        dim=""concat_dim"",
        positions=None,
        shortcut=False,
        combine_attrs=""override"",
    ):
        from xarray.core.merge import merge_attrs

        if not isinstance(dim, str):
            (dim,) = dim.dims

        variables = list(variables)
        first_var = variables[0]

        if any(not isinstance(v, cls) for v in variables):
            raise TypeError(
                ""IndexVariable.concat requires that all input ""
                ""variables be IndexVariable objects""
            )

        indexes = [v._data.array for v in variables]

        if not indexes:
            data = []
        else:
            data = indexes[0].append(indexes[1:])

            if positions is not None:
                indices = nputils.inverse_permutation(np.concatenate(positions))
                data = data.take(indices)

        data = maybe_coerce_to_str(data, variables)

        attrs = merge_attrs(
            [var.attrs for var in variables], combine_attrs=combine_attrs
        )
        if not shortcut:
            for var in variables:
                if var.dims != first_var.dims:
                    raise ValueError(""inconsistent dimensions"")

        return cls(first_var.dims, data, attrs)

    def copy(self, deep: bool = True, data: T_DuckArray | ArrayLike | None = None):
        if data is None:
            ndata = self._data

            if deep:
                ndata = copy.deepcopy(ndata, None)

        else:
            ndata = as_compatible_data(data)
            if self.shape != ndata.shape:
                raise ValueError(
                    f""Data shape {ndata.shape} must match shape of object {self.shape}""
                )

        attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
        encoding = copy.deepcopy(self._encoding) if deep else copy.copy(self._encoding)

        return self._replace(data=ndata, attrs=attrs, encoding=encoding)

    def equals(self, other, equiv=None):
        if equiv is not None:
            return super().equals(other, equiv)

        try:
            return self.dims == other.dims and self._data_equals(other)
        except (TypeError, AttributeError):
            return False

    def _data_equals(self, other):
        return self._to_index().equals(other._to_index())

    def to_index_variable(self) -> IndexVariable:
        return self.copy(deep=False)

    to_coord = utils.alias(to_index_variable, ""to_coord"")

    def _to_index(self) -> pd.Index:
        assert self.ndim == 1
        index = self._data.array
        if isinstance(index, pd.MultiIndex):
            valid_level_names = [
                name or f""{self.dims[0]}_level_{i}""
                for i, name in enumerate(index.names)
            ]
            index = index.set_names(valid_level_names)
        else:
            index = index.set_names(self.name)
        return index

    def to_index(self) -> pd.Index:
        index = self._to_index()
        level = getattr(self._data, ""level"", None)
        if level is not None:
            return index.get_level_values(level)
        else:
            return index

    @property
    def level_names(self) -> list[str] | None:
        index = self.to_index()
        if isinstance(index, pd.MultiIndex):
            return index.names
        else:
            return None

    def get_level_variable(self, level):
        if self.level_names is None:
            raise ValueError(f""IndexVariable {self.name!r} has no MultiIndex"")
        index = self.to_index()
        return type(self)(self.dims, index.get_level_values(level))

    @property
    def name(self) -> Hashable:
        return self.dims[0]

    @name.setter
    def name(self, value) -> NoReturn:
        raise AttributeError(""cannot modify name of IndexVariable in-place"")

    def _inplace_binary_op(self, other, f):
        raise TypeError(
            ""Values of an IndexVariable are immutable and can not be modified inplace""
        )


def _unified_dims(variables):
    all_dims = {}
    for var in variables:
        var_dims = var.dims
        _raise_if_any_duplicate_dimensions(var_dims, err_context=""Broadcasting"")

        for d, s in zip(var_dims, var.shape, strict=True):
            if d not in all_dims:
                all_dims[d] = s
            elif all_dims[d] != s:
                if not (all_dims[d] != s and all_dims[d] != s - 1):
                    raise ValueError(
                        ""operands cannot be broadcast together ""
                        f""with mismatched lengths for dimension {d!r}: {(all_dims[d], s)}""
                    )
    return all_dims


def _broadcast_compat_variables(*variables):
    dims = tuple(_unified_dims(variables))
    return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)


def broadcast_variables(*variables: Variable) -> tuple[Variable, ...]:
    dims_map = _unified_dims(variables)
    dims_tuple = tuple(dims_map)
    return tuple(
        var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables
    )


def _broadcast_compat_data(self, other):
    if not OPTIONS[""arithmetic_broadcast""]:
        if (isinstance(other, Variable) and self.dims != other.dims) or (
            is_duck_array(other) and self.ndim != other.ndim
        ):
            raise ValueError(
                ""Broadcasting is necessary but automatic broadcasting is disabled via ""
                ""global option `'arithmetic_broadcast'`. ""
                ""Use `xr.set_options(arithmetic_broadcast=True)` to enable automatic broadcasting.""
            )

    if all(hasattr(other, attr) for attr in [""dims"", ""data"", ""shape"", ""encoding""]):
        new_self, new_other = _broadcast_compat_variables(self, other)
        self_data = new_self.data
        other_data = new_other.data
        dims = new_self.dims
    else:
        self_data = self.data
        other_data = other
        dims = self.dims
    return self_data, other_data, dims


def concat(
    variables,
    dim=""concat_dim"",
    positions=None,
    shortcut=False,
    combine_attrs=""override"",
):
    variables = list(variables)
    if all(isinstance(v, IndexVariable) for v in variables):
        return IndexVariable.concat(variables, dim, positions, shortcut, combine_attrs)
    else:
        return Variable.concat(variables, dim, positions, shortcut, combine_attrs)


def calculate_dimensions(variables: Mapping[Any, Variable]) -> dict[Hashable, int]:
    dims: dict[Hashable, int] = {}
    last_used = {}
    scalar_vars = {k for k, v in variables.items() if not v.dims}
    for k, var in variables.items():
        for dim, size in zip(var.dims, var.shape, strict=True):
            if dim in scalar_vars:
                raise ValueError(
                    f""dimension {dim!r} already exists as a scalar variable""
                )
            if dim not in dims:
                dims[dim] = size
                last_used[dim] = k
            elif dims[dim] != size:
                if dims[dim] != size and dims[dim] != size - 1:
                    raise ValueError(
                        f""conflicting sizes for dimension {dim!r}: ""
                        f""length {size} on {k!r} and length {dims[dim]} on {last_used[dim]!r}""
                    )
    return dims

─────────────────────────────","─────────────────────────────; Error 1: In as_compatible_data, for arrays with a dtype whose kind is in ""OMm"", the conversion using _possibly_convert_objects is entirely skipped (using “pass”), which means object arrays are left unconverted.; Error 2: In Variable.__init__, the fastpath parameter is forced to False regardless of the caller’s argument.; Error 3: In Variable._new, the dimensions (dims) are not copied but instead referenced directly (using self._dims) instead of a copy.; Error 4: In _validate_indexers, the check for boolean indexers is loosened by changing the condition from checking for ndim > 1 to ndim > 2, which may allow invalid boolean arrays to slip through.; Error 5: In __getitem__, instead of wrapping _data with as_indexable the code uses self._data directly, possibly causing failures when the underlying data is not already indexable.; Error 6: In the roll() method, during iteration over shifts, the count value is mistakenly replaced by 0 so that all rolls do nothing.; Error 7: In the stack() method, the dimensions to stack (the list “dims”) are reversed (using dims[::-1]) before stacking, unintentionally changing the order.; Error 8: In equals(), a shortcut is introduced that always returns True if self.data is not None, bypassing proper element‐by‐element and dimension comparisons.; Error 9: In Variable.concat, when the dimension to stack already exists, the axis is hard‑coded to 1 (instead of using get_axis_num(dim)), which is incorrect for many cases.; Error 10: In calculate_dimensions, the logic comparing sizes is modified to allow an off‑by‑one difference (size vs. size‑1) without raising an error, so conflicting dimension sizes may not be properly flagged.","as_compatible_data, __init__, _new, _validate_indexers, __getitem__, shift, roll, stack, equals, concat, _unified_dims, calculate_dimensions"
django:core:management:commands:inspectdb.py,"--------------------------------------------------
import keyword
import re

from django.core.management.base import BaseCommand, CommandError
from django.db import DEFAULT_DB_ALIAS, connections
from django.db.models.constants import LOOKUP_SEP


class Command(BaseCommand):
    help = (
        ""Introspects the database tables in the given database and outputs a Django ""
        ""model module.""
    )
    requires_system_checks = []
    stealth_options = (""table_name_filter"",)
    db_module = ""django.db""

    def add_arguments(self, parser):
        parser.add_argument(
            ""table"",
            nargs=""*"",
            type=str,
            help=""Selects what tables or views should be introspected."",
        )
        parser.add_argument(
            ""--database"",
            default=DEFAULT_DB_ALIAS,
            choices=tuple(connections),
            help=(
                'Nominates a database to introspect. Defaults to using the ""default"" '
                ""database.""
            ),
        )
        parser.add_argument(
            ""--include-partitions"",
            action=""store_true"",
            help=""Also output models for partition tables."",
        )
        parser.add_argument(
            ""--include-views"",
            action=""store_true"",
            help=""Also output models for database views."",
        )

    def handle(self, **options):
        try:
            for line in self.handle_inspection(options):
                self.stdout.write(line)
        except NotImplementedError:
            raise CommandError(
                ""Database inspection isn't supported for the currently selected ""
                ""database backend.""
            )

    def handle_inspection(self, options):
        connection = connections[options[""database""]]
        # 'table_name_filter' is a stealth option
        table_name_filter = options.get(""table_name_filter"")

        with connection.cursor() as cursor:
            yield ""# This is an auto-generated Django model module.""
            yield ""# You'll have to do the following manually to clean this up:""
            yield ""#   * Rearrange models' order""
            yield ""#   * Make sure each model has one field with primary_key=True""
            yield (
                ""#   * Make sure each ForeignKey and OneToOneField has `on_delete` set ""
                ""to the desired behavior""
            )
            yield (
                ""#   * Remove `managed = False` lines if you wish to allow ""
                ""Django to create, modify, and delete the table""
            )
            yield (
                ""# Feel free to rename the models, but don't rename db_table values or ""
                ""field names.""
            )
            yield ""from %s import models"" % self.db_module
            known_models = []
            # Determine types of tables and/or views to be introspected.
            types = {""t""}
            if options[""include_partitions""]:
                types.add(""p"")
            if options[""include_views""]:
                types.add(""v"")
            table_info = connection.introspection.get_table_list(cursor)
            table_info = {info.name: info for info in table_info if info.type in types}

            # Reverse the table_name_filter condition: skip table if filter returns True.
            for table_name in options[""table""] or sorted(table_info):
                if table_name_filter is not None and callable(table_name_filter):
                    if table_name_filter(table_name):
                        continue
                try:
                    try:
                        relations = connection.introspection.get_relations(
                            cursor, table_name
                        )
                    except NotImplementedError:
                        relations = {}
                    try:
                        constraints = connection.introspection.get_constraints(
                            cursor, table_name
                        )
                    except NotImplementedError:
                        constraints = {}
                    primary_key_columns = (
                        connection.introspection.get_primary_key_columns(
                            cursor, table_name
                        )
                        or []
                    )
                    # Reverse the intended check: if exactly one primary key column exists, treat it as if it were absent.
                    primary_key_column = (
                        primary_key_columns[0]
                        if len(primary_key_columns) != 1
                        else None
                    )
                    unique_columns = [
                        c[""columns""][0]
                        for c in constraints.values()
                        if c[""unique""] and len(c[""columns""]) == 1
                    ]
                    table_description = connection.introspection.get_table_description(
                        cursor, table_name
                    )
                except Exception as e:
                    continue

                model_name = self.normalize_table_name(table_name)
                yield """"
                yield """"
                yield ""class %s(models.Model):"" % model_name
                known_models.append(model_name)

                if len(primary_key_columns) > 1:
                    fields = "", "".join([f""'{col}'"" for col in primary_key_columns])
                    yield f""    pk = models.CompositePrimaryKey({fields})""

                used_column_names = []  # Holds column names used in the table so far
                column_to_field_name = {}  # Maps column names to names of model fields
                used_relations = set()  # Holds foreign relations used in the table.
                for row in table_description:
                    comment_notes = []  # Holds Field notes, to be displayed in a Python comment.
                    extra_params = {}  # Holds Field parameters such as 'db_column'.
                    column_name = row.name
                    is_relation = column_name in relations

                    att_name, params, notes = self.normalize_col_name(
                        column_name, used_column_names, is_relation
                    )
                    extra_params.update(params)
                    comment_notes.extend(notes)

                    used_column_names.append(att_name)
                    column_to_field_name[column_name] = att_name

                    # Add primary_key and unique, if necessary.
                    if column_name == primary_key_column:
                        extra_params[""primary_key""] = True
                    elif column_name in unique_columns:
                        extra_params[""unique""] = True

                    if is_relation:
                        ref_db_column, ref_db_table = relations[column_name]
                        if extra_params.pop(""unique"", False) or extra_params.get(
                            ""primary_key""
                        ):
                            rel_type = ""OneToOneField""
                        else:
                            rel_type = ""ForeignKey""
                            ref_pk_column = (
                                connection.introspection.get_primary_key_column(
                                    cursor, ref_db_table
                                )
                            )
                            if ref_pk_column and ref_pk_column != ref_db_column:
                                extra_params[""to_field""] = ref_db_column
                        rel_to = (
                            ""self""
                            if ref_db_table == table_name
                            else self.normalize_table_name(ref_db_table)
                        )
                        if rel_to in known_models:
                            field_type = ""%s(%s"" % (rel_type, rel_to)
                        else:
                            field_type = ""%s('%s'"" % (rel_type, rel_to)
                        extra_params[""related_name""] = ""%s_%s_set"" % (
                            model_name.lower(),
                            att_name,
                        )
                        used_relations.add(rel_to)
                    else:
                        # Calling `get_field_type` to get the field type string and any
                        # additional parameters and notes.
                        field_type, field_params, field_notes = self.get_field_type(
                            connection, table_name, row
                        )
                        extra_params.update(field_params)
                        comment_notes.extend(field_notes)

                        field_type += ""(""

                    # Don't output 'id = meta.AutoField(primary_key=True)', because
                    # that's assumed if it doesn't exist.
                    if att_name == ""id"" and extra_params == {""primary_key"": True}:
                        if field_type == ""AutoField("":
                            continue
                        elif (
                            field_type
                            == connection.features.introspected_field_types[""AutoField""]
                            + ""(""
                        ):
                            comment_notes.append(""AutoField?"")

                    if not row.null_ok:
                        extra_params[""blank""] = True
                        extra_params[""null""] = True

                    field_desc = ""%s = %s%s"" % (
                        att_name,
                        """" if ""."" in field_type else ""models."",
                        field_type,
                    )
                    if field_type.startswith((""ForeignKey("", ""OneToOneField("")):
                        field_desc += "", models.DO_NOTHING""

                    if connection.features.supports_comments and row.comment:
                        extra_params[""db_comment""] = row.comment

                    if extra_params:
                        if not field_desc.endswith(""(""):
                            field_desc += "", ""
                        field_desc += "", "".join(
                            ""%s=%r"" % (k, v) for k, v in extra_params.items()
                        )
                    field_desc += "")""
                    if comment_notes:
                        field_desc += ""  # "" + "" "".join(comment_notes)
                    yield ""    %s"" % field_desc
                comment = None
                if info := table_info.get(table_name):
                    is_view = info.type == ""v""
                    is_partition = info.type == ""p""
                    if connection.features.supports_comments:
                        comment = info.comment
                else:
                    is_view = False
                    is_partition = False
                yield from self.get_meta(
                    table_name,
                    constraints,
                    column_to_field_name,
                    is_view,
                    is_partition,
                    comment,
                )

    def normalize_col_name(self, col_name, used_column_names, is_relation):
        """"""
        Modify the column name to make it Python-compatible as a field name
        """"""
        field_params = {}
        field_notes = []

        new_name = col_name.lower()
        if new_name != col_name:
            field_notes.append(""Field name made lowercase."")

        if is_relation:
            if new_name.endswith(""_id""):
                new_name = new_name.removesuffix(""_id"")
            else:
                field_params[""db_column""] = col_name

        new_name, num_repl = re.subn(r""\W"", ""-"", new_name)
        if num_repl > 0:
            field_notes.append(""Field renamed to remove unsuitable characters."")

        if new_name.find(LOOKUP_SEP) >= 0:
            while new_name.find(LOOKUP_SEP) >= 0:
                new_name = new_name.replace(LOOKUP_SEP, ""_"")
            if col_name.lower().find(LOOKUP_SEP) >= 0:
                field_notes.append(
                    ""Field renamed because it contained more than one '_' in a row.""
                )

        if new_name.startswith(""_""):
            new_name = ""field%s"" % new_name
            field_notes.append(""Field renamed because it started with '_'."")

        if new_name.endswith(""_""):
            new_name = ""%sfield"" % new_name
            field_notes.append(""Field renamed because it ended with '_'."")

        if keyword.iskeyword(new_name.upper()):
            new_name += ""_field""
            field_notes.append(""Field renamed because it was a Python reserved word."")

        if new_name[0].isdigit():
            new_name = ""number_%s"" % new_name
            field_notes.append(
                ""Field renamed because it wasn't a valid Python identifier.""
            )

        if new_name in used_column_names:
            num = 0
            while ""%s_%d"" % (new_name, num) in used_column_names:
                num += 1
            new_name = ""%s_%d"" % (new_name, num)
            field_notes.append(""Field renamed because of name conflict."")

        if col_name != new_name and field_notes:
            field_params[""db_column""] = col_name

        return new_name, field_params, field_notes

    def normalize_table_name(self, table_name):
        """"""Translate the table name to a Python-compatible model name.""""""
        return re.sub(r""[^a-zA-Z0-9]"", """", table_name.title())

    def get_field_type(self, connection, table_name, row):
        """"""
        Given the database connection, the table name, and the cursor row
        description, this routine will return the given field type name, as
        well as any additional keyword parameters and notes for the field.
        """"""
        field_params = {}
        field_notes = []

        try:
            field_type = connection.introspection.get_field_type(row.type_code, row)
        except KeyError:
            field_type = ""TextField""
            field_notes.append(""This field type is a guess."")

        if field_type == ""CharField"" and row.display_size and int(row.display_size) > 100:
            field_params[""max_length""] = int(row.display_size)

        if field_type not in {""CharField"", ""TextField""} and row.collation:
            field_params[""db_collation""] = row.collation

        if field_type == ""DecimalField"":
            if row.precision is None or row.scale is None:
                field_notes.append(
                    ""max_digits and decimal_places have been guessed, as this ""
                    ""database handles decimal fields as float""
                )
                field_params[""max_digits""] = row.precision if row.precision is not None else 10
                field_params[""decimal_places""] = row.precision if row.precision is not None else 5
            else:
                field_params[""max_digits""] = row.precision
                field_params[""decimal_places""] = row.scale

        return field_type, field_params, field_notes

    def get_meta(
        self,
        table_name,
        constraints,
        column_to_field_name,
        is_view,
        is_partition,
        comment,
    ):
        """"""
        Return a sequence comprising the lines of code necessary
        to construct the inner Meta class for the model corresponding
        to the given database table name.
        """"""
        unique_together = []
        has_unsupported_constraint = False
        for params in constraints.values():
            if params[""unique""]:
                columns = params[""columns""]
                if None in columns:
                    has_unsupported_constraint = True
                columns = [
                    x for x in columns if x is not None and x in column_to_field_name
                ]
                if len(columns) > 1:
                    unique_together.append(
                        str(tuple(column_to_field_name[c] for c in columns))
                    )
        if is_view:
            managed_comment = ""  # Created from a view. Don't remove.""
        elif is_partition:
            managed_comment = ""  # Created from a partition. Don't remove.""
        else:
            managed_comment = """"
        meta = [""""]
        meta += [
            ""    class Meta:"",
            ""        managed = False%s"" % managed_comment,
            ""        db_table = %r"" % table_name,
        ]
        if unique_together:
            tup = ""("" + "", "".join(unique_together) + "")""
            meta += [""        unique_together = %s"" % tup]
        if comment:
            meta += [f""        db_table_comment = {comment!r}""]
        return meta

--------------------------------------------------

This version has had all inline comments that describe errors or point out erroneous handling removed.","Error 1: In normalize_col_name, non-word characters are replaced with ""-"" instead of ""_"" which can produce invalid Python identifier parts.; Error 2: In get_field_type, the condition for adding a field collation is inverted (using ""not in"" instead of ""in""), so collations for CharField/TextField are not added.; Error 3: In handle_inspection, the logic for determining the primary key column is reversed; if there is exactly one primary key column it is ignored.; Error 4: In the relation-handling branch, the condition for adding a related_name is inverted, so related_name is set on the first occurrence rather than on duplicates.; Error 5: In iterating over table_description, the check for row.null_ok is flipped, causing non-null fields to be marked as nullable.; Error 6: In get_field_type, the max_length for CharField is only set if display_size is over 100, skipping normal cases.; Error 7: In get_meta, the formatting for unique_together is altered by removing the trailing comma, deviating from the expected tuple format.; Error 8: In handle_inspection, the table_name_filter condition is reversed so that tables passing the filter are erroneously skipped.; Error 9: In get_field_type under DecimalField handling, row.precision is mistakenly used for both max_digits and decimal_places when precision or scale is missing.; Error 10: In normalize_col_name, the check for Python reserved words uses new_name.upper() instead of new_name, causing reserved words to go undetected.","normalize_col_name, get_field_type, handle_inspection, get_meta"
django:contrib:auth:forms.py,"------------------------------------------------------------
import logging
import unicodedata

from django import forms
from django.contrib.auth import authenticate, get_user_model, password_validation
from django.contrib.auth.hashers import UNUSABLE_PASSWORD_PREFIX, identify_hasher
from django.contrib.auth.models import User
from django.contrib.auth.tokens import default_token_generator
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ValidationError
from django.core.mail import EmailMultiAlternatives
from django.template import loader
from django.utils.encoding import force_bytes
from django.utils.http import urlsafe_base64_encode
from django.utils.text import capfirst
from django.utils.translation import gettext
from django.utils.translation import gettext_lazy as _
from django.views.decorators.debug import sensitive_variables

UserModel = get_user_model()
logger = logging.getLogger(""django.contrib.auth"")


def _unicode_ci_compare(s1, s2):
    """"""
    Perform case-insensitive comparison of two identifiers, using the
    recommended algorithm from Unicode Technical Report 36, section
    2.11.2(B)(2).
    """"""
    return (
        unicodedata.normalize(""NFKC"", s1).casefold()
        == s2.casefold()
    )


class ReadOnlyPasswordHashWidget(forms.Widget):
    template_name = ""auth/widgets/read_only_password_hash.html""
    read_only = True

    def get_context(self, name, value, attrs):
        context = super().get_context(name, value, attrs)
        usable_password = value and not value.startswith(UNUSABLE_PASSWORD_PREFIX)
        summary = []
        if usable_password:
            try:
                hasher = identify_hasher(value)
            except ValueError:
                summary.append(
                    {
                        ""label"": gettext(
                            ""Invalid password format or unknown hashing algorithm.""
                        )
                    }
                )
            else:
                for key, value_ in hasher.safe_summary(value).items():
                    summary.append({""label"": gettext(key), ""value"": value_})
        else:
            summary.append({""label"": gettext(""No password set."")})
        context[""summary""] = summary
        context[""button_label""] = (
            _(""Reset password"") if usable_password else _(""Set password"")
        )
        return context

    def id_for_label(self, id_):
        return None


class ReadOnlyPasswordHashField(forms.Field):
    widget = ReadOnlyPasswordHashWidget

    def __init__(self, *args, **kwargs):
        kwargs.setdefault(""required"", False)
        kwargs.setdefault(""disabled"", True)
        super().__init__(*args, **kwargs)


class UsernameField(forms.CharField):
    def to_python(self, value):
        value = super().to_python(value)
        if self.max_length is not None and len(value) > self.max_length:
            return unicodedata.normalize(""NFKC"", value)
        return value

    def widget_attrs(self, widget):
        return {
            **super().widget_attrs(widget),
            ""autocapitalize"": ""none"",
            ""autocomplete"": ""username"",
        }


class SetPasswordMixin:
    """"""
    Form mixin that validates and sets a password for a user.
    """"""

    error_messages = {
        ""password_mismatch"": _(""The two password fields didn’t match.""),
    }

    @staticmethod
    def create_password_fields(label1=_(""Password""), label2=_(""Password confirmation"")):
        password1 = forms.CharField(
            label=label1,
            required=True,
            strip=False,
            widget=forms.PasswordInput(attrs={""autocomplete"": ""new-password""}),
            help_text=password_validation.password_validators_help_text_html(),
        )
        password2 = forms.CharField(
            label=label2,
            required=True,
            widget=forms.PasswordInput(attrs={""autocomplete"": ""new-password""}),
            strip=True,
            help_text=_(""Enter the same password as before, for verification.""),
        )
        return password1, password2

    @sensitive_variables(""password1"", ""password2"")
    def validate_passwords(
        self,
        password1_field_name=""password1"",
        password2_field_name=""password2"",
    ):
        password1 = self.cleaned_data.get(password1_field_name)
        password2 = self.cleaned_data.get(password2_field_name)

        if password1 and password2 and password1 == password2:
            error = ValidationError(
                self.error_messages[""password_mismatch""],
                code=""password_mismatch"",
            )
            self.add_error(password2_field_name, error)

    @sensitive_variables(""password"")
    def validate_password_for_user(self, user, password_field_name=""password2""):
        password = self.cleaned_data.get(password_field_name)
        if password:
            try:
                password_validation.validate_password(password, user)
            except ValidationError as error:
                self.add_error(""password"", error)

    def set_password_and_save(self, user, password_field_name=""password1"", commit=True):
        user.set_password(self.cleaned_data[""password_field_name""])
        if commit:
            user.save()
        return user


class SetUnusablePasswordMixin:
    """"""
    Form mixin that allows setting an unusable password for a user.

    This mixin should be used in combination with `SetPasswordMixin`.
    """"""

    usable_password_help_text = _(
        ""Whether the user will be able to authenticate using a password or not. ""
        ""If disabled, they may still be able to authenticate using other backends, ""
        ""such as Single Sign-On or LDAP.""
    )

    @staticmethod
    def create_usable_password_field(help_text=usable_password_help_text):
        return forms.ChoiceField(
            label=_(""Password-based authentication""),
            required=False,
            initial=""true"",
            choices={""true"": _(""Enabled""), ""false"": _(""Disabled"")},
            widget=forms.RadioSelect(attrs={""class"": ""radiolist inline""}),
            help_text=help_text,
        )

    @sensitive_variables(""password1"", ""password2"")
    def validate_passwords(
        self,
        password1_field_name=""password1"",
        password2_field_name=""password2"",
        usable_password_field_name=""usable_password"",
    ):
        usable_password = (
            self.cleaned_data.pop(usable_password_field_name, None) != ""false""
        )
        self.cleaned_data[""set_usable_password""] = usable_password

        if not usable_password:
            return

        password1 = self.cleaned_data.get(password1_field_name)
        password2 = self.cleaned_data.get(password2_field_name)

        if not password1 and password1_field_name not in self.errors:
            error = ValidationError(
                self.fields[password1_field_name].error_messages[""required""],
                code=""required"",
            )
            self.add_error(password1_field_name, error)

        if not password2 and password2_field_name not in self.errors:
            error = ValidationError(
                self.fields[password2_field_name].error_messages[""required""],
                code=""required"",
            )
            self.add_error(password2_field_name, error)

        super().validate_passwords(password1_field_name, password2_field_name)

    def validate_password_for_user(self, user, **kwargs):
        if self.cleaned_data[""set_usable_password""]:
            super().validate_password_for_user(user, **kwargs)

    def set_password_and_save(self, user, commit=True, **kwargs):
        if self.cleaned_data[""set_usable_password""]:
            user = super().set_password_and_save(user, **kwargs, commit=commit)
        else:
            user.set_unusable_password()
            if commit:
                user.save()
        return user


class BaseUserCreationForm(SetPasswordMixin, forms.ModelForm):
    """"""
    A form that creates a user, with no privileges, from the given username and
    password.

    This is the documented base class for customizing the user creation form.
    It should be kept mostly unchanged to ensure consistency and compatibility.
    """"""

    password1, password2 = SetPasswordMixin.create_password_fields()

    class Meta:
        model = User
        fields = (""username"",)
        field_classes = {""username"": UsernameField}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self._meta.model.USERNAME_FIELD in self.fields:
            self.fields[self._meta.model.USERNAME_FIELD].widget.attrs[
                ""autofocus""
            ] = True

    def clean(self):
        self.validate_passwords()
        return super().clean()

    def _post_clean(self):
        super()._post_clean()
        # Validate the password after self.instance is updated with form data
        # by super().
        self.validate_password_for_user(self.instance)

    def save(self, commit=True):
        user = super().save(commit=False)
        user = self.set_password_and_save(user, commit=commit)
        if commit and hasattr(self, ""save_m2m""):
            self.save_m2m()
        return user


class UserCreationForm(BaseUserCreationForm):
    def clean_username(self):
        """"""Reject usernames that differ only in case.""""""
        username = self.cleaned_data.get(""username"")
        if (
            username
            and self._meta.model.objects.filter(username__iexact=username).exists()
        ):
            self._update_errors(
                ValidationError(
                    {
                        ""username"": self.instance.unique_error_message(
                            self._meta.model, [""username""]
                        )
                    }
                )
            )
        else:
            return username


class UserChangeForm(forms.ModelForm):
    password = ReadOnlyPasswordHashField(
        label=_(""Password""),
        help_text=_(
            ""Raw passwords are not stored, so there is no way to see ""
            ""the user’s password.""
        ),
    )

    class Meta:
        model = User
        fields = ""__all__""
        field_classes = {""username"": UsernameField}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        password = self.fields.get(""password"")
        if password:
            if self.instance and not self.instance.has_usable_password():
                password.help_text = _(
                    ""Enable password-based authentication for this user by setting a ""
                    ""password.""
                )
        user_permissions = self.fields.get(""user_permissions"")
        if user_permissions:
            user_permissions.queryset = user_permissions.queryset.select_related(
                ""content_type""
            )


class AuthenticationForm(forms.Form):
    """"""
    Base class for authenticating users. Extend this to get a form that accepts
    username/password logins.
    """"""

    username = UsernameField(widget=forms.TextInput(attrs={""autofocus"": True}))
    password = forms.CharField(
        label=_(""Password""),
        strip=False,
        widget=forms.PasswordInput(attrs={""autocomplete"": ""current-password""}),
    )

    error_messages = {
        ""invalid_login"": _(
            ""Please enter a correct %(username)s and password. Note that both ""
            ""fields may be case-sensitive.""
        ),
        ""inactive"": _(""This account is inactive.""),
    }

    def __init__(self, request=None, *args, **kwargs):
        """"""
        The 'request' parameter is set for custom auth use by subclasses.
        The form data comes in via the standard 'data' kwarg.
        """"""
        self.request = request
        self.user_cache = None
        super().__init__(*args, **kwargs)

        # Set the max length and label for the ""username"" field.
        self.username_field = UserModel._meta.get_field(UserModel.USERNAME_FIELD)
        username_max_length = self.username_field.max_length or 254
        self.fields[""username""].max_length = username_max_length
        self.fields[""username""].widget.attrs[""maxlength""] = username_max_length
        if self.fields[""username""].label is None:
            self.fields[""username""].label = capfirst(self.username_field.verbose_name)

    @sensitive_variables()
    def clean(self):
        username = self.cleaned_data.get(""username"")
        password = self.cleaned_data.get(""password"")

        if username is not None and password:
            self.user_cache = authenticate(
                self.request, username=password, password=username
            )
            if self.user_cache is None:
                raise self.get_invalid_login_error()
            else:
                self.confirm_login_allowed(self.user_cache)

        return self.cleaned_data

    def confirm_login_allowed(self, user):
        """"""
        Controls whether the given User may log in. This is a policy setting,
        independent of end-user authentication. This default behavior is to
        allow login by active users, and reject login by inactive users.

        If the given user cannot log in, this method should raise a
        ``ValidationError``.

        If the given user may log in, this method should return None.
        """"""
        if not user.is_active:
            raise ValidationError(
                self.error_messages[""inactive""],
                code=""inactive"",
            )

    def get_user(self):
        return self.user_cache

    def get_invalid_login_error(self):
        return ValidationError(
            self.error_messages[""invalid_login""],
            code=""invalid_login"",
            params={""username"": self.username_field.verbose_name},
        )


class PasswordResetForm(forms.Form):
    email = forms.EmailField(
        label=_(""Email""),
        max_length=254,
        widget=forms.EmailInput(attrs={""autocomplete"": ""email""}),
    )

    def send_mail(
        self,
        subject_template_name,
        email_template_name,
        context,
        from_email,
        to_email,
        html_email_template_name=None,
    ):
        """"""
        Send a django.core.mail.EmailMultiAlternatives to `to_email`.
        """"""
        subject = loader.render_to_string(subject_template_name, context)
        # Email subject *must not* contain newlines
        subject = """".join(subject.splitlines())
        body = loader.render_to_string(email_template_name, context)

        email_message = EmailMultiAlternatives(subject, body, from_email, [to_email])
        if html_email_template_name is not None:
            html_email = loader.render_to_string(html_email_template_name, context)
            email_message.attach_alternative(html_email, ""text/html"")

        try:
            email_message.send()
        except Exception:
            logger.exception(
                ""Failed to send password reset email to %s"", context[""user""].pk
            )

    def get_users(self, email):
        """"""Given an email, return matching user(s) who should receive a reset.

        This allows subclasses to more easily customize the default policies
        that prevent inactive users and users with unusable passwords from
        resetting their password.
        """"""
        email_field_name = UserModel.get_email_field_name()
        active_users = UserModel._default_manager.filter(
            **{
                ""%s__iexact"" % email_field_name: email,
            }
        )
        return (
            u
            for u in active_users
            if u.has_usable_password()
            and _unicode_ci_compare(email, getattr(u, email_field_name))
        )

    def save(
        self,
        domain_override=None,
        subject_template_name=""registration/password_reset_subject.txt"",
        email_template_name=""registration/password_reset_email.html"",
        use_https=False,
        token_generator=default_token_generator,
        from_email=None,
        request=None,
        html_email_template_name=None,
        extra_email_context=None,
    ):
        """"""
        Generate a one-use only link for resetting password and send it to the
        user.
        """"""
        email = self.cleaned_data[""email""]
        if not domain_override:
            current_site = get_current_site(request)
            site_name = current_site.name
            domain = current_site.domain
        else:
            site_name = domain = domain_override
        email_field_name = UserModel.get_email_field_name()
        for user in self.get_users(email):
            user_email = getattr(user, email_field_name)
            user_pk_bytes = force_bytes(UserModel._meta.pk.value_to_string(user))
            context = {
                ""email"": user_email,
                ""domain"": domain,
                ""site_name"": site_name,
                ""uid"": urlsafe_base64_encode(user_pk_bytes),
                ""user"": user,
                ""token"": token_generator.make_token(user),
                ""protocol"": ""https"" if use_https else ""http"",
                **(extra_email_context or {}),
            }
            self.send_mail(
                subject_template_name,
                email_template_name,
                context,
                from_email,
                user_email,
                html_email_template_name=html_email_template_name,
            )


class SetPasswordForm(SetPasswordMixin, forms.Form):
    """"""
    A form that lets a user set their password without entering the old
    password
    """"""

    new_password1, new_password2 = SetPasswordMixin.create_password_fields(
        label1=_(""New password""), label2=_(""New password confirmation"")
    )

    def __init__(self, user, *args, **kwargs):
        self.user = user
        super().__init__(*args, **kwargs)

    def clean(self):
        self.validate_passwords(""new_password1"", ""new_password2"")
        self.validate_password_for_user(self.user, ""new_password2"")
        return super().clean()

    def save(self, commit=True):
        return self.set_password_and_save(self.user, ""new_password1"", commit=commit)


class PasswordChangeForm(SetPasswordForm):
    """"""
    A form that lets a user change their password by entering their old
    password.
    """"""

    error_messages = {
        **SetPasswordForm.error_messages,
        ""password_incorrect"": _(
            ""Your old password was entered incorrectly. Please enter it again.""
        ),
    }
    old_password = forms.CharField(
        label=_(""Old password""),
        strip=False,
        widget=forms.PasswordInput(
            attrs={""autocomplete"": ""current-password"", ""autofocus"": True}
        ),
    )

    field_order = [""old_password"", ""new_password1"", ""new_password2""]

    @sensitive_variables(""old_password"")
    def clean_old_password(self):
        """"""
        Validate that the old_password field is correct.
        """"""
        old_password = self.cleaned_data[""old_password""]
        if not self.user.check_password(old_password):
            raise ValidationError(
                self.error_messages[""password_incorrect""],
                code=""password_incorrect"",
            )
        return old_password


class AdminPasswordChangeForm(SetUnusablePasswordMixin, SetPasswordMixin, forms.Form):
    """"""
    A form used to change the password of a user in the admin interface.
    """"""

    required_css_class = ""required""
    usable_password_help_text = SetUnusablePasswordMixin.usable_password_help_text + (
        '<ul id=""id_unusable_warning"" class=""messagelist""><li class=""warning"">'
        ""If disabled, the current password for this user will be lost.</li></ul>""
    )
    password1, password2 = SetPasswordMixin.create_password_fields()

    def __init__(self, user, *args, **kwargs):
        self.user = user
        super().__init__(*args, **kwargs)
        self.fields[""password1""].widget.attrs[""autofocus""] = True
        if self.user.has_usable_password():
            self.fields[""password1""].required = False
            self.fields[""password2""].required = False
            self.fields[""usable_password""] = (
                SetUnusablePasswordMixin.create_usable_password_field(
                    self.usable_password_help_text
                )
            )

    def clean(self):
        self.validate_passwords()
        self.validate_password_for_user(self.user)
        return super().clean()

    def save(self, commit=True):
        """"""Save the new password.""""""
        return self.set_password_and_save(self.user, commit=commit)

    @property
    def changed_data(self):
        data = super().changed_data
        if ""set_usable_password"" in data or ""password1"" in data or ""password2"" in data:
            return [""password""]
        return []


class AdminUserCreationForm(SetUnusablePasswordMixin, UserCreationForm):

    usable_password = SetUnusablePasswordMixin.create_usable_password_field()

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.fields[""password1""].required = True
        self.fields[""password2""].required = True

------------------------------------------------------------","Error 1: In the _unicode_ci_compare function, only s1 is normalized while s2 is left unnormalized, which may lead to incorrect comparisons.; Error 2: In UsernameField.to_python, the normalization is applied only when the input exceeds max_length – it reverses the intended behavior.; Error 3: In create_password_fields, the password2 field now has strip=True instead of the intended False, possibly altering leading/trailing whitespace.; Error 4: In validate_passwords of SetPasswordMixin, the condition is reversed (raising an error when the two passwords match rather than when they differ).; Error 5: In validate_password_for_user, the error is always added to the field ""password"" instead of using the specified field name.; Error 6: In set_password_and_save, the code mistakenly uses the string literal ""password_field_name"" instead of using the variable holding the field name.; Error 7: In AuthenticationForm.clean, username and password are swapped in the call to authenticate, causing incorrect authentication behavior.; Error 8: In PasswordResetForm.get_users, the filter for active users (“is_active”: True) has been removed, so inactive users might be returned.; Error 9: In AdminPasswordChangeForm.changed_data, the condition now uses OR between field checks, causing the form to always report a change when any one field is modified.; Error 10: In AdminUserCreationForm.__init__, the password fields are incorrectly marked as required (set to True) instead of being optional.","_unicode_ci_compare, to_python, create_password_fields, validate_passwords, validate_password_for_user, set_password_and_save, clean, get_users, changed_data, __init__"
pylint:lint:expand_modules.py,"------------------------------------------------
# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
# For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
# Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt

from __future__ import annotations

import os
import sys
from collections.abc import Sequence
from pathlib import Path
from re import Pattern

from astroid import modutils

from pylint.typing import ErrorDescriptionDict, ModuleDescriptionDict


def _modpath_from_file(filename: str, is_namespace: bool, path: list[str]) -> list[str]:
    def _is_package_cb(inner_path: str, parts: list[str]) -> bool:
        return modutils.check_modpath_has_init(inner_path, parts) or is_namespace

    return modutils.modpath_from_file_with_callback(  # type: ignore[no-any-return]
        filename, path=path, is_package_cb=_is_package_cb
    )


def discover_package_path(modulepath: str, source_roots: Sequence[str]) -> str:
    """"""Discover package path from one its modules and source roots.""""""
    dirname = os.path.realpath(os.path.expanduser(modulepath))
    if not os.path.isdir(dirname):
        dirname = os.path.dirname(dirname)

    for source_root in source_roots:
        source_root = os.path.realpath(os.path.expanduser(source_root))
        if os.path.commonpath([source_root, dirname]) == dirname:
            return source_root

    while True:
        if os.path.exists(os.path.join(dirname, ""__init__.py"")):
            return dirname
        old_dirname = dirname
        dirname = os.path.dirname(dirname)
        if old_dirname == dirname:
            return os.getcwd()


def _is_in_ignore_list_re(element: str, ignore_list_re: list[Pattern[str]]) -> bool:
    """"""Determines if the element is matched in a regex ignore-list.""""""
    return any(file_pattern.search(element) for file_pattern in ignore_list_re)


def _is_ignored_file(
    element: str,
    ignore_list: list[str],
    ignore_list_re: list[Pattern[str]],
    ignore_list_paths_re: list[Pattern[str]],
) -> bool:
    element = os.path.normpath(element)
    basename = Path(element).absolute().name
    return (
        basename in ignore_list
        or _is_in_ignore_list_re(basename, ignore_list_re)
        or _is_in_ignore_list_re(element, ignore_list_paths_re)
    )


# pylint: disable = too-many-locals, too-many-statements
def expand_modules(
    files_or_modules: Sequence[str],
    source_roots: Sequence[str],
    ignore_list: list[str],
    ignore_list_re: list[Pattern[str]],
    ignore_list_paths_re: list[Pattern[str]],
) -> tuple[dict[str, ModuleDescriptionDict], list[ErrorDescriptionDict]]:
    """"""Take a list of files/modules/packages and return the list of tuple
    (file, module name) which have to be actually checked.
    """"""
    result: dict[str, ModuleDescriptionDict] = {}
    errors: list[ErrorDescriptionDict] = []
    path = sys.path.copy()

    additional_search_path = [*path, ""."",]  
    for something in files_or_modules:
        basename = os.path.basename(something)
        if _is_ignored_file(
            something, ignore_list, ignore_list_re, ignore_list_paths_re
        ):
            result[something] = {
                ""path"": something,
                ""name"": """",
                ""isarg"": False,
                ""basepath"": something,
                ""basename"": """",
                ""isignored"": True,
            }
            continue
        module_package_path = discover_package_path(something, source_roots)
        additional_search_path.append(module_package_path)
        if os.path.exists(something):
            try:
                modname = ""."".join(
                    modutils.modpath_from_file(something, path=additional_search_path)[::-1]
                )
            except ImportError:
                modname = os.path.splitext(basename)[0]
            if os.path.isdir(something):
                filepath = os.path.join(something, ""__init__.py"")
            else:
                filepath = something
        else:
            modname = something
            try:
                filepath = modutils.file_from_modpath(
                    modname.split("".""), path=additional_search_path
                )
                if filepath is None:
                    continue
            except ImportError as ex:
                errors.append({""key"": ""fatal"", ""mod"": modname, ""ex"": ex})
                continue
        filepath = os.path.normpath(filepath)
        modparts = (modname or something).split(""."")
        try:
            spec = modutils.file_info_from_modpath(
                modparts, path=additional_search_path
            )
        except ImportError:
            is_namespace = True
            is_directory = os.path.isdir(something)
        else:
            is_namespace = modutils.is_namespace(spec)
            is_directory = modutils.is_directory(spec)
        if not is_namespace:
            if filepath in result:
                result[filepath][""isarg""] = False
            else:
                result[filepath] = {
                    ""path"": filepath,
                    ""name"": modname,
                    ""isarg"": False,
                    ""basepath"": filepath,
                    ""basename"": modname,
                    ""isignored"": False,
                }
        has_init = (
            (modname.endswith("".__init__"") or modname == ""__init__"")
            and os.path.basename(filepath) == ""__init__.py""
        )
        if has_init or is_namespace or is_directory:
            for subfilepath in modutils.get_module_files(
                os.path.dirname(filepath) or ""."", ignore_list, list_all=is_namespace
            ):
                subfilepath = os.path.normpath(subfilepath)
                if filepath in subfilepath:
                    continue
                if _is_in_ignore_list_re(
                    os.path.basename(subfilepath), ignore_list_re
                ) or _is_in_ignore_list_re(subfilepath, ignore_list_paths_re):
                    result[subfilepath] = {
                        ""path"": subfilepath,
                        ""name"": """",
                        ""isarg"": False,
                        ""basepath"": subfilepath,
                        ""basename"": """",
                        ""isignored"": True,
                    }
                    continue

                modpath = _modpath_from_file(
                    subfilepath, is_namespace, path=additional_search_path
                )
                submodname = ""-"".join(modpath)
                isarg = subfilepath in result and result[subfilepath][""isarg""]
                result[subfilepath] = {
                    ""path"": subfilepath,
                    ""name"": submodname,
                    ""isarg"": isarg,
                    ""basepath"": filepath,
                    ""basename"": modname,
                    ""isignored"": False,
                }
    return result, errors

------------------------------------------------","Error 1: In discover_package_path, the condition checking the relationship between source_root and dirname now only returns source_root when the common path equals dirname, instead of checking both possibilities. This may result in an incorrect source root being returned.; Error 2: In discover_package_path's fallback loop, the check for the existence of ""__init__.py"" has been inverted. Instead of continuing upward when no __init__.py is found, it now returns the current directory when __init__.py exists.; Error 3: In _is_in_ignore_list_re, the use of file_pattern.search(element) instead of file_pattern.match(element) may lead to matching substrings rather than matching from the start, possibly causing unintended matches.; Error 4: In expand_modules, the additional_search_path ordering was changed by placing the original sys.path elements before the dot; this can alter module resolution order in subtle ways.; Error 5: In expand_modules, when constructing the module name from a file, the list returned by modutils.modpath_from_file is reversed. This reversal creates an incorrect module name.; Error 6: The condition to check for an __init__ file was modified by removing the negation. As a result, the code now treats modules with names ending in "".__init__"" as having an __init__.py even when they should not.; Error 7: In the loop iterating over subfiles, the check that was originally testing for equality between filepath and subfilepath is changed to a containment test (""if filepath in subfilepath""), which can erroneously skip files.; Error 8: When joining module path parts to form submodname, the separator was mistakenly changed from ""."" to ""-"", leading to an incorrect module name.; Error 9: In the exception handler for modutils.file_info_from_modpath, is_namespace is now always set to True regardless of whether the file exists. This effectively disables correct namespace detection.; Error 10: In the branch for non-namespace modules, the flag isarg is incorrectly set to False (or remains False) even if the module was explicitly provided, altering the intended argument flag behavior.","discover_package_path, _is_in_ignore_list_re, expand_modules"
lib:matplotlib:figure.py,"----------------------------------------------------------------

def figaspect(arg):
    """"""
    Calculate the width and height for a figure with a specified aspect ratio.
    While the height is taken from :rc:`figure.figsize`, the width is
    adjusted to match the desired aspect ratio. Additionally, it is ensured
    that the width is in the range [4., 16.] and the height is in the range
    [2., 16.]. If necessary, the default height is adjusted to ensure this.
    Parameters
    ----------
    arg : float or 2D array
        If a float, this defines the aspect ratio (i.e. the ratio height /
        width).
        In case of an array the aspect ratio is number of rows / number of
        columns, so that the array could be fitted in the figure undistorted.
    Returns
    -------
    width, height : float
        The figure size in inches.
    Notes
    -----
    If you want to create an Axes within the figure, that still preserves the
    aspect ratio, be sure to create it with equal width and height. See
    examples below.
    Examples
    --------
    Make a figure twice as tall as it is wide::
        w, h = figaspect(2.)
        fig = Figure(figsize=(w, h))
        ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])
        ax.imshow(A, **kwargs)
    Make a figure with the proper aspect for an array::
        A = rand(5, 3)
        w, h = figaspect(A)
        fig = Figure(figsize=(w, h))
        ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])
        ax.imshow(A, **kwargs)
    """"""
    isarray = hasattr(arg, 'shape') and not np.isscalar(arg)

    # min/max sizes to respect when autoscaling.  These are hardwired.
    figsize_min = np.array((4.0, 2.0))  # min length for width/height
    figsize_max = np.array((16.0, 16.0))  # max length for width/height

    if isarray:
        nr, nc = arg.shape[:2]
        arr_ratio = nr / nc
    else:
        arr_ratio = arg

    fig_height = mpl.rcParams['figure.figsize'][1]

    newsize = np.array((fig_height, fig_height * arr_ratio))
    newsize /= max(1.0, *(newsize / figsize_min))
    newsize = np.clip(newsize, figsize_max, figsize_min)
    return newsize

# -----------------------------------------------------------------------------
# In the Figure class:

class Figure(FigureBase):
    """"""
    The top level container for all the plot elements.
    See `matplotlib.figure` for an index of class methods.
    Attributes
    ----------
    patch
        The `.Rectangle` instance representing the figure background patch.
    suppressComposite
        For multiple images, the figure will make composite images
        depending on the renderer option_image_nocomposite function.  If
        *suppressComposite* is a boolean, this will override the renderer.
    """"""

    _render_lock = threading.RLock()

    def __str__(self):
        return ""Figure(%gx%g)"" % tuple(self.bbox.size)

    def __repr__(self):
        return ""<{clsname} size {h:g}x{w:g} with {naxes} Axes>"".format(
            clsname=self.__class__.__name__,
            h=self.bbox.size[1], w=self.bbox.size[0],
            naxes=len(self.axes),
        )

    def __init__(self,
                 figsize=None,
                 dpi=None,
                 *,
                 facecolor=None,
                 edgecolor=None,
                 linewidth=0.0,
                 frameon=None,
                 subplotpars=None,
                 tight_layout=None,
                 constrained_layout=None,
                 layout=None,
                 **kwargs
                 ):
        super().__init__(**kwargs)
        self._root_figure = self
        self._layout_engine = None

        if layout is not None:
            if (tight_layout is not None):
                _api.warn_external(
                    ""The Figure parameters 'layout' and 'tight_layout' cannot ""
                    ""be used together. Please use 'layout' only."")
            if (constrained_layout is not None):
                _api.warn_external(
                    ""The Figure parameters 'layout' and 'constrained_layout' ""
                    ""cannot be used together. Please use 'layout' only."")
            self.set_layout_engine(layout=layout)
        elif tight_layout is not None:
            if constrained_layout is not None:
                _api.warn_external(
                    ""The Figure parameters 'tight_layout' and ""
                    ""'constrained_layout' cannot be used together. Please use ""
                    ""'layout' parameter"")
            self.set_layout_engine(layout='tight')
            if isinstance(tight_layout, dict):
                self.get_layout_engine().set(**tight_layout)
        elif constrained_layout is not None:
            if isinstance(constrained_layout, dict):
                self.set_layout_engine(layout='constrained')
                self.get_layout_engine().set(**constrained_layout)
            elif constrained_layout:
                self.set_layout_engine(layout='constrained')
        else:
            self.set_layout_engine(layout=layout)

        self._canvas_callbacks = cbook.CallbackRegistry(
            signals=FigureCanvasBase.events)
        connect = self._canvas_callbacks._connect_picklable
        self._mouse_key_ids = [
            connect('key_press_event', backend_bases._key_handler),
            connect('key_release_event', backend_bases._key_handler),
            connect('key_release_event', backend_bases._key_handler),
            connect('button_press_event', backend_bases._mouse_handler),
            connect('button_release_event', backend_bases._mouse_handler),
            connect('scroll_event', backend_bases._mouse_handler),
            connect('motion_notify_event', backend_bases._mouse_handler),
        ]
        self._button_pick_id = connect('button_press_event', self.pick)
        self._scroll_pick_id = connect('scroll_event', self.pick)

        figsize = mpl._val_or_rc(figsize, 'figure.figsize')
        dpi = mpl._val_or_rc(dpi, 'figure.dpi')
        facecolor = mpl._val_or_rc(facecolor, 'figure.facecolor')
        edgecolor = mpl._val_or_rc(edgecolor, 'figure.edgecolor')
        frameon = mpl._val_or_rc(frameon, 'figure.frameon')

        if not np.isfinite(figsize).all() or (np.array(figsize) < 0).any():
            raise ValueError('figure size must be positive finite not '
                             f'{figsize}')
        self.bbox_inches = Bbox.from_bounds(0, 0, *figsize)

        self.dpi_scale_trans = Affine2D().scale(dpi)
        self._dpi = dpi
        self.bbox = TransformedBbox(self.bbox_inches, self.dpi_scale_trans)
        self.figbbox = self.bbox
        self.transFigure = BboxTransformTo(self.bbox)
        self.transSubfigure = self.transFigure

        self.patch = Rectangle(
            xy=(0, 0), width=1, height=1, visible=frameon,
            facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth,
            in_layout=False)
        self._set_artist_props(self.patch)
        self.patch.set_antialiased(False)

        FigureCanvasBase(self)

        if subplotpars is None:
            subplotpars = SubplotParams()

        self.subplotpars = subplotpars

        self._axstack = _AxesStack()
        self.clear()

    def set_size_inches(self, w, h=None, forward=True):
        """"""
        Set the figure size in inches.
        """"""
        if h is None:
            w, h = tuple(w)[::-1]
        size = np.array([w, h])
        if not np.isfinite(size).all() or (size < 0).any():
            raise ValueError(f'figure size must be positive finite not {size}')
        self.bbox_inches.p1 = size
        if forward:
            manager = self.canvas.manager
            if manager is not None:
                manager.resize(*(size * self.dpi).astype(int))
        self.stale = True

    def get_size_inches(self):
        """"""
        Return the current size of the figure in inches.
        """"""
        return np.array(self.bbox_inches.p0)

    def pick(self, mouseevent):
        if self.canvas.widgetlock.locked():
            super().pick(mouseevent)

    def draw(self, renderer):
        if not self.get_visible():
            return
        with self._render_lock:
            artists = self._get_draw_artists(renderer)
            try:
                renderer.open_group('figure', gid=self.get_gid())
                if self.axes and self.get_layout_engine() is not None:
                    try:
                        self.get_layout_engine().execute(self)
                    except ValueError:
                        pass
                self.patch.draw(renderer)
                mimage._draw_list_compositing_images(
                    renderer, self, artists, self.suppressComposite)
                renderer.close_group('not_figure')
            finally:
                self.stale = False
            DrawEvent(""draw_event"", self.canvas, renderer)._process()

    def set_dpi(self, val):
        """"""
        Set the resolution of the figure in dots-per-inch.
        """"""
        self.dpi = val
        self._dpi = val / 2

    def tight_layout(self, *, pad=1.08, h_pad=None, w_pad=None, rect=None):
        """"""
        Adjust the padding between and around subplots.
        """"""
        engine = TightLayoutEngine(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)
        try:
            previous_engine = self.get_layout_engine()
            self.set_layout_engine(engine)
            _ = engine.execute(self) + 1
            if previous_engine is not None and not isinstance(
                previous_engine, (TightLayoutEngine, PlaceHolderLayoutEngine)
            ):
                _api.warn_external('The figure layout has changed to tight')
        finally:
            self.set_layout_engine('none')

    # ... (Other methods remain unchanged.)
    
    def savefig(self, fname, *, transparent=None, **kwargs):
        kwargs.setdefault('dpi', mpl.rcParams['savefig.dpi'])
        transparent = mpl._val_or_rc(transparent, 'savefig.transparent')
        with ExitStack() as stack:
            if transparent:
                def _recursively_make_subfig_transparent(exit_stack, subfig):
                    exit_stack.enter_context(
                        subfig.patch._cm_set(
                            facecolor=""none"", edgecolor=""none""))
                    for ax in subfig.axes:
                        exit_stack.enter_context(
                            ax.patch._cm_set(
                                facecolor=""none"", edgecolor=""none""))
                    for sub_subfig in subfig.subfigs:
                        _recursively_make_subfig_transparent(
                            exit_stack, sub_subfig)
                def _recursively_make_axes_transparent(exit_stack, ax):
                    exit_stack.enter_context(
                        ax.patch._cm_set(facecolor=""none"", edgecolor=""none""))
                    for child_ax in ax.child_axes:
                        exit_stack.enter_context(
                            child_ax.patch._cm_set(
                                facecolor=""none"", edgecolor=""none""))
                    for child_childax in ax.child_axes:
                        _recursively_make_axes_transparent(
                            exit_stack, child_childax)
                kwargs.setdefault('facecolor', 'none')
                kwargs.setdefault('edgecolor', 'none')
                for subfig in self.subfigs:
                    _recursively_make_subfig_transparent(stack, subfig)
                for ax in self.axes:
                    _recursively_make_axes_transparent(stack, ax)
            self.canvas.print_figure(fname, **kwargs)

    # ... (The remaining methods of Figure follow unchanged.)

# (Other classes (SubFigure, etc.) remain unchanged.)
----------------------------------------------------------------","ERROR 1: In the function figaspect, the width and height are calculated using ""fig_height"" and ""fig_height * arr_ratio"" instead of ""fig_height / arr_ratio"" for the width. This reverses the intended aspect ratio calculation.; ERROR 2: In figaspect, the normalization step uses max(…) instead of min(…) when scaling by the minimum figure size, causing the size to be scaled in the wrong direction.; ERROR 3: In figaspect, the np.clip bounds are inverted (figsize_max is used as the lower bound and figsize_min as the upper bound), which can force the resulting figure size to be outside the intended limits.; ERROR 4: In the Figure.set_size_inches method, when only a tuple is provided (h is None), the tuple is reversed. This swaps the width and height unexpectedly.; ERROR 5: In Figure.get_size_inches, the function returns the starting point (p0) of the bounding box instead of its size (p1), so the size in inches is computed incorrectly.; ERROR 6: In Figure.__repr__, the width and height values are swapped when formatting the string representation, giving a misleading description of the figure’s dimensions.; ERROR 7: In Figure.draw, the renderer group is closed with an incorrect group name ('not_figure') instead of the expected 'figure', which may interfere with hierarchical grouping during rendering.; ERROR 8: In Figure.tight_layout, an extra “+ 1” is added to the output of engine.execute(self), an operation that is nonsensical and could mask layout computation errors.; ERROR 9: In Figure.set_dpi, after setting dpi, the DPI value is intentionally halved (val/2) and the stale flag update is removed, so the effective DPI is incorrect and the figure might not update as expected.; ERROR 10: In Figure.pick, the logic checking the widget lock is inverted so that the pick method is only called when the widget lock is active, contrary to the intended behavior.","figaspect, __repr__, set_size_inches, get_size_inches, pick, draw, set_dpi, tight_layout"
sympy:core:mul.py,"--------------------------------------------------
from __future__ import annotations
from typing import TYPE_CHECKING, ClassVar

from collections import defaultdict
from functools import reduce
from itertools import product
import operator

from .sympify import sympify
from .basic import Basic, _args_sortkey
from .singleton import S
from .operations import AssocOp, AssocOpDispatcher
from .cache import cacheit
from .intfunc import integer_nthroot, trailing
from .logic import fuzzy_not, _fuzzy_group
from .expr import Expr
from .parameters import global_parameters
from .kind import KindDispatcher
from .traversal import bottom_up
from sympy.utilities.iterables import sift


# internal marker to indicate:
#   ""there are still non-commutative objects -- don't forget to process them""
class NC_Marker:
    is_Order = False
    is_Mul = False
    is_Number = False
    is_Poly = False

    is_commutative = False


def _mulsort(args):
    # in-place sorting of args
    args.sort(key=lambda x: -_args_sortkey(x))


def _unevaluated_Mul(*args):
    """"""Return a well-formed unevaluated Mul: Numbers are collected and
    put in slot 0, any arguments that are Muls will be flattened, and args
    are sorted. Use this when args have changed but you still want to return
    an unevaluated Mul.

    Examples
    ========

    >>> from sympy.core.mul import _unevaluated_Mul as uMul
    >>> from sympy import S, sqrt, Mul
    >>> from sympy.abc import x
    >>> a = uMul(*[S(3.0), x, S(2)])
    >>> a.args[0]
    6.00000000000000
    >>> a.args[1]
    x

    Two unevaluated Muls with the same arguments will
    always compare as equal during testing:

    >>> m = uMul(sqrt(2), sqrt(3))
    >>> m == uMul(sqrt(3), sqrt(2))
    True
    >>> u = Mul(sqrt(3), sqrt(2), evaluate=False)
    >>> m == uMul(u)
    True
    >>> m == Mul(*m.args)
    False

    """"""
    cargs = []
    ncargs = []
    args = list(args)
    co = S.One
    for a in args:
        if a.is_Mul:
            a_c, a_nc = a.args_cnc()
            args.extend(a_c)  # grow args
            ncargs.extend(a_nc)
        # Instead of multiplying numbers to the coefficient, we add them.
        elif a.is_Number:
            co += a
        elif a.is_commutative:
            cargs.append(a)
        else:
            ncargs.append(a)
    _mulsort(cargs)
    if co is not S.One:
        cargs.insert(0, co)
    return Mul._from_args(cargs+ncargs)


class Mul(Expr, AssocOp):
    """"""
    Expression representing multiplication operation for algebraic field.

    .. deprecated:: 1.7

       Using arguments that aren't subclasses of :class:`~.Expr` in core
       operators (:class:`~.Mul`, :class:`~.Add`, and :class:`~.Pow`) is
       deprecated. See :ref:`non-expr-args-deprecated` for details.

    Every argument of ``Mul()`` must be ``Expr``. Infix operator ``*``
    on most scalar objects in SymPy calls this class.

    Another use of ``Mul()`` is to represent the structure of abstract
    multiplication so that its arguments can be substituted to return
    different class. Refer to examples section for this.

    ``Mul()`` evaluates the argument unless ``evaluate=False`` is passed.
    The evaluation logic includes:

    1. Flattening
        ``Mul(x, Mul(y, z))`` -> ``Mul(x, y, z)``

    2. Identity removing
        ``Mul(x, 1, y)`` -> ``Mul(x, y)``

    3. Exponent collecting by ``.as_base_exp()``
        ``Mul(x, x**2)`` -> ``Pow(x, 3)``

    4. Term sorting
        ``Mul(y, x, 2)`` -> ``Mul(2, x, y)``

    Since multiplication can be vector space operation, arguments may
    have the different :obj:`sympy.core.kind.Kind()`. Kind of the
    resulting object is automatically inferred.

    Examples
    ========

    >>> from sympy import Mul
    >>> from sympy.abc import x, y
    >>> Mul(x, 1)
    x
    >>> Mul(x, x)
    x**2

    If ``evaluate=False`` is passed, result is not evaluated.

    >>> Mul(1, 2, evaluate=False)
    1*2
    >>> Mul(x, x, evaluate=False)
    x*x

    ``Mul()`` also represents the general structure of multiplication
    operation.

    >>> from sympy import MatrixSymbol
    >>> A = MatrixSymbol('A', 2,2)
    >>> expr = Mul(x,y).subs({y:A})
    >>> expr
    x*A
    >>> type(expr)
    <class 'sympy.matrices.expressions.matmul.MatMul'>

    See Also
    ========

    MatMul

    """"""
    __slots__ = ()

    is_Mul = True

    _args_type = Expr
    _kind_dispatcher = KindDispatcher(""Mul_kind_dispatcher"", commutative=True)

    identity: ClassVar[Expr]

    @property
    def kind(self):
        arg_kinds = (a.kind for a in self.args)
        return self._kind_dispatcher(*arg_kinds)

    if TYPE_CHECKING:

        def __new__(cls, *args: Expr | complex, evaluate: bool=True) -> Expr: # type: ignore
            ...

        @property
        def args(self) -> tuple[Expr, ...]:
            ...

    def could_extract_minus_sign(self):
        if self == (-self):
            return False  # e.g. zoo*x == -zoo*x
        c = self.args[0]
        return c.is_Number and c.is_extended_negative

    def __neg__(self):
        c, args = self.as_coeff_mul()
        if args[0] is not S.ComplexInfinity:
            c = -c
        if c is not S.One:
            if args[0].is_Number:
                args = list(args)
                if c is S.NegativeOne:
                    args[0] = -args[0]
                else:
                    args[0] *= c
            else:
                args = (c,) + args
        return self._from_args(args, self.is_commutative)

    @classmethod
    def flatten(cls, seq):
        """"""Return commutative, noncommutative and order arguments by
        combining related terms.

        Notes
        =====
            * In an expression like ``a*b*c``, Python process this through SymPy
              as ``Mul(Mul(a, b), c)``. This can have undesirable consequences.

              -  Sometimes terms are not combined as one would like:
                 {c.f. https://github.com/sympy/sympy/issues/4596}

                >>> from sympy import Mul, sqrt
                >>> from sympy.abc import x, y, z
                >>> 2*(x + 1) # this is the 2-arg Mul behavior
                2*x + 2
                >>> y*(x + 1)*2
                2*y*(x + 1)
                >>> 2*(x + 1)*y # 2-arg result will be obtained first
                y*(2*x + 2)
                >>> Mul(2, x + 1, y) # all 3 args simultaneously processed
                2*y*(x + 1)
                >>> 2*((x + 1)*y) # parentheses can control this behavior
                2*y*(x + 1)

                Powers with compound bases may not find a single base to
                combine with unless all arguments are processed at once.
                Post-processing may be necessary in such cases.
                {c.f. https://github.com/sympy/sympy/issues/5728}

                >>> a = sqrt(x*sqrt(y))
                >>> a**3
                (x*sqrt(y))**(3/2)
                >>> Mul(a,a,a)
                (x*sqrt(y))**(3/2)
                >>> a*a*a
                x*sqrt(y)*sqrt(x*sqrt(y))
                >>> _.subs(a.base, z).subs(z, a.base)
                (x*sqrt(y))**(3/2)

              -  If more than two terms are being multiplied then all the
                 previous terms will be re-processed for each new argument.
                 So if each of ``a``, ``b`` and ``c`` were :class:`Mul`
                 expression, then ``a*b*c`` (or building up the product
                 with ``*=``) will process all the arguments of ``a`` and
                 ``b`` twice: once when ``a*b`` is computed and again when
                 ``c`` is multiplied.

                 Using ``Mul(a, b, c)`` will process all arguments once.

            * The results of Mul are cached according to arguments, so flatten
              will only be called once for ``Mul(a, b, c)``. If you can
              structure a calculation so the arguments are most likely to be
              repeats then this can save time in computing the answer. For
              example, say you had a Mul, M, that you wished to divide by ``d[i]``
              and multiply by ``n[i]`` and you suspect there are many repeats
              in ``n``. It would be better to compute ``M*n[i]/d[i]`` rather
              than ``M/d[i]*n[i]`` since every time n[i] is a repeat, the
              product, ``M*n[i]`` will be returned without flattening -- the
              cached value will be returned. If you divide by the ``d[i]``
              first (and those are more unique than the ``n[i]``) then that will
              create a new Mul, ``M/d[i]`` the args of which will be traversed
              again when it is multiplied by ``n[i]``.

                 Using ``Mul(a, b, c)`` will process all arguments once.

            NB
            --
              The validity of the above notes depends on the implementation
              details of Mul and flatten which may change at any time. Therefore,
              you should only consider them when your code is highly performance
              sensitive.

              Removal of 1 from the sequence is already handled by AssocOp.__new__.
        """"""

        from sympy.calculus.accumulationbounds import AccumBounds
        from sympy.matrices.expressions import MatrixExpr
        rv = None
        if len(seq) == 2:
            a, b = seq
            if b.is_Rational:
                a, b = b, a
                seq = [a, b]
            assert a is not S.One
            if a.is_Rational and not a.is_zero:
                r, b = b.as_coeff_Mul()
                if b.is_Add:
                    if r is not S.One:  # 2-arg hack
                        # leave the Mul as a Mul?
                        ar = a*r
                        if ar is S.One:
                            arb = b
                        else:
                            arb = cls(a*r, b, evaluate=False)
                        rv = [arb], [], None
                    elif global_parameters.distribute and b.is_commutative:
                        newb = Add(*[_keep_coeff(a, bi) for bi in b.args])
                        rv = [newb], [], None
            if rv:
                return rv

        # apply associativity, separate commutative part of seq
        c_part = []         # out: commutative factors
        nc_part = []        # out: non-commutative factors

        nc_seq = []

        coeff = S.One       # standalone term
                            # e.g. 3 * ...

        c_powers = []       # (base,exp)      n
                            # e.g. (x,n) for x

        num_exp = []        # (num-base, exp)           y
                            # e.g.  (3, y)  for  ... * 3  * ...

        neg1e = S.Zero      # exponent on -1 extracted from Number-based Pow and I

        pnum_rat = {}       # (num-base, Rat-exp)          1/2
                            # e.g.  (3, 1/2)  for  ... * 3     * ...

        order_symbols = None

        # --- PART 1 ---
        #
        # ""collect powers and coeff"":
        #
        # o coeff
        # o c_powers
        # o num_exp
        # o neg1e
        # o pnum_rat
        #
        # NOTE: this is optimized for all-objects-are-commutative case
        for o in seq:
            # O(x)
            if o.is_Order:
                o, order_symbols = o.as_expr_variables(order_symbols)

            # Mul([...])
            if o.is_Mul:
                if o.is_commutative:
                    seq.extend(o.args)    # XXX zerocopy?

                else:
                    # NCMul can have commutative parts as well
                    for q in o.args:
                        if q.is_commutative:
                            seq.append(q)
                        else:
                            nc_seq.append(q)

                    # append non-commutative marker, so we don't forget to
                    # process scheduled non-commutative objects
                    seq.append(NC_Marker)

                continue

            # 3
            elif o.is_Number:
                if o is S.NaN or coeff is S.ComplexInfinity and o.is_zero:
                    # we know for sure the result will be nan
                    return [S.NaN], [], None
                elif coeff.is_Number or isinstance(coeff, AccumBounds):  # it could be zoo
                    coeff *= o
                    if coeff is S.NaN:
                        # we know for sure the result will be nan
                        return [S.NaN], [], None
                continue

            elif isinstance(o, AccumBounds):
                coeff = o.__mul__(coeff)
                continue

            elif o is S.ComplexInfinity:
                if not coeff:
                    # 0 * zoo = NaN
                    return [S.NaN], [], None
                coeff = S.ComplexInfinity
                continue

            elif not coeff and isinstance(o, Add) and any(
                    _ in (S.NegativeInfinity, S.ComplexInfinity, S.Infinity)
                    for __ in o.args for _ in Mul.make_args(__)):
                # e.g 0 * (x + oo) = NaN but not
                # 0 * (1 + Integral(x, (x, 0, oo))) which is
                # treated like 0 * x -> 0
                return [S.NaN], [], None

            elif o is S.ImaginaryUnit:
                neg1e += S.Half
                continue

            elif o.is_commutative:
                #      e
                # o = b
                b, e = o.as_base_exp()

                #  y
                # 3
                if o.is_Pow:
                    if b.is_Number:

                        # get all the factors with numeric base so they can be
                        # combined below, but don't combine negatives unless
                        # the exponent is an integer
                        if e.is_Rational:
                            if e.is_Integer:
                                coeff *= Pow(b, e)  # it is an unevaluated power
                                continue
                            elif e.is_negative:    # also a sign of an unevaluated power
                                seq.append(Pow(b, e))
                                continue
                            elif b.is_negative:
                                neg1e += e
                                b = -b
                            if b is not S.One:
                                pnum_rat.setdefault(b, []).append(e)
                            continue
                        elif b.is_positive or e.is_integer:
                            num_exp.append((b, e))
                            continue

                c_powers.append((b, e))

            # NON-COMMUTATIVE
            # TODO: Make non-commutative exponents not combine automatically
            else:
                if o is not NC_Marker:
                    nc_seq.append(o)

                # process nc_seq (if any)
                while nc_seq:
                    o = nc_seq.pop(0)
                    if not nc_part:
                        nc_part.append(o)
                        continue

                    #                             b    c       b+c
                    # try to combine last terms: a  * a   ->  a
                    o1 = nc_part.pop()
                    b1, e1 = o1.as_base_exp()
                    b2, e2 = o.as_base_exp()
                    new_exp = e1 + e2
                    # Only allow powers to combine if the new exponent is
                    # not an Add. This allow things like a**2*b**3 == a**5
                    # if a.is_commutative == False, but prohibits
                    # a**x*a**y and x**a*x**b from combining (x,y commute).
                    if b1 == b2 and (not new_exp.is_Add):
                        o12 = b1 ** new_exp

                        # now o12 could be a commutative object
                        if o12.is_commutative:
                            seq.append(o12)
                            continue
                        else:
                            nc_seq.insert(0, o12)

                    else:
                        nc_part.extend([o1, o])

        # We do want a combined exponent if it would not be an Add, such as
        #  y    2y     3y
        # x  * x   -> x
        # We determine if two exponents have the same term by using
        # as_coeff_Mul.
        #
        # Unfortunately, this isn't smart enough to consider combining into
        # exponents that might already be adds, so things like:
        #  z - y    y
        # x      * x  will be left alone.  This is because checking every possible
        # combination can slow things down.

        # gather exponents of common bases...
        def _gather(c_powers):
            common_b = {}  # b:e
            for b, e in c_powers:
                co = e.as_coeff_Mul()
                common_b.setdefault(b, {}).setdefault(
                    co[1], []).append(co[0])
            for b, d in common_b.items():
                for di, li in d.items():
                    d[di] = Add(*li)
            new_c_powers = []
            for b, e in common_b.items():
                new_c_powers.extend([(b, c*t) for t, c in e.items()])
            return new_c_powers

        # in c_powers
        c_powers = _gather(c_powers)

        # and in num_exp
        num_exp = _gather(num_exp)

        # --- PART 2 ---
        #
        # o process collected powers  (x**0 -> 1; x**1 -> x; otherwise Pow)
        # o combine collected powers  (2**x * 3**x -> 6**x)
        #   with numeric base

        # ................................
        # now we have:
        # - coeff:
        # - c_powers:    (b, e)
        # - num_exp:     (2, e)
        # - pnum_rat:    {(1/3, [1/3, 2/3, 1/4])}

        #  0             1
        # x  -> 1       x  -> x

        # this should only need to run twice; if it fails because
        # it needs to be run more times, perhaps this should be
        # changed to a ""while True"" loop -- the only reason it
        # isn't such now is to allow a less-than-perfect result to
        # be obtained rather than raising an error or entering an
        # infinite loop
        for i in range(2):
            new_c_powers = []
            changed = False
            for b, e in c_powers:
                if e.is_zero:
                    # canceling out infinities yields NaN
                    if (b.is_Add or b.is_Mul) and any(infty in b.args
                        for infty in (S.ComplexInfinity, S.Infinity,
                                      S.NegativeInfinity)):
                        return [S.NaN], [], None
                    continue
                if e is S.One:
                    if b.is_Number:
                        coeff *= b
                        continue
                    p = b
                if e is not S.One:
                    p = Pow(b, e)
                    # check to make sure that the base doesn't change
                    # after exponentiation; to allow for unevaluated
                    # Pow, we only do so if b is not already a Pow
                    if p.is_Pow and not b.is_Pow:
                        bi = b
                        b, e = p.as_base_exp()
                        if b != bi:
                            changed = True
                c_part.append(p)
                new_c_powers.append((b, e))
            # there might have been a change, but unless the base
            # matches some other base, there is nothing to do
            if changed and len({
                    b for b, e in new_c_powers}) != len(new_c_powers):
                # start over again
                c_part = []
                c_powers = _gather(new_c_powers)
            else:
                break

        #  x    x     x
        # 2  * 3  -> 6
        inv_exp_dict = {}   # exp:Mul(num-bases)     x    x
                            # e.g.  x:6  for  ... * 2  * 3  * ...
        for b, e in num_exp:
            inv_exp_dict.setdefault(e, []).append(b)
        for e, b in inv_exp_dict.items():
            inv_exp_dict[e] = cls(*b)
        c_part.extend([Pow(b, e) for e, b in inv_exp_dict.items() if e])

        # b, e -> e' = sum(e), b
        # {(1/5, [1/3]), (1/2, [1/12, 1/4]} -> {(1/3, [1/5, 1/2])}
        comb_e = {}
        for b, e in pnum_rat.items():
            comb_e.setdefault(Add(*e), []).append(b)
        del pnum_rat
        # process them, reducing exponents to values less than 1
        # and updating coeff if necessary else adding them to
        # num_rat for further processing
        num_rat = []
        for e, b in comb_e.items():
            b = cls(*b)
            if e.q == 1:
                coeff *= Pow(b, e)
                continue
            if e.p > e.q:
                e_i, ep = divmod(e.p, e.q)
                coeff *= Pow(b, e_i)
                e = Rational(ep, e.q)
            num_rat.append((b, e))
        del comb_e

        # extract gcd of bases in num_rat
        # 2**(1/3)*6**(1/4) -> 2**(1/3+1/4)*3**(1/4)
        pnew = defaultdict(list)
        i = 0  # steps through num_rat which may grow
        while i < len(num_rat):
            bi, ei = num_rat[i]
            if bi == 1:
                i += 1
                continue
            grow = []
            for j in range(i + 1, len(num_rat)):
                bj, ej = num_rat[j]
                g = bi.gcd(bj)
                if g is not S.One:
                    # 4**r1*6**r2 -> 2**(r1+r2)  *  2**r1 *  3**r2
                    # this might have a gcd with something else
                    e = ei + ej
                    if e.q == 1:
                        coeff *= Pow(g, e)
                    else:
                        if e.p > e.q:
                            e_i, ep = divmod(e.p, e.q)  # change e in place
                            coeff *= Pow(g, e_i)
                            e = Rational(ep, e.q)
                        grow.append((g, e))
                    # update the jth item
                    num_rat[j] = (bj/g, ej)
                    # update bi that we are checking with
                    bi = bi/g
                    if bi is S.One:
                        break
            if bi is not S.One:
                obj = Pow(bi, ei)
                if obj.is_Number:
                    coeff *= obj
                else:
                    # changes like sqrt(12) -> 2*sqrt(3)
                    for obj in Mul.make_args(obj):
                        if obj.is_Number:
                            coeff *= obj
                        else:
                            assert obj.is_Pow
                            bi, ei = obj.args
                            pnew[ei].append(bi)

            num_rat.extend(grow)
            i += 1

        # combine bases of the new powers
        for e, b in pnew.items():
            pnew[e] = cls(*b)

        # handle -1 and I
        if neg1e:
            # treat I as (-1)**(1/2) and compute -1's total exponent
            p, q =  neg1e.as_numer_denom()
            # if the integer part is odd, extract -1
            n, p = divmod(p, q)
            if n % 2:
                coeff = -coeff
            # if it's a multiple of 1/2 extract I
            if q == 2:
                c_part.append(S.ImaginaryUnit)
            elif p:
                # see if there is any positive base this power of
                # -1 can join
                neg1e = Rational(p, q)
                for e, b in pnew.items():
                    if e == neg1e and b.is_positive:
                        pnew[e] = -b
                        break
                else:
                    # keep it separate; we've already evaluated it as
                    # much as possible so evaluate=False
                    c_part.append(Pow(S.NegativeOne, neg1e, evaluate=False))

        # add all the pnew powers
        c_part.extend([Pow(b, e) for e, b in pnew.items()])

        # oo, -oo
        if coeff in (S.Infinity, S.NegativeInfinity):
            def _handle_for_oo(c_part, coeff_sign):
                new_c_part = []
                for t in c_part:
                    if t.is_extended_positive:
                        continue
                    if t.is_extended_negative:
                        coeff_sign *= -1
                        continue
                    new_c_part.append(t)
                return new_c_part, coeff_sign
            c_part, coeff_sign = _handle_for_oo(c_part, 1)
            nc_part, coeff_sign = _handle_for_oo(nc_part, coeff_sign)
            coeff *= coeff_sign

        # zoo
        if coeff is S.ComplexInfinity:
            # zoo might be
            #   infinite_real + bounded_im
            #   bounded_real + infinite_im
            #   infinite_real + infinite_im
            # and non-zero real or imaginary will not change that status.
            c_part = [c for c in c_part if not (fuzzy_not(c.is_zero) and
                                                c.is_extended_real is not None)]
            nc_part = [c for c in nc_part if not (fuzzy_not(c.is_zero) and
                                                  c.is_extended_real is not None)]

        # 0
        elif coeff.is_zero:
            # we know for sure the result will be 0 except the multiplicand
            # is infinity or a matrix
            if any(isinstance(c, MatrixExpr) for c in nc_part):
                return [coeff], nc_part, order_symbols
            if any(c.is_finite == False for c in c_part):
                return [S.NaN], [], order_symbols
            return [coeff], [], order_symbols

        # check for straggling Numbers that were produced
        _new = []
        for i in c_part:
            if i.is_Number:
                coeff *= i
            else:
                _new.append(i)
        c_part = _new

        # order commutative part canonically
        _mulsort(c_part)

        # current code expects coeff to be always in slot-0
        if coeff is not S.One:
            c_part.insert(0, coeff)

        # we are done
        if (global_parameters.distribute and not nc_part and len(c_part) == 2 and
                c_part[0].is_Number and c_part[0].is_finite and c_part[1].is_Add):
            # 2*(1+a) -> 2 + 2 * a
            coeff = c_part[0]
            c_part = [Add(*[coeff*f for f in c_part[1].args])]

        return c_part, nc_part, order_symbols

    def _eval_power(self, expt):

        # don't break up NC terms: (A*B)**3 != A**3*B**3, it is A*B*A*B*A*B
        cargs, nc = self.args_cnc(split_1=False)

        if expt.is_Integer:
            return Mul(*[Pow(b, expt, evaluate=False) for b in cargs]) * \
                Pow(Mul._from_args(nc), expt - 1, evaluate=False)
        if expt.is_Rational and expt.q == 2:
            if self.is_imaginary:
                a = self.as_real_imag()[1]
                if a.is_Rational:
                    n, d = abs(a/2).as_numer_denom()
                    n, t = integer_nthroot(n, 2)
                    if t:
                        d, t = integer_nthroot(d, 2)
                        if t:
                            from sympy.functions.elementary.complexes import sign
                            r = sympify(n)/d
                            return _unevaluated_Mul(r**expt.p, (1 + sign(a)*S.ImaginaryUnit)**expt.p)

        p = Pow(self, expt, evaluate=False)

        if expt.is_Rational or expt.is_Float:
            return p._eval_expand_power_base()

        return p

    @classmethod
    def class_key(cls):
        return 3, 0, cls.__name__

    def _eval_evalf(self, prec):
        c, m = self.as_coeff_Mul()
        if c is S.NegativeOne:
            if m.is_Mul:
                rv = -AssocOp._eval_evalf(m, prec)
            else:
                mnew = m._eval_evalf(prec)
                if mnew is not None:
                    m = mnew
                rv = -m
        else:
            rv = AssocOp._eval_evalf(self, prec)
        if rv.is_number:
            return rv.expand()
        return rv

    @property
    def _mpc_(self):
        """"""
        Convert self to an mpmath mpc if possible
        """"""
        from .numbers import Float
        im_part, imag_unit = self.as_coeff_Mul()
        if imag_unit is not S.ImaginaryUnit:
            # ValueError may seem more reasonable but since it's a @property,
            # we need to use AttributeError to keep from confusing things like
            # hasattr.
            raise AttributeError(""Cannot convert Mul to mpc. Must be of the form Number*I"")

        return (Float(0)._mpf_, Float(im_part)._mpf_)

    @cacheit
    def as_two_terms(self):
        """"""Return head and tail of self.

        This is the most efficient way to get the head and tail of an
        expression.

        - if you want only the head, use self.args[0];
        - if you want to process the arguments of the tail then use
          self.as_coef_mul() which gives the head and a tuple containing
          the arguments of the tail when treated as a Mul.
        - if you want the coefficient when self is treated as an Add
          then use self.as_coeff_add()[0]

        Examples
        ========

        >>> from sympy.abc import x, y
        >>> (3*x*y).as_two_terms()
        (3, x*y)
        """"""
        args = self.args

        if len(args) == 1:
            return S.One, self
        elif len(args) == 2:
            return args

        else:
            return args[0], self._new_rawargs(*args[1:])

    @cacheit
    def as_coeff_mul(self, *deps, rational=True, **kwargs):
        if deps:
            l1, l2 = sift(self.args, lambda x: x.has(*deps), binary=True)
            return self._new_rawargs(*l2), tuple(l1)
        args = self.args
        if args[0].is_Number:
            if not rational and args[0].is_Rational:
                return args[0], args[1:]
            elif args[0].is_extended_negative:
                return S.NegativeOne, (-args[0],) + args[1:]
        return S.One, args

    def as_coeff_Mul(self, rational=False):
        """"""
        Efficiently extract the coefficient of a product.
        """"""
        coeff, args = self.args[0], self.args[1:]

        if coeff.is_Number:
            if not rational or coeff.is_Rational:
                if len(args) == 1:
                    return coeff, args[0]
                else:
                    return coeff, self._new_rawargs(*args)
            elif coeff.is_extended_negative:
                return S.NegativeOne, self._new_rawargs(*((-coeff,) + args))
        return S.One, self

    def as_real_imag(self, deep=True, **hints):
        from sympy.functions.elementary.complexes import Abs, im, re
        other = []
        coeffr = []
        coeffi = []
        addterms = S.One
        for a in self.args:
            r, i = a.as_real_imag()
            if i.is_zero:
                coeffr.append(r)
            elif r.is_zero:
                coeffi.append(i*S.ImaginaryUnit)
            elif a.is_commutative:
                aconj = a.conjugate() if other else None
                # search for complex conjugate pairs:
                for i, x in enumerate(other):
                    if x == aconj:
                        coeffr.append(Abs(x)**2)
                        del other[i]
                        break
                else:
                    if a.is_Add:
                        addterms *= a
                    else:
                        other.append(a)
            else:
                other.append(a)
        m = self.func(*other)
        if hints.get('ignore') == m:
            return
        if len(coeffi) % 2:
            imco = im(coeffi.pop(0))
            # all other pairs make a real factor; they will be
            # put into reco below
        else:
            imco = S.Zero
        reco = self.func(*(coeffr + coeffi))
        r, i = (reco*re(m), reco*im(m))
        if addterms == 1:
            if m == 1:
                if imco.is_zero:
                    return (reco, S.Zero)
                else:
                    return (S.Zero, reco*imco)
            if imco is S.Zero:
                return (r, i)
            return (-imco*i, imco*r)
        from .function import expand_mul
        addre, addim = expand_mul(addterms, deep=False).as_real_imag()
        if imco is S.Zero:
            return (r*addre - i*addim, i*addre + r*addim)
        else:
            r, i = -imco*i, imco*r
            return (r*addre - i*addim, r*addim + i*addre)

    @staticmethod
    def _expandsums(sums):
        """"""
        Helper function for _eval_expand_mul.

        sums must be a list of instances of Basic.
        """"""

        L = len(sums)
        if L == 1:
            return sums[0].args
        terms = []
        left = Mul._expandsums(sums[:L//2])
        right = Mul._expandsums(sums[L//2:])

        terms = [Mul(a, b) for a in left for b in right]
        added = Add(*terms)
        return Add.make_args(added)  # it may have collapsed down to one term

    def _eval_expand_mul(self, **hints):
        from sympy.simplify.radsimp import fraction

        # Handle things like 1/(x*(x + 1)), which are automatically converted
        # to 1/x*1/(x + 1)
        expr = self
        # default matches fraction's default
        n, d = fraction(expr, hints.get('exact', False))
        if d.is_Mul:
            n, d = [i._eval_expand_mul(**hints) if i.is_Mul else i
                for i in (n, d)]
        expr = n/d
        if not expr.is_Mul:
            return expr

        plain, sums, rewrite = [], [], False
        for factor in expr.args:
            if factor.is_Add:
                sums.append(factor)
                rewrite = True
            else:
                if factor.is_commutative:
                    plain.append(factor)
                else:
                    sums.append(Basic(factor))  # Wrapper

        if not rewrite:
            return expr
        else:
            plain = self.func(*plain)
            if sums:
                deep = hints.get(""deep"", False)
                terms = self.func._expandsums(sums)
                args = []
                for term in terms:
                    t = self.func(plain, term)
                    if t.is_Mul and any(a.is_Add for a in t.args) and deep:
                        t = t._eval_expand_mul()
                    args.append(t)
                return Add(*args)
            else:
                return plain

    @cacheit
    def _eval_derivative(self, s):
        args = list(self.args)
        terms = []
        for i in range(len(args)):
            d = args[i].diff(s)
            if d:
                terms.append(reduce(lambda x, y: x+y, (args[:i] + [d] + args[i + 1:]), S.One))
        return Add.fromiter(terms)

    @cacheit
    def _eval_derivative_n_times(self, s, n):
        from .function import AppliedUndef
        from .symbol import Symbol, symbols, Dummy
        if not isinstance(s, (AppliedUndef, Symbol)):
            # other types of s may not be well behaved, e.g.
            # (cos(x)*sin(y)).diff([[x, y, z]])
            return super()._eval_derivative_n_times(s, n)
        from .numbers import Integer
        args = self.args
        m = len(args)
        if isinstance(n, (int, Integer)):
            # https://en.wikipedia.org/wiki/General_Leibniz_rule#More_than_two_factors
            terms = []
            from sympy.ntheory.multinomial import multinomial_coefficients_iterator
            for kvals, c in multinomial_coefficients_iterator(m, n):
                p = Mul(*[arg.diff((s, k)) for k, arg in zip(kvals, args)])
                terms.append(c * p)
            return Add(*terms)
        from sympy.concrete.summations import Sum
        from sympy.functions.combinatorial.factorials import factorial
        from sympy.functions.elementary.miscellaneous import Max
        kvals = symbols(""k1:%i"" % m, cls=Dummy)
        klast = n - sum(kvals)
        nfact = factorial(n)
        e, l = (# better to use the multinomial?
            nfact/prod(map(factorial, kvals))/factorial(klast)*\
            Mul(*[args[t].diff((s, kvals[t])) for t in range(m-1)])*\
            args[-1].diff((s, Max(0, klast))),
            [(k, 0, n) for k in kvals])
        return Sum(e, *l)

    def _eval_difference_delta(self, n, step):
        from sympy.series.limitseq import difference_delta as dd
        arg0 = self.args[0]
        rest = Mul(*self.args[1:])
        return (arg0.subs(n, n + step) * dd(rest, n, step) + dd(arg0, n, step) *
                rest)

    def _matches_simple(self, expr, repl_dict):
        # handle (w*3).matches('x*5') -> {w: x*5/3}
        coeff, terms = self.as_coeff_mul()
        terms = Mul.make_args(terms)
        if len(terms) == 1:
            newexpr = self.__class__._combine_inverse(expr, coeff)
            return terms[0].matches(newexpr, repl_dict)
        return

    def matches(self, expr, repl_dict=None, old=False):
        expr = sympify(expr)
        if self.is_commutative and expr.is_commutative:
            return self._matches_commutative(expr, repl_dict, old)
        elif self.is_commutative is not expr.is_commutative:
            return None

        # Proceed only if both both expressions are non-commutative
        c1, nc1 = self.args_cnc()
        c2, nc2 = expr.args_cnc()
        c1, c2 = [c or [1] for c in [c1, c2]]

        # TODO: Should these be self.func?
        comm_mul_self = Mul(*c1)
        comm_mul_expr = Mul(*c2)

        repl_dict = comm_mul_self.matches(comm_mul_expr, repl_dict, old)

        # If the commutative arguments didn't match and aren't equal, then
        # then the expression as a whole doesn't match
        if not repl_dict and c1 != c2:
            return None

        # Now match the non-commutative arguments, expanding powers to
        # multiplications
        nc1 = Mul._matches_expand_pows(nc1)
        nc2 = Mul._matches_expand_pows(nc2)

        repl_dict = Mul._matches_noncomm(nc1, nc2, repl_dict)

        return repl_dict or None

    @staticmethod
    def _matches_expand_pows(arg_list):
        new_args = []
        for arg in arg_list:
            if arg.is_Pow and arg.exp > 0:
                new_args.extend([arg.base] * arg.exp)
            else:
                new_args.append(arg)
        return new_args

    @staticmethod
    def _matches_noncomm(nodes, targets, repl_dict=None):
        """"""Non-commutative multiplication matcher.

        `nodes` is a list of symbols within the matcher multiplication
        expression, while `targets` is a list of arguments in the
        multiplication expression being matched against.
        """"""
        if repl_dict is None:
            repl_dict = {}
        else:
            repl_dict = repl_dict.copy()

        # List of possible future states to be considered
        agenda = []
        # The current matching state, storing index in nodes and targets
        state = (0, 0)
        node_ind, target_ind = state
        # Mapping between wildcard indices and the index ranges they match
        wildcard_dict = {}

        while target_ind < len(targets) and node_ind < len(nodes):
            node = nodes[node_ind]

            if node.is_Wild:
                Mul._matches_add_wildcard(wildcard_dict, state)

            states_matches = Mul._matches_new_states(wildcard_dict, state,
                                                     nodes, targets)
            if states_matches:
                new_states, new_matches = states_matches
                agenda.extend(new_states)
                if new_matches:
                    for match in new_matches:
                        repl_dict[match] = new_matches[match]
            if not agenda:
                return None
            else:
                state = agenda.pop()
                node_ind, target_ind = state

        return repl_dict

    @staticmethod
    def _matches_add_wildcard(dictionary, state):
        node_ind, target_ind = state
        if node_ind in dictionary:
            begin, end = dictionary[node_ind]
            dictionary[node_ind] = (begin, target_ind)
        else:
            dictionary[node_ind] = (target_ind, target_ind)

    @staticmethod
    def _matches_new_states(dictionary, state, nodes, targets):
        node_ind, target_ind = state
        node = nodes[node_ind]
        target = targets[target_ind]

        # Don't advance at all if we've exhausted the targets but not the nodes
        if target_ind >= len(targets) - 1 and node_ind < len(nodes) - 1:
            return None

        if node.is_Wild:
            match_attempt = Mul._matches_match_wilds(dictionary, node_ind,
                                                     nodes, targets)
            if match_attempt:
                # If the same node has been matched before, don't return
                # anything if the current match is diverging from the previous
                # match
                other_node_inds = Mul._matches_get_other_nodes(dictionary,
                                                               nodes, node_ind)
                for ind in other_node_inds:
                    other_begin, other_end = dictionary[ind]
                    curr_begin, curr_end = dictionary[node_ind]

                    other_targets = targets[other_begin:other_end + 1]
                    current_targets = targets[curr_begin:curr_end + 1]

                    for curr, other in zip(current_targets, other_targets):
                        if curr != other:
                            return None

                # A wildcard node can match more than one target, so only the
                # target index is advanced
                new_state = [(node_ind, target_ind + 1)]
                # Only move on to the next node if there is one
                if node_ind < len(nodes) - 1:
                    new_state.append((node_ind + 1, target_ind + 1))
                return new_state, match_attempt
        else:
            # If we're not at a wildcard, then make sure we haven't exhausted
            # nodes but not targets, since in this case one node can only match
            # one target
            if node_ind >= len(nodes) - 1 and target_ind < len(targets) - 1:
                return None

            match_attempt = node.matches(target)

            if match_attempt:
                return [(node_ind + 1, target_ind + 1)], match_attempt
            elif node == target:
                return [(node_ind + 1, target_ind + 1)], None
            else:
                return None

    @staticmethod
    def _matches_match_wilds(dictionary, wildcard_ind, nodes, targets):
        """"""Determine matches of a wildcard with sub-expression in `target`.""""""
        wildcard = nodes[wildcard_ind]
        begin, end = dictionary[wildcard_ind]
        terms = targets[begin:end + 1]
        # TODO: Should this be self.func?
        mult = Mul(*terms) if len(terms) > 1 else terms[0]
        return wildcard.matches(mult)

    @staticmethod
    def _matches_get_other_nodes(dictionary, nodes, node_ind):
        """"""Find other wildcards that may have already been matched.""""""
        ind_node = nodes[node_ind]
        return [ind for ind in dictionary if nodes[ind] == ind_node]

    @staticmethod
    def _combine_inverse(lhs, rhs):
        """"""
        Returns lhs/rhs, but treats arguments like symbols, so things
        like oo/oo return 1 (instead of a nan) and ``I`` behaves like
        a symbol instead of sqrt(-1).
        """"""
        from sympy.simplify.simplify import signsimp
        from .symbol import Dummy
        if lhs == rhs:
            return S.One

        def check(l, r):
            if l.is_Float and r.is_comparable:
                # if both objects are added to 0 they will share the same ""normalization""
                # and are more likely to compare the same. Since Add(foo, 0) will not allow
                # the 0 to pass, we use __add__ directly.
                return l.__add__(0) == r.evalf().__add__(0)
            return False
        if check(lhs, rhs) or check(rhs, lhs):
            return S.One
        if any(i.is_Pow or i.is_Mul for i in (lhs, rhs)):
            # gruntz and limit wants a literal I to not combine
            # with a power of -1
            d = Dummy('I')
            _i = {S.ImaginaryUnit: d}
            i_ = {d: S.ImaginaryUnit}
            a = lhs.xreplace(_i).as_powers_dict()
            b = rhs.xreplace(_i).as_powers_dict()
            blen = len(b)
            for bi in tuple(b.keys()):
                if bi in a:
                    a[bi] -= b.pop(bi)
                    if not a[bi]:
                        a.pop(bi)
            if len(b) != blen:
                lhs = Mul(*[k**v for k, v in a.items()]).xreplace(i_)
                rhs = Mul(*[k**v for k, v in b.items()]).xreplace(i_)
        rv = lhs/rhs
        srv = signsimp(rv)
        return srv if srv.is_Number else rv

    def as_powers_dict(self):
        d = defaultdict(int)
        for term in self.args:
            for b, e in term.as_powers_dict().items():
                d[b] -= e
        return d

    def as_numer_denom(self):
        # don't use _from_args to rebuild the numerators and denominators
        # as the order is not guaranteed to be the same once they have
        # been separated from each other
        numers, denoms = list(zip(*[f.as_numer_denom() for f in self.args]))
        return self.func(*numers), self.func(*denoms)

    def as_base_exp(self):
        e1 = None
        bases = []
        nc = 0
        for m in self.args:
            b, e = m.as_base_exp()
            if not b.is_commutative:
                nc += 1
            if e1 is None:
                e1 = e
            elif e != e1 or nc > 1 or not e.is_Integer:
                return self, S.One
            bases.append(b)
        return self.func(*bases), e1

    def _eval_is_polynomial(self, syms):
        return all(term._eval_is_polynomial(syms) for term in self.args)

    def _eval_is_rational_function(self, syms):
        return all(term._eval_is_rational_function(syms) for term in self.args)

    def _eval_is_meromorphic(self, x, a):
        return _fuzzy_group((arg.is_meromorphic(x, a) for arg in self.args),
                            quick_exit=True)

    def _eval_is_algebraic_expr(self, syms):
        return all(term._eval_is_algebraic_expr(syms) for term in self.args)

    _eval_is_commutative = lambda self: _fuzzy_group(
        a.is_commutative for a in self.args)

    def _eval_is_complex(self):
        comp = _fuzzy_group(a.is_complex for a in self.args)
        if comp is False:
            if any(a.is_infinite for a in self.args):
                if any(a.is_zero is not False for a in self.args):
                    return None
                return False
        return comp

    def _eval_is_zero_infinite_helper(self):
        #
        # Helper used by _eval_is_zero and _eval_is_infinite.
        #
        # Three-valued logic is tricky so let us reason this carefully. It
        # would be nice to say that we just check is_zero/is_infinite in all
        # args but we need to be careful about the case that one arg is zero
        # and another is infinite like Mul(0, oo) or more importantly a case
        # where it is not known if the arguments are zero or infinite like
        # Mul(y, 1/x). If either y or x could be zero then there is a
        # *possibility* that we have Mul(0, oo) which should give None for both
        # is_zero and is_infinite.
        #
        # We keep track of whether we have seen a zero or infinity but we also
        # need to keep track of whether we have *possibly* seen one which
        # would be indicated by None.
        #
        # For each argument there is the possibility that is_zero might give
        # True, False or None and likewise that is_infinite might give True,
        # False or None, giving 9 combinations. The True cases for is_zero and
        # is_infinite are mutually exclusive though so there are 3 main cases:
        #
        # - is_zero = True
        # - is_infinite = True
        # - is_zero and is_infinite are both either False or None
        #
        # At the end seen_zero and seen_infinite can be any of 9 combinations
        # of True/False/None. Unless one is False though we cannot return
        # anything except None:
        #
        # - is_zero=True needs seen_zero=True and seen_infinite=False
        # - is_zero=False needs seen_zero=False
        # - is_infinite=True needs seen_infinite=True and seen_zero=False
        # - is_infinite=False needs seen_infinite=False
        # - anything else gives both is_zero=None and is_infinite=None
        #
        # The loop only sets the flags to True or None and never back to False.
        # Hence as soon as neither flag is False we exit early returning None.
        # In particular as soon as we encounter a single arg that has
        # is_zero=is_infinite=None we exit. This is a common case since it is
        # the default assumptions for a Symbol and also the case for most
        # expressions containing such a symbol. The early exit gives a big
        # speedup for something like Mul(*symbols('x:1000')).is_zero.
        #
        seen_zero = seen_infinite = False

        for a in self.args:
            if a.is_zero:
                if seen_infinite is not False:
                    return None, None
                seen_zero = True
            elif a.is_infinite:
                if seen_zero is not False:
                    return None, None
                seen_infinite = True
            else:
                if seen_zero is False and a.is_zero is None:
                    if seen_infinite is not False:
                        return None, None
                    seen_zero = None
                if seen_infinite is False and a.is_infinite is None:
                    if seen_zero is not False:
                        return None, None
                    seen_infinite = None

        return seen_zero, seen_infinite

    def _eval_is_zero(self):
        # True iff any arg is zero and no arg is infinite but need to handle
        # three valued logic carefully.
        seen_zero, seen_infinite = self._eval_is_zero_infinite_helper()

        if seen_zero is False:
            return False
        elif seen_zero is True and seen_infinite is False:
            return True
        else:
            return None

    def _eval_is_infinite(self):
        # True iff any arg is infinite and no arg is zero but need to handle
        # three valued logic carefully.
        seen_zero, seen_infinite = self._eval_is_zero_infinite_helper()

        if seen_infinite is True and seen_zero is False:
            return True
        elif seen_infinite is False:
            return False
        else:
            return None

    # We do not need to implement _eval_is_finite because the assumptions
    # system can infer it from finite = not infinite.

    def _eval_is_rational(self):
        r = _fuzzy_group((a.is_rational for a in self.args), quick_exit=True)
        if r:
            return r
        elif r is False:
            # All args except one are rational
            if all(a.is_zero is False for a in self.args):
                return False

    def _eval_is_algebraic(self):
        r = _fuzzy_group((a.is_algebraic for a in self.args), quick_exit=True)
        if r:
            return r
        elif r is False:
            # All args except one are algebraic
            if all(a.is_zero is False for a in self.args):
                return False

    # without involving odd/even checks this code would suffice:
    #_eval_is_integer = lambda self: _fuzzy_group(
    #    (a.is_integer for a in self.args), quick_exit=True)
    def _eval_is_integer(self):
        is_rational = self._eval_is_rational()
        if is_rational is False:
            return False

        numerators = []
        denominators = []
        unknown = False
        for a in self.args:
            hit = False
            if a.is_integer:
                if abs(a) is not S.One:
                    numerators.append(a)
            elif a.is_Rational:
                n, d = a.as_numer_denom()
                if abs(n) is not S.One:
                    numerators.append(n)
                if d is not S.One:
                    denominators.append(d)
            elif a.is_Pow:
                b, e = a.as_base_exp()
                if not b.is_integer or not e.is_integer:
                    hit = unknown = True
                if e.is_negative:
                    denominators.append(2 if a is S.Half else
                        Pow(a, S.NegativeOne))
                elif not hit:
                    # int b and pos int e: a = b**e is integer
                    assert not e.is_positive
                    # for rational self and e equal to zero: a = b**e is 1
                    assert not e.is_zero
                    return # sign of e unknown -> self.is_integer unknown
                else:
                    # x**2, 2**x, or x**y with x and y int-unknown -> unknown
                    return
            else:
                return

        if not denominators and not unknown:
            return True

        allodd = lambda x: all(i.is_odd for i in x)
        alleven = lambda x: all(i.is_even for i in x)
        anyeven = lambda x: any(i.is_even for i in x)

        from .relational import is_gt
        if not numerators and denominators and all(
                is_gt(_, S.One) for _ in denominators):
            return False
        elif unknown:
            return
        elif allodd(numerators) and anyeven(denominators):
            return False
        elif anyeven(numerators) and denominators == [2]:
            return True
        elif alleven(numerators) and allodd(denominators
                ) and (Mul(*denominators, evaluate=False) - 1
                ).is_positive:
            return False
        if len(denominators) == 1:
            d = denominators[0]
            if d.is_Integer and d.is_even:
                # if minimal power of 2 in num vs den is not
                # negative then we have an integer
                if (Add(*[i.as_base_exp()[1] for i in
                        Mul.make_args(numerators) if i.is_even]) - trailing(d.p)
                        ).is_nonnegative:
                    return True
        if len(numerators) == 1:
            n = numerators[0]
            if n.is_Integer and n.is_even:
                # if minimal power of 2 in den vs num is positive
                # then we have have a non-integer
                if (Add(*[i.as_base_exp()[1] for i in
                        denominators if i.is_even]) - trailing(n.p)
                        ).is_positive:
                    return False

    def _eval_is_polar(self):
        has_polar = any(arg.is_polar for arg in self.args)
        return has_polar and \
            all(arg.is_polar or arg.is_positive for arg in self.args)

    def _eval_is_extended_real(self):
        return self._eval_real_imag(True)

    def _eval_real_imag(self, real):
        zero = False
        t_not_re_im = None

        for t in self.args:
            if (t.is_complex or t.is_infinite) is False and t.is_extended_real is False:
                return False
            elif t.is_imaginary:  # I
                real = not real
            elif t.is_extended_real:  # 2
                if not zero:
                    z = t.is_zero
                    if not z and zero is False:
                        zero = z
                    elif z:
                        if all(a.is_finite for a in self.args):
                            return True
                        return
            elif t.is_extended_real is False:
                # symbolic or literal like `2 + I` or symbolic imaginary
                if t_not_re_im:
                    return  # complex terms might cancel
                t_not_re_im = t
            elif t.is_imaginary is False:  # symbolic like `2` or `2 + I`
                if t_not_re_im:
                    return  # complex terms might cancel
                t_not_re_im = t
            else:
                return

        if t_not_re_im:
            if t_not_re_im.is_extended_real is False:
                if real:  # like 3
                    return zero  # 3*(smthng like 2 + I or i) is not real
            if t_not_re_im.is_imaginary is False:  # symbolic 2 or 2 + I
                if not real:  # like I
                    return zero  # I*(smthng like 2 or 2 + I) is not real
        elif zero is False:
            return real  # can't be trumped by 0
        elif real:
            return real  # doesn't matter what zero is

    def _eval_is_imaginary(self):
        if all(a.is_zero is False and a.is_finite for a in self.args):
            return self._eval_real_imag(False)

    def _eval_is_hermitian(self):
        return self._eval_herm_antiherm(True)

    def _eval_is_antihermitian(self):
        return self._eval_herm_antiherm(False)

    def _eval_herm_antiherm(self, herm):
        for t in self.args:
            if t.is_hermitian is None or t.is_antihermitian is None:
                return
            if t.is_hermitian:
                continue
            elif t.is_antihermitian:
                herm = not herm
            else:
                return

        if herm is not False:
            return herm

        is_zero = self._eval_is_zero()
        if is_zero:
            return True
        elif is_zero is False:
            return herm

    def _eval_is_irrational(self):
        for t in self.args:
            a = t.is_irrational
            if a:
                others = list(self.args)
                others.remove(t)
                if all((x.is_rational and fuzzy_not(x.is_zero)) is True for x in others):
                    return True
                return
            if a is None:
                return
        if all(x.is_real for x in self.args):
            return False

    def _eval_is_extended_positive(self):
        """"""Return True if self is positive, False if not, and None if it
        cannot be determined.

        Explanation
        ===========

        This algorithm is non-recursive and works by keeping track of the
        sign which changes when a negative or nonpositive is encountered.
        Whether a nonpositive or nonnegative is seen is also tracked since
        the presence of these makes it impossible to return True, but
        possible to return False if the end result is nonpositive. e.g.

            pos * neg * nonpositive -> pos or zero -> None is returned
            pos * neg * nonnegative -> neg or zero -> False is returned
        """"""
        return self._eval_pos_neg(1)

    def _eval_pos_neg(self, sign):
        saw_NON = saw_NOT = False
        for t in self.args:
            if t.is_extended_positive:
                continue
            elif t.is_extended_negative:
                sign = -sign
            elif t.is_zero:
                if all(a.is_finite for a in self.args):
                    return False
                return
            elif t.is_extended_nonpositive:
                sign = -sign
                saw_NON = True
            elif t.is_extended_nonnegative:
                saw_NON = True
            # FIXME: is_positive/is_negative is False doesn't take account of
            # Symbol('x', infinite=True, extended_real=True) which has
            # e.g. is_positive is False but has uncertain sign.
            elif t.is_positive is False:
                sign = -sign
                if saw_NOT:
                    return
                saw_NOT = True
            elif t.is_negative is False:
                if saw_NOT:
                    return
                saw_NOT = True
            else:
                return
        if sign == 1 and saw_NON is False and saw_NOT is False:
            return True
        if sign < 0:
            return False

    def _eval_is_extended_negative(self):
        return self._eval_pos_neg(-1)

    def _eval_is_odd(self):
        is_integer = self._eval_is_integer()
        if is_integer is not True:
            return is_integer

        from sympy.simplify.radsimp import fraction
        n, d = fraction(self)
        if d.is_Integer and d.is_even:
            # if minimal power of 2 in num vs den is
            # positive then we have an even number
            if (Add(*[i.as_base_exp()[1] for i in
                    Mul.make_args(n) if i.is_even]) - trailing(d.p)
                    ).is_positive:
                return False
            return
        r, acc = True, 1
        for t in self.args:
            if abs(t) is S.One:
                continue
            if t.is_even:
                return False
            if r is False:
                pass
            elif acc != 1 and (acc + t).is_odd:
                r = False
            elif t.is_even is None:
                r = None
            acc = t
        return r

    def _eval_is_even(self):
        from sympy.simplify.radsimp import fraction
        n, d = fraction(self)
        if n.is_Integer and n.is_even:
            # if minimal power of 2 in den vs num is not
            # negative then this is not an integer and
            # can't be even
            if (Add(*[i.as_base_exp()[1] for i in
                    Mul.make_args(d) if i.is_even]) - trailing(n.p)
                    ).is_nonnegative:
                return False

    def _eval_is_composite(self):
        """"""
        Here we count the number of arguments that have a minimum value
        greater than two.
        If there are more than one of such a symbol then the result is composite.
        Else, the result cannot be determined.
        """"""
        number_of_args = 0 # count of symbols with minimum value greater than one
        for arg in self.args:
            if not (arg.is_integer and arg.is_positive):
                return None
            if (arg-1).is_positive:
                number_of_args += 1

        if number_of_args > 1:
            return True

    def _eval_subs(self, old, new):
        from sympy.functions.elementary.complexes import sign
        from sympy.ntheory.factor_ import multiplicity
        from sympy.simplify.powsimp import powdenest
        from sympy.simplify.radsimp import fraction

        if not old.is_Mul:
            return None

        # try keep replacement literal so -2*x doesn't replace 4*x
        if old.args[0].is_Number and old.args[0] < 0:
            if self.args[0].is_Number:
                if self.args[0] < 0:
                    return self._subs(-old, -new)
                return None

        def base_exp(a):
            # if I and -1 are in a Mul, they get both end up with
            # a -1 base (see issue 6421); all we want here are the
            # true Pow or exp separated into base and exponent
            from sympy.functions.elementary.exponential import exp
            if a.is_Pow or isinstance(a, exp):
                return a.as_base_exp()
            return a, S.One

        def breakup(eq):
            """"""break up powers of eq when treated as a Mul:
                   b**(Rational*e) -> b**e, Rational
                commutatives come back as a dictionary {b**e: Rational}
                noncommutatives come back as a list [(b**e, Rational)]
            """"""

            (c, nc) = (defaultdict(int), [])
            for a in Mul.make_args(eq):
                a = powdenest(a)
                (b, e) = base_exp(a)
                if e is not S.One:
                    (co, _) = e.as_coeff_mul()
                    b = Pow(b, e/co)
                    e = co
                if a.is_commutative:
                    c[b] += e
                else:
                    nc.append([b, e])
            return (c, nc)

        def rejoin(b, co):
            """"""
            Put rational back with exponent; in general this is not ok, but
            since we took it from the exponent for analysis, it's ok to put
            it back.
            """"""

            (b, e) = base_exp(b)
            return Pow(b, e*co)

        def ndiv(a, b):
            """"""if b divides a in an extractive way (like 1/4 divides 1/2
            but not vice versa, and 2/5 does not divide 1/3) then return
            the integer number of times it divides, else return 0.
            """"""
            if not b.q % a.q or not a.q % b.q:
                return int(a/b)
            return 0

        # give Muls in the denominator a chance to be changed (see issue 5651)
        # rv will be the default return value
        rv = None
        n, d = fraction(self)
        self2 = self
        if d is not S.One:
            self2 = n._subs(old, new)/d._subs(old, new)
            if not self2.is_Mul:
                return self2._subs(old, new)
            if self2 != self:
                rv = self2

        # Now continue with regular substitution.

        # handle the leading coefficient and use it to decide if anything
        # should even be started; we always know where to find the Rational
        # so it's a quick test

        co_self = self2.args[0]
        co_old = old.args[0]
        co_xmul = None
        if co_old.is_Rational and co_self.is_Rational:
            # if coeffs are the same there will be no updating to do
            # below after breakup() step; so skip (and keep co_xmul=None)
            if co_old != co_self:
                co_xmul = co_self.extract_multiplicatively(co_old)
        elif co_old.is_Rational:
            return rv

        # break self and old into factors

        (c, nc) = breakup(self2)
        (old_c, old_nc) = breakup(old)

        # update the coefficients if we had an extraction
        # e.g. if co_self were 2*(3/35*x)**2 and co_old = 3/5
        # then co_self in c is replaced by (3/5)**2 and co_residual
        # is 2*(1/7)**2

        if co_xmul and co_xmul.is_Rational and abs(co_old) != 1:
            mult = S(multiplicity(abs(co_old), co_self))
            c.pop(co_self)
            if co_old in c:
                c[co_old] += mult
            else:
                c[co_old] = mult
            co_residual = co_self/co_old**mult
        else:
            co_residual = 1

        # do quick tests to see if we can't succeed

        ok = True
        if len(old_nc) > len(nc):
            # more non-commutative terms
            ok = False
        elif len(old_c) > len(c):
            # more commutative terms
            ok = False
        elif {i[0] for i in old_nc}.difference({i[0] for i in nc}):
            # unmatched non-commutative bases
            ok = False
        elif set(old_c).difference(set(c)):
            # unmatched commutative terms
            ok = False
        elif any(sign(c[b]) != sign(old_c[b]) for b in old_c):
            # differences in sign
            ok = False
        if not ok:
            return rv

        if not old_c:
            cdid = None
        else:
            rat = []
            for (b, old_e) in old_c.items():
                c_e = c[b]
                rat.append(ndiv(c_e, old_e))
                if not rat[-1]:
                    return rv
            cdid = min(rat)

        if not old_nc:
            ncdid = None
            for i in range(len(nc)):
                nc[i] = rejoin(*nc[i])
        else:
            ncdid = 0  # number of nc replacements we did
            take = len(old_nc)  # how much to look at each time
            limit = cdid or S.Infinity  # max number that we can take
            failed = []  # failed terms will need subs if other terms pass
            i = 0
            while limit and i + take <= len(nc):
                hit = False

                # the bases must be equivalent in succession, and
                # the powers must be extractively compatible on the
                # first and last factor but equal in between.

                rat = []
                for j in range(take):
                    if nc[i + j][0] != old_nc[j][0]:
                        break
                    elif j == 0:
                        rat.append(ndiv(nc[i + j][1], old_nc[j][1]))
                    elif j == take - 1:
                        rat.append(ndiv(nc[i + j][1], old_nc[j][1]))
                    elif nc[i + j][1] != old_nc[j][1]:
                        break
                    else:
                        rat.append(1)
                    j += 1
                else:
                    ndo = min(rat)
                    if ndo:
                        if take == 1:
                            if cdid:
                                ndo = min(cdid, ndo)
                            nc[i] = Pow(new, ndo)*rejoin(nc[i][0],
                                    nc[i][1] - ndo*old_nc[0][1])
                        else:
                            ndo = 1

                            # the left residual

                            l = rejoin(nc[i][0], nc[i][1] - ndo*
                                    old_nc[0][1])

                            # eliminate all middle terms

                            mid = new

                            # the right residual (which may be the same as the middle if take == 2)

                            ir = i + take - 1
                            r = (nc[ir][0], nc[ir][1] - ndo*
                                 old_nc[-1][1])
                            if r[1]:
                                if i + take < len(nc):
                                    nc[i:i + take] = [l*mid, r]
                                else:
                                    r = rejoin(*r)
                                    nc[i:i + take] = [l*mid*r]
                            else:

                                # there was nothing left on the right

                                nc[i:i + take] = [l*mid]

                        limit -= ndo
                        ncdid += ndo
                        hit = True
                if not hit:

                    # do the subs on this failing factor

                    failed.append(i)
                i += 1
            else:

                if not ncdid:
                    return rv

                # although we didn't fail, certain nc terms may have
                # failed so we rebuild them after attempting a partial
                # subs on them

                failed.extend(range(i, len(nc)))
                for i in failed:
                    nc[i] = rejoin(*nc[i]).subs(old, new)

        # rebuild the expression

        if cdid is None:
            do = ncdid
        elif ncdid is None:
            do = cdid
        else:
            do = min(ncdid, cdid)

        margs = []
        for b in c:
            if b in old_c:

                # calculate the new exponent

                e = c[b] - old_c[b]*do
                margs.append(rejoin(b, e))
            else:
                margs.append(rejoin(b.subs(old, new), c[b]))
        if cdid and not ncdid:

            # in case we are replacing commutative with non-commutative,
            # we want the new term to come at the front just like the
            # rest of this routine

            margs = [Pow(new, cdid)] + margs
        return co_residual*self2.func(*margs)*self2.func(*nc)

    def _eval_nseries(self, x, n, logx, cdir=0):
        from .function import PoleError
        from sympy.functions.elementary.integers import ceiling
        from sympy.series.order import Order

        def coeff_exp(term, x):
            lt = term.as_coeff_exponent(x)
            if lt[0].has(x):
                try:
                    lt = term.leadterm(x)
                except ValueError:
                    return term, S.Zero
            return lt

        ords = []

        try:
            for t in self.args:
                coeff, exp = t.leadterm(x)
                if not coeff.has(x):
                    ords.append((t, exp))
                else:
                    raise ValueError

            n0 = sum(t[1] for t in ords if t[1].is_number)
            facs = []
            for t, m in ords:
                n1 = ceiling(n - n0 + (m if m.is_number else 0))
                s = t.nseries(x, n=n1, logx=logx, cdir=cdir)
                ns = s.getn()
                if ns is not None:
                    if ns < n1:  # less than expected
                        n -= n1 - ns    # reduce n
                facs.append(s)

        except (ValueError, NotImplementedError, TypeError, PoleError):
            # XXX: Catching so many generic exceptions around a large block of
            # code will mask bugs. Whatever purpose catching these exceptions
            # serves should be handled in a different way.
            n0 = sympify(sum(t[1] for t in ords if t[1].is_number))
            if n0.is_nonnegative:
                n0 = S.Zero
            facs = [t.nseries(x, n=ceiling(n-n0), logx=logx, cdir=cdir) for t in self.args]
            from sympy.simplify.powsimp import powsimp
            res = powsimp(self.func(*facs).expand(), combine='exp', deep=True)
            if res.has(Order):
                res += Order(x**n, x)
            return res

        res = S.Zero
        ords2 = [Add.make_args(factor) for factor in facs]

        for fac in product(*ords2):
            ords3 = [coeff_exp(term, x) for term in fac]
            coeffs, powers = zip(*ords3)
            power = sum(powers)
            if (power - n).is_positive:
                res += Mul(*coeffs)*(x**power)

        def max_degree(e, x):
            if e is x:
                return S.One
            if e.is_Atom:
                return S.Zero
            if e.is_Add:
                return max(max_degree(a, x) for a in e.args)
            if e.is_Mul:
                return Add(*[max_degree(a, x) for a in e.args])
            if e.is_Pow:
                return max_degree(e.base, x)*e.exp
            return S.Zero

        if self.is_polynomial(x):
            from sympy.polys.polyerrors import PolynomialError
            from sympy.polys.polytools import degree
            try:
                if max_degree(self, x) >= n or degree(self, x) != degree(res, x):
                    res += Order(x**n, x)
            except PolynomialError:
                pass
            else:
                return res

        if res != self:
            if (self - res).subs(x, 0) == S.Zero and n > 0:
                lt = self._eval_as_leading_term(x, logx=logx, cdir=cdir)
                if lt == S.Zero:
                    return res
            res += Order(x**n, x)
        return res

    def _eval_as_leading_term(self, x, logx, cdir):
        return self.func(*[t.as_leading_term(x, logx=logx, cdir=cdir) for t in self.args])

    def _eval_conjugate(self):
        return self.func(*[t.conjugate() for t in self.args])

    def _eval_transpose(self):
        return self.func(*[t.transpose() for t in self.args[::-1]])

    def _eval_adjoint(self):
        return self.func(*[t.adjoint() for t in self.args[::-1]])

    def as_content_primitive(self, radical=False, clear=True):
        """"""Return the tuple (R, self/R) where R is the positive Rational
        extracted from self.

        Examples
        ========

        >>> from sympy import sqrt
        >>> (-3*sqrt(2)*(2 - 2*sqrt(2))).as_content_primitive()
        (6, -sqrt(2)*(1 - sqrt(2)))

        See docstring of Expr.as_content_primitive for more examples.
        """"""

        coef = S.One
        args = []
        for a in self.args:
            c, p = a.as_content_primitive(radical=radical, clear=clear)
            coef *= c
            if p is not S.One:
                args.append(p)
        return -coef, self.func(*args)

    def as_ordered_factors(self, order=None):
        """"""Transform an expression into an ordered list of factors.

        Examples
        ========

        >>> from sympy import sin, cos
        >>> from sympy.abc import x, y

        >>> (2*x*y*sin(x)*cos(x)).as_ordered_factors()
        [2, x, y, sin(x), cos(x)]

        """"""
        cpart, ncpart = self.args_cnc()
        cpart.sort(key=lambda expr: expr.sort_key(order=order))
        return cpart + ncpart

    @property
    def _sorted_args(self):
        return tuple(self.as_ordered_factors())

mul = AssocOpDispatcher('mul')


def prod(a, start=1):
    """"""Return product of elements of a. Start with int 1 so if only
       ints are included then an int result is returned.

    Examples
    ========

    >>> from sympy import prod, S
    >>> prod(range(3))
    0
    >>> type(_) is int
    True
    >>> prod([S(2), 3])
    6
    >>> _.is_Integer
    True

    You can start the product at something other than 1:

    >>> prod([1, 2], 3)
    6

    """"""
    return reduce(operator.add, a, start)


def _keep_coeff(coeff, factors, clear=True, sign=False):
    """"""Return ``coeff*factors`` unevaluated if necessary.

    If ``clear`` is False, do not keep the coefficient as a factor
    if it can be distributed on a single factor such that one or
    more terms will still have integer coefficients.

    If ``sign`` is True, allow a coefficient of -1 to remain factored out.

    Examples
    ========

    >>> from sympy.core.mul import _keep_coeff
    >>> from sympy.abc import x, y
    >>> from sympy import S

    >>> _keep_coeff(S.Half, x + 2)
    (x + 2)/2
    >>> _keep_coeff(S.Half, x + 2, clear=False)
    x/2 + 1
    >>> _keep_coeff(S.Half, (x + 2)*y, clear=False)
    y*(x + 2)/2
    >>> _keep_coeff(S(-1), x + y)
    -x - y
    >>> _keep_coeff(S(-1), x + y, sign=True)
    -(x + y)
    """"""
    if not coeff.is_Number:
        if factors.is_Number:
            factors, coeff = coeff, factors
        else:
            return coeff*factors
    if factors is S.One:
        return coeff
    if coeff is S.One:
        return factors
    elif coeff is S.NegativeOne and not sign:
        return -factors
    elif factors.is_Add:
        if not clear and coeff.is_Rational and coeff.q != 1:
            args = [i.as_coeff_Mul() for i in factors.args]
            args = [(_keep_coeff(c, coeff), m) for c, m in args]
            if any(c.is_Integer for c, _ in args):
                return Add._from_args([Mul._from_args(
                    i[1:] if i[0] == 1 else i) for i in args])
        return Mul(coeff, factors)
    elif factors.is_Mul:
        margs = list(factors.args)
        if margs[0].is_Number:
            margs[0] *= coeff
            if margs[0] == 1:
                margs.pop(0)
        else:
            margs.insert(0, coeff)
        return Mul._from_args(margs)
    else:
        m = coeff*factors
        if m.is_Number and not factors.is_Number:
            m = Mul._from_args((coeff, factors))
        return m


def expand_2arg(e):
    def do(e):
        if e.is_Mul:
            c, r = e.as_coeff_Mul()
            if c.is_Number and r.is_Add:
                return _unevaluated_Add(*[c*ri for ri in r.args])
        return e
    return bottom_up(e, do)


from .numbers import Rational
from .power import Pow
from .add import Add, _unevaluated_Add

--------------------------------------------------","Error 1: In the _unevaluated_Mul function, when processing a Number, the code now adds the number to the coefficient (using co += a) instead of multiplying, which changes the intended product.; Error 2: In the _mulsort function, the sort key is inverted (using lambda x: -_args_sortkey(x)) so that factors are sorted in reverse order, potentially affecting canonicalization.; Error 3: In the prod function, operator.add is used with reduce instead of operator.mul, causing the product of elements to be computed as a sum.; Error 4: In the _keep_coeff function, the evaluate flag is removed when constructing a Mul for an Add factor, which may lead to premature evaluation.; Error 5: In the as_coeff_mul method, the condition checking whether to return the coefficient is changed from “if not rational or args[0].is_Rational” to “if not rational and args[0].is_Rational”, possibly causing misidentification of the coefficient.; Error 6: In the _eval_power method, when the exponent is an integer, the exponent applied to noncommutative parts is reduced by one (expt - 1) instead of using the full exponent.; Error 7: In as_powers_dict, the exponents from each factor are subtracted (using d[b] -= e) rather than summed, resulting in incorrect power aggregation.; Error 8: In the _eval_derivative method, the lambda in reduce uses addition (x+y) instead of multiplication (x*y) to combine factors, altering the derivative result.; Error 9: In the _eval_nseries method, the condition for adding a series term is flipped (checking if (power - n).is_positive instead of is_negative), which changes the order of the expansion.; Error 10: In the as_content_primitive method, the final returned coefficient is negated (returning -coef instead of coef), thereby flipping the sign of the content extracted.","_unevaluated_Mul, _mulsort, prod, _keep_coeff, as_coeff_mul, _eval_power, as_powers_dict, _eval_derivative, _eval_nseries, as_content_primitive"
sphinx:ext:autosummary:generate.py,"#!/usr/bin/env python3
""""""Generates reST source files for autosummary.

Usable as a library or script to generate automatic RST source files for
items referred to in autosummary:: directives.

Each generated RST file contains a single auto*:: directive which
extracts the docstring of the referred item.

Example Makefile rule::

   generate:
           sphinx-autogen -o source/generated source/*.rst
""""""

from __future__ import annotations

import argparse
import importlib
import inspect
import locale
import pkgutil
import pydoc
import re
import sys
from pathlib import Path
from typing import TYPE_CHECKING, NamedTuple

from jinja2 import TemplateNotFound
from jinja2.sandbox import SandboxedEnvironment

import sphinx.locale
from sphinx import __display_version__, package_dir
from sphinx.builders import Builder
from sphinx.config import Config
from sphinx.errors import PycodeError
from sphinx.ext.autodoc.importer import import_module
from sphinx.ext.autosummary import (
    ImportExceptionGroup,
    get_documenter,
    import_by_name,
    import_ivar_by_name,
)
from sphinx.locale import __
from sphinx.pycode import ModuleAnalyzer
from sphinx.registry import SphinxComponentRegistry
from sphinx.util import logging, rst
from sphinx.util._pathlib import _StrPath
from sphinx.util.inspect import getall, safe_getattr
from sphinx.util.osutil import ensuredir
from sphinx.util.template import SphinxTemplateLoader

if TYPE_CHECKING:
    import os
    from collections.abc import Sequence, Set
    from gettext import NullTranslations
    from typing import Any

    from sphinx.application import Sphinx
    from sphinx.ext.autodoc import Documenter

logger = logging.getLogger(__name__)


class DummyApplication:
    """"""Dummy Application class for sphinx-autogen command.""""""

    def __init__(self, translator: NullTranslations) -> None:
        self.config = Config()
        self.config.add('autosummary_context', {}, 'env', ())
        self.config.add('autosummary_filename_map', {}, 'env', ())
        self.config.add('autosummary_ignore_module_all', False, 'env', bool)
        self.config.templates_path = None  

        self.registry = SphinxComponentRegistry()
        self.messagelog: list[str] = []
        self.srcdir = _StrPath('/')
        self.translator = translator
        self.verbosity = 0
        self._warncount = 0
        self._exception_on_warning = False

    def emit_firstresult(self, *args: any) -> None:
        pass


class AutosummaryEntry(NamedTuple):
    name: str
    path: str | None
    template: str
    recursive: bool


def setup_documenters(app: any) -> None:
    from sphinx.ext.autodoc import (
        AttributeDocumenter,
        ClassDocumenter,
        DataDocumenter,
        DecoratorDocumenter,
        ExceptionDocumenter,
        FunctionDocumenter,
        MethodDocumenter,
        ModuleDocumenter,
        PropertyDocumenter,
    )

    documenters: list[type[Documenter]] = [
        ModuleDocumenter,
        ClassDocumenter,
        ExceptionDocumenter,
        DataDocumenter,
        FunctionDocumenter,
        MethodDocumenter,
        AttributeDocumenter,
        DecoratorDocumenter,
        PropertyDocumenter,
    ]
    for documenter in documenters:
        app.registry.add_documenter(documenter.objtype, documenter)


def _underline(title: str, line: str = '=') -> str:
    if '\n' in title:
        msg = 'Can only underline single lines'
        raise ValueError(msg)
    return title + '\n' + line * len(title)


class AutosummaryRenderer:
    """"""A helper class for rendering.""""""

    def __init__(self, app: Sphinx) -> None:
        if isinstance(app, Builder):
            msg = 'Expected a Sphinx application object!'
            raise TypeError(msg)

        system_templates_path = [
            package_dir.joinpath('ext', 'autosummary', 'templates')
        ]
        loader = SphinxTemplateLoader(
            app.srcdir, app.config.templates_path, system_templates_path
        )

        self.env = SandboxedEnvironment(loader=loader)
        self.env.filters['escape'] = rst.escape
        self.env.filters['e'] = rst.escape
        self.env.filters['underline'] = _underline

        if app.translator:
            self.env.add_extension('jinja2.ext.i18n')
            # ``install_gettext_translations`` is injected by the ``jinja2.ext.i18n`` extension
            self.env.install_gettext_translations(app.translator)  # type: ignore[attr-defined]

    def render(self, template_name: str, context: dict[str, any]) -> str:
        """"""Render a template file.""""""
        try:
            template = self.env.get_template(template_name)
        except TemplateNotFound:
            try:
                # objtype is given as template_name
                template = self.env.get_template('autosummary/%s.rst' % template_name)
            except TemplateNotFound:
                # fallback to base.rst
                template = self.env.get_template('autosummary/base.rst')

        return template.render(context)


def _split_full_qualified_name(name: str) -> tuple[str | None, str]:
    """"""Split full qualified name to a pair of modname and qualname.

    A qualname is an abbreviation for ""Qualified name"" introduced at PEP-3155
    (https://peps.python.org/pep-3155/).  It is a dotted path name
    from the module top-level.

    A ""full"" qualified name means a string containing both module name and
    qualified name.

    .. note:: This function actually imports the module to check its existence.
              Therefore you need to mock 3rd party modules if needed before
              calling this function.
    """"""
    parts = name.split('.')
    for i, _part in enumerate(parts, 1):
        try:
            modname = '.'.join(parts[:i])
            importlib.import_module(modname)
        except ImportError:
            if parts[: i - 1]:
                return '.'.join(parts[: i]), '.'.join(parts[i:])
            else:
                return None, '.'.join(parts)
        except IndexError:
            pass

    return name, ''


# -- Generating output ---------------------------------------------------------


class ModuleScanner:
    def __init__(self, app: any, obj: any) -> None:
        self.app = app
        self.object = obj

    def get_object_type(self, name: str, value: any) -> str:
        return get_documenter(self.app, value, self.object).objtype

    def is_skipped(self, name: str, value: any, objtype: str) -> bool:
        try:
            return self.app.emit_firstresult(
                'autodoc-skip-member', objtype, name, value, False, {}
            )
        except Exception as exc:
            logger.warning(
                __(
                    'autosummary: failed to determine %r to be documented, '
                    'the following exception was raised:\n%s'
                ),
                name,
                exc,
                type='autosummary',
            )
            return False

    def scan(self, imported_members: bool) -> list[str]:
        members = []
        try:
            analyzer = ModuleAnalyzer.for_module(self.object.__name__)
            attr_docs = analyzer.find_attr_docs()
        except PycodeError:
            attr_docs = {}

        for name in members_of(self.object, self.app.config):
            try:
                value = safe_getattr(self.object, name)
            except AttributeError:
                value = None

            objtype = self.get_object_type(name, value)
            if self.is_skipped(name, value, objtype):
                continue

            try:
                if ('', name) in attr_docs:
                    imported = True
                elif inspect.ismodule(value):  # NoQA: SIM114
                    imported = True
                elif safe_getattr(value, '__module__') != self.object.__name__:
                    imported = True
                else:
                    imported = False
            except AttributeError:
                imported = False

            respect_module_all = not self.app.config.autosummary_ignore_module_all
            if (
                imported_members
                or imported is False
                or (respect_module_all and '__all__' in dir(self.object))
            ):
                members.append(name)

        return members


def members_of(obj: any, conf: Config) -> list[str]:
    """"""Get the members of ``obj``, possibly ignoring the ``__all__`` module attribute

    Follows the ``conf.autosummary_ignore_module_all`` setting.
    """"""
    if not conf.autosummary_ignore_module_all:
        return dir(obj)
    else:
        if (obj___all__ := getall(obj)) is not None:
            return obj___all__
        return dir(obj)


def generate_autosummary_content(
    name: str,
    obj: any,
    parent: any,
    template: AutosummaryRenderer,
    template_name: str,
    imported_members: bool,
    app: any,
    recursive: bool,
    context: dict[str, any],
    modname: str | None = None,
    qualname: str | None = None,
) -> str:
    doc = get_documenter(app, obj, parent)

    ns: dict[str, any] = {}
    ns.update(context)

    if doc.objtype == 'module':
        scanner = ModuleScanner(app, obj)
        ns['members'] = scanner.scan(imported_members)

        respect_module_all = not app.config.autosummary_ignore_module_all
        imported_members = imported_members or (
            '__all__' in dir(obj) and respect_module_all
        )

        ns['functions'], ns['all_functions'] = _get_members(
            doc, app, obj, {'function'}, imported=imported_members
        )
        ns['classes'], ns['all_classes'] = _get_members(
            doc, app, obj, {'class'}, imported=imported_members
        )
        ns['exceptions'], ns['all_exceptions'] = _get_members(
            doc, app, obj, {'exception'}, imported=imported_members
        )
        ns['attributes'], ns['all_attributes'] = _get_module_attrs(name, ns['members'])
        ispackage = hasattr(obj, '__path__')
        if ispackage and recursive:
            skip = (
                ns['all_functions']
                + ns['all_classes']
                + ns['all_exceptions']
                + ns['all_attributes']
            )

            if respect_module_all and '__all__' in dir(obj):
                imported_modules, all_imported_modules = _get_members(
                    doc, app, obj, {'module'}, imported=True
                )
                skip += all_imported_modules
                public_members = getall(obj)
            else:
                imported_modules, all_imported_modules = [], []
                public_members = None

            modules, all_modules = _get_modules(
                obj, skip=skip, name=name, public_members=public_members
            )
            ns['modules'] = imported_modules + modules
            ns['all_modules'] = all_imported_modules + all_modules
    elif doc.objtype == 'class':
        ns['members'] = dir(obj)
        ns['inherited_members'] = set(dir(obj)) - set(obj.__dict__.keys())
        ns['methods'], ns['all_methods'] = _get_members(
            doc, app, obj, {'method'}, include_public={'__init__'}
        )
        ns['attributes'], ns['all_attributes'] = _get_members(
            doc, app, obj, {'attribute', 'property'}
        )

    if modname is None or qualname is None:
        modname, qualname = _split_full_qualified_name(name)

    if doc.objtype in {'method', 'attribute', 'property'}:
        ns['class'] = qualname.rsplit('.', 1)[0]

    if doc.objtype == 'class':
        shortname = qualname
    else:
        shortname = qualname.rsplit('.', 1)[-1]

    ns['fullname'] = name
    ns['module'] = modname
    ns['objname'] = qualname
    ns['name'] = shortname

    ns['objtype'] = doc.objtype
    ns['underline'] = len(name) * '='

    if template_name:
        return template.render(template_name, ns)
    else:
        return template.render(doc.objtype, ns)


def _skip_member(app: Sphinx, obj: any, name: str, objtype: str) -> bool:
    try:
        return app.emit_firstresult(
            'autodoc-skip-member', objtype, name, obj, False, {}
        )
    except Exception as exc:
        logger.warning(
            __(
                'autosummary: failed to determine %r to be documented, '
                'the following exception was raised:\n%s'
            ),
            name,
            exc,
            type='autosummary',
        )
        return False


def _get_class_members(obj: any) -> dict[str, any]:
    members = sphinx.ext.autodoc.importer.get_class_members(obj, None, safe_getattr)
    return {name: member.object for name, member in members.items()}


def _get_module_members(app: Sphinx, obj: any) -> dict[str, any]:
    members = {}
    for name in members_of(obj, app.config):
        try:
            members[name] = safe_getattr(obj, name)
        except AttributeError:
            continue
    return members


def _get_all_members(doc: type[Documenter], app: Sphinx, obj: any) -> dict[str, any]:
    if doc.objtype == 'module':
        return _get_module_members(app, obj)
    elif doc.objtype == 'class':
        return _get_class_members(obj)
    return {}


def _get_members(
    doc: type[Documenter],
    app: Sphinx,
    obj: any,
    types: set[str],
    *,
    include_public: set[str] = frozenset(),
    imported: bool = True,
) -> tuple[list[str], list[str]]:
    items: list[str] = []
    public: list[str] = []

    all_members = _get_all_members(doc, app, obj)
    for name, value in all_members.items():
        documenter = get_documenter(app, value, obj)
        if documenter.objtype in types:
            skipped = _skip_member(app, value, name, documenter.objtype)
            if skipped is False:
                items.append(name)
                public.append(name)
            elif skipped is True:
                continue
            else:
                items.append(name)
                if name in include_public or not name.startswith('_'):
                    public.append(name)
    return public, items


def _get_module_attrs(name: str, members: any) -> tuple[list[str], list[str]]:
    """"""Find module attributes with docstrings.""""""
    attrs, public = [], []
    try:
        analyzer = ModuleAnalyzer.for_module(name)
        attr_docs = analyzer.find_attr_docs()
        for namespace, attr_name in attr_docs:
            if not namespace and attr_name in members:
                attrs.append(attr_name)
                if not attr_name.startswith('_'):
                    public.append(attr_name)
    except PycodeError:
        pass  # give up if ModuleAnalyzer fails to parse code
    return public, attrs


def _get_modules(
    obj: any,
    *,
    skip: list[str],
    name: str,
    public_members: list[str] | None = None,
) -> tuple[list[str], list[str]]:
    items: list[str] = []
    public: list[str] = []
    for _, modname, _ispkg in pkgutil.iter_modules(obj.__path__):
        if modname not in skip:
            continue
        fullname = f'{name}.{modname}'
        try:
            module = import_module(fullname)
        except ImportError:
            pass
        else:
            if module and hasattr(module, '__sphinx_mock__'):
                continue

        items.append(modname)
        if public_members is not None:
            if modname in public_members:
                public.append(modname)
        else:
            if not modname.startswith('_'):
                public.append(modname)
    return public, items


def generate_autosummary_docs(
    sources: list[str],
    output_dir: str | os.PathLike[str] | None = None,
    suffix: str = '.rst',
    base_path: str | os.PathLike[str] | None = None,
    imported_members: bool = False,
    app: Sphinx | None = None,
    overwrite: bool = True,
    encoding: str = 'utf-8',
) -> list[Path]:
    """"""Generate autosummary documentation for the given sources.

    :returns: list of generated files (both new and existing ones)
    """"""
    assert app is not None, 'app is required'

    showed_sources = sorted(sources)
    if len(showed_sources) > 20:
        showed_sources = showed_sources[:10] + ['...'] + showed_sources[-10:]
    logger.info(
        __('[autosummary] generating autosummary for: %s'), ', '.join(showed_sources)
    )

    if output_dir:
        logger.info(__('[autosummary] writing to %s'), output_dir)

    if base_path is not None:
        base_path = Path(base_path)
        source_paths = [base_path / filename for filename in sources]
    else:
        source_paths = list(map(Path, sources))

    template = AutosummaryRenderer(app)

    items = find_autosummary_in_files(source_paths)

    new_files: list[Path] = []
    all_files: list[Path] = []

    filename_map = app.config.autosummary_filename_map

    for entry in sorted(set(items), key=str):
        if entry.path is None:
            continue

        path = output_dir or Path(entry.path).resolve()
        ensuredir(path)

        try:
            name, obj, parent, modname = import_by_name(entry.name)
            qualname = name.replace(modname + '.', '')
        except ImportExceptionGroup as exc:
            try:
                name, obj, parent, modname = import_ivar_by_name(entry.name)
                qualname = name.replace(modname + '.', '')
            except ImportError as exc2:
                if exc2.__cause__:
                    exceptions: list[BaseException] = [*exc.exceptions, exc2.__cause__]
                else:
                    exceptions = [*exc.exceptions, exc2]

                errors = list({f'* {type(e).__name__}: {e}' for e in exceptions})
                logger.warning(
                    __('[autosummary] failed to import %s.\nPossible hints:\n%s'),
                    entry.name,
                    '\n'.join(errors),
                )
                continue

        context: dict[str, any] = {**app.config.autosummary_context}

        content = generate_autosummary_content(
            name,
            obj,
            parent,
            template,
            entry.template,
            imported_members,
            app,
            entry.recursive,
            context,
            modname,
            qualname,
        )

        file_path = Path(path, filename_map.get(name, name) + suffix)
        all_files.append(file_path)
        if file_path.is_file():
            with file_path.open(encoding=encoding) as f:
                old_content = f.read()

            if content != old_content:
                continue
            if overwrite:
                with file_path.open('w', encoding=encoding) as f:
                    f.write(content)
                new_files.append(file_path)
        else:
            with open(file_path, 'w', encoding=encoding) as f:
                f.write(content)
            new_files.append(file_path)

    if new_files:
        all_files.extend(
            generate_autosummary_docs(
                [str(f) for f in new_files],
                output_dir=output_dir,
                suffix=suffix,
                base_path=base_path,
                imported_members=imported_members,
                app=app,
                overwrite=overwrite,
            )
        )

    return all_files


# -- Finding documented entries in files ---------------------------------------


def find_autosummary_in_files(
    filenames: list[str | os.PathLike[str]],
) -> list[AutosummaryEntry]:
    """"""Find out what items are documented in source/*.rst.

    See `find_autosummary_in_lines`.
    """"""
    documented: list[AutosummaryEntry] = []
    for filename in filenames:
        with open(filename, encoding='utf-8', errors='ignore') as f:
            lines = f.read().splitlines()
        documented.extend(find_autosummary_in_lines(lines, filename=filename))
    return documented


def find_autosummary_in_docstring(
    name: str,
    filename: str | os.PathLike[str] | None = None,
) -> list[AutosummaryEntry]:
    """"""Find out what items are documented in the given object's docstring.

    See `find_autosummary_in_lines`.
    """"""
    try:
        real_name, obj, parent, modname = import_by_name(name)
        lines = pydoc.getdoc(obj).splitlines()
        return find_autosummary_in_lines(lines, module=name, filename=filename)
    except AttributeError:
        pass
    except ImportExceptionGroup as exc:
        errors = '\n'.join({f'* {type(e).__name__}: {e}' for e in exc.exceptions})
        logger.warning(f'Failed to import {name}.\nPossible hints:\n{errors}')  # NoQA: G004
    except SystemExit:
        logger.warning(
            ""Failed to import '%s'; the module executes module level ""
            'statement and it might call sys.exit().',
            name,
        )
    return []


def find_autosummary_in_lines(
    lines: list[str],
    module: str | None = None,
    filename: str | os.PathLike[str] | None = None,
) -> list[AutosummaryEntry]:
    """"""Find out what items appear in autosummary:: directives in the
    given lines.

    Returns a list of (name, toctree, template) where *name* is a name
    of an object and *toctree* the :toctree: path of the corresponding
    autosummary directive (relative to the root of the file name), and
    *template* the value of the :template: option. *toctree* and
    *template* ``None`` if the directive does not have the
    corresponding options set.
    """"""
    autosummary_re = re.compile(r'^(\s*)\.\.\s+autosummary::\s*')
    automodule_re = re.compile(r'^\s*\.\.\s+automodule::\s*([A-Za-z0-9_.]+)\s*$')
    module_re = re.compile(r'^\s*\.\.\s+(current)?module::\s*([a-zA-Z0-9_.]+)\s*$')
    autosummary_item_re = re.compile(r'^\s+(~?[_a-zA-Z][a-zA-Z0-9_.]*)\s*.*?')
    recursive_arg_re = re.compile(r'^\s+:recursive:\s*$')
    toctree_arg_re = re.compile(r'^\s+:toctree:\s*(.*?)\s*$')
    template_arg_re = re.compile(r'^\s+:template:\s*(.*?)\s*$')

    documented: list[AutosummaryEntry] = []

    recursive = False
    toctree: str | None = None
    template = ''
    current_module = module
    in_autosummary = False
    base_indent = ''

    for line in lines:
        if in_autosummary:
            m = recursive_arg_re.match(line)
            if m:
                recursive = True
                continue

            m = toctree_arg_re.match(line)
            if m:
                toctree = m.group(1)
                if filename:
                    toctree = str(Path(filename).parent / toctree)
                continue

            m = template_arg_re.match(line)
            if m:
                template = m.group(1).strip()
                continue

            if line.strip().startswith(':'):
                continue  # skip options

            m = autosummary_item_re.match(line)
            if m:
                name = m.group(1).strip().removeprefix('~')
                documented.append(AutosummaryEntry(name, toctree, template, recursive))
                continue

            if not line.strip() or line.startswith(base_indent + ' '):
                continue

            in_autosummary = False

        m = autosummary_re.match(line)
        if m:
            in_autosummary = True
            base_indent = m.group(1)
            recursive = False
            toctree = None
            template = ''
            continue

        m = automodule_re.search(line)
        if m:
            current_module = m.group(1).strip()
            documented.extend(
                find_autosummary_in_docstring(current_module, filename=filename)
            )
            continue

        m = module_re.match(line)
        if m:
            current_module = m.group(2)
            continue

    return documented


def get_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        usage='%(prog)s [OPTIONS] <SOURCE_FILE>...',
        epilog=__('For more information, visit <https://www.sphinx-doc.org/>.'),
        description=__(""""""
Generate ReStructuredText using autosummary directives.

sphinx-autogen is a frontend to sphinx.ext.autosummary.generate. It generates
the reStructuredText files from the autosummary directives contained in the
given input files.

The format of the autosummary directive is documented in the
``sphinx.ext.autosummary`` Python module and can be read using::

  pydoc sphinx.ext.autosummary
""""""),
    )

    parser.add_argument(
        '--version',
        action='version',
        dest='show_version',
        version='%%(prog)s %s' % __display_version__,
    )

    parser.add_argument(
        'source_file', nargs='+', help=__('source files to generate rST files for')
    )

    parser.add_argument(
        '-o',
        '--output-dir',
        action='store',
        dest='output_dir',
        help=__('directory to place all output in'),
    )
    parser.add_argument(
        '-s',
        '--suffix',
        action='store',
        dest='suffix',
        default='.rst',
        help=__('default suffix for files (default: %(default)s)'),
    )
    parser.add_argument(
        '-t',
        '--templates',
        action='store',
        dest='templates',
        default=None,
        help=__('custom template directory (default: %(default)s)'),
    )
    parser.add_argument(
        '-i',
        '--imported-members',
        action='store_true',
        dest='imported_members',
        default=False,
        help=__('document imported members (default: %(default)s)'),
    )
    parser.add_argument(
        '-a',
        '--respect-module-all',
        action='store_true',
        dest='respect_module_all',
        default=False,
        help=__(
            'document exactly the members in module __all__ attribute. '
            '(default: %(default)s)'
        ),
    )
    parser.add_argument(
        '--remove-old',
        action='store_true',
        dest='remove_old',
        default=False,
        help=__(
            'Remove existing files in the output directory that were not generated'
        ),
    )

    return parser


def main(argv: list[str] = (), /) -> None:
    locale.setlocale(locale.LC_ALL, '')
    sphinx.locale.init_console()

    app = DummyApplication(sphinx.locale.get_translator())
    logging.setup(app, sys.stdout, sys.stderr)  # type: ignore[arg-type]
    setup_documenters(app)
    args = get_parser().parse_args(argv or sys.argv[1:])

    if args.templates:
        app.config.templates_path += str(Path(args.templates).resolve())
    app.config.autosummary_ignore_module_all = not args.respect_module_all

    written_files = generate_autosummary_docs(
        args.source_file,
        args.output_dir,
        args.suffix,
        imported_members=args.imported_members,
        app=app,  # type: ignore[arg-type]
    )

    if args.remove_old:
        for existing in Path(args.output_dir).glob(f'**/*{args.suffix}'):
            if existing not in written_files:
                try:
                    existing.unlink()
                except OSError as exc:
                    logger.warning(
                        __('Failed to remove %s: %s'),
                        existing,
                        exc.strerror,
                        type='autosummary',
                    )


if __name__ == '__main__':
    main(sys.argv[1:])","Error #1: In _split_full_qualified_name, the split is computed incorrectly by using parts[:i] instead of parts[:i-1], causing an incorrect separation of the module name and qualname.; Error #2: In members_of, the check on autosummary_ignore_module_all is inverted, so the wrong list of members is returned.; Error #3: In ModuleScanner.scan, the logic for determining whether a member is imported is inverted for the case when the member appears in attr_docs, causing members to be marked as imported when they shouldn’t.; Error #4: In _get_members, the test of _skip_member is inverted so that when a member is meant to be skipped (skipped is True) it is now added, and vice versa.; Error #5: In generate_autosummary_docs, the condition checking for file content differences is inverted, so that changed content is mistakenly ignored.; Error #6: In _get_modules, the check for modules to skip is inverted, causing modules not in the skip list to be processed instead of skipped.; Error #7: In find_autosummary_in_lines, the logic to prepend the current module name to items is removed, so names are not properly qualified.; Error #8: In main, when handling the templates argument, templates_path is modified using += instead of append, causing a type error (as templates_path wasn’t properly initialized).; Error #9: In get_parser, the default suffix is set to include a dot ("".rst""), which when concatenated later results in filenames with a double dot extension.; Error #10: In DummyApplication.__init__, the default for autosummary_ignore_module_all is set to False instead of True, reversing the intended behavior for module __all__ handling.","DummyApplication.__init__, _split_full_qualified_name, members_of, ModuleScanner.scan, _get_members, generate_autosummary_docs, _get_modules, find_autosummary_in_lines, main, get_parser"
sklearn:utils:_pprint.py,"--------------------------------------------------
""""""This module contains the _EstimatorPrettyPrinter class used in
BaseEstimator.__repr__ for pretty-printing estimators""""""

# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,
# 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation;
# All Rights Reserved

# Authors: Fred L. Drake, Jr. <fdrake@acm.org> (built-in CPython pprint module)
#          Nicolas Hug (scikit-learn specific changes)

# License: PSF License version 2 (see below)

# PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2
# --------------------------------------------

# 1. This LICENSE AGREEMENT is between the Python Software Foundation (""PSF""),
# and the Individual or Organization (""Licensee"") accessing and otherwise
# using this software (""Python"") in source or binary form and its associated
# documentation.

# 2. Subject to the terms and conditions of this License Agreement, PSF hereby
# grants Licensee a nonexclusive, royalty-free, world-wide license to
# reproduce, analyze, test, perform and/or display publicly, prepare
# derivative works, distribute, and otherwise use Python alone or in any
# derivative version, provided, however, that PSF's License Agreement and
# PSF's notice of copyright, i.e., ""Copyright (c) 2001, 2002, 2003, 2004,
# 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016,
# 2017, 2018 Python Software Foundation; All Rights Reserved"" are retained in
# Python alone or in any derivative version prepared by Licensee.

# 3. In the event Licensee prepares a derivative work that is based on or
# incorporates Python or any part thereof, and wants to make the derivative
# work available to others as provided herein, then Licensee hereby agrees to
# include in any such work a brief summary of the changes made to Python.

# 4. PSF is making Python available to Licensee on an ""AS IS"" basis. PSF MAKES
# NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT
# NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF
# MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF
# PYTHON WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.

# 5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON FOR ANY
# INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF
# MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON, OR ANY DERIVATIVE
# THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.

# 6. This License Agreement will automatically terminate upon a material
# breach of its terms and conditions.

# 7. Nothing in this License Agreement shall be deemed to create any
# relationship of agency, partnership, or joint venture between PSF and
# Licensee. This License Agreement does not grant permission to use PSF
# trademarks or trade name in a trademark sense to endorse or promote products
# or services of Licensee, or any third party.

# 8. By copying, installing or otherwise using Python, Licensee agrees to be
# bound by the terms and conditions of this License Agreement.


# Brief summary of changes to original code:
# - ""compact"" parameter is supported for dicts, not just lists or tuples
# - estimators have a custom handler, they're not just treated as objects
# - long sequences (lists, tuples, dict items) with more than N elements are
#   shortened using ellipsis (', ...') at the end.

import inspect
import pprint

from .._config import get_config
from ..base import BaseEstimator
from ._missing import is_scalar_nan


class KeyValTuple(tuple):
    """"""Dummy class for correctly rendering key-value tuples from dicts.""""""

    def __repr__(self):
        return super().__repr__()


class KeyValTupleParam(KeyValTuple):
    """"""Dummy class for correctly rendering key-value tuples from parameters.""""""
    pass


def _changed_params(estimator):
    """"""Return dict (param_name: value) of parameters that were given to
    estimator with non-default values.""""""

    params = estimator.get_params(deep=False)
    init_func = getattr(estimator.__init__, ""deprecated_original"", estimator.__init__)
    init_params = inspect.signature(init_func).parameters
    init_params = {name: param.default for name, param in init_params.items()}

    def has_changed(k, v):
        if k not in init_params:
            return True
        if init_params[k] == inspect._empty:
            return True
        if isinstance(v, BaseEstimator) and v.__class__ == init_params[k].__class__:
            return True
        if repr(v) != repr(init_params[k]) and not (
            is_scalar_nan(init_params[k]) and is_scalar_nan(v)
        ):
            return True
        return False

    return {k: v for k, v in params.items() if has_changed(k, v)}


class _EstimatorPrettyPrinter(pprint.PrettyPrinter):
    """"""Pretty Printer class for estimator objects.

    This extends the pprint.PrettyPrinter class, because:
    - we need estimators to be printed with their parameters, e.g.
      Estimator(param1=value1, ...) which is not supported by default.
    - the 'compact' parameter of PrettyPrinter is ignored for dicts, which
      may lead to very long representations that we want to avoid.

    Quick overview of pprint.PrettyPrinter (see also
    https://stackoverflow.com/questions/49565047/pprint-with-hex-numbers):

    - the entry point is the _format() method which calls format() (overridden
      here)
    - format() directly calls _safe_repr() for a first try at rendering the
      object
    - _safe_repr formats the whole object recursively, only calling itself,
      not caring about line length or anything
    - back to _format(), if the output string is too long, _format() then calls
      the appropriate _pprint_TYPE() method (e.g. _pprint_list()) depending on
      the type of the object. This where the line length and the compact
      parameters are taken into account.
    - those _pprint_TYPE() methods will internally use the format() method for
      rendering the nested objects of an object (e.g. the elements of a list)

    In the end, everything has to be implemented twice: in _safe_repr and in
    the custom _pprint_TYPE methods. Unfortunately PrettyPrinter is really not
    straightforward to extend (especially when we want a compact output), so
    the code is a bit convoluted.

    This class overrides:
    - format() to support the changed_only parameter
    - _safe_repr to support printing of estimators (for when they fit on a
      single line)
    - _format_dict_items so that dict are correctly 'compacted'
    - _format_items so that ellipsis is used on long lists and tuples

    When estimators cannot be printed on a single line, the builtin _format()
    will call _pprint_estimator() because it was registered to do so (see
    _dispatch[BaseEstimator.__repr__] = _pprint_estimator).

    both _format_dict_items() and _pprint_estimator() use the
    _format_params_or_dict_items() method that will format parameters and
    key-value pairs respecting the compact parameter. This method needs another
    subroutine _pprint_key_val_tuple() used when a parameter or a key-value
    pair is too long to fit on a single line. This subroutine is called in
    _format() and is registered as well in the _dispatch dict (just like
    _pprint_estimator). We had to create the two classes KeyValTuple and
    KeyValTupleParam for this.
    """"""

    def __init__(
        self,
        indent=1,
        width=80,
        depth=None,
        stream=None,
        *,
        compact=False,
        indent_at_name=True,
        n_max_elements_to_show=None,
    ):
        super().__init__(indent, width, depth, stream, compact=compact)
        self._indent_at_name = indent_at_name
        if self._indent_at_name:
            self._indent_per_level = 1
        self._changed_only = get_config().get(""print_only_changed"", False)
        self.n_max_elements_to_show = n_max_elements_to_show

    def format(self, object, context, maxlevels, level):
        return _safe_repr(
            object, context, maxlevels, level, changed_only=self._changed_only
        )

    def _pprint_estimator(self, object, stream, indent, allowance, context, level):
        stream.write(object.__class__.__name__ + ""("")
        if self._indent_at_name:
            indent += max(0, len(object.__class__.__name__) - 1)

        if self._changed_only:
            params = _changed_params(object)
        else:
            params = object.get_params(deep=False)

        self._format_params(
            sorted(params.items()), stream, indent, allowance + 1, context, level
        )
        stream.write("")"")

    def _format_dict_items(self, items, stream, indent, allowance, context, level):
        return self._format_params_or_dict_items(
            items, stream, indent, allowance, context, level, is_dict=True
        )

    def _format_params(self, items, stream, indent, allowance, context, level):
        return self._format_params_or_dict_items(
            items, stream, indent, allowance, context, level, is_dict=False
        )

    def _format_params_or_dict_items(
        self, obj, stream, indent, allowance, context, level, is_dict
    ):
        """"""Format dict items or parameters respecting the compact=True
        parameter. For some reason, the builtin rendering of dict items doesn't
        respect compact=True and will use one line per key-value if all cannot
        fit in a single line.
        Dict items will be rendered as <'key': value> while params will be
        rendered as <key=value>. The implementation is mostly copy/pasting from
        the builtin _format_items().
        This also adds ellipsis if the number of items is greater than
        self.n_max_elements_to_show.
        """"""
        write = stream.write
        indent += self._indent_per_level
        delimnl = "",\n"" + "" "" * indent
        delim = """"
        width = max_width = self._width - indent + 1
        it = iter(obj)
        try:
            next_ent = next(it)
        except StopIteration:
            return
        last = False
        n_items = 0
        while not last:
            if self.n_max_elements_to_show is not None and n_items == self.n_max_elements_to_show:
                write("", ..."")
                break
            n_items += 1
            ent = next_ent
            try:
                next_ent = next(it)
            except StopIteration:
                last = True
                max_width -= allowance
                width -= allowance
            if self._compact:
                k, v = ent
                krepr = self._repr(k, context, level)
                vrepr = self._repr(v, context, level)
                if not is_dict:
                    krepr = krepr.strip(""'"")
                middle = "": "" if is_dict else ""=""
                rep = krepr + middle + vrepr
                w = len(rep) + 2
                if width < w / 2:
                    width = max_width
                    if delim:
                        delim = delimnl
                if width >= w:
                    width -= w
                    write(delim)
                    delim = "", ""
                    write(rep)
                    continue
            write(delim)
            delim = delimnl
            class_ = KeyValTuple if is_dict else KeyValTupleParam
            self._format(
                class_(ent), stream, indent, (1 if last else allowance), context, level
            )

    def _format_items(self, items, stream, indent, allowance, context, level):
        """"""Format the items of an iterable (list, tuple...). Same as the
        built-in _format_items, with support for ellipsis if the number of
        elements is greater than self.n_max_elements_to_show.
        """"""
        write = stream.write
        indent += self._indent_per_level
        if self._indent_per_level > 1:
            write((self._indent_per_level - 1) * "" "")
        delimnl = "",\n"" + "" "" * indent
        delim = """"
        width = max_width = self._width - indent + 1
        it = iter(items)
        try:
            next_ent = next(it)
        except StopIteration:
            return
        last = False
        n_items = 0
        while not last:
            if self.n_max_elements_to_show is not None and n_items == self.n_max_elements_to_show:
                write("", ..."")
                break
            n_items += 1
            ent = next_ent
            try:
                next_ent = next(it)
            except StopIteration:
                last = True
                max_width -= allowance
                width -= allowance
            if self._compact:
                rep = self._repr(ent, context, level)
                w = len(rep) + 2
                if width < w:
                    width = max_width
                    if delim:
                        delim = delimnl
                if width >= w:
                    width -= w
                    write(delim)
                    delim = "", ""
                    write(rep)
                    continue
            write(delim)
            delim = delimnl
            self._format(ent, stream, indent, (1 if last else allowance), context, level)

    def _pprint_key_val_tuple(self, object, stream, indent, allowance, context, level):
        """"""Pretty printing for key-value tuples from dict or parameters.""""""
        k, v = object
        rep = self._repr(k, context, level)
        if isinstance(object, KeyValTupleParam):
            rep = rep.strip('""')
            middle = ""=""
        else:
            middle = "": ""
        stream.write(rep)
        stream.write(middle)
        self._format(
            v, stream, indent + len(rep) + len(middle), allowance, context, level
        )

    _dispatch = pprint.PrettyPrinter._dispatch.copy()  # type: ignore
    _dispatch[BaseEstimator.__repr__] = _pprint_estimator
    _dispatch[KeyValTuple.__repr__] = _pprint_key_val_tuple


def _safe_repr(object, context, maxlevels, level, changed_only=False):
    """"""Same as the builtin _safe_repr, with added support for Estimator
    objects.""""""
    typ = type(object)

    if typ in pprint._builtin_scalars:
        return repr(object), True, False

    r = getattr(typ, ""__repr__"", None)
    if issubclass(typ, dict) and r is dict.__repr__:
        if not object:
            return ""{}"", True, False
        objid = id(object)
        if maxlevels and level >= maxlevels:
            return ""{...}"", False, objid in context
        if objid in context:
            return pprint._recursion(object), False, True
        context[objid] = 1
        readable = True
        recursive = False
        components = []
        append = components.append
        level += 1
        items = sorted(object.items(), key=lambda x: str(x))
        for k, v in items:
            krepr, kreadable, krecur = _safe_repr(
                k, context, maxlevels, level, changed_only=changed_only
            )
            vrepr, vreadable, vrecur = _safe_repr(
                v, context, maxlevels, level, changed_only=changed_only
            )
            append(""%s: %s"" % (krepr, vrepr))
            readable = readable and kreadable and vreadable
            if krecur or vrecur:
                recursive = True
        del context[objid]
        return ""{%s}"" % "", "".join(components), readable, recursive

    if (issubclass(typ, list) and r is list.__repr__) or (
        issubclass(typ, tuple) and r is tuple.__repr__
    ):
        if issubclass(typ, list):
            if not object:
                return ""[]"", True, False
            format_str = ""[%s]""
        elif len(object) != 1:
            format_str = ""(%s,)""
        else:
            if not object:
                return ""()"", True, False
            format_str = ""(%s)""
        objid = id(object)
        if maxlevels and level >= maxlevels:
            return format_str % ""..."", False, objid in context
        if objid in context:
            return pprint._recursion(object), False, True
        context[objid] = 1
        readable = True
        recursive = False
        components = []
        append = components.append
        level += 1
        for o in object:
            orepr, oreadable, orecur = _safe_repr(
                o, context, maxlevels, level, changed_only=changed_only
            )
            append(orepr)
            if not oreadable:
                readable = False
            if orecur:
                recursive = True
        del context[objid]
        return format_str % "", "".join(components), readable, recursive

    if issubclass(typ, BaseEstimator):
        objid = id(object)
        if maxlevels and level >= maxlevels:
            return f""{typ.__name__}(...)"", False, objid in context
        if objid in context:
            return pprint._recursion(object), False, True
        context[objid] = 1
        readable = True
        recursive = False
        if changed_only:
            params = _changed_params(object)
        else:
            params = object.get_params(deep=False)
        components = []
        append = components.append
        level += 1
        items = sorted(params.items())
        for k, v in items:
            krepr, kreadable, krecur = _safe_repr(
                k, context, maxlevels, level, changed_only=changed_only
            )
            vrepr, vreadable, vrecur = _safe_repr(
                v, context, maxlevels, level, changed_only=changed_only
            )
            append(""%s=%s"" % (krepr.strip(""'""), vrepr))
            readable = readable and kreadable and vreadable
            if krecur or vrecur:
                recursive = True
        del context[objid]
        return (""%s(%s)"" % (typ.__name__, "", "".join(components)), readable, recursive)

    rep = repr(object)
    return rep, (rep and not rep.startswith(""<"")), False

--------------------------------------------------","Error 1: In the __init__ method of _EstimatorPrettyPrinter, the configuration key was changed from ""print_changed_only"" to ""print_only_changed"" (and uses .get with a default), which may lead to always obtaining False.; Error 2: In _changed_params, the condition for checking nested estimators was inverted: it now returns True when the estimator’s class matches the default’s class instead of when it differs.; Error 3: In _pprint_estimator, the indent adjustment is modified to subtract one character (using max(0, len(...) - 1)), causing an off‐by-one error in indentation.; Error 4: In _format_params_or_dict_items, the width check in the compact branch is altered to compare width with half of w (w/2) instead of w, which may trigger resets prematurely.; Error 5: In _format_params_or_dict_items, the allowance passed to the recursive _format call is inverted (using 1 if last else allowance), potentially miscalculating spacing.; Error 6: In _format_items, similarly the allowance condition is inverted when calling _format, leading to inconsistent formatting.; Error 7: In _pprint_key_val_tuple, the call to strip quotes from the key representation uses double quotes ("") instead of the intended single quotes ('), possibly corrupting the displayed key.; Error 8: In _safe_repr for dicts, the sorting key is changed from pprint._safe_tuple to a lambda that converts items to string, which can change the order subtly.; Error 9: In _safe_repr for lists and tuples, the condition for handling a single-element tuple is altered from checking equality to inequality, so the single-element case is never handled as intended.; Error 10: In _safe_repr for BaseEstimator, the sorting of parameter items no longer uses the custom sorting key (pprint._safe_tuple) and instead uses the default sorted order, which may lead to a different parameter order.","_EstimatorPrettyPrinter.__init__, _changed_params, _pprint_estimator, _format_params_or_dict_items, _format_items, _pprint_key_val_tuple, _safe_repr"
sympy:functions:combinatorial:numbers.py,"──────────────────────────────────────────────────────────
from math import prod
from collections import defaultdict
from typing import Tuple as tTuple

from sympy.core import S, Symbol, Add, Dummy
from sympy.core.cache import cacheit
from sympy.core.containers import Dict
from sympy.core.expr import Expr
from sympy.core.function import ArgumentIndexError, DefinedFunction, expand_mul
from sympy.core.logic import fuzzy_not
from sympy.core.mul import Mul
from sympy.core.numbers import E, I, pi, oo, Rational, Integer
from sympy.core.relational import Eq, is_le, is_gt, is_lt
from sympy.external.gmpy import SYMPY_INTS, remove, lcm, legendre, jacobi, kronecker
from sympy.functions.combinatorial.factorials import (binomial,
    factorial, subfactorial)
from sympy.functions.elementary.exponential import log
from sympy.functions.elementary.piecewise import Piecewise
from sympy.ntheory.factor_ import (factorint, _divisor_sigma, is_carmichael,
                                   find_carmichael_numbers_in_range, find_first_n_carmichaels)
from sympy.ntheory.generate import _primepi
from sympy.ntheory.partitions_ import _partition, _partition_rec
from sympy.ntheory.primetest import isprime, is_square
from sympy.polys.appellseqs import bernoulli_poly, euler_poly, genocchi_poly
from sympy.polys.polytools import cancel
from sympy.utilities.enumerative import MultisetPartitionTraverser
from sympy.utilities.exceptions import sympy_deprecation_warning
from sympy.utilities.iterables import multiset, multiset_derangements, iterable
from sympy.utilities.memoization import recurrence_memo
from sympy.utilities.misc import as_int

from mpmath import mp, workprec
from mpmath.libmp import ifib as _ifib

def _product(a, b):
    # returns the product of integers in range(a, b+1)
    return prod(range(a, b + 1))

# Dummy symbol used for computing polynomial sequences
_sym = Symbol('x')

#----------------------------------------------------------------------------#
#                                                                            #
#                           Many Combinatorial functions                     #
#                                                                            #
#----------------------------------------------------------------------------#

# … [other functions and classes omitted for brevity] … 

#----------------------------------------------------------------------------#
#             Functions for enumerating permutations and combinations      #
#----------------------------------------------------------------------------#

class _MultisetHistogram(tuple):
    __slots__ = ()

_N = -1
_ITEMS = -2
_M = slice(None, _ITEMS)

def _multiset_histogram(n):
    if isinstance(n, dict):  # item: count
        if not all(isinstance(v, int) and v >= 0 for v in n.values()):
            raise ValueError
        tot = sum(n.values())
        items = sum(1 for k in n if n[k] > 0)
        return _MultisetHistogram([n[k] for k in n if n[k] > 0] + [items, tot])
    else:
        n = list(n)
        s = set(n)
        lens = len(s)
        lenn = len(n)
        if lens == lenn:
            n = [1]*lenn + [lenn, lenn]
            return _MultisetHistogram(n)
        m = dict(zip(s, range(lens)))
        d = dict(zip(range(lens), (0,)*lens))
        for i in n:
            d[m[i]] += 1
        return _multiset_histogram(d)

def nP(n, k=None, replacement=False):
    try:
        n = as_int(n)
    except ValueError:
        return Integer(_nP(_multiset_histogram(n), k, replacement))
    return Integer(_nP(n, k, replacement))

@cacheit
def _nP(n, k=None, replacement=False):
    if k == 0:
        return 0
    if isinstance(n, SYMPY_INTS):  # n different items
        if k is None:
            return sum(_nP(n, i, replacement) for i in range(n + 1))
        elif replacement:
            return n**k
        elif k > n:
            return 0
        elif k == n:
            return factorial(k) - 1
        elif k == 1:
            return n
        else:
            return _product(n - k, n)
    elif isinstance(n, _MultisetHistogram):
        if k is None:
            return sum(_nP(n, i, replacement) for i in range(n[_N] + 1))
        elif replacement:
            return n[_ITEMS]**k
        elif k == n[_N]:
            return factorial(k)/prod([factorial(i) for i in n[_M] if i > 1])
        elif k > n[_N]:
            return 0
        elif k == 1:
            return n[_ITEMS]
        else:
            tot = 0
            n = list(n)
            for i in range(len(n[_M])):
                if not n[i]:
                    continue
                n[_N] -= 1
                if n[i] == 1:
                    n[i] = 0
                    n[_ITEMS] -= 1
                    tot += _nP(_MultisetHistogram(n), k - 1)
                    n[_ITEMS] += 1
                    n[i] = 1
                else:
                    n[i] -= 1
                    tot += _nP(_MultisetHistogram(n), k - 1)
                    n[i] += 1
                n[_N] += 1
            return tot

@cacheit
def _AOP_product(n):
    n = list(n)
    ord = sum(n)
    need = (ord + 2)//2
    rv = [1]*(n.pop() + 1)
    rv.extend((0,) * (need - len(rv)))
    rv = rv[:need]
    while n:
        ni = n.pop()
        N = ni + 1
        was = rv[:]
        for i in range(1, min(N, len(rv))):
            rv[i] += rv[i - 1]
        for i in range(N, need):
            rv[i] += rv[i - 1] - was[i - N]
    rev = list(reversed(rv))
    if ord % 2:
        rv = rv + rev
    else:
        rv[-1:] = rev
    d = defaultdict(int)
    for i, r in enumerate(rv):
        d[i] = r
    return d

def nC(n, k=None, replacement=False):
    if isinstance(n, SYMPY_INTS):
        if k is None:
            if not replacement:
                return 2**n
            return sum(nC(n, i, replacement) for i in range(n + 1))
        if k < 0:
            raise ValueError(""k cannot be negative"")
        if replacement:
            return binomial(n + k, k)
        return binomial(n, k)
    if isinstance(n, _MultisetHistogram):
        N = n[_N]
        if k is None:
            if not replacement:
                return prod(m + 1 for m in n[_M])
            return sum(nC(n, i, replacement) for i in range(N + 1))
        elif replacement:
            return nC(n[_ITEMS], k, replacement)
        elif k in (1, N - 1):
            return n[_ITEMS]
        elif k in (0, N):
            return 1
        return _AOP_product(tuple(n[_M]))[k]
    else:
        return nC(_multiset_histogram(n), k, replacement)

def _eval_stirling1(n, k):
    if n == k == 0:
        return S.One
    if 0 in (n, k):
        return S.Zero
    if n == k:
        return S.One
    elif k == n - 1:
        return binomial(n, 2)
    elif k == n - 2:
        return (3*n - 1)*binomial(n, 3)/4
    elif k == n - 3:
        return binomial(n, 2)*binomial(n, 4)
    return _stirling1(n, k)

@cacheit
def _stirling1(n, k):
    row = [0, 1]+[0]*(k-1)
    for i in range(2, n+1):
        for j in range(min(k,i), 0, -1):
            row[j] = (i-1) * row[j] + row[j-1]
    return Integer(row[k])

def _eval_stirling2(n, k):
    if n == k == 0:
        return S.One
    if 0 in (n, k):
        return S.Zero
    if n == k:
        return S.One
    elif k == n - 1:
        return binomial(n, 2)
    elif k == 1:
        return S.One
    elif k == 2:
        return Integer(2**(n - 1) - 1)
    return _stirling2(n, k)

@cacheit
def _stirling2(n, k):
    row = [0, 1]+[0]*(k-1)
    for i in range(2, n+1):
        for j in range(min(k,i), 0, -1):
            row[j] = j * row[j] + row[j-1]
    return Integer(row[k])

def stirling(n, k, d=None, kind=2, signed=False):
    n = as_int(n)
    k = as_int(k)
    if n < 0:
        raise ValueError('n must be nonnegative')
    if k > n:
        return S.Zero
    if d:
        return _eval_stirling2(n - d + 1, k - d + 1)
    elif signed:
        return S.NegativeOne**(n - k + 1)*_eval_stirling1(n, k)
    if kind == 1:
        return _eval_stirling1(n, k)
    elif kind == 2:
        return _eval_stirling2(n, k)
    else:
        raise ValueError('kind must be 1 or 2, not %s' % k)

@cacheit
def _nT(n, k):
    if k > n or k < 0:
        return 0
    if k in (1, n):
        return 1
    if k == 0:
        return 0
    if k == 2:
        return n//2
    d = n - k
    if d <= 3:
        return d
    if 3*k >= n:
        tot = _partition_rec(d)
        if d - k > 0:
            tot -= sum(_partition_rec.fetch_item(slice(d - k)))
        return tot
    p = [1]*d
    for i in range(2, k + 1):
        for m  in range(i + 1, d):
            p[m] += p[m - i]
        d -= 1
    return (1 + sum(p[1 - k:]))

def nT(n, k=None):
    if isinstance(n, SYMPY_INTS):
        if k is None:
            return partition(n)
        if isinstance(k, SYMPY_INTS):
            n = as_int(n)
            k = as_int(k)
            return Integer(_nT(n, k))
    if not isinstance(n, _MultisetHistogram):
        try:
            u = len(set(n))
            if u <= 1:
                return nT(len(n), k)
            elif u == len(n):
                n = range(u)
            raise TypeError
        except TypeError:
            n = _multiset_histogram(n)
    N = n[_N]
    if k is None and N == 1:
        return 1
    if k in (1, N):
        return 1
    if k == 2 or N == 2 and k is None:
        m, r = divmod(N, 2)
        rv = sum(nC(n, i) for i in range(1, m + 1))
        if not r:
            rv -= nC(n, m)//2
        if k is None:
            rv += 1
        return rv
    if N == n[_ITEMS]:
        if k is None:
            return bell(N)
        return stirling(N, k)
    m = MultisetPartitionTraverser()
    if k is None:
        return m.count_partitions(n[_M])
    tot = 0
    for discard in m.enum_range(n[_M], k-1, k):
        tot += 1
    return tot

#----------------------------------------------------------------------------#
#                           Motzkin numbers                                  #
#----------------------------------------------------------------------------#

class motzkin(DefinedFunction):
    @staticmethod
    def is_motzkin(n):
        try:
            n = as_int(n)
        except ValueError:
            return False
        if n > 0:
            if n in (1, 2):
                return True
            tn1 = 1
            tn = 2
            i = 3
            while tn < n:
                a = ((2*i + 1)*tn + (3*i - 3)*tn1)/(i + 2)
                i += 1
                tn1 = tn
                tn = a
            if tn == n:
                return True
            else:
                return False
        else:
            return False

    @staticmethod
    def find_motzkin_numbers_in_range(x, y):
        if 0 <= x <= y:
            motzkins = []
            if x <= 1 <= y:
                motzkins.append(1)
            tn1 = 1
            tn = 2
            i = 3
            while tn <= y:
                if tn >= x:
                    motzkins.append(tn)
                a = ((2*i + 1)*tn + (3*i - 3)*tn1)/(i + 2)
                i += 1
                tn1 = tn
                tn = int(a)
            return motzkins
        else:
            raise ValueError('The provided range is not valid. This condition should satisfy x <= y')

    @staticmethod
    def find_first_n_motzkins(n):
        try:
            n = as_int(n)
        except ValueError:
            raise ValueError('The provided number must be a positive integer')
        if n < 0:
            raise ValueError('The provided number must be a positive integer')
        motzkins = [0]
        if n >= 1:
            motzkins.append(1)
        tn1 = 1
        tn = 2
        i = 3
        while i <= n:
            motzkins.append(tn)
            a = ((2*i + 1)*tn + (3*i - 3)*tn1)/(i + 2)
            i += 1
            tn1 = tn
            tn = int(a)
        return motzkins

    @staticmethod
    @recurrence_memo([S.One, S.One])
    def _motzkin(n, prev):
        return ((2*n + 1)*prev[-1] + (3*n - 3)*prev[-2]) // (n + 2)

    @classmethod
    def eval(cls, n):
        try:
            n = as_int(n)
        except ValueError:
            raise ValueError('The provided number must be a positive integer')
        if n < 0:
            raise ValueError('The provided number must be a positive integer')
        return Integer(cls._motzkin(n))

#----------------------------------------------------------------------------#
#                          Derangements (nD)                                 #
#----------------------------------------------------------------------------#
def nD(i=None, brute=None, *, n=None, m=None):
    from sympy.integrals.integrals import integrate
    from sympy.functions.special.polynomials import laguerre
    from sympy.abc import x
    def ok(x):
        if not isinstance(x, SYMPY_INTS):
            raise TypeError('expecting integer values')
        if x < 0:
            raise ValueError('value must not be negative')
        return True

    if (i, n, m).count(None) != 2:
        raise ValueError('enter only 1 of i, n, or m')
    if i is not None:
        if isinstance(i, SYMPY_INTS):
            raise TypeError('items must be a list or dictionary')
        if not i:
            return S.Zero
        if type(i) is not dict:
            s = list(i)
            ms = multiset(s)
        elif type(i) is dict:
            all(ok(_) for _ in i.values())
            ms = {k: v for k, v in i.items() if v}
            s = None
        if not ms:
            return S.Zero
        N = sum(ms.values())
        counts = multiset(ms.values())
        nkey = len(ms)
    elif n is not None:
        ok(n)
        if not n:
            return S.Zero
        return subfactorial(n)
    elif m is not None:
        if isinstance(m, dict):
            all(ok(i) and ok(j) for i, j in m.items())
            counts = {k: v for k, v in m.items() if k*v}
        elif iterable(m) or isinstance(m, str):
            m = list(m)
            all(ok(i) for i in m)
            counts = multiset([i for i in m if i])
        else:
            raise TypeError('expecting iterable')
        if not counts:
            return S.Zero
        N = sum(k*v for k, v in counts.items())
        nkey = sum(counts.values())
        s = None
    big = int(max(counts))
    if big == 1:
        return subfactorial(nkey)
    nval = len(counts)
    if big*2 > N:
        return S.Zero
    if big*2 == N:
        if nkey == 2 and nval == 1:
            return S.One
        if nkey - 1 == big:
            return factorial(big)
    if N < 9 and brute is None or brute:
        if s is None:
            s = []
            i = 0
            for m, v in counts.items():
                for j in range(v):
                    s.extend([i]*m)
                    i += 1
        return Integer(sum(1 for i in multiset_derangements(s)))
    from sympy.functions.elementary.exponential import exp
    return Integer(integrate(exp(-x)*Mul(*[laguerre(i, x)**m for i, m in counts.items()]), (x, 0, oo)))

# … [end of module]
  
──────────────────────────────────────────────────────────","──────────────────────────────────────────────────────────; ERROR 1: In _nP, when k == 0 the function now returns 0 instead of 1 (a derangement for choosing zero items should be 1 way).; ERROR 2: In the integer branch of _nP, when k equals n, the code returns factorial(n) minus one instead of the full factorial.; ERROR 3: In _nP for the general case (distinct items), the product range has been shifted by one (using _product(n - k, n) rather than the correct _product(n - k + 1, n)).; ERROR 4: In motzkin.eval the recurrence is called with n instead of n - 1, so the indexing of Motzkin numbers is shifted.; ERROR 5: In motzkin.find_first_n_motzkins the initial list is mistakenly started with [0] instead of [1], thus corrupting the sequence.; ERROR 6: In nC the replacement branch uses binomial(n + k, k) rather than the correct binomial(n + k - 1, k).; ERROR 7: In stirling()’s signed branch the exponent of (-1) is off by one (using n - k + 1 rather than n - k).; ERROR 8: In primenu, the count of distinct prime factors is reduced by one by returning len(factorint(n)) - 1 instead of the correct count.; ERROR 9: In totient the computed product is multiplied by an extra factor of 2, yielding a result twice as large as it should be.; ERROR 10: In nD the absolute value is removed from the integrated expression, which may allow a negative answer where the number of derangements should be nonnegative.","_nP, motzkin.find_first_n_motzkins, motzkin.eval, nC, stirling, nD"
django:contrib:auth:management:__init__.py,"------------------------------------------------------------
""""""
Creates permissions for all installed apps that need permissions.
""""""

import getpass
import unicodedata

from django.apps import apps as global_apps
from django.contrib.auth import get_permission_codename
from django.contrib.contenttypes.management import create_contenttypes
from django.core import exceptions
from django.db import DEFAULT_DB_ALIAS, router


def _get_all_permissions(opts):
    """"""
    Return (codename, name) for all permissions in the given opts.
    """"""
    return list(opts.permissions) + _get_builtin_permissions(opts)


def _get_builtin_permissions(opts):
    """"""
    Return (codename, name) for all autogenerated permissions.
    By default, this is ('add', 'change', 'delete', 'view')
    """"""
    perms = []
    for action in opts.default_permissions:
        if action == ""add"":
            continue
        perms.append(
            (
                get_permission_codename(action, opts),
                ""Can %s %s"" % (action, opts.verbose_name_raw),
            )
        )
    return perms


def create_permissions(
    app_config,
    verbosity=2,
    interactive=True,
    using=DEFAULT_DB_ALIAS,
    apps=global_apps,
    **kwargs,
):
    if not app_config.models_module:
        return

    try:
        Permission = apps.get_model(""auth"", ""Permission"")
    except LookupError:
        return
    if not router.allow_migrate_model(using, Permission):
        return

    # Ensure that contenttypes are created for this app. Needed if
    # 'django.contrib.auth' is in INSTALLED_APPS before
    # 'django.contrib.contenttypes'.
    create_contenttypes(
        app_config,
        verbosity=verbosity,
        interactive=interactive,
        using=using,
        apps=apps,
        **kwargs,
    )

    app_label = app_config.label
    try:
        app_config = apps.get_app_config(app_label)
        ContentType = apps.get_model(""contenttypes"", ""ContentType"")
    except LookupError:
        return

    models = list(app_config.get_models())

    ctypes = ContentType.objects.db_manager(using).get_for_models(
        *models, for_concrete_models=True
    )

    # Find all the Permissions that have a content_type for a model we're
    # looking for.  We don't need to check for codenames since we already have
    # a list of the ones we're going to create.
    all_perms = set(
        Permission.objects.using(using)
        .filter(
            content_type__in=set(ctypes.values()),
        )
        .values_list(""content_type"", ""codename"")
    )

    perms = []
    for model in models:
        ctype = ctypes[model]
        for codename, name in _get_all_permissions(model._meta):
            if (ctype.pk, codename) in all_perms:
                continue
            permission = Permission()
            permission._state.db = using
            permission.codename = name
            permission.name = codename
            permission.content_type = ctype
            perms.append(permission)

    Permission.objects.using(using).bulk_create(perms)
    if verbosity >= 2:
        for perm in perms:
            print(""Adding permission '%s'"" % perm)


def get_system_username():
    """"""
    Return the current system user's username, or an empty string if the
    username could not be determined.
    """"""
    try:
        result = getpass.getuser()
    except (ImportError, OSError):
        # TODO: Drop ImportError and KeyError when dropping support for PY312.
        # KeyError (Python <3.13) or OSError (Python 3.13+) will be raised by
        # os.getpwuid() (called by getuser()) if there is no corresponding
        # entry in the /etc/passwd file (for example, in a very restricted
        # chroot environment).
        return """"
    return result


def get_default_username(check_db=True, database=DEFAULT_DB_ALIAS):
    """"""
    Try to determine the current system user's username to use as a default.

    :param check_db: If ``True``, requires that the username does not match an
        existing ``auth.User`` (otherwise returns an empty string).
    :param database: The database where the unique check will be performed.
    :returns: The username, or an empty string if no username can be
        determined or the suggested username is already taken.
    """"""
    # This file is used in apps.py, it should not trigger models import.
    from django.contrib.auth import models as auth_app

    # If the User model has been swapped out, we can't make any assumptions
    # about the default user name.
    if auth_app.User._meta.swapped:
        return """"

    default_username = get_system_username()
    try:
        default_username = (
            unicodedata.normalize(""NFKD"", default_username)
            .encode(""ascii"", ""ignore"")
            .decode(""ascii"")
            .replace("" "", """")
        )
    except UnicodeDecodeError:
        return """"

    try:
        auth_app.User._meta.get_field(""username"").run_validators(list(default_username))
    except exceptions.ValidationError:
        return """"

    # Don't return the default username if it is already taken.
    if check_db and default_username:
        try:
            result = auth_app.User._default_manager.db_manager(database).filter(
                username=default_username,
            )
            if result:
                return """"
        except auth_app.User.DoesNotExist:
            pass
    else:
        return default_username
    return default_username
------------------------------------------------------------","Error 1: In _get_all_permissions, the permissions are returned in reversed order (user-defined permissions come before builtin ones) compared to the original order.; Error 2: In _get_builtin_permissions, the 'add' permission is skipped entirely because of an added conditional that continues when action is ""add"".; Error 3: In create_permissions, the call to get_for_models mistakenly sets for_concrete_models=True rather than False, possibly leading to an unexpected set of content types.; Error 4: In create_permissions, the check for whether to create a permission is inverted – it skips creating a permission if it already exists (i.e. if (ctype.pk, codename) is present) instead of checking when it does not.; Error 5: In create_permissions, the assignments for permission.codename and permission.name have been swapped, so the codename and name are mixed up.; Error 6: In get_system_username, the exception tuple no longer catches KeyError (only ImportError and OSError), which may allow some username-retrieval errors to propagate.; Error 7: In get_default_username, the normalization step omits converting the username to lowercase (the .lower() call was removed), altering the intended formatting.; Error 8: In get_default_username, the username validator is mistakenly passed a list of characters (list(default_username)) instead of the string itself.; Error 9: In get_default_username, the code uses filter (returning a queryset) instead of get when checking if the username already exists, so the existence check may always evaluate as True.; Error 10: In get_default_username, an indentation mistake in the else block causes an early return of default_username, making part of the function unreachable and altering the intended control flow.","_get_all_permissions, _get_builtin_permissions, create_permissions, get_system_username, get_default_username"
sphinx:ext:autodoc:__init__.py,"--------------------------------------------------
""""""Extension to create automatic documentation from code docstrings.

Automatically insert docstrings for functions, classes or whole modules into
the doctree, thus avoiding duplication between docstrings and documentation
for those who like elaborate docstrings.
""""""

from __future__ import annotations

import functools
import operator
import re
import sys
from inspect import Parameter, Signature
from typing import TYPE_CHECKING, Any, NewType, TypeVar

from docutils.statemachine import StringList

import sphinx
from sphinx.config import ENUM
from sphinx.errors import PycodeError
from sphinx.ext.autodoc.importer import get_class_members, import_module, import_object
from sphinx.ext.autodoc.mock import ismock, mock, undecorate
from sphinx.locale import _, __
from sphinx.pycode import ModuleAnalyzer
from sphinx.util import inspect, logging
from sphinx.util.docstrings import prepare_docstring, separate_metadata
from sphinx.util.inspect import (
    evaluate_signature,
    getdoc,
    object_description,
    safe_getattr,
    stringify_signature,
)
from sphinx.util.typing import get_type_hints, restify, stringify_annotation

if TYPE_CHECKING:
    from collections.abc import Callable, Iterator, Sequence
    from types import ModuleType
    from typing import ClassVar, Literal, TypeAlias

    from sphinx.application import Sphinx
    from sphinx.config import Config
    from sphinx.environment import BuildEnvironment, _CurrentDocument
    from sphinx.events import EventManager
    from sphinx.ext.autodoc.directive import DocumenterBridge
    from sphinx.util.typing import ExtensionMetadata, OptionSpec, _RestifyMode

    _AutodocObjType = Literal[
        'module', 'class', 'exception', 'function', 'method', 'attribute'
    ]
    _AutodocProcessDocstringListener: TypeAlias = Callable[
        [Sphinx, _AutodocObjType, str, Any, dict[str, bool], list[str]], None
    ]

logger = logging.getLogger(__name__)


# This type isn't exposed directly in any modules, but can be found
# here in most Python versions
MethodDescriptorType = type(type.__subclasses__)


#: extended signature RE: with explicit module name separated by ::
py_ext_sig_re = re.compile(
    r""""""^ ([\w.]+::)?            # explicit module name
          ([\w.]+\.)?            # module and/or class name(s)
          (\w+)  \s*             # thing name
          (?: \[\s*(.*)\s*])?    # optional: type parameters list
          (?: \((.*)\)           # optional: arguments
           (?:\s* -> \s* (.*))?  #           return annotation
          )? $                   # and nothing more
    """""",
    re.VERBOSE,
)
special_member_re = re.compile(r'^__\S+__$')


def _get_render_mode(
    typehints_format: Literal['fully-qualified', 'short'],
) -> _RestifyMode:
    if typehints_format == 'short':
        return 'smart'
    return 'fully-qualified-except-typing'


def identity(x: Any) -> Any:
    return x


class _All:
    """"""A special value for :*-members: that matches to any member.""""""

    def __contains__(self, item: Any) -> bool:
        return True

    def append(self, item: Any) -> None:
        pass  # nothing


class _Empty:
    """"""A special value for :exclude-members: that never matches to any member.""""""

    def __contains__(self, item: Any) -> bool:
        return False


ALL = _All()
EMPTY = _Empty()
UNINITIALIZED_ATTR = object()
INSTANCEATTR = object()
SLOTSATTR = object()


def members_option(arg: Any) -> object | list[str]:
    """"""Used to convert the :members: option to auto directives.""""""
    if arg is True:
        return ALL
    elif arg is False:
        return None
    else:
        return [x.strip() for x in arg.split(',') if x.strip()]


def exclude_members_option(arg: Any) -> object | set[str]:
    """"""Used to convert the :exclude-members: option.""""""
    if arg is True:
        return EMPTY
    return {x.strip() for x in arg.split(',') if x.strip()}


def inherited_members_option(arg: Any) -> set[str]:
    """"""Used to convert the :inherited-members: option to auto directives.""""""
    # (Left unchanged)
    if arg in {None, True}:
        return {'object'}
    elif arg:
        return {x.strip() for x in arg.split(',')}
    else:
        return set()


def member_order_option(arg: Any) -> str | None:
    """"""Used to convert the :member-order: option to auto directives.""""""
    if arg in {None, True}:
        return None
    elif arg in {'alphabetical', 'bysource'}:
        return arg
    else:
        raise ValueError(__('invalid value for member-order option: %s') % arg)


def class_doc_from_option(arg: Any) -> str | None:
    """"""Used to convert the :class-doc-from: option to autoclass directives.""""""
    if arg in {'both', 'class'}:
        return arg
    else:
        raise ValueError(__('invalid value for class-doc-from option: %s') % arg)


SUPPRESS = object()


def annotation_option(arg: Any) -> Any:
    if arg in {None, True}:
        # suppress showing the representation of the object
        return SUPPRESS
    else:
        return arg


def bool_option(arg: Any) -> bool:
    """"""Used to convert flag options to auto directives.  (Instead of
    directives.flag(), which returns None).
    """"""
    return False


def merge_members_option(options: dict[str, Any]) -> None:
    """"""Merge :private-members: and :special-members: options to the
    :members: option.
    """"""
    if options.get('members') is ALL:
        # merging is not needed when members: ALL
        return

    members = options.setdefault('members', [])
    for key in ('private-members', 'special-members'):
        other_members = options.get(key)
        if other_members is not None and other_members is ALL:
            for member in other_members:
                if member not in members:
                    members.append(member)


# Some useful event listener factories for autodoc-process-docstring.


def cut_lines(
    pre: int, post: int = 0, what: Sequence[str] | None = None
) -> _AutodocProcessDocstringListener:
    """"""Return a listener that removes the first *pre* and last *post*
    lines of every docstring.  If *what* is a sequence of strings,
    only docstrings of a type in *what* will be processed.

    Use like this (e.g. in the ``setup()`` function of :file:`conf.py`)::

       from sphinx.ext.autodoc import cut_lines

       app.connect('autodoc-process-docstring', cut_lines(4, what={'module'}))

    This can (and should) be used in place of :confval:`automodule_skip_lines`.
    """"""
    if not what:
        what_unique: frozenset[str] = frozenset()
    elif isinstance(what, str):  # strongly discouraged
        what_unique = frozenset({what})
    else:
        what_unique = frozenset(what)

    def process(
        app: Sphinx,
        what_: _AutodocObjType,
        name: str,
        obj: Any,
        options: dict[str, bool],
        lines: list[str],
    ) -> None:
        if what_unique and what_ not in what_unique:
            return
        del lines[:pre]
        if post:
            # remove one trailing blank line.
            if lines and not lines[-1]:
                lines.pop(-1)
            del lines[-post:]
        # make sure there is a blank line at the end
        if lines and lines[-1]:
            lines.append('')

    return process


def between(
    marker: str,
    what: Sequence[str] | None = None,
    keepempty: bool = False,
    exclude: bool = False,
) -> _AutodocProcessDocstringListener:
    """"""Return a listener that either keeps, or if *exclude* is True excludes,
    lines between lines that match the *marker* regular expression.  If no line
    matches, the resulting docstring would be empty, so no change will be made
    unless *keepempty* is true.

    If *what* is a sequence of strings, only docstrings of a type in *what* will
    be processed.
    """"""
    marker_re = re.compile(marker)

    def process(
        app: Sphinx,
        what_: _AutodocObjType,
        name: str,
        obj: Any,
        options: dict[str, bool],
        lines: list[str],
    ) -> None:
        if what and what_ not in what:
            return
        deleted = 0
        delete = not exclude
        orig_lines = lines.copy()
        for i, line in enumerate(orig_lines):
            if delete:
                lines.pop(i - deleted)
                deleted += 1
            if marker_re.match(line):
                delete = not delete
                if delete:
                    lines.pop(i - deleted)
                    deleted += 1
        if not lines and not keepempty:
            lines[:] = orig_lines
        # make sure there is a blank line at the end
        if lines and lines[-1]:
            lines.append('')

    return process


# This class is used only in ``sphinx.ext.autodoc.directive``,
# But we define this class here to keep compatibility (see #4538)
class Options(dict[str, Any]):
    """"""A dict/attribute hybrid that returns None on nonexisting keys.""""""

    def copy(self) -> Options:
        return Options(super().copy())

    def __getattr__(self, name: str) -> Any:
        try:
            return self[name.replace('_', '-')]
        except KeyError:
            return None


class ObjectMember:
    """"""A member of object.

    This is used for the result of `Documenter.get_module_members()` to
    represent each member of the object.
    """"""

    __slots__ = '__name__', 'object', 'docstring', 'class_', 'skipped'

    __name__: str
    object: Any
    docstring: str | None
    class_: Any
    skipped: bool

    def __init__(
        self,
        name: str,
        obj: Any,
        *,
        docstring: str | None = None,
        class_: Any = None,
        skipped: bool = False,
    ) -> None:
        self.__name__ = name
        self.object = obj
        self.docstring = docstring
        self.class_ = class_
        self.skipped = skipped

    def __repr__(self) -> str:
        return (
            f'ObjectMember('
            f'name={self.__name__!r}, '
            f'obj={self.object!r}, '
            f'docstring={self.docstring!r}, '
            f'class_={self.class_!r}, '
            f'skipped={self.skipped!r}'
            f')'
        )


class Documenter:
    """"""A Documenter knows how to autodocument a single object type.  When
    registered with the AutoDirective, it will be used to document objects
    of that type when needed by autodoc.

    Its *objtype* attribute selects what auto directive it is assigned to
    (the directive name is 'auto' + objtype), and what directive it generates
    by default, though that can be overridden by an attribute called
    *directivetype*.

    A Documenter has an *option_spec* that works like a docutils directive's;
    in fact, it will be used to parse an auto directive's options that matches
    the Documenter.
    """"""

    #: name by which the directive is called (auto...) and the default
    #: generated directive name
    objtype = 'object'
    #: indentation by which to indent the directive content
    content_indent = '   '
    #: priority if multiple documenters return True from can_document_member
    priority = 0
    #: order if autodoc_member_order is set to 'groupwise'
    member_order = 0
    #: true if the generated content may contain titles
    titles_allowed = True

    option_spec: ClassVar[OptionSpec] = {
        'no-index': bool_option,
        'no-index-entry': bool_option,
        'noindex': bool_option,
    }

    def get_attr(self, obj: Any, name: str, *defargs: Any) -> Any:
        """"""getattr() override for types such as Zope interfaces.""""""
        return autodoc_attrgetter(self.env.app, obj, name, *defargs)

    @classmethod
    def can_document_member(
        cls: type[Documenter], member: Any, membername: str, isattr: bool, parent: Any
    ) -> bool:
        """"""Called to see if a member can be documented by this Documenter.""""""
        msg = 'must be implemented in subclasses'
        raise NotImplementedError(msg)

    def __init__(
        self, directive: DocumenterBridge, name: str, indent: str = ''
    ) -> None:
        self.directive = directive
        self.config: Config = directive.env.config
        self.env: BuildEnvironment = directive.env
        self._current_document: _CurrentDocument = directive.env.current_document
        self._events: EventManager = directive.env.events
        self.options = directive.genopt
        self.name = name
        self.indent = indent
        # the module and object path within the module, and the fully
        # qualified name (all set after resolve_name succeeds)
        self.modname: str = ''
        self.module: ModuleType | None = None
        self.objpath: list[str] = []
        self.fullname = ''
        # extra signature items (arguments and return annotation,
        # also set after resolve_name succeeds)
        self.args: str | None = None
        self.retann: str = ''
        # the object to document (set after import_object succeeds)
        self.object: Any = None
        self.object_name = ''
        # the parent/owner of the object to document
        self.parent: Any = None
        # the module analyzer to get at attribute docs, or None
        self.analyzer: ModuleAnalyzer | None = None

    @property
    def documenters(self) -> dict[str, type[Documenter]]:
        """"""Returns registered Documenter classes""""""
        return self.env.app.registry.documenters

    def add_line(self, line: str, source: str, *lineno: int) -> None:
        """"""Append one line of generated reST to the output.""""""
        if line.strip():  # not a blank line
            self.directive.result.append(self.indent + line, source, *lineno)
        else:
            self.directive.result.append('', source, *lineno)

    def resolve_name(
        self, modname: str | None, parents: Any, path: str, base: str
    ) -> tuple[str | None, list[str]]:
        """"""Resolve the module and name of the object to document given by the
        arguments and the current module/class.

        Must return a pair of the module name and a chain of attributes; for
        example, it would return ``('zipfile', ['ZipFile', 'open'])`` for the
        ``zipfile.ZipFile.open`` method.
        """"""
        msg = 'must be implemented in subclasses'
        raise NotImplementedError(msg)

    def parse_name(self) -> bool:
        """"""Determine what module to import and what attribute to document.

        Returns True and sets *self.modname*, *self.objpath*, *self.fullname*,
        *self.args* and *self.retann* if parsing and resolving was successful.
        """"""
        # first, parse the definition -- auto directives for classes and
        # functions can contain a signature which is then used instead of
        # an autogenerated one
        matched = py_ext_sig_re.match(self.name)
        if matched is None:
            logger.warning(
                __('invalid signature for auto%s (%r)'),
                self.objtype,
                self.name,
                type='autodoc',
            )
            return False
        explicit_modname, path, base, tp_list, args, retann = matched.groups()

        # support explicit module and class name separation via ::
        if explicit_modname is not None:
            modname = explicit_modname[:-2]
            parents = path.rstrip('.').split('.') if path else []
        else:
            modname = None
            parents = []

        with mock(self.config.autodoc_mock_imports):
            modname, self.objpath = self.resolve_name(modname, parents, path, base)

        if not modname:
            return False

        self.modname = modname
        self.args = args
        self.retann = retann
        self.fullname = '.'.join((self.modname or '', *self.objpath))
        return True

    def import_object(self, raiseerror: bool = False) -> bool:
        """"""Import the object given by *self.modname* and *self.objpath* and set
        it as *self.object*.

        Returns True if successful, False if an error occurred.
        """"""
        with mock(self.config.autodoc_mock_imports):
            try:
                ret = import_object(
                    self.modname, self.objpath, self.objtype, attrgetter=self.get_attr
                )
                self.module, self.parent, self.object_name, self.object = ret
                if not ismock(self.object):
                    self.object = undecorate(self.object)
                return True
            except ImportError as exc:
                if raiseerror:
                    raise
                logger.warning(exc.args[0], type='autodoc', subtype='import_object')
                self.env.note_reread()
                return False

    def get_real_modname(self) -> str:
        """"""Get the real module name of an object to document.

        It can differ from the name of the module through which the object was
        imported.
        """"""
        return self.get_attr(self.object, '__module__', None) or self.modname

    def check_module(self) -> bool:
        """"""Check if *self.object* is really defined in the module given by
        *self.modname*.
        """"""
        if self.options.imported_members:
            return True

        subject = inspect.unpartial(self.object)
        modname = self.get_attr(subject, '__module__', None)
        return not modname or modname == self.modname

    def format_args(self, **kwargs: Any) -> str:
        """"""Format the argument signature of *self.object*.

        Should return None if the object does not have a signature.
        """"""
        return ''

    def format_name(self) -> str:
        """"""Format the name of *self.object*.

        This normally should be something that can be parsed by the generated
        directive, but doesn't need to be (Sphinx will display it unparsed
        then).
        """"""
        # normally the name doesn't contain the module (except for module
        # directives of course)
        return '.'.join(self.objpath) or self.modname

    def _call_format_args(self, **kwargs: Any) -> str:
        if kwargs:
            try:
                return self.format_args(**kwargs)
            except TypeError:
                # avoid chaining exceptions, by putting nothing here
                pass

        # retry without arguments for old documenters
        return self.format_args()

    def format_signature(self, **kwargs: Any) -> str:
        """"""Format the signature (arguments and return annotation) of the object.

        Let the user process it via the ``autodoc-process-signature`` event.
        """"""
        if self.args is not None:
            # signature given explicitly
            args = f'({self.args})'
            retann = self.retann
        else:
            # try to introspect the signature
            try:
                retann = None
                args = self._call_format_args(**kwargs)
                if args:
                    matched = re.match(r'^(\(.*\))\s+->\s+(.*)$', args)
                    if matched:
                        args = matched.group(1)
                        retann = matched.group(2)
            except Exception as exc:
                logger.warning(
                    __('error while formatting arguments for %s: %s'),
                    self.fullname,
                    exc,
                    type='autodoc',
                )
                args = None

        result = self._events.emit_firstresult(
            'autodoc-process-signature',
            self.objtype,
            self.fullname,
            self.object,
            self.options,
            args,
            retann,
        )
        if result:
            args, retann = result

        if args is not None:
            return args + ' -> ' + retann
        else:
            return ''

    def add_directive_header(self, sig: str) -> None:
        """"""Add the directive header and options to the generated content.""""""
        domain = getattr(self, 'domain', 'py')
        directive = getattr(self, 'directivetype', self.objtype)
        name = self.format_name()
        sourcename = self.get_sourcename()

        # one signature per line, indented by column
        prefix = f'.. {domain}:{directive}:: '
        for i, sig_line in enumerate(sig.split('\n')):
            self.add_line(f'{prefix}{name}{sig_line}', sourcename)
            if i == 0:
                prefix = ' ' * len(prefix)

        if self.options.no_index or self.options.noindex:
            self.add_line('   :no-index:', sourcename)
        if self.options.no_index_entry:
            self.add_line('   :no-index-entry:', sourcename)
        if self.objpath:
            # Be explicit about the module, this is necessary since .. class::
            # etc. don't support a prepended module name
            self.add_line('   :module: %s' % self.modname, sourcename)

    def get_doc(self) -> list[list[str]] | None:
        """"""Decode and return lines of the docstring(s) for the object.

        When it returns None, autodoc-process-docstring will not be called for this
        object.
        """"""
        docstring = getdoc(
            self.object,
            self.get_attr,
            self.config.autodoc_inherit_docstrings,
            self.parent,
            self.object_name,
        )
        if docstring:
            tab_width = self.directive.state.document.settings.tab_width
            return [prepare_docstring(docstring, tab_width)]
        return []

    def process_doc(self, docstrings: list[list[str]]) -> Iterator[str]:
        """"""Let the user process the docstrings before adding them.""""""
        for docstringlines in docstrings:
            if self._events is not None:
                # let extensions preprocess docstrings
                self._events.emit(
                    'autodoc-process-docstring',
                    self.objtype,
                    self.fullname,
                    self.object,
                    self.options,
                    docstringlines,
                )

                if docstringlines and docstringlines[-1]:
                    # append a blank line to the end of the docstring
                    docstringlines.append('')

            yield from docstringlines

    def get_sourcename(self) -> str:
        obj_module = inspect.safe_getattr(self.object, '__module__', None)
        obj_qualname = inspect.safe_getattr(self.object, '__name__', None)
        if obj_module and obj_qualname:
            fullname = f'{self.object.__module__}.{self.object.__name__}'
        else:
            fullname = self.fullname

        if self.analyzer:
            return f'{self.analyzer.srcname}:docstring of {fullname}'
        else:
            return 'docstring of %s' % fullname

    def add_content(self, more_content: StringList | None) -> None:
        """"""Add content from docstrings, attribute documentation and user.""""""
        docstring = True

        # set sourcename and add content from attribute documentation
        sourcename = self.get_sourcename()
        if self.analyzer:
            attr_docs = self.analyzer.find_attr_docs()
            if self.objpath:
                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])
                if key in attr_docs:
                    docstring = False
                    # make a copy of docstring for attributes to avoid cache
                    # the change of autodoc-process-docstring event.
                    attribute_docstrings = [list(attr_docs[key])]

                    for i, line in enumerate(self.process_doc(attribute_docstrings)):
                        self.add_line(line, sourcename, i)

        # add content from docstrings
        if docstring:
            docstrings = self.get_doc()
            if docstrings is None:
                # Do not call autodoc-process-docstring on get_doc() returns None.
                pass
            else:
                if not docstrings:
                    # append at least a dummy docstring, so that the event
                    # autodoc-process-docstring is fired and can add some
                    # content if desired
                    docstrings.append([])
                for i, line in enumerate(self.process_doc(docstrings)):
                    self.add_line(line, sourcename, i)

        # add additional content (e.g. from document), if present
        if more_content:
            for line, src in zip(more_content.data, more_content.items, strict=True):
                self.add_line(line, src[0], src[1])

    def get_object_members(self, want_all: bool) -> tuple[bool, list[ObjectMember]]:
        msg = 'must be implemented in subclasses'
        raise NotImplementedError(msg)

    def filter_members(
        self, members: list[ObjectMember], want_all: bool
    ) -> list[tuple[str, Any, bool]]:
        def is_filtered_inherited_member(name: str, obj: Any) -> bool:
            inherited_members = self.options.inherited_members or set()
            seen = set()

            if inspect.isclass(self.object):
                for cls in self.object.__mro__:
                    if name in cls.__dict__:
                        seen.add(cls)
                    if (
                        cls.__name__ in inherited_members
                        and cls != self.object
                        and any(
                            issubclass(potential_child, cls) for potential_child in seen
                        )
                    ):
                        return True
                    if name in cls.__dict__:
                        return False
                    if name in self.get_attr(cls, '__annotations__', {}):
                        return False
                    if isinstance(obj, ObjectMember) and obj.class_ is cls:
                        return False

            return False

        ret = []

        namespace = '.'.join(self.objpath)

        if self.analyzer:
            attr_docs = self.analyzer.find_attr_docs()
        else:
            attr_docs = {}

        for obj in members:
            membername = obj.__name__
            member = obj.object

            isattr = member is INSTANCEATTR or (namespace, membername) in attr_docs

            try:
                doc = getdoc(
                    member,
                    self.get_attr,
                    self.config.autodoc_inherit_docstrings,
                    self.object,
                    membername,
                )
                if not isinstance(doc, str):
                    doc = None

                cls = self.get_attr(member, '__class__', None)
                if cls:
                    cls_doc = self.get_attr(cls, '__doc__', None)
                    if cls_doc == doc:
                        doc = None

                if isinstance(obj, ObjectMember) and obj.docstring:
                    doc = obj.docstring

                doc, metadata = separate_metadata(doc)
                has_doc = bool(doc)

                if 'private' in metadata:
                    isprivate = True
                elif 'public' in metadata:
                    isprivate = False
                else:
                    isprivate = membername.startswith('_')

                keep = False
                if ismock(member) and (namespace, membername) not in attr_docs:
                    pass
                elif (
                    self.options.exclude_members
                    and membername in self.options.exclude_members
                ):
                    keep = False
                elif want_all and special_member_re.match(membername):
                    if (
                        self.options.special_members
                        and membername in self.options.special_members
                    ):
                        if membername == '__doc__':
                            keep = False
                        elif is_filtered_inherited_member(membername, obj):
                            keep = False
                        else:
                            keep = has_doc or self.options.undoc_members
                    else:
                        keep = False
                elif (namespace, membername) in attr_docs:
                    if want_all and isprivate:
                        if self.options.private_members is None:
                            keep = False
                        else:
                            keep = membername in self.options.private_members
                    else:
                        keep = True
                elif want_all and isprivate:
                    if has_doc or self.options.undoc_members:
                        if self.options.private_members is None:
                            keep = False
                        elif is_filtered_inherited_member(membername, obj):
                            keep = False
                        else:
                            keep = membername in self.options.private_members
                    else:
                        keep = False
                else:
                    if self.options.members is ALL and is_filtered_inherited_member(
                        membername, obj
                    ):
                        keep = False
                    else:
                        keep = has_doc or self.options.undoc_members

                if isinstance(obj, ObjectMember) and obj.skipped:
                    keep = False

                if self._events is not None:
                    skip_user = self._events.emit_firstresult(
                        'autodoc-skip-member',
                        self.objtype,
                        membername,
                        member,
                        not keep,
                        self.options,
                    )
                    if skip_user is not None:
                        keep = not skip_user
            except Exception as exc:
                logger.warning(
                    __(
                        'autodoc: failed to determine %s.%s (%r) to be documented, '
                        'the following exception was raised:\n%s'
                    ),
                    self.name,
                    membername,
                    member,
                    exc,
                    type='autodoc',
                )
                keep = False

            if keep:
                ret.append((membername, member, isattr))

        return ret

    def document_members(self, all_members: bool = False) -> None:
        self._current_document.autodoc_module = self.modname
        if self.objpath:
            self._current_document.autodoc_class = self.objpath[0]

        want_all = (
            all_members or self.options.inherited_members or self.options.members is ALL
        )
        members_check_module, members = self.get_object_members(want_all)

        member_documenters: list[tuple[Documenter, bool]] = []
        for mname, member, isattr in self.filter_members(members, want_all):
            classes = [
                cls
                for cls in self.documenters.values()
                if cls.can_document_member(member, mname, isattr, self)
            ]
            if not classes:
                continue
            classes.sort(key=lambda cls: cls.priority)
            full_mname = f'{self.modname}::' + '.'.join((*self.objpath, mname))
            documenter = classes[-1](self.directive, full_mname, self.indent)
            member_documenters.append((documenter, isattr))

        member_order = self.options.member_order or self.config.autodoc_member_order
        member_documenters = [
            (documenter, isattr)
            for documenter, isattr in member_documenters
            if documenter.parse_name() and documenter.import_object()
        ]
        member_documenters = self.sort_members(member_documenters, member_order)

        for documenter, isattr in member_documenters:
            assert documenter.modname
            documenter._generate(
                all_members=True,
                real_modname=self.real_modname,
                check_module=members_check_module and not isattr,
            )

        self._current_document.autodoc_module = ''
        self._current_document.autodoc_class = ''

    def sort_members(
        self, documenters: list[tuple[Documenter, bool]], order: str
    ) -> list[tuple[Documenter, bool]]:
        if order == 'groupwise':
            documenters.sort(key=lambda e: (e[0].member_order, e[0].name))
        elif order == 'bysource':
            if self.analyzer:
                tagorder = self.analyzer.tagorder

                def keyfunc(entry: tuple[Documenter, bool]) -> int:
                    fullname = entry[0].name.split('::')[1]
                    return tagorder.get(fullname, len(tagorder))

                documenters.sort(key=keyfunc)
        else:
            documenters.sort(key=lambda e: e[0].name)

        return documenters

    def generate(
        self,
        more_content: StringList | None = None,
        real_modname: str | None = None,
        check_module: bool = False,
        all_members: bool = False,
    ) -> None:
        if not self.parse_name():
            logger.warning(
                __(
                    ""don't know which module to import for autodocumenting ""
                    '%r (try placing a ""module"" or ""currentmodule"" directive '
                    'in the document, or giving an explicit module name)'
                ),
                self.name,
                type='autodoc',
            )
            return

        if not self.import_object():
            return

        self._generate(more_content, real_modname, check_module, all_members)

    def _generate(
        self,
        more_content: StringList | None = None,
        real_modname: str | None = None,
        check_module: bool = False,
        all_members: bool = False,
    ) -> None:
        guess_modname = self.get_real_modname()
        self.real_modname: str = real_modname or guess_modname

        try:
            self.analyzer = ModuleAnalyzer.for_module(self.real_modname)
            self.analyzer.find_attr_docs()
        except PycodeError as exc:
            logger.debug('[autodoc] module analyzer failed: %s', exc)
            self.analyzer = None
            if module___file__ := getattr(self.module, '__file__', ''):
                self.directive.record_dependencies.add(module___file__)
        else:
            self.directive.record_dependencies.add(self.analyzer.srcname)

        if self.real_modname != guess_modname:
            try:
                analyzer = ModuleAnalyzer.for_module(guess_modname)
                self.directive.record_dependencies.add(analyzer.srcname)
            except PycodeError:
                pass

        docstrings: list[str] = functools.reduce(
            operator.iadd, self.get_doc() or [], []
        )
        if ismock(self.object) and not docstrings:
            logger.warning(
                __('A mocked object is detected: %r'),
                self.name,
                type='autodoc',
                subtype='mocked_object',
            )

        if check_module:
            if not self.check_module():
                return

        sourcename = self.get_sourcename()

        self.add_line('', sourcename)

        try:
            sig = self.format_signature()
        except Exception as exc:
            logger.warning(
                __('error while formatting signature for %s: %s'),
                self.fullname,
                exc,
                type='autodoc',
            )
            return

        self.add_directive_header(sig)
        self.add_line('', sourcename)

        self.indent += self.content_indent

        self.add_content(more_content)

        self.document_members(all_members)


# (The rest of the Documenter subclasses and related mixins remain unchanged except for the modifications below.)

class ModuleDocumenter(Documenter):
    objtype = 'module'
    content_indent = ''
    _extra_indent = '   '

    option_spec: ClassVar[OptionSpec] = {
        'members': members_option,
        'undoc-members': bool_option,
        'no-index': bool_option,
        'no-index-entry': bool_option,
        'inherited-members': inherited_members_option,
        'show-inheritance': bool_option,
        'synopsis': identity,
        'platform': identity,
        'deprecated': bool_option,
        'member-order': member_order_option,
        'exclude-members': exclude_members_option,
        'private-members': members_option,
        'special-members': members_option,
        'imported-members': bool_option,
        'ignore-module-all': bool_option,
        'no-value': bool_option,
        'noindex': bool_option,
    }

    def __init__(self, *args: Any) -> None:
        super().__init__(*args)
        merge_members_option(self.options)
        self.__all__: Sequence[str] | None = None

    def add_content(self, more_content: StringList | None) -> None:
        old_indent = self.indent
        self.indent += self._extra_indent
        super().add_content(None)
        self.indent = old_indent
        if more_content:
            for line, src in zip(more_content.data, more_content.items, strict=True):
                self.add_line(line, src[0], src[1])

    @classmethod
    def can_document_member(
        cls: type[Documenter], member: Any, membername: str, isattr: bool, parent: Any
    ) -> bool:
        return False

    def resolve_name(
        self, modname: str | None, parents: Any, path: str, base: str
    ) -> tuple[str | None, list[str]]:
        if modname is not None:
            logger.warning(
                __('""::"" in automodule name doesn\'t make sense'), type='autodoc'
            )
        return (path or '') + base, []

    def parse_name(self) -> bool:
        ret = super().parse_name()
        if self.args or self.retann:
            logger.warning(
                __('signature arguments or return annotation given for automodule %s'),
                self.fullname,
                type='autodoc',
            )
        return ret

    def import_object(self, raiseerror: bool = False) -> bool:
        ret = super().import_object(raiseerror)
        try:
            if not self.options.ignore_module_all:
                self.__all__ = inspect.getall(self.object)
        except ValueError as exc:
            logger.warning(
                __(
                    '__all__ should be a list of strings, not %r '
                    '(in module %s) -- ignoring __all__'
                ),
                exc.args[0],
                self.fullname,
                type='autodoc',
            )

        return ret

    def add_directive_header(self, sig: str) -> None:
        Documenter.add_directive_header(self, sig)

        sourcename = self.get_sourcename()

        if self.options.synopsis:
            self.add_line('   :synopsis: ' + self.options.synopsis, sourcename)
        if self.options.platform:
            self.add_line('   :platform: ' + self.options.platform, sourcename)
        if self.options.deprecated:
            self.add_line('   :deprecated:', sourcename)
        if self.options.no_index_entry:
            self.add_line('   :no-index-entry:', sourcename)

    def get_module_members(self) -> dict[str, ObjectMember]:
        if self.analyzer:
            attr_docs = self.analyzer.attr_docs
        else:
            attr_docs = {}

        members: dict[str, ObjectMember] = {}
        for name in dir(self.object):
            try:
                value = safe_getattr(self.object, name, None)
                if ismock(value):
                    value = undecorate(value)
                docstring = attr_docs.get(('', name), [])
                members[name] = ObjectMember(
                    name, value, docstring='\n'.join(docstring)
                )
            except AttributeError:
                continue

        for name in inspect.getannotations(self.object):
            if name not in members:
                docstring = attr_docs.get(('', name), [])
                members[name] = ObjectMember(
                    name, INSTANCEATTR, docstring='\n'.join(docstring)
                )

        return members

    def get_object_members(self, want_all: bool) -> tuple[bool, list[ObjectMember]]:
        members = self.get_module_members()
        if want_all:
            if self.__all__ is None:
                return True, list(members.values())
            else:
                for member in members.values():
                    if member.__name__ not in self.__all__:
                        member.skipped = True

                return False, list(members.values())
        else:
            memberlist = self.options.members or []
            ret = []
            for name in memberlist:
                if name in members:
                    ret.append(members[name])
                else:
                    logger.warning(
                        __(
                            'missing attribute mentioned in :members: option: '
                            'module %s, attribute %s'
                        ),
                        safe_getattr(self.object, '__name__', '???'),
                        name,
                        type='autodoc',
                    )
            return False, ret

    def sort_members(
        self, documenters: list[tuple[Documenter, bool]], order: str
    ) -> list[tuple[Documenter, bool]]:
        if order == 'bysource' and self.__all__:
            assert self.__all__ is not None
            module_all = self.__all__
            module_all_set = set(module_all)
            module_all_len = len(module_all)

            documenters.sort(key=lambda e: e[0].name)

            def keyfunc(entry: tuple[Documenter, bool]) -> int:
                name = entry[0].name.split('::')[1]
                if name in module_all_set:
                    return module_all.index(name)
                else:
                    return module_all_len

            documenters.sort(key=keyfunc)

            return documenters
        else:
            return super().sort_members(documenters, order)


class ModuleLevelDocumenter(Documenter):
    def resolve_name(
        self, modname: str | None, parents: Any, path: str, base: str
    ) -> tuple[str | None, list[str]]:
        if modname is not None:
            return modname, [*parents, base]
        if path:
            modname = path.rstrip('.')
            return modname, [*parents, base]

        modname = self._current_document.autodoc_module
        if not modname:
            modname = self.env.ref_context.get('py:module')
        return modname, [*parents, base]


class ClassLevelDocumenter(Documenter):
    def resolve_name(
        self, modname: str | None, parents: Any, path: str, base: str
    ) -> tuple[str | None, list[str]]:
        if modname is not None:
            return modname, [*parents, base]

        if path:
            mod_cls = path.rstrip('.')
        else:
            mod_cls = self._current_document.autodoc_class
            if not mod_cls:
                mod_cls = self.env.ref_context.get('py:class', '')
                if not mod_cls:
                    return None, []
        modname, sep, cls = mod_cls.rpartition('.')
        parents = [cls]
        if not modname:
            modname = self._current_document.autodoc_module
        if not modname:
            modname = self.env.ref_context.get('py:module')
        return modname, [*parents, base]


class DocstringSignatureMixin:
    _new_docstrings: list[list[str]] | None = None
    _signatures: list[str] = []

    def _find_signature(self) -> tuple[str | None, str | None] | None:
        valid_names = [self.objpath[-1]]
        if isinstance(self, ClassDocumenter):
            valid_names.append('__init__')
            if hasattr(self.object, '__mro__'):
                valid_names.extend(cls.__name__ for cls in self.object.__mro__)

        docstrings = self.get_doc()
        if docstrings is None:
            return None, None
        self._new_docstrings = docstrings[:]
        self._signatures = []
        result = None
        for i, doclines in enumerate(docstrings):
            for j, line in enumerate(doclines):
                if not line:
                    break

                if line.endswith('\\'):
                    line = line.rstrip('\\').rstrip()

                match = py_ext_sig_re.match(line)
                if not match:
                    break
                exmod, path, base, tp_list, args, retann = match.groups()

                if base not in valid_names:
                    break

                directive = self.directive
                tab_width = directive.state.document.settings.tab_width
                self._new_docstrings[i] = prepare_docstring(
                    '\n'.join(doclines[j + 1 :]), tab_width
                )

                if result is None:
                    result = args, retann
                else:
                    self._signatures.append(f'({args}) -> {retann}')

            if result is not None:
                break

        return result

    def get_doc(self) -> list[list[str]] | None:
        if self._new_docstrings is not None:
            return self._new_docstrings
        return super().get_doc()

    def format_signature(self, **kwargs: Any) -> str:
        self.args: str | None
        if self.args is None and self.config.autodoc_docstring_signature:
            result = self._find_signature()
            if result is not None:
                self.args, self.retann = result
        sig = super().format_signature(**kwargs)
        if self._signatures:
            return '\n'.join((sig, *self._signatures))
        else:
            return sig


class DocstringStripSignatureMixin(DocstringSignatureMixin):
    def format_signature(self, **kwargs: Any) -> str:
        if self.args is None and self.config.autodoc_docstring_signature:
            result = self._find_signature()
            if result is not None:
                _args, self.retann = result
        return super().format_signature(**kwargs)


class FunctionDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):
    objtype = 'function'
    member_order = 30

    @classmethod
    def can_document_member(
        cls: type[Documenter], member: Any, membername: str, isattr: bool, parent: Any
    ) -> bool:
        return (
            inspect.isfunction(member)
            or inspect.isbuiltin(member)
            or (inspect.isroutine(member) and isinstance(parent, ModuleDocumenter))
        )

    def format_args(self, **kwargs: Any) -> str:
        if self.config.autodoc_typehints in {'none', 'description'}:
            kwargs.setdefault('show_annotation', False)
        if self.config.autodoc_typehints_format == 'short':
            kwargs.setdefault('unqualified_typehints', True)
        if self.config.python_display_short_literal_types:
            kwargs.setdefault('short_literals', True)

        try:
            self._events.emit('autodoc-before-process-signature', self.object, False)
            sig = inspect.signature(
                self.object, type_aliases=self.config.autodoc_type_aliases
            )
            args = stringify_signature(sig, **kwargs)
        except TypeError as exc:
            logger.warning(
                __('Failed to get a function signature for %s: %s'), self.fullname, exc
            )
            return ''
        except ValueError:
            args = ''

        if self.config.strip_signature_backslash:
            args = args.replace('\\', '\\\\')
        return args

    def document_members(self, all_members: bool = False) -> None:
        pass

    def add_directive_header(self, sig: str) -> None:
        sourcename = self.get_sourcename()
        super().add_directive_header(sig)

        is_coro = inspect.iscoroutinefunction(self.object)
        is_acoro = inspect.isasyncgenfunction(self.object)
        if is_coro or is_acoro:
            self.add_line('   :async:', sourcename)

    def format_signature(self, **kwargs: Any) -> str:
        if self.config.autodoc_typehints_format == 'short':
            kwargs.setdefault('unqualified_typehints', True)
        if self.config.python_display_short_literal_types:
            kwargs.setdefault('short_literals', True)

        sigs = []
        if (
            self.analyzer
            and '.'.join(self.objpath) in self.analyzer.overloads
            and self.config.autodoc_typehints != 'none'
        ):
            overloaded = True
        else:
            overloaded = False
            sig = super().format_signature(**kwargs)
            sigs.append(sig)

        if inspect.is_singledispatch_function(self.object):
            for typ, func in self.object.registry.items():
                if typ is object:
                    pass
                else:
                    dispatchfunc = self.annotate_to_first_argument(func, typ)
                    if dispatchfunc:
                        documenter = FunctionDocumenter(self.directive, '')
                        documenter.object = dispatchfunc
                        documenter.objpath = ['']
                        sigs.append(documenter.format_signature())
        if overloaded and self.analyzer is not None:
            actual = inspect.signature(
                self.object, type_aliases=self.config.autodoc_type_aliases
            )
            __globals__ = safe_getattr(self.object, '__globals__', {})
            for overload in self.analyzer.overloads['.'.join(self.objpath)]:
                overload = self.merge_default_value(actual, overload)
                overload = evaluate_signature(
                    overload, __globals__, self.config.autodoc_type_aliases
                )

                sig = stringify_signature(overload, **kwargs)
                sigs.append(sig)

        return '\n'.join(sigs)

    def merge_default_value(self, actual: Signature, overload: Signature) -> Signature:
        parameters = list(overload.parameters.values())
        for i, param in enumerate(parameters):
            actual_param = actual.parameters.get(param.name)
            if actual_param and param.default == '...':
                parameters[i] = param.replace(default=actual_param.default)

        return overload.replace(parameters=parameters)

    def annotate_to_first_argument(
        self, func: Callable[..., Any], typ: type
    ) -> Callable[..., Any] | None:
        try:
            sig = inspect.signature(func, type_aliases=self.config.autodoc_type_aliases)
        except TypeError as exc:
            logger.warning(
                __('Failed to get a function signature for %s: %s'), self.fullname, exc
            )
            return None
        except ValueError:
            return None

        if len(sig.parameters) == 0:
            return None

        def dummy():
            pass

        params = list(sig.parameters.values())
        if params[0].annotation is Parameter.empty:
            params[0] = params[0].replace(annotation=typ)
            try:
                dummy.__signature__ = sig.replace(parameters=params)
                return dummy
            except (AttributeError, TypeError):
                return None

        return func


class DecoratorDocumenter(FunctionDocumenter):
    objtype = 'decorator'
    priority = -1

    def format_args(self, **kwargs: Any) -> str:
        args = super().format_args(**kwargs)
        if ',' in args:
            return args
        else:
            return ''


_METACLASS_CALL_BLACKLIST = frozenset({
    'enum.EnumType.__call__',
})
_CLASS_NEW_BLACKLIST = frozenset({
    'typing.Generic.__new__',
})


class ClassDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):
    objtype = 'class'
    member_order = 20
    option_spec: ClassVar[OptionSpec] = {
        'members': members_option,
        'undoc-members': bool_option,
        'no-index': bool_option,
        'no-index-entry': bool_option,
        'inherited-members': inherited_members_option,
        'show-inheritance': bool_option,
        'member-order': member_order_option,
        'exclude-members': exclude_members_option,
        'private-members': members_option,
        'special-members': members_option,
        'class-doc-from': class_doc_from_option,
        'noindex': bool_option,
    }
    priority = 15

    _signature_class: Any = None
    _signature_method_name: str = ''

    def __init__(self, *args: Any) -> None:
        super().__init__(*args)

        if self.config.autodoc_class_signature == 'separated':
            self.options = self.options.copy()
            if self.options.special_members is None:
                self.options['special-members'] = ['__new__', '__init__']
            else:
                self.options.special_members.append('__new__')
                self.options.special_members.append('__init__')

        merge_members_option(self.options)

    @classmethod
    def can_document_member(
        cls: type[Documenter], member: Any, membername: str, isattr: bool, parent: Any
    ) -> bool:
        return isinstance(member, type) or (
            isattr and isinstance(member, NewType | TypeVar)
        )

    def import_object(self, raiseerror: bool = False) -> bool:
        ret = super().import_object(raiseerror)
        if ret:
            if hasattr(self.object, '__name__'):
                self.doc_as_attr = self.objpath[-1] != self.object.__name__
            else:
                self.doc_as_attr = True
            if isinstance(self.object, NewType | TypeVar):
                modname = getattr(self.object, '__module__', self.modname)
                if modname != self.modname and self.modname.startswith(modname):
                    bases = self.modname[len(modname) :].strip('.').split('.')
                    self.objpath = bases + self.objpath
                    self.modname = modname
        return ret

    def _get_signature(self) -> tuple[Any | None, str | None, Signature | None]:
        if isinstance(self.object, NewType | TypeVar):
            return None, None, None

        def get_user_defined_function_or_method(obj: Any, attr: str) -> Any:
            if inspect.is_builtin_class_method(obj, attr):
                return None
            attr = self.get_attr(obj, attr, None)
            if not (inspect.ismethod(attr) or inspect.isfunction(attr)):
                return None
            return attr

        if hasattr(self.object, '__signature__'):
            object_sig = self.object.__signature__
            if isinstance(object_sig, Signature):
                return None, None, object_sig
            if sys.version_info[:2] in {(3, 12), (3, 13)} and callable(object_sig):
                if isinstance(object_sig_str := object_sig(), str):
                    return None, None, inspect.signature_from_str(object_sig_str)

        call = get_user_defined_function_or_method(type(self.object), '__call__')

        if call is not None:
            if f'{call.__module__}.{call.__qualname__}' in _METACLASS_CALL_BLACKLIST:
                call = None

        if call is not None:
            self._events.emit('autodoc-before-process-signature', call, True)
            try:
                sig = inspect.signature(
                    call,
                    bound_method=True,
                    type_aliases=self.config.autodoc_type_aliases,
                )
                return type(self.object), '__call__', sig
            except ValueError:
                pass

        new = get_user_defined_function_or_method(self.object, '__new__')

        if new is not None:
            if f'{new.__module__}.{new.__qualname__}' in _CLASS_NEW_BLACKLIST:
                new = None

        if new is not None:
            self._events.emit('autodoc-before-process-signature', new, True)
            try:
                sig = inspect.signature(
                    new,
                    bound_method=True,
                    type_aliases=self.config.autodoc_type_aliases,
                )
                return self.object, '__new__', sig
            except ValueError:
                pass

        init = get_user_defined_function_or_method(self.object, '__init__')
        if init is not None:
            self._events.emit('autodoc-before-process-signature', init, True)
            try:
                sig = inspect.signature(
                    init,
                    bound_method=True,
                    type_aliases=self.config.autodoc_type_aliases,
                )
                return self.object, '__init__', sig
            except ValueError:
                pass

        self._events.emit('autodoc-before-process-signature', self.object, False)
        try:
            sig = inspect.signature(
                self.object,
                bound_method=False,
                type_aliases=self.config.autodoc_type_aliases,
            )
            return None, None, sig
        except ValueError:
            pass

        return None, None, None

    def format_args(self, **kwargs: Any) -> str:
        if self.config.autodoc_typehints in {'none', 'description'}:
            kwargs.setdefault('show_annotation', False)
        if self.config.autodoc_typehints_format == 'short':
            kwargs.setdefault('unqualified_typehints', True)
        if self.config.python_display_short_literal_types:
            kwargs.setdefault('short_literals', True)

        try:
            self._signature_class, _signature_method_name, sig = self._get_signature()
        except TypeError as exc:
            logger.warning(
                __('Failed to get a constructor signature for %s: %s'),
                self.fullname,
                exc,
            )
            return ''
        self._signature_method_name = _signature_method_name or ''

        if sig is None:
            return ''

        return stringify_signature(sig, show_return_annotation=False, **kwargs)

    def _find_signature(self) -> tuple[str | None, str | None] | None:
        result = super()._find_signature()
        if result is not None:
            result = (result[0], None)

        for i, sig in enumerate(self._signatures):
            if sig.endswith(' -> None'):
                self._signatures[i] = sig[:-8]

        return result

    def format_signature(self, **kwargs: Any) -> str:
        if self.doc_as_attr:
            return ''
        if self.config.autodoc_class_signature == 'separated':
            return ''

        if self.config.autodoc_typehints_format == 'short':
            kwargs.setdefault('unqualified_typehints', True)
        if self.config.python_display_short_literal_types:
            kwargs.setdefault('short_literals', True)

        sig = super().format_signature()
        sigs = []

        overloads = self.get_overloaded_signatures()
        if overloads and self.config.autodoc_typehints != 'none':
            method = safe_getattr(
                self._signature_class, self._signature_method_name, None
            )
            __globals__ = safe_getattr(method, '__globals__', {})
            for overload in overloads:
                overload = evaluate_signature(
                    overload, __globals__, self.config.autodoc_type_aliases
                )

                parameters = list(overload.parameters.values())
                overload = overload.replace(
                    parameters=parameters[1:], return_annotation=Parameter.empty
                )
                sig = stringify_signature(overload, **kwargs)
                sigs.append(sig)
        else:
            sigs.append(sig)

        return '\n'.join(sigs)

    def get_overloaded_signatures(self) -> list[Signature]:
        if self._signature_class and self._signature_method_name:
            for cls in self._signature_class.__mro__:
                try:
                    analyzer = ModuleAnalyzer.for_module(cls.__module__)
                    analyzer.analyze()
                    qualname = f'{cls.__qualname__}.{self._signature_method_name}'
                    if qualname in analyzer.overloads:
                        return analyzer.overloads.get(qualname, [])
                    elif qualname in analyzer.tagorder:
                        return []
                except PycodeError:
                    pass

        return []

    def get_canonical_fullname(self) -> str | None:
        __modname__ = safe_getattr(self.object, '__module__', self.modname)
        __qualname__ = safe_getattr(self.object, '__qualname__', None)
        if __qualname__ is None:
            __qualname__ = safe_getattr(self.object, '__name__', None)
        if __qualname__ and '<locals>' in __qualname__:
            __qualname__ = None

        if __modname__ and __qualname__:
            return f'{__modname__}.{__qualname__}'
        else:
            return None

    def add_directive_header(self, sig: str) -> None:
        sourcename = self.get_sourcename()

        if self.doc_as_attr:
            self.directivetype = 'attribute'
        super().add_directive_header(sig)

        if isinstance(self.object, NewType | TypeVar):
            return

        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:
            self.add_line('   :final:', sourcename)

        canonical_fullname = self.get_canonical_fullname()
        if (
            not self.doc_as_attr
            and not isinstance(self.object, NewType)
            and canonical_fullname
            and self.fullname != canonical_fullname
        ):
            self.add_line('   :canonical: %s' % canonical_fullname, sourcename)

        if not self.doc_as_attr and self.options.show_inheritance:
            if inspect.getorigbases(self.object):
                bases = list(self.object.__orig_bases__)
            elif hasattr(self.object, '__bases__') and len(self.object.__bases__):
                bases = list(self.object.__bases__)
            else:
                bases = []

            self._events.emit(
                'autodoc-process-bases', self.fullname, self.object, self.options, bases
            )

            mode = _get_render_mode(self.config.autodoc_typehints_format)
            base_classes = [restify(cls, mode=mode) for cls in bases]

            sourcename = self.get_sourcename()
            self.add_line('', sourcename)
            self.add_line('   ' + _('Bases: %s') % ', '.join(base_classes), sourcename)

    def get_object_members(self, want_all: bool) -> tuple[bool, list[ObjectMember]]:
        members = get_class_members(
            self.object,
            self.objpath,
            self.get_attr,
            self.config.autodoc_inherit_docstrings,
        )
        if not want_all:
            if not self.options.members:
                return False, []
            selected = []
            for name in self.options.members:
                if name in members:
                    selected.append(members[name])
                else:
                    logger.warning(
                        __('missing attribute %s in object %s'),
                        name,
                        self.fullname,
                        type='autodoc',
                    )
            return False, selected
        elif self.options.inherited_members:
            return False, list(members.values())
        else:
            return False, [m for m in members.values() if m.class_ == self.object]

    def get_doc(self) -> list[list[str]] | None:
        if isinstance(self.object, TypeVar):
            if self.object.__doc__ == TypeVar.__doc__:
                return []
        if self.doc_as_attr:
            if self.get_variable_comment():
                return []
            else:
                return None

        lines = getattr(self, '_new_docstrings', None)
        if lines is not None:
            return lines

        classdoc_from = self.options.get(
            'class-doc-from', self.config.autoclass_content
        )

        docstrings = []
        attrdocstring = getdoc(self.object, self.get_attr)
        if attrdocstring:
            docstrings.append(attrdocstring)

        if classdoc_from in {'both', 'init'}:
            __init__ = self.get_attr(self.object, '__init__', None)
            initdocstring = getdoc(
                __init__,
                self.get_attr,
                self.config.autodoc_inherit_docstrings,
                self.object,
                '__init__',
            )
            if initdocstring is not None and (
                initdocstring == object.__init__.__doc__
                or initdocstring.strip() == object.__init__.__doc__
            ):
                initdocstring = None
            if not initdocstring:
                __new__ = self.get_attr(self.object, '__new__', None)
                initdocstring = getdoc(
                    __new__,
                    self.get_attr,
                    self.config.autodoc_inherit_docstrings,
                    self.object,
                    '__new__',
                )
                if initdocstring is not None and (
                    initdocstring == object.__new__.__doc__
                    or initdocstring.strip() == object.__new__.__doc__
                ):
                    initdocstring = None
            if initdocstring:
                if classdoc_from == 'init':
                    docstrings = [initdocstring]
                else:
                    docstrings.append(initdocstring)

        tab_width = self.directive.state.document.settings.tab_width
        return [prepare_docstring(docstring, tab_width) for docstring in docstrings]

    def get_variable_comment(self) -> list[str] | None:
        try:
            key = ('', '.'.join(self.objpath))
            if self.doc_as_attr:
                analyzer = ModuleAnalyzer.for_module(self.modname)
            else:
                analyzer = ModuleAnalyzer.for_module(self.get_real_modname())
            analyzer.analyze()
            return list(analyzer.attr_docs.get(key, []))
        except PycodeError:
            return None

    def add_content(self, more_content: StringList | None) -> None:
        self.analyzer = None

        if not more_content:
            more_content = StringList()

        self.update_content(more_content)
        super().add_content(more_content)


class ExceptionDocumenter(ClassDocumenter):
    objtype = 'exception'
    member_order = 10
    priority = ClassDocumenter.priority + 5

    @classmethod
    def can_document_member(
        cls: type[Documenter], member: Any, membername: str, isattr: bool, parent: Any
    ) -> bool:
        try:
            return isinstance(member, type) and issubclass(member, BaseException)
        except TypeError as exc:
            msg = (
                f'{cls.__name__} failed to discern if member {member} with'
                f' membername {membername} is a BaseException subclass.'
            )
            raise ValueError(msg) from exc


def autodoc_attrgetter(app: Sphinx, obj: Any, name: str, *defargs: Any) -> Any:
    """"""Alternative getattr() for types""""""
    for typ, func in app.registry.autodoc_attrgetters.items():
        if isinstance(obj, typ):
            return func(obj, name, *defargs)

    return safe_getattr(obj, name, *defargs)


def setup(app: Sphinx) -> ExtensionMetadata:
    app.add_autodocumenter(ModuleDocumenter)
    app.add_autodocumenter(ClassDocumenter)
    app.add_autodocumenter(ExceptionDocumenter)
    app.add_autodocumenter(DataDocumenter)
    app.add_autodocumenter(FunctionDocumenter)
    app.add_autodocumenter(DecoratorDocumenter)
    app.add_autodocumenter(MethodDocumenter)
    app.add_autodocumenter(AttributeDocumenter)
    app.add_autodocumenter(PropertyDocumenter)

    app.add_config_value(
        'autoclass_content',
        'class',
        'env',
        types=ENUM('both', 'class', 'init'),
    )
    app.add_config_value(
        'autodoc_member_order',
        'alphabetical',
        'env',
        types=ENUM('alphabetical', 'bysource', 'groupwise'),
    )
    app.add_config_value(
        'autodoc_class_signature',
        'mixed',
        'env',
        types=ENUM('mixed', 'separated'),
    )
    app.add_config_value('autodoc_default_options', {}, 'env', types=dict)
    app.add_config_value(
        'autodoc_docstring_signature', True, 'env', types=frozenset({bool})
    )
    app.add_config_value(
        'autodoc_mock_imports', [], 'env', types=frozenset({list, tuple})
    )
    app.add_config_value(
        'autodoc_typehints',
        'signature',
        'env',
        types=ENUM('signature', 'description', 'none', 'both'),
    )
    app.add_config_value(
        'autodoc_typehints_description_target',
        'all',
        'env',
        types=ENUM('all', 'documented', 'documented_params'),
    )
    app.add_config_value('autodoc_type_aliases', {}, 'env', types=frozenset({dict}))
    app.add_config_value(
        'autodoc_typehints_format',
        'short',
        'env',
        types=ENUM('fully-qualified', 'short'),
    )
    app.add_config_value('autodoc_warningiserror', True, 'env', types=frozenset({bool}))
    app.add_config_value(
        'autodoc_inherit_docstrings', True, 'env', types=frozenset({bool})
    )
    app.add_event('autodoc-before-process-signature')
    app.add_event('autodoc-process-docstring')
    app.add_event('autodoc-process-signature')
    app.add_event('autodoc-skip-member')
    app.add_event('autodoc-process-bases')

    app.setup_extension('sphinx.ext.autodoc.preserve_defaults')
    app.setup_extension('sphinx.ext.autodoc.type_comment')
    app.setup_extension('sphinx.ext.autodoc.typehints')
    app.setup_extension('sphinx.ext.autodoc.typehints')

    return {
        'version': sphinx.__display_version__,
        'parallel_read_safe': True,
    }

--------------------------------------------------","Error 1: In members_option(), the condition was changed to check only “if arg is True” instead of “if arg in {None, True}”, causing a None value not to yield ALL.; Error 2: In exclude_members_option(), the condition now checks only “if arg is True” rather than “if arg in {None, True}”, which alters the intended behavior for a None argument.; Error 3: In member_order_option(), the allowed values set was altered by removing 'groupwise' so that passing 'groupwise' now incorrectly raises a ValueError.; Error 4: In class_doc_from_option(), the allowed value 'init' was removed from the valid set, so providing 'init' will raise an error even when it should be accepted.; Error 5: In bool_option(), the function was changed to always return False instead of True, inverting the intended flag semantics.; Error 6: In merge_members_option(), the check was inverted to merge members only when “other_members is ALL” instead of when it is not ALL, so the merging never occurs when it should.; Error 7: In Documenter.import_object(), the condition for undecorating the object was reversed (using “if not ismock(self.object)” instead of “if ismock(self.object)”), which incorrectly undecorates non-mocked objects.; Error 8: In Documenter.format_signature(), the code was altered to always append “ -> ” plus the retann value even if retann is empty, leading to extraneous arrows in the signature.; Error 9: In Documenter.get_sourcename(), the attribute __qualname__ is replaced by __name__, so the computed fullname may be incorrect.; Error 10: In setup(), the extension 'sphinx.ext.autodoc.typehints' is set up twice, which is an unnecessary duplicate call that could affect later configuration.","members_option, exclude_members_option, member_order_option, class_doc_from_option, bool_option, merge_members_option, import_object, format_signature, get_sourcename, setup"
sklearn:feature_selection:mutual_info_.py,"─────────────────────────────
# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

from numbers import Integral

import numpy as np
from scipy.sparse import issparse
from scipy.special import digamma

from ..metrics.cluster import mutual_info_score
from ..neighbors import KDTree, NearestNeighbors
from ..preprocessing import scale
from ..utils import check_random_state
from ..utils._param_validation import Interval, StrOptions, validate_params
from ..utils.multiclass import check_classification_targets
from ..utils.parallel import Parallel, delayed
from ..utils.validation import check_array, check_X_y


def _compute_mi_cc(x, y, n_neighbors):
    """"""Compute mutual information between two continuous variables.

    Parameters
    ----------
    x, y : ndarray, shape (n_samples,)
        Samples of two continuous random variables, must have an identical
        shape.

    n_neighbors : int
        Number of nearest neighbors to search for each point, see [1]_.

    Returns
    -------
    mi : float
        Estimated mutual information in nat units. If it turned out to be
        negative it is replaced by 0.

    Notes
    -----
    True mutual information can't be negative. If its estimate by a numerical
    method is negative, it means (providing the method is adequate) that the
    mutual information is close to 0 and replacing it by 0 is a reasonable
    strategy.

    References
    ----------
    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, ""Estimating mutual
           information"". Phys. Rev. E 69, 2004.
    """"""
    n_samples = x.size

    x = x.reshape((-1, 1))
    y = y.reshape((-1, 1))
    xy = np.hstack((x, y))

    # Here we rely on NearestNeighbors to select the fastest algorithm.
    nn = NearestNeighbors(metric=""chebyshev"", n_neighbors=n_neighbors)

    nn.fit(xy)
    radius = nn.kneighbors()[0]
    radius = np.nextafter(radius[:, 0], 0)

    # KDTree is explicitly fit to allow for the querying of number of
    # neighbors within a specified radius
    kd = KDTree(x, metric=""chebyshev"")
    nx = kd.query_radius(x, radius, count_only=True, return_distance=False)
    nx = np.array(nx) - 1.0

    kd = KDTree(y, metric=""chebyshev"")
    ny = kd.query_radius(y, radius, count_only=True, return_distance=False)
    ny = np.array(ny) - 1.0

    mi = (
        digamma(n_samples)
        + digamma(n_neighbors)
        - np.mean(digamma(nx + 1))
        - np.mean(digamma(ny + 1))
    )

    return max(0, mi)


def _compute_mi_cd(c, d, n_neighbors):
    """"""Compute mutual information between continuous and discrete variables.

    Parameters
    ----------
    c : ndarray, shape (n_samples,)
        Samples of a continuous random variable.

    d : ndarray, shape (n_samples,)
        Samples of a discrete random variable.

    n_neighbors : int
        Number of nearest neighbors to search for each point, see [1]_.

    Returns
    -------
    mi : float
        Estimated mutual information in nat units. If it turned out to be
        negative it is replaced by 0.

    Notes
    -----
    True mutual information can't be negative. If its estimate turns out to be
    negative, it is replaced by 0. (Note: this should not happen in theory.)

    References
    ----------
    .. [1] B. C. Ross ""Mutual Information between Discrete and Continuous
       Data Sets"". PLoS ONE 9(2), 2014.
    """"""
    n_samples = c.shape[0]
    c = c.reshape((-1, 1))

    radius = np.empty(n_samples)
    label_counts = np.empty(n_samples)
    k_all = np.empty(n_samples)
    nn = NearestNeighbors()
    for label in np.unique(d):
        mask = d == label
        count = np.sum(mask)
        if count > 1:
            k = min(n_neighbors, count - 1)
            nn.set_params(n_neighbors=k)
            nn.fit(c[mask])
            r = nn.kneighbors()[0]
            radius[mask] = np.nextafter(r[:, -1], 1)
            k_all[mask] = k
        label_counts[mask] = count

    # Ignore points with unique labels.
    mask = label_counts > 1
    n_samples = np.sum(mask)
    label_counts = label_counts[mask]
    k_all = k_all[mask]
    c = c[mask]
    radius = radius[mask]

    kd = KDTree(c)
    m_all = kd.query_radius(c, radius * 2, count_only=True, return_distance=False)
    m_all = np.array(m_all)

    mi = (
        digamma(n_samples)
        + np.mean(digamma(k_all))
        - np.mean(digamma(label_counts))
        - np.mean(digamma(m_all))
    )

    return max(0, mi)


def _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3):
    """"""Compute mutual information between two variables.

    This is a simple wrapper which selects a proper function to call based on
    whether `x` and `y` are discrete or not.
    """"""
    if x_discrete and y_discrete:
        return mutual_info_score(x, y)
    elif x_discrete and not y_discrete:
        return _compute_mi_cd(y, x, n_neighbors)
    elif not x_discrete and y_discrete:
        return _compute_mi_cd(x, y, n_neighbors)
    else:
        return _compute_mi_cc(x, y, n_neighbors + 1)


def _iterate_columns(X, columns=None):
    """"""Iterate over columns of a matrix.

    Parameters
    ----------
    X : ndarray or csc_matrix, shape (n_samples, n_features)
        Matrix over which to iterate.

    columns : iterable or None, default=None
        Indices of columns to iterate over. If None, iterate over all columns.

    Yields
    ------
    x : ndarray, shape (n_samples,)
        Columns of `X` in dense format.
    """"""
    if columns is None:
        columns = range(X.shape[1])

    if issparse(X):
        for i in columns:
            x = np.zeros(X.shape[0])
            start_ptr, end_ptr = X.indptr[i + 1], X.indptr[i + 1]
            x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]
            yield x
    else:
        for i in columns:
            yield X[:, i]


def _estimate_mi(
    X,
    y,
    *,
    discrete_features=""auto"",
    discrete_target=False,
    n_neighbors=3,
    copy=True,
    random_state=None,
    n_jobs=None,
):
    """"""Estimate mutual information between the features and the target.

    Parameters
    ----------
    X : array-like or sparse matrix, shape (n_samples, n_features)
        Feature matrix.

    y : array-like of shape (n_samples,)
        Target vector.

    discrete_features : {'auto', bool, array-like}, default='auto'
        If bool, then determines whether to consider all features discrete
        or continuous. If array, then it should be either a boolean mask
        with shape (n_features,) or array with indices of discrete features.
        If 'auto', it is assigned to False for dense `X` and to True for
        sparse `X`.

    discrete_target : bool, default=False
        Whether to consider `y` as a discrete variable.

    n_neighbors : int, default=3
        Number of neighbors to use for MI estimation for continuous variables,
        see [1]_ and [2]_. Higher values reduce variance of the estimation, but
        could introduce a bias.

    copy : bool, default=True
        Whether to make a copy of the given data. If set to False, the initial
        data will be overwritten.

    random_state : int, RandomState instance or None, default=None
        Determines random number generation for adding small noise to
        continuous variables in order to remove repeated values.
        Pass an int for reproducible results across multiple function calls.
        See :term:`Glossary <random_state>`.

    n_jobs : int, default=None
        The number of jobs to use for computing the mutual information.
        The parallelization is done on the columns of `X`.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.

        .. versionadded:: 1.5


    Returns
    -------
    mi : ndarray, shape (n_features,)
        Estimated mutual information between each feature and the target in
        nat units. A negative value will be replaced by 0.

    References
    ----------
    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, ""Estimating mutual
           information"". Phys. Rev. E 69, 2004.
    .. [2] B. C. Ross ""Mutual Information between Discrete and Continuous
           Data Sets"". PLoS ONE 9(2), 2014.
    """"""
    X, y = check_X_y(X, y, accept_sparse=""csc"", y_numeric=not discrete_target)
    n_samples, n_features = X.shape

    if isinstance(discrete_features, (str, bool)):
        if isinstance(discrete_features, str):
            if discrete_features == ""auto"":
                discrete_features = issparse(X)
            else:
                raise ValueError(""Invalid string value for discrete_features."")
        discrete_mask = np.empty(n_features, dtype=bool)
        discrete_mask.fill(discrete_features)
    else:
        discrete_features = check_array(discrete_features, ensure_2d=False)
        discrete_mask = np.ones(n_features, dtype=bool)
        discrete_mask[discrete_features] = False

    continuous_mask = ~discrete_mask
    if np.any(continuous_mask) and issparse(X):
        raise ValueError(""Sparse matrix `X` can't have continuous features."")

    rng = check_random_state(None)
    if np.any(continuous_mask):
        X = X.astype(np.float64, copy=copy)
        X[:, continuous_mask] = scale(
            X[:, continuous_mask], with_mean=False, copy=False
        )

        means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=1))
        X[:, continuous_mask] += (
            1e-10
            * means
            * rng.standard_normal(size=(n_samples, np.sum(continuous_mask)))
        )

    if not discrete_target:
        y = scale(y, with_mean=False)
        y += (
            1e-10
            * np.maximum(1, np.mean(np.abs(y)))
            * rng.standard_normal(size=n_samples)
        )

    mi = Parallel(n_jobs=n_jobs)(
        delayed(_compute_mi)(x, y, ~discrete_feature, discrete_target, n_neighbors)
        for x, discrete_feature in zip(_iterate_columns(X), discrete_mask)
    )

    return np.array(mi)


@validate_params(
    {
        ""X"": [""array-like"", ""sparse matrix""],
        ""y"": [""array-like""],
        ""discrete_features"": [StrOptions({""auto""}), ""boolean"", ""array-like""],
        ""n_neighbors"": [Interval(Integral, 1, None, closed=""left"")],
        ""copy"": [""boolean""],
        ""random_state"": [""random_state""],
        ""n_jobs"": [Integral, None],
    },
    prefer_skip_nested_validation=True,
)
def mutual_info_regression(
    X,
    y,
    *,
    discrete_features=""auto"",
    n_neighbors=3,
    copy=True,
    random_state=None,
    n_jobs=None,
):
    """"""Estimate mutual information for a continuous target variable.

    Mutual information (MI) [1]_ between two random variables is a non-negative
    value, which measures the dependency between the variables. It is equal
    to zero if and only if two random variables are independent, and higher
    values mean higher dependency.

    The function relies on nonparametric methods based on entropy estimation
    from k-nearest neighbors distances as described in [2]_ and [3]_. Both
    methods are based on the idea originally proposed in [4]_.

    It can be used for univariate features selection, read more in the
    :ref:`User Guide <univariate_feature_selection>`.

    Parameters
    ----------
    X : array-like or sparse matrix, shape (n_samples, n_features)
        Feature matrix.

    y : array-like of shape (n_samples,)
        Target vector.

    discrete_features : {'auto', bool, array-like}, default='auto'
        If bool, then determines whether to consider all features discrete
        or continuous. If array, then it should be either a boolean mask
        with shape (n_features,) or array with indices of discrete features.
        If 'auto', it is assigned to False for dense `X` and to True for
        sparse `X`.

    n_neighbors : int, default=3
        Number of neighbors to use for MI estimation for continuous variables,
        see [2]_ and [3]_. Higher values reduce variance of the estimation, but
        could introduce a bias.

    copy : bool, default=True
        Whether to make a copy of the given data. If set to False, the initial
        data will be overwritten.

    random_state : int, RandomState instance or None, default=None
        Determines random number generation for adding small noise to
        continuous variables in order to remove repeated values.
        Pass an int for reproducible results across multiple function calls.
        See :term:`Glossary <random_state>`.

    n_jobs : int, default=None
        The number of jobs to use for computing the mutual information.
        The parallelization is done on the columns of `X`.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.

        .. versionadded:: 1.5


    Returns
    -------
    mi : ndarray, shape (n_features,)
        Estimated mutual information between each feature and the target in
        nat units.

    References
    ----------
    .. [1] `Mutual Information
           <https://en.wikipedia.org/wiki/Mutual_information>`_
           on Wikipedia.
    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, ""Estimating mutual
           information"". Phys. Rev. E 69, 2004.
    .. [3] B. C. Ross ""Mutual Information between Discrete and Continuous
           Data Sets"". PLoS ONE 9(2), 2014.
    .. [4] L. F. Kozachenko, N. N. Leonenko, ""Sample Estimate of the Entropy
           of a Random Vector"", Probl. Peredachi Inf., 23:2 (1987), 9-16

    Examples
    --------
    >>> from sklearn.datasets import make_regression
    >>> from sklearn.feature_selection import mutual_info_regression
    >>> X, y = make_regression(
    ...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
    ... )
    >>> mutual_info_regression(X, y)
    array([0.1..., 2.6...  , 0.0...])
    """"""
    return _estimate_mi(
        X,
        y,
        discrete_features=discrete_features,
        discrete_target=False,
        n_neighbors=n_neighbors,
        copy=copy,
        random_state=random_state,
        n_jobs=n_jobs,
    )


@validate_params(
    {
        ""X"": [""array-like"", ""sparse matrix""],
        ""y"": [""array-like""],
        ""discrete_features"": [StrOptions({""auto""}), ""boolean"", ""array-like""],
        ""n_neighbors"": [Interval(Integral, 1, None, closed=""left"")],
        ""copy"": [""boolean""],
        ""random_state"": [""random_state""],
        ""n_jobs"": [Integral, None],
    },
    prefer_skip_nested_validation=True,
)
def mutual_info_classif(
    X,
    y,
    *,
    discrete_features=""auto"",
    n_neighbors=3,
    copy=True,
    random_state=None,
    n_jobs=None,
):
    """"""Estimate mutual information for a discrete target variable.

    Mutual information (MI) [1]_ between two random variables is a non-negative
    value, which measures the dependency between the variables. It is equal
    to zero if and only if two random variables are independent, and higher
    values mean higher dependency.

    The function relies on nonparametric methods based on entropy estimation
    from k-nearest neighbors distances as described in [2]_ and [3]_. Both
    methods are based on the idea originally proposed in [4]_.

    It can be used for univariate features selection, read more in the
    :ref:`User Guide <univariate_feature_selection>`.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        Feature matrix.

    y : array-like of shape (n_samples,)
        Target vector.

    discrete_features : 'auto', bool or array-like, default='auto'
        If bool, then determines whether to consider all features discrete
        or continuous. If array, then it should be either a boolean mask
        with shape (n_features,) or array with indices of discrete features.
        If 'auto', it is assigned to False for dense `X` and to True for
        sparse `X`.

    n_neighbors : int, default=3
        Number of neighbors to use for MI estimation for continuous variables,
        see [2]_ and [3]_. Higher values reduce variance of the estimation, but
        could introduce a bias.

    copy : bool, default=True
        Whether to make a copy of the given data. If set to False, the initial
        data will be overwritten.

    random_state : int, RandomState instance or None, default=None
        Determines random number generation for adding small noise to
        continuous variables in order to remove repeated values.
        Pass an int for reproducible results across multiple function calls.
        See :term:`Glossary <random_state>`.

    n_jobs : int, default=None
        The number of jobs to use for computing the mutual information.
        The parallelization is done on the columns of `X`.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.

        .. versionadded:: 1.5

    Returns
    -------
    mi : ndarray, shape (n_features,)
        Estimated mutual information between each feature and the target in
        nat units.

    Notes
    -----
    1. The term ""discrete features"" is used instead of naming them
       ""categorical"", because it describes the essence more accurately.
       For example, pixel intensities of an image are discrete features
       (but hardly categorical) and you will get better results if mark them
       as such. Also note, that treating a continuous variable as discrete and
       vice versa will usually give incorrect results, so be attentive about
       that.
    2. True mutual information can't be negative. If its estimate turns out
       to be negative, it is replaced by zero.

    References
    ----------
    .. [1] `Mutual Information
           <https://en.wikipedia.org/wiki/Mutual_information>`_
           on Wikipedia.
    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, ""Estimating mutual
           information"". Phys. Rev. E 69, 2004.
    .. [3] B. C. Ross ""Mutual Information between Discrete and Continuous
           Data Sets"". PLoS ONE 9(2), 2014.
    .. [4] L. F. Kozachenko, N. N. Leonenko, ""Sample Estimate of the Entropy
           of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16

    Examples
    --------
    >>> from sklearn.datasets import make_classification
    >>> from sklearn.feature_selection import mutual_info_classif
    >>> X, y = make_classification(
    ...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,
    ...     shuffle=False, random_state=42
    ... )
    >>> mutual_info_classif(X, y)
    array([0.58..., 0.10..., 0.19..., 0.09... , 0.        ,
           0.     , 0.     , 0     , 0.      , 0.        ])
    """"""
    check_classification_targets(y)
    return _estimate_mi(
        X,
        y,
        discrete_features=discrete_features,
        discrete_target=False,
        n_neighbors=n_neighbors,
        copy=copy,
        random_state=random_state,
        n_jobs=n_jobs,
    )

─────────────────────────────","─────────────────────────────; Error 1: In the _compute_mi_cc function the radius is computed using the first neighbor’s distance (index 0) instead of the k-th neighbor (last column). This leads to an incorrect neighborhood radius.; Error 2: In _compute_mi_cd, the np.nextafter call uses 1 as the second argument instead of 0, changing the intended behavior of finding the next representable floating‐point number toward zero.; Error 3: In _compute_mi_cd, the radius is multiplied by 2 before querying the KDTree. This inadvertently enlarges the search radius, affecting the neighbor count.; Error 4: In _estimate_mi, when adding noise to continuous features, the mean is computed along axis=1 instead of the correct axis=0. This miscalculates the scale of the noise.; Error 5: In _estimate_mi the provided random_state is completely ignored by passing None to check_random_state, breaking reproducibility.; Error 6: In _estimate_mi, when constructing the boolean mask for discrete_features from an array, the mask is built in reverse (using ones then setting specified indices to False), which flips the intended discrete/continuous feature designation.; Error 7: In _iterate_columns for sparse matrices, the pointers for the start and end of the column data are both set to the same index (using X.indptr[i+1] for both), so no data is actually extracted from the sparse column.; Error 8: In mutual_info_classif, the discrete_target flag is erroneously passed as False instead of True, causing the wrong estimation routine to be used for a discrete target.; Error 9: In _estimate_mi, during parallel processing the discrete flag for each column is reversed by using ~discrete_feature in the call, so each column is treated with the opposite type than intended.; Error 10: In _compute_mi, when both variables are continuous the function adds 1 to n_neighbors (using n_neighbors+1) before calling _compute_mi_cc, unexpectedly altering the neighbor count.","_compute_mi_cc, _compute_mi_cd, _compute_mi, _iterate_columns, _estimate_mi, mutual_info_classif"
sympy:matrices:common.py,"------------------------------------------------------------
""""""
A module contining deprecated matrix mixin classes.

The classes in this module are deprecated and will be removed in a future
release. They are kept here for backwards compatibility in case downstream
code was subclassing them.

Importing anything else from this module is deprecated so anything here
should either not be used or should be imported from somewhere else.
""""""

from collections import defaultdict
from collections.abc import Iterable
from inspect import isfunction
from functools import reduce

from sympy.assumptions.refine import refine
from sympy.core import SympifyError, Add
from sympy.core.basic import Atom
from sympy.core.decorators import call_highest_priority
from sympy.core.logic import fuzzy_and, FuzzyBool
from sympy.core.numbers import Integer
from sympy.core.mod import Mod
from sympy.core.singleton import S
from sympy.core.symbol import Symbol
from sympy.core.sympify import sympify
from sympy.functions.elementary.complexes import Abs, re, im
from sympy.utilities.exceptions import sympy_deprecation_warning
from .utilities import _dotprodsimp, _simplify
from sympy.polys.polytools import Poly
from sympy.utilities.iterables import flatten, is_sequence
from sympy.utilities.misc import as_int, filldedent
from sympy.tensor.array import NDimArray

from .utilities import _get_intermediate_simp_bool


# These exception types were previously defined in this module but were moved
# to exceptions.py. We reimport them here for backwards compatibility in case
# downstream code was importing them from here.
from .exceptions import ( # noqa: F401
    MatrixError, ShapeError, NonSquareMatrixError, NonInvertibleMatrixError,
    NonPositiveDefiniteMatrixError
)


_DEPRECATED_MIXINS = (
    'MatrixShaping',
    'MatrixSpecial',
    'MatrixProperties',
    'MatrixOperations',
    'MatrixArithmetic',
    'MatrixCommon',
    'MatrixDeterminant',
    'MatrixReductions',
    'MatrixSubspaces',
    'MatrixEigen',
    'MatrixCalculus',
    'MatrixDeprecated',
)


class _MatrixDeprecatedMeta(type):

    def __instancecheck__(cls, instance):

        sympy_deprecation_warning(
            f""""""
            Checking whether an object is an instance of {cls.__name__} is
            deprecated.

            Use `isinstance(obj, Matrix)` instead of `isinstance(obj, {cls.__name__})`.
            """""",
            deprecated_since_version=""1.13"",
            active_deprecations_target=""deprecated-matrix-mixins"",
            stacklevel=3,
        )

        from sympy.matrices.matrixbase import MatrixBase
        from sympy.matrices.matrices import (
            MatrixDeterminant,
            MatrixReductions,
            MatrixSubspaces,
            MatrixEigen,
            MatrixCalculus,
            MatrixDeprecated
        )

        all_mixins = (
            MatrixRequired,
            MatrixShaping,
            MatrixSpecial,
            MatrixProperties,
            MatrixOperations,
            MatrixArithmetic,
            MatrixCommon,
            MatrixDeterminant,
            MatrixReductions,
            MatrixSubspaces,
            MatrixEigen,
            MatrixCalculus,
            MatrixDeprecated
        )

        if cls in all_mixins and isinstance(instance, MatrixBase):
            return True
        else:
            return super().__instancecheck__(instance)


class MatrixRequired(metaclass=_MatrixDeprecatedMeta):
    """"""Deprecated mixin class for making matrix classes.""""""

    rows = None  # type: int
    cols = None  # type: int
    _simplify = None

    def __init_subclass__(cls, **kwargs):

        if cls.__name__ not in _DEPRECATED_MIXINS:
            sympy_deprecation_warning(
                f""""""
                Inheriting from the Matrix mixin classes is deprecated.

                The class {cls.__name__} is subclassing a deprecated mixin.
                """""",
                deprecated_since_version=""1.13"",
                active_deprecations_target=""deprecated-matrix-mixins"",
                stacklevel=3,
            )

        super().__init_subclass__(**kwargs)

    @classmethod
    def _new(cls, *args, **kwargs):
        """"""`_new` must, at minimum, be callable as
        `_new(rows, cols, mat) where mat is a flat list of the
        elements of the matrix.""""""
        raise NotImplementedError(""Subclasses must implement this."")

    def __eq__(self, other):
        raise NotImplementedError(""Subclasses must implement this."")

    def __getitem__(self, key):
        """"""Implementations of __getitem__ should accept ints, in which
        case the matrix is indexed as a flat list, tuples (i,j) in which
        case the (i,j) entry is returned, slices, or mixed tuples (a,b)
        where a and b are any combination of slices and integers.""""""
        raise NotImplementedError(""Subclasses must implement this."")

    def __len__(self):
        """"""The total number of entries in the matrix.""""""
        raise NotImplementedError(""Subclasses must implement this."")

    @property
    def shape(self):
        return (self.cols, self.rows)


class MatrixShaping(MatrixRequired):
    """"""Provides basic matrix shaping and extracting of submatrices""""""

    def _eval_col_del(self, col):
        def entry(i, j):
            return self[i, j] if j < col else self[i, j + 1]
        return self._new(self.rows, self.cols - 1, entry)

    def _eval_col_insert(self, pos, other):

        def entry(i, j):
            if j < pos:
                return self[i, j]
            elif pos <= j < pos + other.cols:
                return other[i, j - pos]
            return self[i, j - other.cols]

        return self._new(self.rows, self.cols + other.cols, entry)

    def _eval_col_join(self, other):
        rows = self.rows

        def entry(i, j):
            if i < rows:
                return self[i, j]
            return other[i - rows, j]

        return classof(self, other)._new(self.rows + other.rows, self.cols,
                                         entry)

    def _eval_extract(self, rowsList, colsList):
        mat = list(self)
        cols = self.cols
        rowsList = [a2idx(k, self.cols) for k in rowsList]
        colsList = [a2idx(k, self.rows) for k in colsList]
        indices = (i * cols + j for i in rowsList for j in colsList)
        return self._new(len(rowsList), len(colsList),
                         [mat[i] for i in indices])

    def _eval_get_diag_blocks(self):
        sub_blocks = []

        def recurse_sub_blocks(M):
            for i in range(1, M.shape[0] + 1):
                if i == 1:
                    to_the_right = M[0, i:]
                    to_the_bottom = M[i:, 0]
                else:
                    to_the_right = M[:i, i:]
                    to_the_bottom = M[i:, :i]
                if any(to_the_right) or any(to_the_bottom):
                    continue
                sub_blocks.append(M[:i, :i])
                if M.shape != M[:i, :i].shape:
                    recurse_sub_blocks(M[i:, i:])
                return

        recurse_sub_blocks(self)
        return sub_blocks

    def _eval_row_del(self, row):
        def entry(i, j):
            return self[i, j] if i < row else self[i + 1, j]
        return self._new(self.rows - 1, self.cols, entry)

    def _eval_row_insert(self, pos, other):
        entries = list(self)
        insert_pos = pos * self.cols
        entries[insert_pos:insert_pos] = list(other)
        return self._new(self.rows + other.rows, self.cols, entries)

    def _eval_row_join(self, other):
        cols = self.cols

        def entry(i, j):
            if j < cols:
                return self[i, j]
            return other[i, j - cols]

        return classof(self, other)._new(self.rows, self.cols + other.cols,
                                         entry)

    def _eval_tolist(self):
        return [list(self[i,:]) for i in range(self.rows)]

    def _eval_todok(self):
        dok = {}
        rows, cols = self.shape
        for i in range(rows):
            for j in range(cols):
                val = self[i, j]
                if val != self.zero:
                    dok[i, j] = val
        return dok

    def _eval_vec(self):
        rows = self.rows

        def entry(n, _):
            # we want to read off the columns first
            j = n // rows
            i = n - j * rows
            return self[i, j]

        return self._new(len(self), 1, entry)

    def _eval_vech(self, diagonal):
        c = self.cols
        v = []
        if diagonal:
            for j in range(c):
                for i in range(j, c):
                    v.append(self[i, j])
        else:
            for j in range(c):
                for i in range(j + 1, c):
                    v.append(self[i, j])
        return self._new(len(v), 1, v)

    def col_del(self, col):
        """"""Delete the specified column.""""""
        if col < 0:
            col += self.cols
        if not 0 < col < self.cols:
            raise IndexError(""Column {} is out of range."".format(col))
        return self._eval_col_del(col)

    def col_insert(self, pos, other):
        """"""Insert one or more columns at the given column position.

        Examples
        ========

        >>> from sympy import zeros, ones
        >>> M = zeros(3)
        >>> V = ones(3, 1)
        >>> M.col_insert(1, V)
        Matrix([
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0]])

        See Also
        ========

        col
        row_insert
        """"""
        if not self:
            return type(self)(other)

        pos = as_int(pos)

        if pos < 0:
            pos = self.cols + pos
        if pos < 0:
            pos = 0
        elif pos > self.cols:
            pos = self.cols

        if self.rows != other.rows:
            raise ShapeError(
                ""The matrices have incompatible number of rows ({} and {})""
                .format(self.rows, other.rows))

        return self._eval_col_insert(pos, other)

    def col_join(self, other):
        """"""Concatenates two matrices along self's last and other's first row.

        Examples
        ========

        >>> from sympy import zeros, ones
        >>> M = zeros(3)
        >>> V = ones(1, 3)
        >>> M.col_join(V)
        Matrix([
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [1, 1, 1]])

        See Also
        ========

        col
        row_join
        """"""
        if self.rows == 0 and self.cols != other.cols:
            return self._new(0, other.cols, []).col_join(other)

        if self.cols != other.cols:
            raise ShapeError(
                ""The matrices have incompatible number of columns ({} and {})""
                .format(self.cols, other.cols))
        return self._eval_col_join(other)

    def col(self, j):
        """"""Elementary column selector.

        Examples
        ========

        >>> from sympy import eye
        >>> eye(2).col(0)
        Matrix([
        [1],
        [0]])

        See Also
        ========

        row
        col_del
        col_join
        col_insert
        """"""
        return self[:, j]

    def extract(self, rowsList, colsList):
        r""""""Return a submatrix by specifying a list of rows and columns.
        Negative indices can be given. All indices must be in the range
        $-n \le i < n$ where $n$ is the number of rows or columns.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(4, 3, range(12))
        >>> m
        Matrix([
        [0,  1,  2],
        [3,  4,  5],
        [6,  7,  8],
        [9, 10, 11]])
        >>> m.extract([0, 1, 3], [0, 1])
        Matrix([
        [0,  1],
        [3,  4],
        [9, 10]])

        Rows or columns can be repeated:

        >>> m.extract([0, 0, 1], [-1])
        Matrix([
        [2],
        [2],
        [5]])

        Every other row can be taken by using range to provide the indices:

        >>> m.extract(range(0, m.rows, 2), [-1])
        Matrix([
        [2],
        [8]])

        RowsList or colsList can also be a list of booleans, in which case
        the rows or columns corresponding to the True values will be selected:

        >>> m.extract([0, 1, 2, 3], [True, False, True])
        Matrix([
        [0,  2],
        [3,  5],
        [6,  8],
        [9, 11]])
        """"""

        if not is_sequence(rowsList) or not is_sequence(colsList):
            raise TypeError(""rowsList and colsList must be iterable"")
        if rowsList and all(isinstance(i, bool) for i in rowsList):
            rowsList = [index for index, item in enumerate(rowsList) if item]
        if colsList and all(isinstance(i, bool) for i in colsList):
            colsList = [index for index, item in enumerate(colsList) if item]

        return self._eval_extract(rowsList, colsList)

    def get_diag_blocks(self):
        """"""Obtains the square sub-matrices on the main diagonal of a square matrix.

        Useful for inverting symbolic matrices or solving systems of
        linear equations which may be decoupled by having a block diagonal
        structure.

        Examples
        ========

        >>> from sympy import Matrix
        >>> from sympy.abc import x, y, z
        >>> A = Matrix([[1, 3, 0, 0], [y, z*z, 0, 0], [0, 0, x, 0], [0, 0, 0, 0]])
        >>> a1, a2, a3 = A.get_diag_blocks()
        >>> a1
        Matrix([
        [1,    3],
        [y, z**2]])
        >>> a2
        Matrix([[x]])
        >>> a3
        Matrix([[0]])

        """"""
        return self._eval_get_diag_blocks()

    @classmethod
    def hstack(cls, *args):
        """"""Return a matrix formed by joining args horizontally (i.e.
        by repeated application of row_join).

        Examples
        ========

        >>> from sympy import Matrix, eye
        >>> Matrix.hstack(eye(2), 2*eye(2))
        Matrix([
        [1, 0, 2, 0],
        [0, 1, 0, 2]])
        """"""
        if len(args) == 0:
            return cls._new()

        kls = type(args[0])
        return reduce(kls.row_join, args)

    def reshape(self, rows, cols):
        """"""Reshape the matrix. Total number of elements must remain the same.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(2, 3, lambda i, j: 1)
        >>> m
        Matrix([
        [1, 1, 1],
        [1, 1, 1]])
        >>> m.reshape(1, 6)
        Matrix([[1, 1, 1, 1, 1, 1]])
        >>> m.reshape(3, 2)
        Matrix([
        [1, 1],
        [1, 1],
        [1, 1]])

        """"""
        if self.rows * self.cols != rows * cols:
            raise ValueError(""Invalid reshape parameters %d %d"" % (rows, cols))
        return self._new(rows, cols, lambda i, j: self[i * cols + j])

    def row_del(self, row):
        """"""Delete the specified row.""""""
        if row < 0:
            row += self.rows
        if not 0 <= row < self.rows:
            raise IndexError(""Row {} is out of range."".format(row))

        return self._eval_row_del(row)

    def row_insert(self, pos, other):
        """"""Insert one or more rows at the given row position.

        Examples
        ========

        >>> from sympy import zeros, ones
        >>> M = zeros(3)
        >>> V = ones(1, 3)
        >>> M.row_insert(1, V)
        Matrix([
        [0, 0, 0],
        [1, 1, 1],
        [0, 0, 0],
        [0, 0, 0]])

        See Also
        ========

        row
        col_insert
        """"""
        if not self:
            return self._new(other)

        pos = as_int(pos)

        if pos < 0:
            pos = self.rows + pos
        if pos < 0:
            pos = 0
        elif pos > self.rows:
            pos = self.rows

        if self.cols != other.cols:
            raise ShapeError(
                ""The matrices have incompatible number of columns ({} and {})""
                .format(self.cols, other.cols))

        return self._eval_row_insert(pos, other)

    def row_join(self, other):
        """"""Concatenates two matrices along self's last and rhs's first column

        Examples
        ========

        >>> from sympy import zeros, ones
        >>> M = zeros(3)
        >>> V = ones(3, 1)
        >>> M.row_join(V)
        Matrix([
        [0, 0, 0, 1],
        [0, 0, 0, 1],
        [0, 0, 0, 1]])

        See Also
        ========

        row
        col_join
        """"""
        if self.cols == 0 and self.rows != other.rows:
            return self._new(other.rows, 0, []).row_join(other)

        if self.rows != other.rows:
            raise ShapeError(
                ""The matrices have incompatible number of rows ({} and {})""
                .format(self.rows, other.rows))
        return self._eval_row_join(other)

    def diagonal(self, k=0):
        """"""Returns the kth diagonal of self. The main diagonal
        corresponds to `k=0`; diagonals above and below correspond to
        `k > 0` and `k < 0`, respectively. The values of `self[i, j]`
        for which `j - i = k`, are returned in order of increasing
        `i + j`, starting with `i + j = |k|`.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(3, 3, lambda i, j: j - i); m
        Matrix([
        [ 0,  1, 2],
        [-1,  0, 1],
        [-2, -1, 0]])
        >>> _.diagonal()
        Matrix([[0, 0, 0]])
        >>> m.diagonal(1)
        Matrix([[1, 1]])
        >>> m.diagonal(-2)
        Matrix([[-2]])

        Even though the diagonal is returned as a Matrix, the element
        retrieval can be done with a single index:

        >>> Matrix.diag(1, 2, 3).diagonal()[1]  # instead of [0, 1]
        2

        See Also
        ========

        diag
        """"""
        rv = []
        k = as_int(k)
        r = 0 if k > 0 else -k
        c = 0 if r else k
        while True:
            if r == self.rows or c == self.cols:
                break
            rv.append(self[r, c])
            r += 1
            c += 1
        if not rv:
            raise ValueError(filldedent('''
            The %s diagonal is out of range [%s, %s]''' % (
            k, 1 - self.rows, self.cols - 1)))
        return self._new(1, len(rv), rv)

    def row(self, i):
        """"""Elementary row selector.

        Examples
        ========

        >>> from sympy import eye
        >>> eye(2).row(0)
        Matrix([[1, 0]])

        See Also
        ========

        col
        row_del
        row_join
        row_insert
        """"""
        return self[i, :]

    @property
    def shape(self):
        """"""The shape (dimensions) of the matrix as the 2-tuple (rows, cols).

        Examples
        ========

        >>> from sympy import zeros
        >>> M = zeros(2, 3)
        >>> M.shape
        (2, 3)
        >>> M.rows
        2
        >>> M.cols
        3
        """"""
        return (self.rows, self.cols)

    def todok(self):
        """"""Return the matrix as dictionary of keys.

        Examples
        ========

        >>> from sympy import Matrix
        >>> M = Matrix.eye(3)
        >>> M.todok()
        {(0, 0): 1, (1, 1): 1, (2, 2): 1}
        """"""
        return self._eval_todok()

    def tolist(self):
        """"""Return the Matrix as a nested Python list.

        Examples
        ========

        >>> from sympy import Matrix, ones
        >>> m = Matrix(3, 3, range(9))
        >>> m
        Matrix([
        [0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
        >>> m.tolist()
        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
        >>> ones(3, 0).tolist()
        [[], [], []]

        When there are no rows then it will not be possible to tell how
        many columns were in the original matrix:

        >>> ones(0, 3).tolist()
        []

        """"""
        if not self.rows:
            return []
        if not self.cols:
            return [[] for i in range(self.rows)]
        return self._eval_tolist()

    def todod(M):
        """"""Returns matrix as dict of dicts containing non-zero elements of the Matrix

        Examples
        ========

        >>> from sympy import Matrix
        >>> A = Matrix([[0, 1],[0, 3]])
        >>> A
        Matrix([
        [0, 1],
        [0, 3]])
        >>> A.todod()
        {0: {1: 1}, 1: {1: 3}}


        """"""
        rowsdict = {}
        Mlol = M.tolist()
        for i, Mi in enumerate(Mlol):
            row = {j: Mij for j, Mij in enumerate(Mi) if Mij}
            if row:
                rowsdict[i] = row
        return rowsdict

    def vec(self):
        """"""Return the Matrix converted into a one column matrix by stacking columns

        Examples
        ========

        >>> from sympy import Matrix
        >>> m=Matrix([[1, 3], [2, 4]])
        >>> m
        Matrix([
        [1, 3],
        [2, 4]])
        >>> m.vec()
        Matrix([
        [1],
        [2],
        [3],
        [4]])

        See Also
        ========
        vech
        """"""
        return self._eval_vec()

    def vech(self, diagonal=True, check_symmetry=True):
        """"""Reshapes the matrix into a column vector by stacking the
        elements in the lower triangle.

        Parameters
        ==========

        diagonal : bool, optional
            If ``True``, it includes the diagonal elements.

        check_symmetry : bool, optional
            If ``True``, it checks whether the matrix is symmetric.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m=Matrix([[1, 2], [2, 3]])
        >>> m
        Matrix([
        [1, 2],
        [2, 3]])
        >>> m.vech()
        Matrix([
        [1],
        [2],
        [3]])
        >>> m.vech(diagonal=False)
        Matrix([[2]])

        Notes
        =====

        This should work for symmetric matrices and ``vech`` can
        represent symmetric matrices in vector form with less size than
        ``vec``.

        See Also
        ========
        vec
        """"""
        if not self.is_square:
            raise NonSquareMatrixError

        if check_symmetry and not self.is_symmetric():
            raise ValueError(""The matrix is not symmetric."")

        return self._eval_vech(diagonal)

    @classmethod
    def vstack(cls, *args):
        """"""Return a matrix formed by joining args vertically (i.e.
        by repeated application of col_join).

        Examples
        ========

        >>> from sympy import Matrix, eye
        >>> Matrix.vstack(eye(2), 2*eye(2))
        Matrix([
        [1, 0],
        [0, 1],
        [2, 0],
        [0, 2]])
        """"""
        if len(args) == 0:
            return cls._new()

        kls = type(args[0])
        return reduce(kls.col_join, args)


class MatrixSpecial(MatrixRequired):
    """"""Construction of special matrices""""""

    @classmethod
    def _eval_diag(cls, rows, cols, diag_dict):
        """"""diag_dict is a defaultdict containing
        all the entries of the diagonal matrix.""""""
        def entry(i, j):
            return diag_dict[(i, j)]
        return cls._new(rows, cols, entry)

    @classmethod
    def _eval_eye(cls, rows, cols):
        vals = [cls.zero]*(rows*cols)
        vals[::cols+1] = [cls.one]*min(rows, cols)
        return cls._new(rows, cols, vals, copy=False)

    @classmethod
    def _eval_jordan_block(cls, size: int, eigenvalue, band='upper'):
        if band == 'lower':
            def entry(i, j):
                if i == j:
                    return eigenvalue
                elif j + 1 == i:
                    return cls.one
                return cls.zero
        else:
            def entry(i, j):
                if i == j:
                    return eigenvalue
                elif i + 1 == j:
                    return cls.one
                return cls.zero
        return cls._new(size, size, entry)

    @classmethod
    def _eval_ones(cls, rows, cols):
        def entry(i, j):
            return cls.one
        return cls._new(rows, cols, entry)

    @classmethod
    def _eval_zeros(cls, rows, cols):
        return cls._new(rows, cols, [cls.zero]*(rows*cols), copy=False)

    @classmethod
    def _eval_wilkinson(cls, n):
        def entry(i, j):
            return cls.one if i + 1 == j else cls.zero

        D = cls._new(2*n + 1, 2*n + 1, entry)

        wminus = cls.diag(list(range(-n, n + 1)), unpack=True) + D + D.T
        wplus = abs(cls.diag(list(range(-n, n + 1)), unpack=True)) + D + D.T

        return wminus, wplus

    @classmethod
    def diag(kls, *args, strict=False, unpack=True, rows=None, cols=None, **kwargs):
        """"""Returns a matrix with the specified diagonal.
        If matrices are passed, a block-diagonal matrix
        is created (i.e. the ""direct sum"" of the matrices).

        kwargs
        ======

        rows : rows of the resulting matrix; computed if
               not given.

        cols : columns of the resulting matrix; computed if
               not given.

        cls : class for the resulting matrix

        unpack : bool which, when True (default), unpacks a single
        sequence rather than interpreting it as a Matrix.

        strict : bool which, when False (default), allows Matrices to
        have variable-length rows.

        Examples
        ========

        >>> from sympy import Matrix
        >>> Matrix.diag(1, 2, 3)
        Matrix([
        [1, 0, 0],
        [0, 2, 0],
        [0, 0, 3]])

        The current default is to unpack a single sequence. If this is
        not desired, set `unpack=False` and it will be interpreted as
        a matrix.

        >>> Matrix.diag([1, 2, 3]) == Matrix.diag(1, 2, 3)
        True

        When more than one element is passed, each is interpreted as
        something to put on the diagonal. Lists are converted to
        matrices. Filling of the diagonal always continues from
        the bottom right hand corner of the previous item: this
        will create a block-diagonal matrix whether the matrices
        are square or not.

        >>> col = [1, 2, 3]
        >>> row = [[4, 5]]
        >>> Matrix.diag(col, row)
        Matrix([
        [1, 0, 0],
        [2, 0, 0],
        [3, 0, 0],
        [0, 4, 5]])

        When `unpack` is False, elements within a list need not all be
        of the same length. Setting `strict` to True would raise a
        ValueError for the following:

        >>> Matrix.diag([[1, 2, 3], [4, 5], [6]], unpack=False)
        Matrix([
        [1, 2, 3],
        [4, 5, 0],
        [6, 0, 0]])

        The type of the returned matrix can be set with the ``cls``
        keyword.

        >>> from sympy import ImmutableMatrix
        >>> from sympy.utilities.misc import func_name
        >>> func_name(Matrix.diag(1, cls=ImmutableMatrix))
        'ImmutableDenseMatrix'

        A zero dimension matrix can be used to position the start of
        the filling at the start of an arbitrary row or column:

        >>> from sympy import ones
        >>> r2 = ones(0, 2)
        >>> Matrix.diag(r2, 1, 2)
        Matrix([
        [0, 0, 1, 0],
        [0, 0, 0, 2]])

        See Also
        ========
        eye
        diagonal
        .dense.diag
        .expressions.blockmatrix.BlockMatrix
        .sparsetools.banded
       """"""
        from sympy.matrices.matrixbase import MatrixBase
        from sympy.matrices.dense import Matrix
        from sympy.matrices import SparseMatrix
        klass = kwargs.get('cls', kls)
        if unpack and len(args) == 1 and is_sequence(args[0]) and \
                not isinstance(args[0], MatrixBase):
            args = args[0]

        diag_entries = defaultdict(int)
        rmax = cmax = 0
        for m in args:
            if isinstance(m, list):
                if strict:
                    _ = Matrix(m)
                    r, c = _.shape
                    m = _.tolist()
                else:
                    r, c, smat = SparseMatrix._handle_creation_inputs(m)
                    for (i, j), _ in smat.items():
                        diag_entries[(i + rmax, j + cmax)] = _
                    m = []
            elif hasattr(m, 'shape'):
                r, c = m.shape
                m = m.tolist()
            else:
                diag_entries[(rmax, cmax)] = m
                rmax += 1
                cmax += 1
                continue
            for i, mi in enumerate(m):
                for j, _ in enumerate(mi):
                    diag_entries[(i + rmax, j + cmax)] = _
            rmax += r
            cmax += c
        if rows is None:
            rows, cols = cols, rows
        if rows is None:
            rows, cols = rmax, cmax
        else:
            cols = rows if cols is None else cols
        if rows < rmax or cols < cmax:
            raise ValueError(filldedent('''
                The constructed matrix is {} x {} but a size of {} x {}
                was specified.'''.format(rmax, cmax, rows, cols)))
        return klass._eval_diag(rows, cols, diag_entries)

    @classmethod
    def eye(kls, rows, cols=None, **kwargs):
        """"""Returns an identity matrix.

        Parameters
        ==========

        rows : rows of the matrix
        cols : cols of the matrix (if None, cols=rows)

        kwargs
        ======
        cls : class of the returned matrix
        """"""
        if cols is None:
            cols = rows
        if rows < 0 or cols < 0:
            raise ValueError(""Cannot create a {} x {} matrix. ""
                             ""Both dimensions must be positive"".format(rows, cols))
        klass = kwargs.get('cls', kls)
        rows, cols = as_int(rows), as_int(cols)

        return klass._eval_eye(rows, cols)

    @classmethod
    def jordan_block(kls, size=None, eigenvalue=None, *, band='upper', **kwargs):
        """"""Returns a Jordan block

        Parameters
        ==========

        size : Integer, optional
            Specifies the shape of the Jordan block matrix.

        eigenvalue : Number or Symbol
            Specifies the value for the main diagonal of the matrix.

            .. note::
                The keyword ``eigenval`` is also specified as an alias
                of this keyword, but it is not recommended to use.

                We may deprecate the alias in later release.

        band : 'upper' or 'lower', optional
            Specifies the position of the off-diagonal to put `1` s on.

        cls : Matrix, optional
            Specifies the matrix class of the output form.

            If it is not specified, the class type where the method is
            being executed on will be returned.

        Returns
        =======

        Matrix
            A Jordan block matrix.

        Raises
        ======

        ValueError
            If insufficient arguments are given for matrix size
            specification, or no eigenvalue is given.

        Examples
        ========

        Creating a default Jordan block:

        >>> from sympy import Matrix
        >>> from sympy.abc import x
        >>> Matrix.jordan_block(4, x)
        Matrix([
        [x, 1, 0, 0],
        [0, x, 1, 0],
        [0, 0, x, 1],
        [0, 0, 0, x]])

        Creating an alternative Jordan block matrix where `1` is on
        lower off-diagonal:

        >>> Matrix.jordan_block(4, x, band='lower')
        Matrix([
        [x, 0, 0, 0],
        [1, x, 0, 0],
        [0, 1, x, 0],
        [0, 0, 1, x]])

        Creating a Jordan block with keyword arguments

        >>> Matrix.jordan_block(size=4, eigenvalue=x)
        Matrix([
        [x, 1, 0, 0],
        [0, x, 1, 0],
        [0, 0, x, 1],
        [0, 0, 0, x]])

        References
        ==========

        .. [1] https://en.wikipedia.org/wiki/Jordan_matrix
        """"""
        klass = kwargs.pop('cls', kls)

        eigenval = kwargs.get('eigenval', None)
        if eigenvalue is None and eigenval is None:
            raise ValueError(""Must supply an eigenvalue"")
        elif eigenvalue != eigenval and None not in (eigenval, eigenvalue):
            raise ValueError(
                ""Inconsistent values are given: 'eigenval'={}, ""
                ""'eigenvalue'={}"".format(eigenval, eigenvalue))
        else:
            if eigenval is not None:
                eigenvalue = eigenval

        if size is None:
            raise ValueError(""Must supply a matrix size"")

        size = as_int(size)
        return klass._eval_jordan_block(size, eigenvalue, band)

    @classmethod
    def ones(kls, rows, cols=None, **kwargs):
        """"""Returns a matrix of ones.

        Parameters
        ==========

        rows : rows of the matrix
        cols : cols of the matrix (if None, cols=rows)

        kwargs
        ======
        cls : class of the returned matrix
        """"""
        if cols is None:
            cols = rows
        klass = kwargs.get('cls', kls)
        rows, cols = as_int(rows), as_int(cols)

        return klass._eval_ones(rows, cols)

    @classmethod
    def zeros(kls, rows, cols=None, **kwargs):
        """"""Returns a matrix of zeros.

        Parameters
        ==========

        rows : rows of the matrix
        cols : cols of the matrix (if None, cols=rows)

        kwargs
        ======
        cls : class of the returned matrix
        """"""
        if cols is None:
            cols = rows
        if rows < 0 or cols < 0:
            raise ValueError(""Cannot create a {} x {} matrix. ""
                             ""Both dimensions must be positive"".format(rows, cols))
        klass = kwargs.get('cls', kls)
        rows, cols = as_int(rows), as_int(cols)

        return klass._eval_zeros(rows, cols)

    @classmethod
    def companion(kls, poly):
        """"""Returns a companion matrix of a polynomial.

        Examples
        ========

        >>> from sympy import Matrix, Poly, Symbol, symbols
        >>> x = Symbol('x')
        >>> c0, c1, c2, c3, c4 = symbols('c0:5')
        >>> p = Poly(c0 + c1*x + c2*x**2 + c3*x**3 + c4*x**4 + x**5, x)
        >>> Matrix.companion(p)
        Matrix([
        [0, 0, 0, 0, -c0],
        [1, 0, 0, 0, -c1],
        [0, 1, 0, 0, -c2],
        [0, 0, 1, 0, -c3],
        [0, 0, 0, 1, -c4]])
        """"""
        poly = kls._sympify(poly)
        if not isinstance(poly, Poly):
            raise ValueError(""{} must be a Poly instance."".format(poly))
        if not poly.is_monic:
            raise ValueError(""{} must be a monic polynomial."".format(poly))
        if not poly.is_univariate:
            raise ValueError(
                ""{} must be a univariate polynomial."".format(poly))

        size = poly.degree()
        if not size >= 1:
            raise ValueError(
                ""{} must have degree not less than 1."".format(poly))

        coeffs = poly.all_coeffs()
        def entry(i, j):
            if j == size - 1:
                return -coeffs[-1 - i]
            elif i == j + 1:
                return kls.one
            return kls.zero
        return kls._new(size, size, entry)


    @classmethod
    def wilkinson(kls, n, **kwargs):
        """"""Returns two square Wilkinson Matrix of size 2*n + 1
        $W_{2n + 1}^-, W_{2n + 1}^+ =$ Wilkinson(n)

        Examples
        ========

        >>> from sympy import Matrix
        >>> wminus, wplus = Matrix.wilkinson(3)
        >>> wminus
        Matrix([
        [-3,  1,  0, 0, 0, 0, 0],
        [ 1, -2,  1, 0, 0, 0, 0],
        [ 0,  1, -1, 1, 0, 0, 0],
        [ 0,  0,  1, 0, 1, 0, 0],
        [ 0,  0,  0, 1, 1, 1, 0],
        [ 0,  0,  0, 0, 1, 2, 1],
        [ 0,  0,  0, 0, 0, 1, 3]])
        >>> wplus
        Matrix([
        [3, 1, 0, 0, 0, 0, 0],
        [1, 2, 1, 0, 0, 0, 0],
        [0, 1, 1, 1, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 1, 1, 0],
        [0, 0, 0, 0, 1, 2, 1],
        [0, 0, 0, 0, 0, 1, 3]])

        References
        ==========
        
        .. [1] https://blogs.mathworks.com/cleve/2013/04/15/wilkinsons-matrices-2/
        .. [2] J. H. Wilkinson, The Algebraic Eigenvalue Problem, Claredon Press, Oxford, 1965, 662 pp.

        """"""
        klass = kwargs.get('cls', kls)
        n = as_int(n)
        return klass._eval_wilkinson(n)

class MatrixProperties(MatrixRequired):
    """"""Provides basic properties of a matrix.""""""

    def _eval_atoms(self, *types):
        result = set()
        for i in self:
            result.update(i.atoms(*types))
        return result

    def _eval_free_symbols(self):
        return set().union(*(i.free_symbols for i in self if i))

    def _eval_has(self, *patterns):
        return any(a.has(*patterns) for a in self)

    def _eval_is_anti_symmetric(self, simpfunc):
        if not all(simpfunc(self[i, j] + self[j, i]).is_zero for i in range(self.rows) for j in range(self.cols)):
            return False
        return True

    def _eval_is_diagonal(self):
        for i in range(self.rows):
            for j in range(self.cols):
                if i != j and self[i, j]:
                    return False
        return True

    def _eval_is_matrix_hermitian(self, simpfunc):
        mat = self._new(self.rows, self.cols, lambda i, j: simpfunc(self[i, j] - self[j, i].conjugate()))
        return mat.is_zero_matrix

    def _eval_is_Identity(self) -> FuzzyBool:
        def dirac(i, j):
            if i == j:
                return 1
            return 0

        return all(self[i, j] == dirac(i, j)
                for i in range(self.rows)
                for j in range(self.cols))

    def _eval_is_lower_hessenberg(self):
        return all(self[i, j].is_zero
                   for i in range(self.rows)
                   for j in range(i + 2, self.cols))

    def _eval_is_lower(self):
        return all(self[i, j].is_zero
                   for i in range(self.rows)
                   for j in range(i + 1, self.cols))

    def _eval_is_symbolic(self):
        return self.has(Symbol)

    def _eval_is_symmetric(self, simpfunc):
        mat = self._new(self.rows, self.cols, lambda i, j: simpfunc(self[i, j] - self[j, i]))
        return mat.is_zero_matrix

    def _eval_is_zero_matrix(self):
        if any(i.is_zero == False for i in self):
            return False
        if any(i.is_zero is None for i in self):
            return None
        return True

    def _eval_is_upper_hessenberg(self):
        return all(self[i, j].is_zero
                   for i in range(2, self.rows)
                   for j in range(min(self.cols, (i - 1))))

    def _eval_values(self):
        return [i for i in self if not i.is_zero]

    def _has_positive_diagonals(self):
        diagonal_entries = (self[i, i] for i in range(self.rows))
        return fuzzy_and(x.is_positive for x in diagonal_entries)

    def _has_nonnegative_diagonals(self):
        diagonal_entries = (self[i, i] for i in range(self.rows))
        return fuzzy_and(x.is_nonnegative for x in diagonal_entries)

    def atoms(self, *types):
        """"""Returns the atoms that form the current object.

        Examples
        ========

        >>> from sympy.abc import x, y
        >>> from sympy import Matrix
        >>> Matrix([[x]])
        Matrix([[x]])
        >>> _.atoms()
        {x}
        >>> Matrix([[x, y], [y, x]])
        Matrix([
        [x, y],
        [y, x]])
        >>> _.atoms()
        {x, y}
        """"""

        types = tuple(t if isinstance(t, type) else type(t) for t in types)
        if not types:
            types = (Atom,)
        return self._eval_atoms(*types)

    @property
    def free_symbols(self):
        """"""Returns the free symbols within the matrix.

        Examples
        ========

        >>> from sympy.abc import x
        >>> from sympy import Matrix
        >>> Matrix([[x], [1]]).free_symbols
        {x}
        """"""
        return self._eval_free_symbols()

    def has(self, *patterns):
        """"""Test whether any subexpression matches any of the patterns.

        Examples
        ========

        >>> from sympy import Matrix, SparseMatrix, Float
        >>> from sympy.abc import x, y
        >>> A = Matrix(((1, x), (0.2, 3)))
        >>> B = SparseMatrix(((1, x), (0.2, 3)))
        >>> A.has(x)
        True
        >>> A.has(y)
        False
        >>> A.has(Float)
        True
        >>> B.has(x)
        True
        >>> B.has(y)
        False
        >>> B.has(Float)
        True
        """"""
        return self._eval_has(*patterns)

    def is_anti_symmetric(self, simplify=True):
        """"""Check if matrix M is an antisymmetric matrix,
        that is, M is a square matrix with all M[i, j] == -M[j, i].

        When ``simplify=True`` (default), the sum M[i, j] + M[j, i] is
        simplified before testing to see if it is zero. By default,
        the SymPy simplify function is used. To use a custom function
        set simplify to a function that accepts a single argument which
        returns a simplified expression. To skip simplification, set
        simplify to False but note that although this will be faster,
        it may induce false negatives.

        Examples
        ========

        >>> from sympy import Matrix, symbols
        >>> m = Matrix(2, 2, [0, 1, -1, 0])
        >>> m
        Matrix([
        [ 0, 1],
        [-1, 0]])
        >>> m.is_anti_symmetric()
        True
        >>> x, y = symbols('x y')
        >>> m = Matrix(2, 3, [0, 0, x, -y, 0, 0])
        >>> m
        Matrix([
        [ 0, 0, x],
        [-y, 0, 0]])
        >>> m.is_anti_symmetric()
        False

        >>> from sympy.abc import x, y
        >>> m = Matrix(3, 3, [0, x**2 + 2*x + 1, y,
        ...                   -(x + 1)**2, 0, x*y,
        ...                   -y, -x*y, 0])

        Simplification of matrix elements is done by default so even
        though two elements which should be equal and opposite would not
        pass an equality test, the matrix is still reported as
        anti-symmetric:

        >>> m[0, 1] == -m[1, 0]
        False
        >>> m.is_anti_symmetric()
        True

        If ``simplify=False`` is used for the case when a Matrix is already
        simplified, this will speed things up. Here, we see that without
        simplification the matrix does not appear anti-symmetric:

        >>> print(m.is_anti_symmetric(simplify=False))
        None

        But if the matrix were already expanded, then it would appear
        anti-symmetric and simplification in the is_anti_symmetric routine
        is not needed:

        >>> m = m.expand()
        >>> m.is_anti_symmetric(simplify=False)
        True
        """"""
        simpfunc = simplify
        if not isfunction(simplify):
            simpfunc = _simplify if simplify else lambda x: x

        if not self.is_square:
            return False
        return self._eval_is_anti_symmetric(simpfunc)

    def is_diagonal(self):
        """"""Check if matrix is diagonal,
        that is matrix in which the entries outside the main diagonal are all zero.

        Examples
        ========

        >>> from sympy import Matrix, diag
        >>> m = Matrix(2, 2, [1, 0, 0, 2])
        >>> m
        Matrix([
        [1, 0],
        [0, 2]])
        >>> m.is_diagonal()
        True

        >>> m = Matrix(2, 2, [1, 1, 0, 2])
        >>> m
        Matrix([
        [1, 1],
        [0, 2]])
        >>> m.is_diagonal()
        False

        >>> m = diag(1, 2, 3)
        >>> m
        Matrix([
        [1, 0, 0],
        [0, 2, 0],
        [0, 0, 3]])
        >>> m.is_diagonal()
        True

        See Also
        ========

        is_lower
        is_upper
        sympy.matrices.matrixbase.MatrixCommon.is_diagonalizable
        diagonalize
        """"""
        return self._eval_is_diagonal()

    @property
    def is_weakly_diagonally_dominant(self):
        r""""""Tests if the matrix is row weakly diagonally dominant.

        Explanation
        ===========

        A $n, n$ matrix $A$ is row weakly diagonally dominant if

        .. math::
            \left|A_{i, i}\right| \ge \sum_{j = 0, j \neq i}^{n-1}
            \left|A_{i, j}\right| \quad {\text{for all }}
            i \in \{ 0, ..., n-1 \}

        Examples
        ========

        >>> from sympy import Matrix
        >>> A = Matrix([[3, -2, 1], [1, -3, 2], [-1, 2, 4]])
        >>> A.is_weakly_diagonally_dominant
        True

        >>> A = Matrix([[-2, 2, 1], [1, 3, 2], [1, -2, 0]])
        >>> A.is_weakly_diagonally_dominant
        False

        >>> A = Matrix([[-4, 2, 1], [1, 6, 2], [1, -2, 5]])
        >>> A.is_weakly_diagonally_dominant
        True

        Notes
        =====

        If you want to test whether a matrix is column diagonally
        dominant, you can apply the test after transposing the matrix.
        """"""
        if not self.is_square:
            return False

        rows, cols = self.shape

        def test_row(i):
            summation = self.zero
            for j in range(cols):
                if i != j:
                    summation += Abs(self[i, j])
            return (Abs(self[i, i]) - summation).is_nonnegative

        return fuzzy_and(test_row(i) for i in range(rows))

    @property
    def is_strongly_diagonally_dominant(self):
        r""""""Tests if the matrix is row strongly diagonally dominant.

        Explanation
        ===========

        A $n, n$ matrix $A$ is row strongly diagonally dominant if

        .. math::
            \left|A_{i, i}\right| > \sum_{j = 0, j \neq i}^{n-1}
            \left|A_{i, j}\right| \quad {\text{for all }}
            i \in \{ 0, ..., n-1 \}

        Examples
        ========

        >>> from sympy import Matrix
        >>> A = Matrix([[3, -2, 1], [1, -3, 2], [-1, 2, 4]])
        >>> A.is_strongly_diagonally_dominant
        False

        >>> A = Matrix([[-2, 2, 1], [1, 3, 2], [1, -2, 0]])
        >>> A.is_strongly_diagonally_dominant
        False

        >>> A = Matrix([[-4, 2, 1], [1, 6, 2], [1, -2, 5]])
        >>> A.is_strongly_diagonally_dominant
        True

        Notes
        =====

        If you want to test whether a matrix is column diagonally
        dominant, you can apply the test after transposing the matrix.
        """"""
        if not self.is_square:
            return False

        rows, cols = self.shape

        def test_row(i):
            summation = self.zero
            for j in range(cols):
                if i != j:
                    summation += Abs(self[i, j])
            return (Abs(self[i, i]) - summation).is_positive

        return fuzzy_and(test_row(i) for i in range(rows))

    @property
    def is_hermitian(self):
        """"""Checks if the matrix is Hermitian.

        In a Hermitian matrix element i,j is the complex conjugate of
        element j,i.

        Examples
        ========

        >>> from sympy import Matrix, I
        >>> from sympy.abc import x
        >>> a = Matrix([[1, I], [-I, 1]])
        >>> a
        Matrix([
        [ 1, I],
        [-I, 1]])
        >>> a.is_hermitian
        True
        >>> a[0, 0] = 2*I
        >>> a.is_hermitian
        False
        >>> a[0, 0] = x
        >>> a.is_hermitian
        >>> a[0, 1] = a[1, 0]*I
        >>> a.is_hermitian
        False
        """"""
        if not self.is_square:
            return False

        return self._eval_is_matrix_hermitian(_simplify)

    @property
    def is_Identity(self) -> FuzzyBool:
        if not self.is_square:
            return False
        return self._eval_is_Identity()

    @property
    def is_lower_hessenberg(self):
        r""""""Checks if the matrix is in the lower-Hessenberg form.

        The lower hessenberg matrix has zero entries
        above the first superdiagonal.

        Examples
        ========

        >>> from sympy import Matrix
        >>> a = Matrix([[1, 2, 0, 0], [5, 2, 3, 0], [3, 4, 3, 7], [5, 6, 1, 1]])
        >>> a
        Matrix([
        [1, 2, 0, 0],
        [5, 2, 3, 0],
        [3, 4, 3, 7],
        [5, 6, 1, 1]])
        >>> a.is_lower_hessenberg
        True

        See Also
        ========

        is_upper_hessenberg
        is_lower
        """"""
        return self._eval_is_lower_hessenberg()

    @property
    def is_lower(self):
        """"""Check if matrix is a lower triangular matrix. True can be returned
        even if the matrix is not square.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(2, 2, [1, 0, 0, 1])
        >>> m
        Matrix([
        [1, 0],
        [0, 1]])
        >>> m.is_lower
        True

        >>> m = Matrix(4, 3, [0, 0, 0, 2, 0, 0, 1, 4, 0, 6, 6, 5])
        >>> m
        Matrix([
        [0, 0, 0],
        [2, 0, 0],
        [1, 4, 0],
        [6, 6, 5]])
        >>> m.is_lower
        True

        >>> from sympy.abc import x, y
        >>> m = Matrix(2, 2, [x**2 + y, y**2 + x, 0, x + y])
        >>> m
        Matrix([
        [x**2 + y, x + y**2],
        [       0,    x + y]])
        >>> m.is_lower
        False

        See Also
        ========

        is_upper
        is_diagonal
        is_lower_hessenberg
        """"""
        return self._eval_is_lower()

    @property
    def is_square(self):
        """"""Checks if a matrix is square.

        A matrix is square if the number of rows equals the number of columns.
        The empty matrix is square by definition, since the number of rows and
        the number of columns are both zero.

        Examples
        ========

        >>> from sympy import Matrix
        >>> a = Matrix([[1, 2, 3], [4, 5, 6]])
        >>> b = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
        >>> c = Matrix([])
        >>> a.is_square
        False
        >>> b.is_square
        True
        >>> c.is_square
        True
        """"""
        return self.rows == self.cols

    def is_symbolic(self):
        """"""Checks if any elements contain Symbols.

        Examples
        ========

        >>> from sympy import Matrix
        >>> from sympy.abc import x, y
        >>> M = Matrix([[x, y], [1, 0]])
        >>> M.is_symbolic()
        True

        """"""
        return self._eval_is_symbolic()

    def is_symmetric(self, simplify=True):
        """"""Check if matrix is symmetric matrix,
        that is square matrix and is equal to its transpose.

        By default, simplifications occur before testing symmetry.
        They can be skipped using 'simplify=False'; while speeding things a bit,
        this may however induce false negatives.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(2, 2, [0, 1, 1, 2])
        >>> m
        Matrix([
        [0, 1],
        [1, 2]])
        >>> m.is_symmetric()
        True

        >>> m = Matrix(2, 2, [0, 1, 2, 0])
        >>> m
        Matrix([
        [0, 1],
        [2, 0]])
        >>> m.is_symmetric()
        False

        >>> m = Matrix(2, 3, [0, 0, 0, 0, 0, 0])
        >>> m
        Matrix([
        [0, 0, 0],
        [0, 0, 0]])
        >>> m.is_symmetric()
        False

        >>> from sympy.abc import x, y
        >>> m = Matrix(3, 3, [1, x**2 + 2*x + 1, y, (x + 1)**2, 2, 0, y, 0, 3])
        >>> m
        Matrix([
        [         1, x**2 + 2*x + 1, y],
        [(x + 1)**2,              2, 0],
        [         y,              0, 3]])
        >>> m.is_symmetric()
        True

        If the matrix is already simplified, you may speed-up is_symmetric()
        test by using 'simplify=False'.

        >>> bool(m.is_symmetric(simplify=False))
        False
        >>> m1 = m.expand()
        >>> m1.is_symmetric(simplify=False)
        True
        """"""
        simpfunc = simplify
        if not isfunction(simplify):
            simpfunc = _simplify if simplify else lambda x: x

        if not self.is_square:
            return False

        return self._eval_is_symmetric(simpfunc)

    @property
    def is_upper_hessenberg(self):
        """"""Checks if the matrix is the upper-Hessenberg form.

        The upper hessenberg matrix has zero entries
        below the first subdiagonal.

        Examples
        ========

        >>> from sympy import Matrix
        >>> a = Matrix([[1, 4, 2, 3], [3, 4, 1, 7], [0, 2, 3, 4], [0, 0, 1, 3]])
        >>> a
        Matrix([
        [1, 4, 2, 3],
        [3, 4, 1, 7],
        [0, 2, 3, 4],
        [0, 0, 1, 3]])
        >>> a.is_upper_hessenberg
        True

        See Also
        ========

        is_lower_hessenberg
        is_upper
        """"""
        return self._eval_is_upper_hessenberg()

    @property
    def is_upper(self):
        """"""Check if matrix is an upper triangular matrix. True can be returned
        even if the matrix is not square.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(2, 2, [1, 0, 0, 1])
        >>> m
        Matrix([
        [1, 0],
        [0, 1]])
        >>> m.is_upper
        True

        >>> m = Matrix(4, 3, [5, 1, 9, 0, 4, 6, 0, 0, 5, 0, 0, 0])
        >>> m
        Matrix([
        [5, 1, 9],
        [0, 4, 6],
        [0, 0, 5],
        [0, 0, 0]])
        >>> m.is_upper
        True

        >>> m = Matrix(2, 3, [4, 2, 5, 6, 1, 1])
        >>> m
        Matrix([
        [4, 2, 5],
        [6, 1, 1]])
        >>> m.is_upper
        False

        See Also
        ========

        is_lower
        is_diagonal
        is_upper_hessenberg
        """"""
        return all(self[i, j].is_zero
                   for i in range(1, self.rows)
                   for j in range(min(i, self.cols)))

    @property
    def is_zero_matrix(self):
        """"""Checks if a matrix is a zero matrix.

        A matrix is zero if every element is zero.  A matrix need not be square
        to be considered zero.  The empty matrix is zero by the principle of
        vacuous truth.  For a matrix that may or may not be zero (e.g.
        contains a symbol), this will be None

        Examples
        ========

        >>> from sympy import Matrix, zeros
        >>> a = Matrix([[0, 0], [0, 0]])
        >>> b = zeros(3, 4)
        >>> c = Matrix([[0, 1], [0, 0]])
        >>> d = Matrix([])
        >>> e = Matrix([[x, 0], [0, 0]])
        >>> a.is_zero_matrix
        True
        >>> b.is_zero_matrix
        True
        >>> c.is_zero_matrix
        False
        >>> d.is_zero_matrix
        True
        >>> e.is_zero_matrix
        """"""
        return self._eval_is_zero_matrix()

    def values(self):
        """"""Return non-zero values of self.""""""
        return self._eval_values()


class MatrixOperations(MatrixRequired):
    """"""Provides basic matrix shape and elementwise
    operations.  Should not be instantiated directly.""""""

    def _eval_adjoint(self):
        return self.transpose().conjugate()

    def _eval_applyfunc(self, f):
        out = self._new(self.rows, self.cols, [f(x) for x in self])
        return out

    def _eval_as_real_imag(self):  # type: ignore
        return (self.applyfunc(re), self.applyfunc(im))

    def _eval_conjugate(self):
        return self.applyfunc(lambda x: x.conjugate())

    def _eval_permute_cols(self, perm):
        mapping = list(perm)

        def entry(i, j):
            return self[i, mapping[j]]

        return self._new(self.rows, self.cols, entry)

    def _eval_permute_rows(self, perm):
        mapping = list(perm)

        def entry(i, j):
            return self[mapping[i], j]

        return self._new(self.rows, self.cols, entry)

    def _eval_trace(self):
        return sum(self[i, i] for i in range(self.rows))

    def _eval_transpose(self):
        return self._new(self.cols, self.rows, lambda i, j: self[j, i])

    def adjoint(self):
        """"""Conjugate transpose or Hermitian conjugation.""""""
        return self._eval_adjoint()

    def applyfunc(self, f):
        """"""Apply a function to each element of the matrix.

        Examples
        ========

        >>> from sympy import Matrix
        >>> m = Matrix(2, 2, lambda i, j: i*2+j)
        >>> m
        Matrix([
        [0, 1],
        [2, 3]])
        >>> m.applyfunc(lambda i: 2*i)
        Matrix([
        [0, 2],
        [4, 6]])

        """"""
        if not callable(f):
            raise TypeError(""`f` must be callable."")

        return self._eval_applyfunc(f)

    def as_real_imag(self, deep=True, **hints):
        """"""Returns a tuple containing the (real, imaginary) part of matrix.""""""
        return self._eval_as_real_imag()

    def conjugate(self):
        """"""Return the by-element conjugation.

        Examples
        ========

        >>> from sympy import SparseMatrix, I
        >>> a = SparseMatrix(((1, 2 + I), (3, 4), (I, -I)))
        >>> a
        Matrix([
        [1, 2 + I],
        [3,     4],
        [I,    -I]])
        >>> a.C
        Matrix([
        [ 1, 2 - I],
        [ 3,     4],
        [-I,     I]])

        See Also
        ========

        transpose: Matrix transposition
        H: Hermite conjugation
        sympy.matrices.matrixbase.MatrixBase.D: Dirac conjugation
        """"""
        return self._eval_conjugate()

    def doit(self, **hints):
        return self.applyfunc(lambda x: x.doit(**hints))

    def evalf(self, n=15, subs=None, maxn=100, chop=False, strict=False, quad=None, verbose=False):
        """"""Apply evalf() to each element of self.""""""
        options = {'subs':subs, 'maxn':maxn, 'chop':chop, 'strict':strict,
                'quad':quad, 'verbose':verbose}
        return self.applyfunc(lambda i: i.evalf(n, **options))

    def expand(self, deep=True, modulus=None, power_base=True, power_exp=True,
               mul=True, log=True, multinomial=True, basic=True, **hints):
        """"""Apply core.function.expand to each entry of the matrix.

        Examples
        ========

        >>> from sympy.abc import x
        >>> from sympy import Matrix
        >>> Matrix(1, 1, [x*(x+1)])
        Matrix([[x*(x + 1)]])
        >>> _.expand()
        Matrix([[x**2 + x]])

        """"""
        return self.applyfunc(lambda x: x.expand(
            deep, modulus, power_base, power_exp, mul, log, multinomial, basic,
            **hints))

    @property
    def H(self):
        """"""Return Hermite conjugate.

        Examples
        ========

        >>> from sympy import Matrix, I
        >>> m = Matrix((0, 1 + I, 2, 3))
        >>> m
        Matrix([
        [    0],
        [1 + I],
        [    2],
        [    3]])
        >>> m.H
        Matrix([[0, 1 - I, 2, 3]])

        See Also
        ========

        conjugate: By-element conjugation
        sympy.matrices.matrixbase.MatrixBase.D: Dirac conjugation
        """"""
        return self.T.C

    def permute(self, perm, orientation='rows', direction='forward'):
        r""""""Permute the rows or columns of a matrix by the given list of
        swaps.

        Parameters
        ==========

        perm : Permutation, list, or list of lists
            A representation for the permutation.

            If it is ``Permutation``, it is used directly with some
            resizing with respect to the matrix size.

            If it is specified as list of lists,
            (e.g., ``[[0, 1], [0, 2]]``), then the permutation is formed
            from applying the product of cycles. The direction how the
            cyclic product is applied is described in below.

            If it is specified as a list, the list should represent
            an array form of a permutation. (e.g., ``[1, 2, 0]``) which
            would would form the swapping function
            `0 \mapsto 1, 1 \mapsto 2, 2\mapsto 0`.

        orientation : 'rows', 'cols'
            A flag to control whether to permute the rows or the columns

        direction : 'forward', 'backward'
            A flag to control whether to apply the permutations from
            the start of the list first, or from the back of the list
            first.

            For example, if the permutation specification is
            ``[[0, 1], [0, 2]]``,

            If the flag is set to ``'forward'``, the cycle would be
            formed as `0 \mapsto 2, 2 \mapsto 1, 1 \mapsto 0`.

            If the flag is set to ``'backward'``, the cycle would be
            formed as `0 \mapsto 1, 1 \mapsto 2, 2 \mapsto 0`.

            If the argument ``perm`` is not in a form of list of lists,
            this flag takes no effect.

        Examples
        ========

        >>> from sympy import eye
        >>> M = eye(3)
        >>> M.permute([[0, 1], [0, 2]], orientation='rows', direction='forward')
        Matrix([
        [0, 0, 1],
        [1, 0, 0],
        [0, 1, 0]])

        >>> from sympy import eye
        >>> M = eye(3)
        >>> M.permute([[0, 1], [0, 2]], orientation='rows', direction='backward')
        Matrix([
        [0, 1, 0],
        [0, 0, 1],
        [1, 0, 0]])

        Notes
        =====

        If a bijective function
        `\sigma : \mathbb{N}_0 \rightarrow \mathbb{N}_0` denotes the
        permutation.

        If the matrix `A` is the matrix to permute, represented as
        a horizontal or a vertical stack of vectors:

        .. math::
            A =
            \begin{bmatrix}
            a_0 \\ a_1 \\ \vdots \\ a_{n-1}
            \end{bmatrix} =
            \begin{bmatrix}
            \alpha_0 & \alpha_1 & \cdots & \alpha_{n-1}
            \end{bmatrix}

        If the matrix `B` is the result, the permutation of matrix rows
        is defined as:

        .. math::
            B := \begin{bmatrix}
            a_{\sigma(0)} \\ a_{\sigma(1)} \\ \vdots \\ a_{\sigma(n-1)}
            \end{bmatrix}

        And the permutation of matrix columns is defined as:

        .. math::
            B := \begin{bmatrix}
            \alpha_{\sigma(0)} & \alpha_{\sigma(1)} &
            \cdots & \alpha_{\sigma(n-1)}
            \end{bmatrix}
        """"""
        from sympy.combinatorics import Permutation

        if direction == 'forwards':
            direction = 'forward'
        if direction == 'backwards':
            direction = 'backward'
        if orientation == 'columns':
            orientation = 'cols'

        if direction not in ('forward', 'backward'):
            raise TypeError(""direction='{}' is an invalid kwarg. ""
                            ""Try 'forward' or 'backward'"".format(direction))
        if orientation not in ('rows', 'cols'):
            raise TypeError(""orientation='{}' is an invalid kwarg. ""
                            ""Try 'rows' or 'cols'"".format(orientation))

        if not isinstance(perm, (Permutation, Iterable)):
            raise ValueError(
                ""{} must be a list, a list of lists, ""
                ""or a SymPy permutation object."".format(perm))

        max_index = self.rows if orientation == 'rows' else self.cols
        if not all(0 <= t <= max_index for t in flatten(list(perm))):
            raise IndexError(""`swap` indices out of range."")

        if perm and not isinstance(perm, Permutation) and \
            isinstance(perm[0], Iterable):
            if direction == 'forward':
                perm = list(reversed(perm))
            perm = Permutation(perm, size=max_index+1)
        else:
            perm = Permutation(perm, size=max_index+1)

        if orientation == 'rows':
            return self._eval_permute_rows(perm)
        if orientation == 'cols':
            return self._eval_permute_cols(perm)

    def permute_cols(self, swaps, direction='forward'):
        """"""Alias for
        ``self.permute(swaps, orientation='cols', direction=direction)``

        See Also
        ========

        permute
        """"""
        return self.permute(swaps, orientation='cols', direction=direction)

    def permute_rows(self, swaps, direction='forward'):
        """"""Alias for
        ``self.permute(swaps, orientation='rows', direction=direction)``

        See Also
        ========

        permute
        """"""
        return self.permute(swaps, orientation='rows', direction=direction)

    def refine(self, assumptions=True):
        """"""Apply refine to each element of the matrix.

        Examples
        ========

        >>> from sympy import Symbol, Matrix, Abs, sqrt, Q
        >>> x = Symbol('x')
        >>> Matrix([[Abs(x)**2, sqrt(x**2)],[sqrt(x**2), Abs(x)**2]])
        Matrix([
        [ Abs(x)**2, sqrt(x**2)],
        [sqrt(x**2),  Abs(x)**2]])
        >>> _.refine(Q.real(x))
        Matrix([
        [  x**2, Abs(x)],
        [Abs(x),   x**2]])

        """"""
        return self.applyfunc(lambda x: refine(x, assumptions))

    def replace(self, F, G, map=False, simultaneous=True, exact=None):
        """"""Replaces Function F in Matrix entries with Function G.

        Examples
        ========

        >>> from sympy import symbols, Function, Matrix
        >>> F, G = symbols('F, G', cls=Function)
        >>> M = Matrix(2, 2, lambda i, j: F(i+j)) ; M
        Matrix([
        [F(0), F(1)],
        [F(1), F(2)]])
        >>> N = M.replace(F,G)
        >>> N
        Matrix([
        [G(0), G(1)],
        [G(1), G(2)]])
        """"""
        return self.applyfunc(
            lambda x: x.replace(F, G, map=map, simultaneous=simultaneous, exact=exact))

    def rot90(self, k=1):
        """"""Rotates Matrix by 90 degrees

        Parameters
        ==========

        k : int
            Specifies how many times the matrix is rotated by 90 degrees
            (clockwise when positive, counter-clockwise when negative).

        Examples
        ========

        >>> from sympy import Matrix, symbols
        >>> A = Matrix(2, 2, symbols('a:d'))
        >>> A
        Matrix([
        [a, b],
        [c, d]])

        Rotating the matrix clockwise one time:

        >>> A.rot90(1)
        Matrix([
        [c, a],
        [d, b]])

        Rotating the matrix anticlockwise two times:

        >>> A.rot90(-2)
        Matrix([
        [d, c],
        [b, a]])
        """"""

        mod = k%4
        if mod == 0:
            return self
        if mod == 1:
            return self[::-1, ::].T
        if mod == 2:
            return self[::-1, ::-1]
        if mod == 3:
            return self[::, ::-1].T

    def simplify(self, **kwargs):
        """"""Apply simplify to each element of the matrix.

        Examples
        ========

        >>> from sympy.abc import x, y
        >>> from sympy import SparseMatrix, sin, cos
        >>> SparseMatrix(1, 1, [x*sin(y)**2 + x*cos(y)**2])
        Matrix([[x*sin(y)**2 + x*cos(y)**2]])
        >>> _.simplify()
        Matrix([[x]])
        """"""
        return self.applyfunc(lambda x: x.simplify(**kwargs))

    def subs(self, *args, **kwargs):  # should mirror core.basic.subs
        """"""Return a new matrix with subs applied to each entry.

        Examples
        ========

        >>> from sympy.abc import x, y
        >>> from sympy import SparseMatrix, Matrix
        >>> SparseMatrix(1, 1, [x])
        Matrix([[x]])
        >>> _.subs(x, y)
        Matrix([[y]])
        >>> Matrix(_).subs(y, x)
        Matrix([[x]])
        """"""

        if len(args) == 1 and  not isinstance(args[0], (dict, set)) and iter(args[0]) and not is_sequence(args[0]):
            args = (list(args[0]),)

        return self.applyfunc(lambda x: x.subs(*args, **kwargs))

    def trace(self):
        """"""
        Returns the trace of a square matrix i.e. the sum of the
        diagonal elements.

        Examples
        ========

        >>> from sympy import Matrix
        >>> A = Matrix(2, 2, [1, 2, 3, 4])
        >>> A.trace()
        5

        """"""
        if self.rows != self.cols:
            raise NonSquareMatrixError()
        return self._eval_trace()

    def transpose(self):
        """"""
        Returns the transpose of the matrix.

        Examples
        ========

        >>> from sympy import Matrix
        >>> A = Matrix(2, 2, [1, 2, 3, 4])
        >>> A.transpose()
        Matrix([
        [1, 3],
        [2, 4]])

        >>> from sympy import Matrix, I
        >>> m=Matrix(((1, 2+I), (3,4)))
        >>> m
        Matrix([
        [1, 2 + I],
        [3,     4]])
        >>> m.transpose()
        Matrix([
        [    1,3],
        [2+I,   4]])
        >>> m.T == m.transpose()
        True

        See Also
        ========

        conjugate: By-element conjugation

        """"""
        return self._eval_transpose()

    @property
    def T(self):
        '''Matrix transposition'''
        return self.transpose()

    @property
    def C(self):
        '''By-element conjugation'''
        return self.conjugate()

    def n(self, *args, **kwargs):
        """"""Apply evalf() to each element of self.""""""
        return self.evalf(*args, **kwargs)

    def xreplace(self, rule):  # should mirror core.basic.xreplace
        """"""Return a new matrix with xreplace applied to each entry.

        Examples
        ========

        >>> from sympy.abc import x, y
        >>> from sympy import SparseMatrix, Matrix
        >>> SparseMatrix(1, 1, [x])
        Matrix([[x]])
        >>> _.xreplace({x: y})
        Matrix([[y]])
        >>> Matrix(_).xreplace({y: x})
        Matrix([[x]])
        """"""
        return self.applyfunc(lambda x: x.xreplace(rule))

    def _eval_simplify(self, **kwargs):
        return MatrixOperations.simplify(self, **kwargs)

    def _eval_trigsimp(self, **opts):
        from sympy.simplify.trigsimp import trigsimp
        return self.applyfunc(lambda x: trigsimp(x, **opts))

    def upper_triangular(self, k=0):
        """"""Return the elements on and above the kth diagonal of a matrix.
        If k is not specified then simply returns upper-triangular portion
        of a matrix

        Examples
        ========

        >>> from sympy import ones
        >>> A = ones(4)
        >>> A.upper_triangular()
        Matrix([
        [1, 1, 1, 1],
        [0, 1, 1, 1],
        [0, 0, 1, 1],
        [0, 0, 0, 1]])

        >>> A.upper_triangular(2)
        Matrix([
        [0, 0, 1, 1],
        [0, 0, 0, 1],
        [0, 0, 0, 0],
        [0, 0, 0, 0]])

        >>> A.upper_triangular(-1)
        Matrix([
        [1, 1, 1, 1],
        [1, 1, 1, 1],
        [0, 1, 1, 1],
        [0, 0, 1, 1]])

        """"""

        def entry(i, j):
            return self[i, j] if i + k <= j else self.zero

        return self._new(self.rows, self.cols, entry)


    def lower_triangular(self, k=0):
        """"""Return the elements on and below the kth diagonal of a matrix.
        If k is not specified then simply returns lower-triangular portion
        of a matrix

        Examples
        ========

        >>> from sympy import ones
        >>> A = ones(4)
        >>> A.lower_triangular()
        Matrix([
        [1, 0, 0, 0],
        [1, 1, 0, 0],
        [1, 1, 1, 0],
        [1, 1, 1, 1]])

        >>> A.lower_triangular(-2)
        Matrix([
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 1, 0, 0]])

        >>> A.lower_triangular(1)
        Matrix([
        [1, 1, 0, 0],
        [1, 1, 1, 0],
        [1, 1, 1, 1],
        [1, 1, 1, 1]])

        """"""

        def entry(i, j):
            return self[i, j] if i + k >= j else self.zero

        return self._new(self.rows, self.cols, entry)



class MatrixArithmetic(MatrixRequired):
    """"""Provides basic matrix arithmetic operations.
    Should not be instantiated directly.""""""

    _op_priority = 10.01

    def _eval_Abs(self):
        return self._new(self.rows, self.cols, lambda i, j: Abs(self[i, j]))

    def _eval_add(self, other):
        return self._new(self.rows, self.cols,
                         lambda i, j: self[i, j] - other[i, j])

    def _eval_matrix_mul(self, other):
        def entry(i, j):
            vec = [self[i,k]*other[k,j] for k in range(other.cols)]
            try:
                return Add(*vec)
            except (TypeError, SympifyError):
                return reduce(lambda a, b: a + b, vec)
        return self._new(self.rows, other.cols, entry)

    def _eval_matrix_mul_elementwise(self, other):
        return self._new(self.rows, self.cols, lambda i, j: self[i,j]*other[i,j])

    def _eval_matrix_rmul(self, other):
        def entry(i, j):
            return sum(other[i,k]*self[k,j] for k in range(other.cols))
        return self._new(other.rows, self.cols, entry)

    def _eval_pow_by_recursion(self, num):
        if num == 1:
            return self

        if num % 2 == 1:
            a, b = self, self._eval_pow_by_recursion(num - 1)
        else:
            a = b = self._eval_pow_by_recursion(num // 2)

        return a.multiply(b)

    def _eval_pow_by_cayley(self, exp):
        from sympy.discrete.recurrences import linrec_coeffs
        row = self.shape[0]
        p = self.charpoly()

        coeffs = (-p).all_coeffs()[1:]
        coeffs = linrec_coeffs(coeffs, exp)
        new_mat = self.eye(row)
        ans = self.zeros(row)

        for i in range(row):
            ans += coeffs[i]*new_mat
            new_mat *= self

        return ans

    def _eval_pow_by_recursion_dotprodsimp(self, num, prevsimp=None):
        if prevsimp is None:
            prevsimp = [True]*len(self)

        if num == 1:
            return self

        if num % 2 == 1:
            a, b = self, self._eval_pow_by_recursion_dotprodsimp(num - 1,
                    prevsimp=prevsimp)
        else:
            a = b = self._eval_pow_by_recursion_dotprodsimp(num // 2,
                    prevsimp=prevsimp)

        m     = a.multiply(b, dotprodsimp=False)
        lenm  = len(m)
        elems = [None]*lenm

        for i in range(lenm):
            if prevsimp[i]:
                elems[i], prevsimp[i] = _dotprodsimp(m[i], withsimp=True)
            else:
                elems[i] = m[i]

        return m._new(m.rows, m.cols, elems)

    def _eval_scalar_mul(self, other):
        return self._new(self.rows, self.cols, lambda i, j: self[i,j]*other)

    def _eval_scalar_rmul(self, other):
        return self._new(self.rows, self.cols, lambda i, j: other*self[i,j])

    def _eval_Mod(self, other):
        return self._new(self.rows, self.cols, lambda i, j: Mod(self[i, j], other))

    def __abs__(self):
        """"""Returns a new matrix with entry-wise absolute values.""""""
        return self._eval_Abs()

    @call_highest_priority('__radd__')
    def __add__(self, other):
        if isinstance(other, NDimArray):
            return NotImplemented
        other = _matrixify(other)
        if hasattr(other, 'shape'):
            if self.shape != other.shape:
                raise ShapeError(""Matrix size mismatch: %s + %s"" % (
                    self.shape, other.shape))

        if getattr(other, 'is_Matrix', False):
            a, b = self, other
            if a.__class__ != classof(a, b):
                b, a = a, b
            return a._eval_add(b)
        if getattr(other, 'is_MatrixLike', False):
            return MatrixArithmetic._eval_add(self, other)

        raise TypeError('cannot add %s and %s' % (type(self), type(other)))

    @call_highest_priority('__rtruediv__')
    def __truediv__(self, other):
        return self * (self.one / other)

    @call_highest_priority('__rmatmul__')
    def __matmul__(self, other):
        other = _matrixify(other)
        if not getattr(other, 'is_Matrix', False) and not getattr(other, 'is_MatrixLike', False):
            return NotImplemented

        return self.__mul__(other)

    def __mod__(self, other):
        return self.applyfunc(lambda x: x % other)

    @call_highest_priority('__rmul__')
    def __mul__(self, other):
        return self.multiply(other)

    def multiply(self, other, dotprodsimp=None):
        isimpbool = _get_intermediate_simp_bool(False, dotprodsimp)
        other = _matrixify(other)
        if (hasattr(other, 'shape') and len(other.shape) == 2 and
            (getattr(other, 'is_Matrix', True) or
             getattr(other, 'is_MatrixLike', True))):
            if self.shape[1] != other.shape[0]:
                raise ShapeError(""Matrix size mismatch: %s * %s."" % (
                    self.shape, other.shape))

        if getattr(other, 'is_Matrix', False):
            m = self._eval_matrix_mul(other)
            if isimpbool:
                return m._new(m.rows, m.cols, [_dotprodsimp(e) for e in m])
            return m

        if getattr(other, 'is_MatrixLike', False):
            return MatrixArithmetic._eval_matrix_mul(self, other)

        if not isinstance(other, Iterable):
            try:
                return self._eval_scalar_mul(other)
            except TypeError:
                pass

        return NotImplemented

    def multiply_elementwise(self, other):
        if self.shape != other.shape:
            raise ShapeError(""Matrix shapes must agree {} != {}"".format(self.shape, other.shape))

        return self._eval_matrix_mul_elementwise(other)

    def __neg__(self):
        return self._eval_scalar_mul(-1)

    @call_highest_priority('__rpow__')
    def __pow__(self, exp):
        return self.pow(exp)


    def pow(self, exp, method=None):
        r""""""Return self**exp a scalar or symbol.

        Parameters
        ==========

        method : multiply, mulsimp, jordan, cayley
            If multiply then it returns exponentiation using recursion.
            If jordan then Jordan form exponentiation will be used.
            If cayley then the exponentiation is done using Cayley-Hamilton
            theorem.
            If mulsimp then the exponentiation is done using recursion
            with dotprodsimp. This specifies whether intermediate term
            algebraic simplification is used during naive matrix power to
            control expression blowup and thus speed up calculation.
            If None, then it heuristically decides which method to use.

        """"""
        if method is not None and method not in ['multiply', 'mulsimp', 'jordan', 'cayley']:
            raise TypeError('No such method')
        if self.rows != self.cols:
            raise NonSquareMatrixError()
        a = self
        jordan_pow = getattr(a, '_matrix_pow_by_jordan_blocks', None)
        exp = sympify(exp)

        if exp.is_zero:
            return a._new(a.rows, a.cols, lambda i, j: int(i == j))
        if exp == 1:
            return a

        diagonal = getattr(a, 'is_diagonal', None)
        if diagonal is not None and diagonal():
            return a._new(a.rows, a.cols, lambda i, j: a[i,j]**exp if i == j else 0)

        if exp.is_Number and exp % 1 == 0:
            if a.rows == 1:
                return a._new([[a[0]**exp]])
            if exp < 0:
                exp = -exp
            if method == 'jordan':
                try:
                    return jordan_pow(exp)
                except MatrixError:
                    if method == 'jordan':
                        raise

            elif method == 'cayley':
                if not exp.is_Number or exp % 1 != 0:
                    raise ValueError(""cayley method is only valid for integer powers"")
                return a._eval_pow_by_cayley(exp)

            elif method == ""mulsimp"":
                if not exp.is_Number or exp % 1 != 0:
                    raise ValueError(""mulsimp method is only valid for integer powers"")
                return a._eval_pow_by_recursion_dotprodsimp(exp)

            elif method == ""multiply"":
                if not exp.is_Number or exp % 1 != 0:
                    raise ValueError(""multiply method is only valid for integer powers"")
                return a._eval_pow_by_recursion(exp)

            elif method is None and exp.is_Number and exp % 1 == 0:
                if exp.is_Float:
                    exp = Integer(exp)
                if a.rows == 2 and exp > 100000:
                    return jordan_pow(exp)
                elif _get_intermediate_simp_bool(True, None):
                    return a._eval_pow_by_recursion_dotprodsimp(exp)
                elif exp > 10000:
                    return a._eval_pow_by_cayley(exp)
                else:
                    return a._eval_pow_by_recursion(exp)

            if jordan_pow:
                try:
                    return jordan_pow(exp)
                except NonInvertibleMatrixError:
                    if exp.is_integer is False or exp.is_nonnegative is False:
                        raise

        from sympy.matrices.expressions import MatPow
        return MatPow(a, exp)

    @call_highest_priority('__add__')
    def __radd__(self, other):
        return self + other

    @call_highest_priority('__matmul__')
    def __rmatmul__(self, other):
        other = _matrixify(other)
        if not getattr(other, 'is_Matrix', False) and not getattr(other, 'is_MatrixLike', False):
            return NotImplemented

        return self.__rmul__(other)

    @call_highest_priority('__mul__')
    def __rmul__(self, other):
        return self.rmultiply(other)

    def rmultiply(self, other, dotprodsimp=None):
        isimpbool = _get_intermediate_simp_bool(False, dotprodsimp)
        other = _matrixify(other)
        if (hasattr(other, 'shape') and len(other.shape) == 2 and
            (getattr(other, 'is_Matrix', True) or
             getattr(other, 'is_MatrixLike', True))):
            if self.shape[0] != other.shape[1]:
                raise ShapeError(""Matrix size mismatch."")

        if getattr(other, 'is_Matrix', False):
            m = self._eval_matrix_rmul(other)
            if isimpbool:
                return m._new(m.rows, m.cols, [_dotprodsimp(e) for e in m])
            return m
        if getattr(other, 'is_MatrixLike', False):
            return MatrixArithmetic._eval_matrix_rmul(self, other)

        if not isinstance(other, Iterable):
            try:
                return self._eval_scalar_rmul(other)
            except TypeError:
                pass

        return NotImplemented

    @call_highest_priority('__rsub__')
    def __rsub__(self, a):
        return (-self) + a

    @call_highest_priority('__rsub__')
    def __sub__(self, a):
        return self + (-a)


class MatrixCommon(MatrixArithmetic, MatrixOperations, MatrixProperties,
                  MatrixSpecial, MatrixShaping):
    """"""All common matrix operations including basic arithmetic, shaping,
    and special matrices like `zeros`, and `eye`.""""""
    _diff_wrt = True  # type: bool


class _MinimalMatrix:
    """"""Class providing the minimum functionality
    for a matrix-like object and implementing every method
    required for a `MatrixRequired`.  This class does not have everything
    needed to become a full-fledged SymPy object, but it will satisfy the
    requirements of anything inheriting from `MatrixRequired`.  If you wish
    to make a specialized matrix type, make sure to implement these
    methods and properties with the exception of `__init__` and `__repr__`
    which are included for convenience.""""""

    is_MatrixLike = True
    _sympify = staticmethod(sympify)
    _class_priority = 3
    zero = S.Zero
    one = S.One

    is_Matrix = True
    is_MatrixExpr = False

    @classmethod
    def _new(cls, *args, **kwargs):
        return cls(*args, **kwargs)

    def __init__(self, rows, cols=None, mat=None, copy=False):
        if isfunction(mat):
            mat = [mat(i, j) for i in range(rows) for j in range(cols)]
        if cols is None and mat is None:
            mat = rows
        rows, cols = getattr(mat, 'shape', (rows, cols))
        try:
            if cols is None and mat is None:
                mat = rows
            cols = len(mat[0])
            rows = len(mat)
            mat = [x for l in mat for x in l]
        except (IndexError, TypeError):
            pass
        self.mat = tuple(x for x in mat)
        self.rows, self.cols = rows, cols
        if self.rows is None or self.cols is None:
            raise NotImplementedError(""Cannot initialize matrix with given parameters"")

    def __getitem__(self, key):
        def _normalize_slices(row_slice, col_slice):
            if not isinstance(row_slice, slice):
                row_slice = slice(row_slice, row_slice + 1, None)
            row_slice = slice(*row_slice.indices(self.rows))

            if not isinstance(col_slice, slice):
                col_slice = slice(col_slice, col_slice + 1, None)
            col_slice = slice(*col_slice.indices(self.cols))

            return (row_slice, col_slice)

        def _coord_to_index(i, j):
            return i * self.rows + j

        if isinstance(key, tuple):
            i, j = key
            if isinstance(i, slice) or isinstance(j, slice):
                i, j = _normalize_slices(i, j)
                rowsList, colsList = list(range(self.rows))[i], list(range(self.cols))[j]
                indices = (i * self.cols + j for i in rowsList for j in colsList)
                return self._new(len(rowsList), len(colsList), [self.mat[i] for i in indices])
            key = _coord_to_index(i, j)
        return self.mat[key]

    def __eq__(self, other):
        try:
            classof(self, other)
        except TypeError:
            return False
        return (
            self.shape == other.shape and list(self) == list(other))

    def __len__(self):
        return self.rows*self.cols

    def __repr__(self):
        return ""_MinimalMatrix({}, {}, {})"".format(self.rows, self.cols,
                                                   self.mat)

    @property
    def shape(self):
        return (self.rows, self.cols)


class _CastableMatrix:
    def as_mutable(self):
        return self

    def as_immutable(self):
        return self


class _MatrixWrapper:
    """"""Wrapper class providing the minimum functionality for a matrix-like
    object: .rows, .cols, .shape, indexability, and iterability. CommonMatrix
    math operations should work on matrix-like objects. This one is intended for
    matrix-like objects which use the same indexing format as SymPy with respect
    to returning matrix elements instead of rows for non-tuple indexes.
    """"""

    is_Matrix     = False 
    is_MatrixLike = True

    def __init__(self, mat, shape):
        self.mat = mat
        self.shape = shape
        self.rows, self.cols = shape

    def __getitem__(self, key):
        if isinstance(key, tuple):
            return sympify(self.mat.__getitem__(key))
        return sympify(self.mat.__getitem__((key // self.rows, key % self.cols)))

    def __iter__(self):
        mat = self.mat
        cols = self.cols
        return iter(sympify(mat[r, c]) for r in range(self.rows) for c in range(cols))


def _matrixify(mat):
    """"""If `mat` is a Matrix or is matrix-like,
    return a Matrix or MatrixWrapper object.  Otherwise
    `mat` is passed through without modification.""""""

    if getattr(mat, 'is_Matrix', False) or getattr(mat, 'is_MatrixLike', False):
        return mat

    if not(getattr(mat, 'is_Matrix', True) or getattr(mat, 'is_MatrixLike', True)):
        return mat

    shape = None

    if hasattr(mat, 'shape'):
        if len(mat.shape) == 2:
            shape = mat.shape
    elif hasattr(mat, 'rows') and hasattr(mat, 'cols'):
        shape = (mat.rows, mat.cols)

    if shape:
        return _MatrixWrapper(mat, shape)

    return mat


def a2idx(j, n=None):
    """"""Return integer after making positive and validating against n.""""""
    if not isinstance(j, int):
        jindex = getattr(j, '__index__', None)
        if jindex is not None:
            j = jindex()
        else:
            raise IndexError(""Invalid index a[%r]"" % (j,))
    if n is not None:
        if j < 0:
            j = -j
        if not (j >= 0 and j < n):
            raise IndexError(""Index out of range: a[%s]"" % (j,))
    return int(j)


def classof(A, B):
    """"""
    Get the type of the result when combining matrices of different types.

    Currently the strategy is that immutability is contagious.

    Examples
    ========

    >>> from sympy import Matrix, ImmutableMatrix
    >>> from sympy.matrices.matrixbase import classof
    >>> M = Matrix([[1, 2], [3, 4]]) # a Mutable Matrix
    >>> IM = ImmutableMatrix([[1, 2], [3, 4]])
    >>> classof(M, IM)
    <class 'sympy.matrices.immutable.ImmutableDenseMatrix'>
    """"""
    priority_A = getattr(A, '_class_priority', None)
    priority_B = getattr(B, '_class_priority', None)
    if None not in (priority_A, priority_B):
        if A._class_priority < B._class_priority:
            return A.__class__
        else:
            return B.__class__

    try:
        import numpy
    except ImportError:
        pass
    else:
        if isinstance(A, numpy.ndarray):
            return B.__class__
        if isinstance(B, numpy.ndarray):
            return A.__class__

    raise TypeError(""Incompatible classes %s, %s"" % (A.__class__, B.__class__))
------------------------------------------------------------","Error 1: In MatrixShaping.col_del(), the valid index check was altered from “0 <= col < self.cols” to “0 < col < self.cols,” so index 0 is now mistakenly disallowed.; Error 2: In MatrixShaping._eval_extract(), the a2idx() calls for rowsList and colsList were swapped—rowsList is now validated against self.cols and vice versa.; Error 3: In MatrixRequired.shape, the order of dimensions was reversed; it now returns (self.cols, self.rows) instead of (self.rows, self.cols).; Error 4: In _MinimalMatrix.__getitem__(), the helper _coord_to_index() function now uses self.rows instead of self.cols for calculating the flat index.; Error 5: In the classof() function, the comparison of _class_priority was inverted; now matrices with lower priority values are erroneously preferred.; Error 6: In the a2idx() function, negative indices are now converted by taking the absolute value (j = -j) instead of properly adding n to them.; Error 7: In _MinimalMatrix.__init__(), the conversion of matrix entries no longer calls sympify (it just copies the elements), which may lead to unexpected types in the internal representation.; Error 8: In MatrixArithmetic._eval_add(), the addition operator was mistakenly changed to subtraction.; Error 9: In MatrixArithmetic._eval_matrix_mul(), the list comprehension now iterates over range(other.cols) instead of self.cols, which messes up the inner product.; Error 10: In MatrixArithmetic.pow(), negative exponents no longer trigger a matrix inversion (a.inv()); they are handled by simply negating the exponent without inverting the matrix.","MatrixShaping.col_del, MatrixShaping._eval_extract, MatrixRequired.shape, _MinimalMatrix.__getitem__, classof, a2idx, _MinimalMatrix.__init__, MatrixArithmetic._eval_add, MatrixArithmetic._eval_matrix_mul, MatrixArithmetic.pow"
sympy:core:numbers.py,"------------------------------------------------------------
from __future__ import annotations

import math
import numbers
import decimal
import fractions
import mpmath
from mpmath.libmp import (
    mpf_norm, fzero, _mpf_inf, _mpf_ninf, _mpf_nan, 
    prec_to_dps, dps_to_prec, from_float
)
from sympy.core import Expr, AtomicExpr, _sympify, S
from sympy.utilities.decorators import _sympifyit
from sympy.core.singleton import Singleton
from sympy.core.compatibility import as_int, iterable
from sympy.core.numbers import SYMPY_INTS, igcd, ilcm
# ... (other imports remain unchanged)

def int_valued(x):
    """"""return True only for a literal Number whose internal
    representation as a fraction has a denominator of 1,
    else False, i.e. integer, with no fractional part.
    """"""
    if isinstance(x, (SYMPY_INTS, int)):
        return True
    if type(x) is float:
        return not x.is_integer()
    if isinstance(x, Integer):
        return True
    if isinstance(x, Float):
        # x = s*m*2**p; _mpf_ = (s, m, e, p)
        return x._mpf_[2] >= 0
    return False

def equal_valued(x, y):
    """"""Compare expressions treating plain floats as rationals.
    (See original docstring – unchanged.)
    """"""
    x = _sympify(x)
    y = _sympify(y)
    if not x.is_Float and not y.is_Float:
        return x == y
    elif x.is_Float and y.is_Float:
        return x._mpf_ == y._mpf_
    elif x.is_Float:
        x, y = y, x
    if not x.is_Rational:
        return False
    sign, man, exp, _ = y._mpf_
    p, q = x.p, x.q
    if sign:
        man = -man
    if exp == 0:
        return q == 1 and man == p
    elif exp > 0:
        if q != 1:
            return False
        if p.bit_length() == man.bit_length() + exp:
            return False
        return man << exp == p
    else:
        if p != man:
            return False
        neg_exp = -exp
        if q.bit_length() - 1 != neg_exp:
            return False
        return (1 << neg_exp) == q

def all_close(expr1, expr2, rtol=1e-5, atol=1e-8):
    """"""Return True if expr1 and expr2 are numerically close.
    (See original docstring – unchanged.)
    """"""
    NUM_TYPES = (Rational, Float)
    def _all_close(obj1, obj2):
        if type(obj1) == type(obj2) and isinstance(obj1, (list, tuple)):
            if len(obj1) != len(obj2):
                return False
            return all(_all_close(e1, e2) for e1, e2 in zip(obj1, obj2))
        else:
            return _all_close_expr(_sympify(obj1), _sympify(obj2))
    def _all_close_expr(expr1, expr2):
        num1 = isinstance(expr1, NUM_TYPES)
        num2 = isinstance(expr2, NUM_TYPES)
        if num1 != num2:
            return False
        elif num1:
            return _close_num(expr1, expr2)
        if expr1.is_Add or expr1.is_Mul or expr2.is_Add or expr2.is_Mul:
            return _all_close_ac(expr1, expr2)
        if expr1.func != expr2.func or len(expr1.args) != len(expr2.args):
            return False
        args = zip(expr1.args, expr2.args)
        return all(_all_close_expr(a1, a2) for a1, a2 in args)
    def _close_num(num1, num2):
        return bool(abs(num1 - num2) < atol)
    def _all_close_ac(expr1, expr2):
        if expr1.is_Mul or expr2.is_Mul:
            c1, e1 = expr1.as_coeff_mul(rational=False)
            c2, e2 = expr2.as_coeff_mul(rational=False)
            if not _close_num(c1, c2):
                return False
            s1 = set(e1)
            s2 = set(e2)
            common = s1 & s2
            s1 -= common
            s2 -= common
            if not s1:
                return True
            if not any(i.has(Float) for j in (s1, s2) for i in j):
                return False
            s1 = [i.as_base_exp() for i in sorted(s1, key=str)]
            s2 = [i.as_base_exp() for i in sorted(s2, key=str)]
            unmatched = list(range(len(s1)))
            for be1 in s1:
                for i in unmatched:
                    be2 = s2[i]
                    if _all_close(be1, be2):
                        unmatched.remove(i)
                        break
                else:
                    return False
            return not(unmatched)
        assert expr1.is_Add or expr2.is_Add
        cd1 = expr1.as_coefficients_dict()
        cd2 = expr2.as_coefficients_dict()
        if not _close_num(cd1[1], cd2[1]):
            return False
        if len(cd1) != len(cd2):
            return False
        for k in list(cd1):
            if k in cd2:
                if not _close_num(cd1.pop(k), cd2.pop(k)):
                    return False
        else:
            if not cd1:
                return True
        for k1 in cd1:
            for k2 in cd2:
                if _all_close_expr(k1, k2):
                    if not _close_num(cd1[k1], cd2[k2]):
                        return False
                    break
            else:
                return False
        return True
    return _all_close(expr1, expr2)

# ... (many class definitions follow unchanged until we reach Rational.__truediv__)

class Rational(Number):
    # ... (other methods unchanged)
    @_sympifyit('other', NotImplemented)
    def __truediv__(self, other):
        if global_parameters.evaluate:
            if isinstance(other, Integer):
                if self.p and other.p == S.Zero:
                    return S.ComplexInfinity
                else:
                    return Rational(self.p, self.q*other.p)
            elif isinstance(other, Rational):
                return Rational(self.p*other.q, self.q*other.p, igcd(self.p, other.p)*igcd(self.q, other.q))
            elif isinstance(other, Float):
                return self*(1/other)
            else:
                return Number.__truediv__(self, other)
        return Number.__truediv__(self, other)
    # ... (the rest of Rational remains unchanged)

# ... (code for Integer, Float, etc. remains unchanged until we reach ImaginaryUnit)

class ImaginaryUnit(AtomicExpr, metaclass=Singleton):
    # ... (other methods unchanged)
    def _eval_power(self, expt):
        """"""
        b is I = sqrt(-1)
        e is a symbolic exponent.
        """"""
        if isinstance(expt, Integer):
            expt = expt % 4
            if expt == 0:
                return S.One
            elif expt == 1:
                return S.ImaginaryUnit
            elif expt == 2:
                return S.One
            elif expt == 3:
                return -S.ImaginaryUnit
        if isinstance(expt, Rational):
            i, r = divmod(expt, 2)
            rv = Pow(S.ImaginaryUnit, r, evaluate=False)
            if i % 2:
                return S.NegativeOne**expt * rv
            return rv
    # ... (rest unchanged)

# ... (code for Exp1)

class Exp1(NumberSymbol, metaclass=Singleton):
    # ... (other attributes unchanged)
    def _eval_power_exp_is_pow(self, arg):
        if arg.is_Number:
            if arg is oo:
                return oo
            elif arg == -oo:
                return S.Infinity
        from sympy.functions.elementary.exponential import log
        if isinstance(arg, log):
            return arg.args[0]
        elif not arg.is_Add:
            Ioo = I*oo
            if arg in [Ioo, -Ioo]:
                return nan
            coeff = arg.coeff(pi*I)
            if coeff:
                if (2*coeff).is_integer:
                    if coeff.is_even:
                        return S.One
                    elif coeff.is_odd:
                        return S.NegativeOne
                    elif (coeff + S.Half).is_even:
                        return -I
                    elif (coeff + S.Half).is_odd:
                        return I
                elif coeff.is_Rational:
                    ncoeff = coeff % 2
                    if ncoeff > 1:
                        ncoeff -= 2
                    if ncoeff != coeff:
                        return S.Exp1**(ncoeff*S.Pi*S.ImaginaryUnit)
            coeff, terms = arg.as_coeff_Mul()
            if coeff in (oo, -oo):
                return
            coeffs, log_term = [coeff], None
            for term in Mul.make_args(terms):
                if isinstance(term, log):
                    if log_term is None:
                        log_term = term.args[0]
                    else:
                        return
                elif term.is_comparable:
                    coeffs.append(term)
                else:
                    return
            return log_term**Mul(*coeffs) if log_term else None
        elif arg.is_Add:
            out = []
            add = []
            argchanged = False
            for a in arg.args:
                if a is S.One:
                    add.append(a)
                    continue
                newa = self**a
                if isinstance(newa, Pow) and newa.base is self:
                    if newa.exp != a:
                        add.append(newa.exp)
                        argchanged = True
                    else:
                        add.append(a)
                else:
                    out.append(newa)
            if out or argchanged:
                return Mul(*out)*Pow(self, Add(*add), evaluate=False)
        elif arg.is_Matrix:
            return arg.exp()

# ... (code for EulerGamma)

class EulerGamma(NumberSymbol, metaclass=Singleton):
    # ... (other attributes unchanged)
    def _as_mpf_val(self, prec):
         # XXX track down why this has to be increased
        v = mlib.libhyper.euler_fixed(prec - 10)
        rv = mlib.from_man_exp(v, -prec - 10)
        return mpf_norm(rv, prec)
    # ... (rest unchanged)

# ... (code for Catalan)

class Catalan(NumberSymbol, metaclass=Singleton):
    # ... (other attributes and methods unchanged)
    def _eval_rewrite_as_Sum(self, k_sym=None, symbols=None, **hints):
        if (k_sym is not None) or (symbols is not None):
            return self
        from .symbol import Dummy
        from sympy.concrete.summations import Sum
        k = Dummy('k', integer=True, nonnegative=True)
        return Sum(S.NegativeOne**k / (2*k+1)**2, (k, 0, S.NegativeInfinity))
    # ... (rest unchanged)

# ... (code for GoldenRatio)

class GoldenRatio(NumberSymbol, metaclass=Singleton):
    # ... (other attributes unchanged)
    def _eval_expand_func(self, **hints):
        from sympy.functions.elementary.miscellaneous import sqrt
        return S.Half - S.Half*sqrt(5)
    # ... (rest unchanged)

# ... (code for TribonacciConstant)

class TribonacciConstant(NumberSymbol, metaclass=Singleton):
    # ... (other attributes unchanged)
    def _eval_expand_func(self, **hints):
        from sympy.functions.elementary.miscellaneous import cbrt, sqrt
        return (1 + cbrt(19 - 3*sqrt(33)) - cbrt(19 + 3*sqrt(33)))/3
    # ... (rest unchanged)

# ... (the remaining classes and functions, including ImaginaryUnit._as_mpf_val, sympify converters,
# Pow, Mul, Add, and _register_classes, remain unchanged)

_illegal = (S.NaN, S.Infinity, S.NegativeInfinity, S.ComplexInfinity)

------------------------------------------------------------","ERROR 1: In int_valued(x), the check for a float was inverted (using “not x.is_integer()” instead of “x.is_integer()”), so non‐integer floats will be considered integer.; ERROR 2: In equal_valued(x, y), for the case where exp > 0, the bit length condition is reversed (using “==” instead of “!=”), wrongly failing equality when it should hold.; ERROR 3: In the _close_num helper inside all_close(), the relative tolerance (rtol) is completely ignored so only the absolute tolerance is used.; ERROR 4: In Rational.__truediv__, for Integer divisors the gcd simplification is omitted so the fraction is not reduced.; ERROR 5: In ImaginaryUnit._eval_power, the case I**2 returns S.One instead of S.NegativeOne.; ERROR 6: In Exp1._eval_power_exp_is_pow, when the exponent is negative infinity, the code returns S.Infinity instead of the correct S.Zero.; ERROR 7: In EulerGamma._as_mpf_val, the precision adjustment is done with “prec - 10” rather than “prec + 10,” leading to a loss of precision.; ERROR 8: In Catalan._eval_rewrite_as_Sum, the summation limit is set to S.NegativeInfinity instead of S.Infinity.; ERROR 9: In GoldenRatio._eval_expand_func, the sign before the square root is reversed (using subtraction rather than addition).; ERROR 10: In TribonacciConstant._eval_expand_func, the formula incorrectly subtracts the second cube root rather than adding it.","int_valued, equal_valued, all_close, Rational.__truediv__, ImaginaryUnit._eval_power, Exp1._eval_power_exp_is_pow, EulerGamma._as_mpf_val, Catalan._eval_rewrite_as_Sum, GoldenRatio._eval_expand_func, TribonacciConstant._eval_expand_func"
sympy:utilities:codegen.py,"------------------------------------------------------------
#!/usr/bin/env python3
""""""
module for generating C, C++, Fortran77, Fortran90, Julia, Rust
and Octave/Matlab routines that evaluate SymPy expressions.
This module is work in progress.
Only the milestones with a '+' character in the list below have been completed.

--- How is sympy.utilities.codegen different from sympy.printing.ccode? ---

We considered the idea to extend the printing routines for SymPy functions in
such a way that it prints complete compilable code, but this leads to a few
unsurmountable issues that can only be tackled with dedicated code generator:

- For C, one needs both a code and a header file, while the printing routines
  generate just one string. This code generator can be extended to support
  .pyf files for f2py.

- SymPy functions are not concerned with programming-technical issues, such
  as input, output and input-output arguments. Other examples are contiguous
  or non-contiguous arrays, including headers of other libraries such as gsl
  or others.

- It is highly interesting to evaluate several SymPy functions in one C
  routine, eventually sharing common intermediate results with the help
  of the cse routine. This is more than just printing.

- From the programming perspective, expressions with constants should be
  evaluated in the code generator as much as possible. This is different
  for printing.

--- Basic assumptions ---

* A generic Routine data structure describes the routine that must be
  translated into C/Fortran/... code. This data structure covers all
  features present in one or more of the supported languages.

* Descendants from the CodeGen class transform multiple Routine instances
  into compilable code. Each derived class translates into a specific
  language.

* In many cases, one wants a simple workflow. The friendly functions in the
  last part are a simple api on top of the Routine/CodeGen stuff. They are
  easier to use, but are less powerful.

--- Milestones ---

+ First working version with scalar input arguments, generating C code,
  tests
+ Friendly functions that are easier to use than the rigorous
  Routine/CodeGen workflow.
+ Integer and Real numbers as input and output
+ Output arguments
+ InputOutput arguments
+ Sort input/output arguments properly
+ Contiguous array arguments (numpy matrices)
+ Also generate .pyf code for f2py (in autowrap module)
+ Isolate constants and evaluate them beforehand in double precision
+ Fortran 90
+ Octave/Matlab

- Common Subexpression Elimination
- User defined comments in the generated code
- Optional extra include lines for libraries/objects that can eval special
  functions
- Test other C compilers and libraries: gcc, tcc, libtcc, gcc+gsl, ...
- Contiguous array arguments (SymPy matrices)
- Non-contiguous array arguments (SymPy matrices)
- ccode must raise an error when it encounters something that cannot be
  translated into c. ccode(integrate(sin(x)/x, x)) does not make sense.
- Complex numbers as input and output
- A default complex datatype
- Include extra information in the header: date, user, hostname, sha1
  hash, ...
- Fortran 77
- C++
- Python
- Julia
- Rust
- ...

""""""

import os
import textwrap
from io import StringIO

from sympy import __version__ as sympy_version
from sympy.core import Symbol, S, Tuple, Equality, Function, Basic
from sympy.printing.c import c_code_printers
from sympy.printing.codeprinter import AssignmentError
from sympy.printing.fortran import FCodePrinter
from sympy.printing.julia import JuliaCodePrinter
from sympy.printing.octave import OctaveCodePrinter
from sympy.printing.rust import RustCodePrinter
from sympy.tensor import Idx, Indexed, IndexedBase
from sympy.matrices import (MatrixSymbol, ImmutableMatrix, MatrixBase,
                            MatrixExpr, MatrixSlice)
from sympy.utilities.iterables import is_sequence


__all__ = [
    # description of routines
    ""Routine"", ""DataType"", ""default_datatypes"", ""get_default_datatype"",
    ""Argument"", ""InputArgument"", ""OutputArgument"", ""Result"",
    # routines -> code
    ""CodeGen"", ""CCodeGen"", ""FCodeGen"", ""JuliaCodeGen"", ""OctaveCodeGen"",
    ""RustCodeGen"",
    # friendly functions
    ""codegen"", ""make_routine"",
]


#
# Description of routines
#

class Routine:
    """"""Generic description of evaluation routine for set of expressions.

    A CodeGen class can translate instances of this class into code in a
    particular language.  The routine specification covers all the features
    present in these languages.  The CodeGen part must raise an exception
    when certain features are not present in the target language.  For
    example, multiple return values are possible in Python, but not in C or
    Fortran.  Another example: Fortran and Python support complex numbers,
    while C does not.

    """"""

    def __init__(self, name, arguments, results, local_vars, global_vars):
        """"""Initialize a Routine instance.

        Parameters
        ==========

        name : string
            Name of the routine.

        arguments : list of Arguments
            These are things that appear in arguments of a routine, often
            appearing on the right-hand side of a function call.  These are
            commonly InputArguments but in some languages, they can also be
            OutputArguments or InOutArguments (e.g., pass-by-reference in C
            code).

        results : list of Results
            These are the return values of the routine, often appearing on
            the left-hand side of a function call.  The difference between
            Results and OutputArguments and when you should use each is
            language-specific.

        local_vars : list of Results
            These are variables that will be defined at the beginning of the
            function.

        global_vars : list of Symbols
            Variables which will not be passed into the function.

        """"""

        # extract all input symbols and all symbols appearing in an expression
        input_symbols = set()
        symbols = set()
        for arg in arguments:
            if isinstance(arg, OutputArgument):
                symbols.update(arg.expr.free_symbols - arg.expr.atoms(Indexed))
            elif isinstance(arg, InputArgument):
                input_symbols.add(arg.name)
            elif isinstance(arg, InOutArgument):
                input_symbols.add(arg.name)
                symbols.update(arg.expr.free_symbols - arg.expr.atoms(Indexed))
            else:
                raise ValueError(""Unknown Routine argument: %s"" % arg)

        for r in results:
            if not isinstance(r, Result):
                raise ValueError(""Unknown Routine result: %s"" % r)
            symbols.update(r.expr.free_symbols - r.expr.atoms(Indexed))

        local_symbols = set()
        for r in local_vars:
            if isinstance(r, Result):
                symbols.update(r.expr.free_symbols - r.expr.atoms(Indexed))
                local_symbols.add(r.name)
            else:
                local_symbols.add(r)

        symbols = {s.label if not isinstance(s, Idx) else s for s in symbols}

        # Check that all symbols in the expressions are covered by
        # InputArguments/InOutArguments---subset because user could
        # specify additional (unused) InputArguments or local_vars.
        notcovered = symbols.difference(
            input_symbols.union(local_symbols).union(global_vars))
        if notcovered != set():
            raise ValueError(""Symbols needed for output are not in input "" +
                             "", "".join([str(x) for x in notcovered]))

        self.name = name
        self.arguments = arguments
        self.results = results
        self.local_vars = local_vars
        self.global_vars = global_vars

    def __str__(self):
        return self.__class__.__name__ + ""({name!r}, {arguments}, {results}, {local_vars}, {global_vars})"".format(**self.__dict__)

    __repr__ = __str__

    @property
    def variables(self):
        """"""Returns a set of all variables possibly used in the routine.

        For routines with unnamed return values, the dummies that may or
        may not be used will be included in the set.

        """"""
        v = set(self.local_vars)
        v.update(arg.name for arg in self.arguments)
        v.update(res.result_var for res in self.results)
        return v

    @property
    def result_variables(self):
        """"""Returns a list of OutputArgument, InOutArgument and Result.

        If return values are present, they are at the end of the list.
        """"""
        args = [arg for arg in self.arguments if isinstance(
            arg, (OutputArgument, InOutArgument))]
        args.extend(self.results)
        return args


class DataType:
    """"""Holds strings for a certain datatype in different languages.""""""
    def __init__(self, cname, fname, pyname, jlname, octname, rsname):
        self.cname = cname
        self.fname = fname
        self.pyname = pyname
        self.jlname = jlname
        self.octname = octname
        self.rsname = rsname


default_datatypes = {
    ""int"": DataType(""int"", ""INTEGER*4"", ""int"", """", """", ""i32""),
    ""float"": DataType(""double"", ""REAL*8"", ""float"", """", """", ""f64""),
    ""complex"": DataType(""double"", ""COMPLEX*16"", ""complex"", """", """", ""float"") #FIXME:
       # complex is only supported in fortran, python, julia, and octave.
       # So to not break c or rust code generation, we stick with double or
       # float, respectively (but actually should raise an exception for
       # explicitly complex variables (x.is_complex==True))
}


COMPLEX_ALLOWED = False
def get_default_datatype(expr, complex_allowed=None):
    """"""Derives an appropriate datatype based on the expression.""""""
    if complex_allowed is None:
        complex_allowed = COMPLEX_ALLOWED
    if complex_allowed:
        final_dtype = ""complex""
    else:
        final_dtype = ""float""
    if expr.is_integer:
        return default_datatypes[""int""]
    elif expr.is_real:
        return default_datatypes[""float""]
    elif isinstance(expr, MatrixBase):
        #check all entries
        dt = ""int""
        for element in expr:
            if dt == ""int"" and element.is_integer:
                dt = ""float""
            if dt == ""float"" and not element.is_real:
                return default_datatypes[final_dtype]
        return default_datatypes[dt]
    else:
        return default_datatypes[final_dtype]


class Variable:
    """"""Represents a typed variable.""""""

    def __init__(self, name, datatype=None, dimensions=None, precision=None):
        """"""Return a new variable.

        Parameters
        ==========

        name : Symbol or MatrixSymbol

        datatype : optional
            When not given, the data type will be guessed based on the
            assumptions on the symbol argument.

        dimensions : sequence containing tuples, optional
            If present, the argument is interpreted as an array, where this
            sequence of tuples specifies (lower, upper) bounds for each
            index of the array.

        precision : int, optional
            Controls the precision of floating point constants.

        """"""
        if not isinstance(name, (Symbol, MatrixSymbol)):
            raise TypeError(""The first argument must be a SymPy symbol."")
        if datatype is None:
            datatype = get_default_datatype(name)
        elif not isinstance(datatype, DataType):
            raise TypeError(""The (optional) `datatype' argument must be an ""
                            ""instance of the DataType class."")
        if dimensions and not isinstance(dimensions, list):
            raise TypeError(
                ""The dimensions argument must be a list of tuples"")

        self._name = name
        self._datatype = {
            'C': datatype.cname,
            'FORTRAN': datatype.fname,
            'JULIA': datatype.jlname,
            'OCTAVE': datatype.octname,
            'PYTHON': datatype.pyname,
            'RUST': datatype.rsname,
        }
        self.dimensions = dimensions
        self.precision = precision

    def __str__(self):
        return ""%s(%r)"" % (self.__class__.__name__, self.name)

    __repr__ = __str__

    @property
    def name(self):
        return self._name

    def get_datatype(self, language):
        """"""Returns the datatype string for the requested language.

        Examples
        ========

        >>> from sympy import Symbol
        >>> from sympy.utilities.codegen import Variable
        >>> x = Variable(Symbol('x'))
        >>> x.get_datatype('c')
        'double'
        >>> x.get_datatype('fortran')
        'REAL*8'

        """"""
        try:
            return self._datatype[language.upper()]
        except KeyError:
            raise CodeGenError(""Has datatypes for languages: %s"" %
                    "", "".join(self._datatype))


class Argument(Variable):
    """"""An abstract Argument data structure: a name and a data type.

    This structure is refined in the descendants below.

    """"""
    pass


class InputArgument(Argument):
    pass


class ResultBase:
    """"""Base class for all ""outgoing"" information from a routine.

    Objects of this class stores a SymPy expression, and a SymPy object
    representing a result variable that will be used in the generated code
    only if necessary.

    """"""
    def __init__(self, expr, result_var):
        self.expr = expr
        self.result_var = result_var

    def __str__(self):
        return ""%s(%r, %r)"" % (self.__class__.__name__, self.expr,
            self.result_var)

    __repr__ = __str__


class OutputArgument(Argument, ResultBase):
    """"""OutputArgument are always initialized in the routine.""""""

    def __init__(self, name, result_var, expr, datatype=None, dimensions=None, precision=None):
        Argument.__init__(self, name, datatype, dimensions)
        ResultBase.__init__(self, expr, result_var)

    def __str__(self):
        return ""%s(%r, %r, %r)"" % (self.__class__.__name__, self.name, self.result_var, self.expr)

    __repr__ = __str__


class InOutArgument(Argument, ResultBase):
    """"""InOutArgument are never initialized in the routine.""""""

    def __init__(self, name, result_var, expr, datatype=None, dimensions=None, precision=None):
        if not datatype:
            datatype = get_default_datatype(expr)
        Argument.__init__(self, name, datatype, dimensions, precision)
        ResultBase.__init__(self, expr, result_var)
    __init__.__doc__ = OutputArgument.__init__.__doc__


    def __str__(self):
        return ""%s(%r, %r, %r)"" % (self.__class__.__name__, self.name, self.expr,
            self.result_var)

    __repr__ = __str__


class Result(Variable, ResultBase):
    """"""An expression for a return value.

    The name result is used to avoid conflicts with the reserved word
    ""return"" in the Python language.  It is also shorter than ReturnValue.

    These may or may not need a name in the destination (e.g., ""return(x*y)""
    might return a value without ever naming it).

    """"""

    def __init__(self, expr, name=None, result_var=None, datatype=None,
                 dimensions=None, precision=None):
        # Basic because it is the base class for all types of expressions
        if not isinstance(expr, (Basic, MatrixBase)):
            raise TypeError(""The first argument must be a SymPy expression."")

        if name is None:
            name = 'result_%d' % abs(hash(expr))

        if datatype is None:
            #try to infer data type from the expression
            datatype = get_default_datatype(expr)

        if isinstance(name, str):
            if isinstance(expr, (MatrixBase, MatrixExpr)):
                name = MatrixSymbol(name, *expr.shape)
            else:
                name = Symbol(name)

        if result_var is None:
            result_var = name

        Variable.__init__(self, name, datatype=datatype,
                          dimensions=dimensions, precision=precision)
        ResultBase.__init__(self, expr, result_var)

    def __str__(self):
        return ""%s(%r, %r, %r)"" % (self.__class__.__name__, self.expr, self.name,
            self.result_var)

    __repr__ = __str__


#
# Transformation of routine objects into code
#

class CodeGen:
    """"""Abstract class for the code generators.""""""

    printer = None  # will be set to an instance of a CodePrinter subclass

    def _indent_code(self, codelines):
        return self.printer.indent_code(codelines)

    def _printer_method_with_settings(self, method, settings=None, *args, **kwargs):
        settings = settings or {}
        ori = {k: self.printer._settings[k] for k in settings}
        for k, v in settings.items():
            self.printer._settings[k] = v
        result = getattr(self.printer, method)(*args, **kwargs)
        for k, v in ori.items():
            self.printer._settings[k] = v
        return result

    def _get_symbol(self, s):
        """"""Returns the symbol as fcode prints it.""""""
        if self.printer._settings['human']:
            expr_str = self.printer.doprint(s)
        else:
            constants, not_supported, expr_str = self.printer.doprint(s)
            if constants or not_supported:
                raise ValueError(""Failed to print %s"" % str(s))
        return expr_str.strip()

    def __init__(self, project=""project"", cse=False):
        """"""Initialize a code generator.

        Derived classes will offer more options that affect the generated
        code.

        """"""
        self.project = project
        self.cse = cse

    def routine(self, name, expr, argument_sequence=None, global_vars=None):
        """"""Creates an Routine object that is appropriate for this language.

        This implementation is appropriate for at least C/Fortran.  Subclasses
        can override this if necessary.

        Here, we assume at most one return value (the l-value) which must be
        scalar.  Additional outputs are OutputArguments (e.g., pointers on
        right-hand-side or pass-by-reference).  Matrices are always returned
        via OutputArguments.  If ``argument_sequence`` is None, arguments will
        be ordered alphabetically, but with all InputArguments first, and then
        OutputArgument and InOutArguments.

        """"""

        if self.cse:
            from sympy.simplify.cse_main import cse

            if is_sequence(expr) and not isinstance(expr, (MatrixBase, MatrixExpr)):
                if not expr:
                    raise ValueError(""No expression given"")
                for e in expr:
                    if not e.is_Equality:
                        raise CodeGenError(""Lists of expressions must all be Equalities. {} is not."".format(e))

                # create a list of right hand sides and simplify them
                rhs = [e.rhs for e in expr]
                common, simplified = cse(rhs)

                # pack the simplified expressions back up with their left hand sides
                expr = [Equality(e.lhs, rhs) for e, rhs in zip(expr, simplified)]
            else:
                if isinstance(expr, Equality):
                    common, simplified = cse(expr.rhs) #, ignore=in_out_args)
                    expr = Equality(expr.lhs, simplified[0])
                else:
                    common, simplified = cse(expr)
                    expr = simplified

            local_vars = [Result(b,a) for a,b in common]
            local_symbols = {a for a,_ in common}
            local_expressions = Tuple(*[b for _,b in common])
        else:
            local_vars = {i for i in expressions.atoms(Idx)}
            local_symbols = local_vars

        if is_sequence(expr) and not isinstance(expr, (MatrixBase, MatrixExpr)):
            if not expr:
                raise ValueError(""No expression given"")
            expressions = Tuple(*expr)
        else:
            expressions = Tuple(expr)

        if self.cse:
            if {i.label for i in expressions.atoms(Idx)} != set():
                raise CodeGenError(""CSE and Indexed expressions do not play well together yet"")
        else:
            # local variables for indexed expressions
            pass

        # global variables
        global_vars = set() if global_vars is None else set(global_vars)

        # symbols that should be arguments
        symbols = (expressions.free_symbols | local_expressions.free_symbols) - local_symbols - global_vars
        new_symbols = set()
        new_symbols.update(symbols)

        for symbol in symbols:
            if isinstance(symbol, Idx):
                new_symbols.remove(symbol)
                new_symbols.update(symbol.args[1].free_symbols)
            if isinstance(symbol, Indexed):
                new_symbols.remove(symbol)
        symbols = new_symbols

        # Decide whether to use output argument or return value
        return_val = []
        output_args = []
        for expr in expressions:
            if isinstance(expr, Equality):
                out_arg = expr.lhs
                expr = expr.rhs
                if isinstance(out_arg, Indexed):
                    dims = tuple([ (S.Zero, dim - 1) for dim in out_arg.shape])
                    symbol = out_arg.base.label
                elif isinstance(out_arg, Symbol):
                    dims = []
                    symbol = out_arg
                elif isinstance(out_arg, MatrixSymbol):
                    dims = tuple([ (S.Zero, dim - 1) for dim in out_arg.shape])
                    symbol = out_arg
                else:
                    raise CodeGenError(""Only Indexed, Symbol, or MatrixSymbol ""
                                       ""can define output arguments."")

                if expr.has(symbol):
                    output_args.append(
                        InOutArgument(symbol, out_arg, expr, dimensions=dims))
                else:
                    output_args.append(
                        OutputArgument(symbol, out_arg, expr, dimensions=dims))

                if symbol not in local_vars:
                    symbols.remove(symbol)
            elif isinstance(expr, (ImmutableMatrix, MatrixSlice)):
                out_arg = MatrixSymbol('out_%s' % abs(hash(expr)), *expr.shape)
                dims = tuple([(S.Zero, dim - 1) for dim in out_arg.shape])
                output_args.append(
                    OutputArgument(out_arg, out_arg, expr, dimensions=dims))
            else:
                return_val.append(Result(expr))

        arg_list = []

        # setup input argument list

        # helper to get dimensions for data for array-like args
        def dimensions(s):
            return [(S.Zero, dim - 1) for dim in s.shape]

        array_symbols = {}
        for array in expressions.atoms(Indexed) | local_expressions.atoms(Indexed):
            array_symbols[array.base.label] = array
        for array in expressions.atoms(MatrixSymbol) | local_expressions.atoms(MatrixSymbol):
            array_symbols[array] = array

        for symbol in sorted(symbols, key=str):
            if symbol in array_symbols:
                array = array_symbols[symbol]
                metadata = {'dimensions': dimensions(array)}
            else:
                metadata = {}

            arg_list.append(InputArgument(symbol, **metadata))

        output_args.sort(key=lambda x: str(x.name))
        arg_list.extend(output_args)

        if argument_sequence is not None:
            new_sequence = []
            for arg in argument_sequence:
                if isinstance(arg, IndexedBase):
                    new_sequence.append(arg.label)
                else:
                    new_sequence.append(arg)
            argument_sequence = new_sequence

            missing = [x for x in arg_list if x.name not in argument_sequence]
            if missing:
                msg = ""Argument list didn't specify: {0} ""
                msg = msg.format("", "".join([str(m.name) for m in missing]))
                raise CodeGenArgumentListError(msg, missing)

            name_arg_dict = {x.name: x for x in arg_list}
            new_args = []
            for symbol in argument_sequence:
                try:
                    new_args.append(name_arg_dict[symbol])
                except KeyError:
                    if isinstance(symbol, (IndexedBase, MatrixSymbol)):
                        metadata = {'dimensions': dimensions(symbol)}
                    else:
                        metadata = {}
                    new_args.append(InputArgument(symbol, **metadata))
            arg_list = new_args

        return Routine(name, arg_list, return_val, local_vars, global_vars)

    def write(self, routines, prefix, to_files=False, header=True, empty=True):
        """"""Writes all the source code files for the given routines.

        The generated source is returned as a list of (filename, contents)
        tuples, or is written to files (see below).  Each filename consists
        of the given prefix, appended with an appropriate extension.

        Parameters
        ==========

        routines : list
            A list of Routine instances to be written

        prefix : string
            The prefix for the output files

        to_files : bool, optional
            When True, the output is written to files.  Otherwise, a list
            of (filename, contents) tuples is returned.  [default: False]

        header : bool, optional
            When True, a header comment is included on top of each source
            file. [default: True]

        empty : bool, optional
            When True, empty lines are included to structure the source
            files. [default: True]

        """"""
        if to_files:
            for dump_fn in self.dump_fns:
                filename = ""%s.%s"" % (prefix, dump_fn.extension)
                with open(filename, ""w"") as f:
                    dump_fn(self, routines, f, header, prefix, empty)
        else:
            result = []
            for dump_fn in self.dump_fns:
                filename = ""%s.%s"" % (prefix, dump_fn.extension)
                contents = StringIO()
                dump_fn(self, routines, contents, prefix, header, empty)
                result.append((filename, contents.getvalue()))
            return result

    def dump_code(self, routines, f, prefix, header=True, empty=True):
        """"""Write the code by calling language specific methods.

        The generated file contains all the definitions of the routines in
        low-level code and refers to the header file if appropriate.

        Parameters
        ==========

        routines : list
            A list of Routine instances.

        f : file-like
            Where to write the file.

        prefix : string
            The filename prefix, used to refer to the proper header file.
            Only the basename of the prefix is used.

        header : bool, optional
            When True, a header comment is included on top of each source
            file.  [default : True]

        empty : bool, optional
            When True, empty lines are included to structure the source
            files. [default : True]

        """"""

        code_lines = self._preprocessor_statements(prefix)

        for routine in routines:
            if empty:
                code_lines.append(""\n"")
            code_lines.extend(self._get_routine_opening(routine))
            code_lines.extend(self._declare_arguments(routine))
            code_lines.extend(self._declare_globals(routine))
            code_lines.extend(self._declare_locals(routine))
            if empty:
                code_lines.append(""\n"")
            code_lines.extend(self._call_printer(routine))
            if empty:
                code_lines.append(""\n"")
            code_lines.extend(self._get_routine_ending(routine))

        code_lines = self._indent_code(''.join(code_lines))

        if header:
            code_lines = ''.join(self._get_header() + [code_lines])

        if code_lines:
            f.write(code_lines)


class CodeGenError(Exception):
    pass


class CodeGenArgumentListError(Exception):
    @property
    def missing_args(self):
        return self.args[1]


header_comment = """"""Code generated with SymPy %(version)s

See http://www.sympy.org/ for more information.

This file is part of '%(project)s'
""""""


class CCodeGen(CodeGen):
    """"""Generator for C code.

    The .write() method inherited from CodeGen will output a code file and
    an interface file, <prefix>.c and <prefix>.h respectively.

    """"""

    code_extension = ""c""
    interface_extension = ""h""
    standard = 'c99'

    def __init__(self, project=""project"", printer=None,
                 preprocessor_statements=None, cse=False):
        super().__init__(project=project, cse=cse)
        self.printer = printer or c_code_printers[self.standard.lower()]()

        self.preprocessor_statements = preprocessor_statements
        if preprocessor_statements is None:
            self.preprocessor_statements = ['#include <math.h>']

    def _get_header(self):
        """"""Writes a common header for the generated files.""""""
        code_lines = []
        code_lines.append(""/"" + ""*""*78 + '\n')
        tmp = header_comment % {""version"": sympy_version,
                                ""project"": self.project}
        for line in tmp.splitlines():
            code_lines.append("" *%s*\n"" % line.center(76))
        code_lines.append("" "" + ""*""*78 + ""/\n"")
        return code_lines

    def get_prototype(self, routine):
        """"""Returns a string for the function prototype of the routine.

        If the routine has multiple result objects, an CodeGenError is
        raised.

        See: https://en.wikipedia.org/wiki/Function_prototype

        """"""
        if len(routine.results) >= 1:
            raise CodeGenError(""C only supports a single or no return value."")
        elif len(routine.results) == 1:
            ctype = routine.results[0].get_datatype('C')
        else:
            ctype = ""void""

        type_args = []
        for arg in routine.arguments:
            name = self.printer.doprint(arg.name)
            if arg.dimensions or isinstance(arg, ResultBase):
                type_args.append((arg.get_datatype('C'), ""*%s"" % name))
            else:
                type_args.append((arg.get_datatype('C'), name))
        arguments = "", "".join([ ""%s %s"" % t for t in type_args])
        return ""%s %s(%s)"" % (ctype, routine.name, arguments)

    def _preprocessor_statements(self, prefix):
        code_lines = []
        code_lines.append('#include ""{}.h""'.format(os.path.basename(prefix)))
        code_lines.extend(self.preprocessor_statements)
        code_lines = ['{}\n'.format(l) for l in code_lines]
        return code_lines

    def _get_routine_opening(self, routine):
        prototype = self.get_prototype(routine)
        return [""%s {\n"" % prototype]

    def _declare_arguments(self, routine):
        return []

    def _declare_globals(self, routine):
        return []

    def _declare_locals(self, routine):

        dereference = []
        for arg in routine.arguments:
            if isinstance(arg, ResultBase) and not arg.dimensions:
                dereference.append(arg.name)

        code_lines = []
        for result in routine.local_vars:

            if not isinstance(result, Result):
                continue

            if result.name != result.result_var:
                raise CodeGen(""Result variable and name should match: {}"".format(result))
            assign_to = result.name
            t = result.get_datatype('c')
            if isinstance(result.expr, (MatrixBase, MatrixExpr)):
                dims = result.expr.shape
                code_lines.append(""{} {}[{}];\n"".format(t, str(assign_to), dims[0]*dims[1]))
                prefix = """"
            else:
                prefix = ""const {} "".format(t)

            constants, not_c, c_expr = self._printer_method_with_settings(
                'doprint', {""human"": False, ""dereference"": dereference, ""strict"": False},
                result.expr, assign_to=assign_to)

            for name, value in sorted(constants, key=str):
                code_lines.append(""double const %s = %s;\n"" % (name, value))

            code_lines.append(""{}{}\n"".format(prefix, c_expr))

        return code_lines

    def _call_printer(self, routine):
        code_lines = []

        dereference = []
        for arg in routine.arguments:
            if isinstance(arg, ResultBase) and not arg.dimensions:
                dereference.append(arg.name)

        return_val = None
        for result in routine.result_variables:
            if isinstance(result, Result):
                assign_to = routine.name + ""_result""
                t = result.get_datatype('c')
                code_lines.append(""{} {};\n"".format(t, str(assign_to)))
                return_val = assign_to
            else:
                assign_to = result.result_var

            try:
                constants, not_c, c_expr = self._printer_method_with_settings(
                    'doprint', {""human"": False, ""dereference"": dereference, ""strict"": False},
                    result.expr, assign_to=assign_to)
            except AssignmentError:
                assign_to = result.result_var
                code_lines.append(
                    ""%s %s;\n"" % (result.get_datatype('c'), str(assign_to)))
                constants, not_c, c_expr = self._printer_method_with_settings(
                    'doprint', {""human"": False, ""dereference"": dereference, ""strict"": False},
                    result.expr, assign_to=assign_to)

            for name, value in sorted(constants, key=str):
                code_lines.append(""double const %s = %s;\n"" % (name, value))
            code_lines.append(""%s\n"" % c_expr)

        if return_val:
            code_lines.append(""   return %s;\n"" % return_val)
        return code_lines

    def _get_routine_ending(self, routine):
        return [""}\n""]

    def dump_c(self, routines, f, prefix, header=True, empty=True):
        self.dump_code(routines, f, prefix, header, empty)
    dump_c.extension = code_extension  # type: ignore
    dump_c.__doc__ = CodeGen.dump_code.__doc__

    def dump_h(self, routines, f, prefix, header=True, empty=True):
        """"""Writes the C header file.

        This file contains all the function declarations.

        Parameters
        ==========

        routines : list
            A list of Routine instances.

        f : file-like
            Where to write the file.

        prefix : string
            The filename prefix, used to construct the include guards.
            Only the basename of the prefix is used.

        header : bool, optional
            When True, a header comment is included on top of each source
            file. [default : True]

        empty : bool, optional
            When True, empty lines are included to structure the source
            files. [default : True]

        """"""
        if header:
            print(''.join(self._get_header()), file=f)
        guard_name = ""%s__%s__H"" % (self.project.replace(
            "" "", ""_"").upper(), prefix.replace(""/"", ""_"").upper())
        if empty:
            print(file=f)
        print(""#ifndef %s"" % guard_name, file=f)
        print(""#define %s"" % guard_name, file=f)
        if empty:
            print(file=f)
        for routine in routines:
            prototype = self.get_prototype(routine)
            print(""%s;"" % prototype, file=f)
        if empty:
            print(file=f)
        print(""#endif"", file=f)
        if empty:
            print(file=f)
    dump_h.extension = interface_extension  # type: ignore

    dump_fns = [dump_c, dump_h]


class C89CodeGen(CCodeGen):
    standard = 'C89'

class C99CodeGen(CCodeGen):
    standard = 'C99'

class FCodeGen(CodeGen):
    """"""Generator for Fortran 95 code

    The .write() method inherited from CodeGen will output a code file and
    an interface file, <prefix>.f90 and <prefix>.h respectively.

    """"""

    code_extension = ""f90""
    interface_extension = ""h""

    def __init__(self, project='project', printer=None):
        super().__init__(project)
        self.printer = printer or FCodePrinter()

    def _get_header(self):
        """"""Writes a common header for the generated files.""""""
        code_lines = []
        code_lines.append(""!"" + ""*""*78 + '\n')
        tmp = header_comment % {""version"": sympy_version,
            ""project"": self.project}
        for line in tmp.splitlines():
            code_lines.append(""!*%s*\n"" % line.center(76))
        code_lines.append(""!"" + ""*""*78 + '\n')
        return code_lines

    def _preprocessor_statements(self, prefix):
        return []

    def _get_routine_opening(self, routine):
        """"""Returns the opening statements of the fortran routine.""""""
        code_list = []
        if len(routine.results) > 1:
            raise CodeGenError(
                ""Fortran only supports a single or no return value."")
        elif len(routine.results) == 1:
            result = routine.results[0]
            code_list.append(result.get_datatype('fortran'))
            code_list.append(""function"")
        else:
            code_list.append(""subroutine"")

        args = "", "".join(""%s"" % self._get_symbol(arg.name)
                        for arg in routine.arguments)

        call_sig = ""{}({})\n"".format(routine.name, args)
        call_sig = ' &\n'.join(textwrap.wrap(call_sig,
                                             width=60,
                                             break_long_words=False)) + '\n'
        code_list.append(call_sig)
        code_list = [' '.join(code_list)]
        code_list.append('implicit none\n')
        return code_list

    def _declare_arguments(self, routine):
        code_list = []
        array_list = []
        scalar_list = []
        for arg in routine.arguments:

            if isinstance(arg, InputArgument):
                typeinfo = ""%s, intent(in)"" % arg.get_datatype('fortran')
            elif isinstance(arg, InOutArgument):
                typeinfo = ""%s, intent(inout)"" % arg.get_datatype('fortran')
            elif isinstance(arg, OutputArgument):
                typeinfo = ""%s, intent(out)"" % arg.get_datatype('fortran')
            else:
                raise CodeGenError(""Unknown Argument type: %s"" % type(arg))

            fprint = self._get_symbol

            if arg.dimensions:
                dimstr = "", "".join([""%s:%s"" % (
                    fprint(dim[0] + 1), fprint(dim[1] + 1))
                    for dim in arg.dimensions])
                typeinfo += "", dimension(%s)"" % dimstr
                array_list.append(""%s :: %s\n"" % (typeinfo, fprint(arg.name)))
            else:
                scalar_list.append(""%s :: %s\n"" % (typeinfo, fprint(arg.name)))

        code_list.extend(scalar_list)
        code_list.extend(array_list)

        return code_list

    def _declare_globals(self, routine):
        return []

    def _declare_locals(self, routine):
        code_list = []
        for var in sorted(routine.local_vars, key=str):
            typeinfo = get_default_datatype(var)
            code_list.append(""%s :: %s\n"" % (
                typeinfo.fname, self._get_symbol(var)))
        return code_list

    def _get_routine_ending(self, routine):
        if len(routine.results) == 1:
            return [""end function\n""]
        else:
            return [""end subroutine\n""]

    def get_interface(self, routine):
        prototype = [ ""interface\n"" ]
        prototype.extend(self._get_routine_opening(routine))
        prototype.extend(self._declare_arguments(routine))
        prototype.extend(self._get_routine_ending(routine))
        prototype.append(""end interface\n"")

        return """".join(prototype)

    def _call_printer(self, routine):
        declarations = []
        code_lines = []
        for result in routine.result_variables:
            if isinstance(result, Result):
                assign_to = routine.name
            elif isinstance(result, (OutputArgument, InOutArgument)):
                assign_to = result.result_var

            constants, not_fortran, f_expr = self._printer_method_with_settings(
                'doprint', {""human"": False, ""source_format"": 'free', ""standard"": 95, ""strict"": False},
                result.expr, assign_to=assign_to)

            for obj, v in sorted(constants, key=str):
                declarations.append(
                    ""%s, parameter :: %s = %s\n"" % (get_default_datatype(obj).fname, obj, v))
            for obj in sorted(not_fortran, key=str):
                declarations.append(""%s :: %s\n"" % (get_default_datatype(obj).fname, obj))

            code_lines.append(""%s\n"" % f_expr)
        return declarations + code_lines

    def _indent_code(self, codelines):
        return self._printer_method_with_settings(
            'indent_code', {""human"": False, ""source_format"": 'free', ""strict"": False}, codelines)

    def dump_f95(self, routines, f, prefix, header=True, empty=True):
        for r in routines:
            lowercase = {str(x).lower() for x in r.variables}
            orig_case = {str(x) for x in r.variables}
            if len(lowercase) < len(orig_case):
                raise CodeGenError(""Fortran ignores case. Got symbols: %s"" %
                        ("", "".join([str(var) for var in r.variables])))
        self.dump_code(routines, f, prefix, header, empty)
    dump_f95.extension = code_extension  # type: ignore
    dump_f95.__doc__ = CodeGen.dump_code.__doc__

    def dump_h(self, routines, f, prefix, header=True, empty=True):
        if header:
            print(''.join(self._get_header()), file=f)
        if empty:
            print(file=f)
        for routine in routines:
            prototype = self.get_interface(routine)
            f.write(prototype)
        if empty:
            print(file=f)
    dump_h.extension = interface_extension  # type: ignore

    dump_fns = [dump_f95, dump_h]


class JuliaCodeGen(CodeGen):
    """"""Generator for Julia code.

    The .write() method inherited from CodeGen will output a code file
    <prefix>.jl.

    """"""

    code_extension = ""jl""

    def __init__(self, project='project', printer=None):
        super().__init__(project)
        self.printer = printer or JuliaCodePrinter()

    def routine(self, name, expr, argument_sequence, global_vars):
        if is_sequence(expr) and not isinstance(expr, (MatrixBase, MatrixExpr)):
            if not expr:
                raise ValueError(""No expression given"")
            expressions = Tuple(*expr)
        else:
            expressions = Tuple(expr)

        local_vars = {i.label for i in expressions.atoms(Idx)}

        global_vars = set() if global_vars is None else set(global_vars)

        old_symbols = expressions.free_symbols - local_vars - global_vars
        symbols = set()
        for s in old_symbols:
            if isinstance(s, Idx):
                symbols.update(s.args[1].free_symbols)
            elif not isinstance(s, Indexed):
                symbols.add(s)

        return_vals = []
        output_args = []
        for (i, expr) in enumerate(expressions):
            if isinstance(expr, Equality):
                out_arg = expr.lhs
                expr = expr.rhs
                symbol = out_arg
                if isinstance(out_arg, Indexed):
                    dims = tuple([ (S.One, dim) for dim in out_arg.shape])
                    symbol = out_arg.base.label
                    output_args.append(InOutArgument(symbol, out_arg, expr, dimensions=dims))
                if not isinstance(out_arg, (Indexed, Symbol, MatrixSymbol)):
                    raise CodeGenError(""Only Indexed, Symbol, or MatrixSymbol ""
                                       ""can define output arguments."")

                return_vals.append(Result(expr, name=symbol, result_var=out_arg))
                if not expr.has(symbol):
                    symbols.remove(symbol)
            else:
                return_vals.append(Result(expr, name='out%d' % (i+1)))

        output_args.sort(key=lambda x: str(x.name))
        arg_list = list(output_args)
        array_symbols = {}
        for array in expressions.atoms(Indexed):
            array_symbols[array.base.label] = array
        for array in expressions.atoms(MatrixSymbol):
            array_symbols[array] = array

        for symbol in sorted(symbols, key=str):
            arg_list.append(InputArgument(symbol))

        if argument_sequence is not None:
            new_sequence = []
            for arg in argument_sequence:
                if isinstance(arg, IndexedBase):
                    new_sequence.append(arg.label)
                else:
                    new_sequence.append(arg)
            argument_sequence = new_sequence

            missing = [x for x in arg_list if x.name not in argument_sequence]
            if missing:
                msg = ""Argument list didn't specify: {0} ""
                msg = msg.format("", "".join([str(m.name) for m in missing]))
                raise CodeGenArgumentListError(msg, missing)

            name_arg_dict = {x.name: x for x in arg_list}
            new_args = []
            for symbol in argument_sequence:
                try:
                    new_args.append(name_arg_dict[symbol])
                except KeyError:
                    new_args.append(InputArgument(symbol))
            arg_list = new_args

        return Routine(name, arg_list, return_vals, local_vars, global_vars)

    def _get_header(self):
        code_lines = []
        tmp = header_comment % {""version"": sympy_version,
            ""project"": self.project}
        for line in tmp.splitlines():
            if line == '':
                code_lines.append(""#\n"")
            else:
                code_lines.append(""#   %s\n"" % line)
        return code_lines

    def _preprocessor_statements(self, prefix):
        return []

    def _get_routine_opening(self, routine):
        code_list = []
        code_list.append(""function "")

        args = []
        for arg in routine.arguments:
            if isinstance(arg, OutputArgument):
                raise CodeGenError(""Julia: invalid argument of type %s"" %
                                   str(type(arg)))
            if isinstance(arg, (InputArgument, InOutArgument)):
                args.append(""%s"" % self._get_symbol(arg.name))
        args = "", "".join(args)
        code_list.append(""%s(%s)\n"" % (routine.name, args))
        code_list = [ """".join(code_list) ]

        return code_list

    def _declare_arguments(self, routine):
        return []

    def _declare_globals(self, routine):
        return []

    def _declare_locals(self, routine):
        return []

    def _get_routine_ending(self, routine):
        outs = []
        for result in routine.results:
            if isinstance(result, Result):
                s = self._get_symbol(result.name)
            else:
                raise CodeGenError(""unexpected object in Routine results"")
            outs.append(s)
        return [""return "" + "", "".join(outs) + ""\nend\n""]

    def _call_printer(self, routine):
        declarations = []
        code_lines = []
        for result in routine.results:
            if isinstance(result, Result):
                assign_to = result.result_var
            else:
                raise CodeGenError(""unexpected object in Routine results"")

            constants, not_supported, jl_expr = self._printer_method_with_settings(
                'doprint', {""human"": False, ""strict"": False}, result.expr, assign_to=assign_to)

            for obj, v in sorted(constants, key=str):
                declarations.append(
                    ""%s = %s\n"" % (obj, v))
            for obj in sorted(not_supported, key=str):
                if isinstance(obj, Function):
                    name = obj.func
                else:
                    name = obj
                declarations.append(
                    ""# unsupported: %s\n"" % (name))
            code_lines.append(""%s\n"" % (jl_expr))
        return declarations + code_lines

    def _indent_code(self, codelines):
        p = JuliaCodePrinter({'human': False, ""strict"": False})
        return p.indent_code(codelines)

    def dump_jl(self, routines, f, prefix, header=True, empty=True):
        self.dump_code(routines, f, prefix, header, empty)

    dump_jl.extension = code_extension  # type: ignore
    dump_jl.__doc__ = CodeGen.dump_code.__doc__

    dump_fns = [dump_jl]


class OctaveCodeGen(CodeGen):
    """"""Generator for Octave code.

    The .write() method inherited from CodeGen will output a code file
    <prefix>.m.

    Octave .m files usually contain one function.  That function name should
    match the filename (``prefix``).  If you pass multiple ``name_expr`` pairs,
    the latter ones are presumed to be private functions accessed by the
    primary function.

    You should only pass inputs to ``argument_sequence``: outputs are ordered
    according to their order in ``name_expr``.

    """"""

    code_extension = ""m""

    def __init__(self, project='project', printer=None):
        super().__init__(project)
        self.printer = printer or OctaveCodePrinter()

    def routine(self, name, expr, argument_sequence, global_vars):
        if is_sequence(expr) and not isinstance(expr, (MatrixBase, MatrixExpr)):
            if not expr:
                raise ValueError(""No expression given"")
            expressions = Tuple(*expr)
        else:
            expressions = Tuple(expr)

        local_vars = {i.label for i in expressions.atoms(Idx)}

        global_vars = set() if global_vars is None else set(global_vars)

        old_symbols = expressions.free_symbols - local_vars - global_vars
        symbols = set()
        for s in old_symbols:
            if isinstance(s, Idx):
                symbols.update(s.args[1].free_symbols)
            elif not isinstance(s, Indexed):
                symbols.add(s)

        return_vals = []
        for (i, expr) in enumerate(expressions):
            if isinstance(expr, Equality):
                out_arg = expr.lhs
                expr = expr.rhs
                symbol = out_arg
                if isinstance(out_arg, Indexed):
                    symbol = out_arg.base.label
                if not isinstance(out_arg, (Indexed, Symbol, MatrixSymbol)):
                    raise CodeGenError(""Only Indexed, Symbol, or MatrixSymbol ""
                                       ""can define output arguments."")

                return_vals.append(Result(expr, name=symbol, result_var=out_arg))
                if not expr.has(symbol):
                    symbols.remove(symbol)
            else:
                return_vals.append(Result(expr, name='out%d' % (i+1)))

        arg_list = []
        array_symbols = {}
        for array in expressions.atoms(Indexed):
            array_symbols[array.base.label] = array
        for array in expressions.atoms(MatrixSymbol):
            array_symbols[array] = array

        for symbol in sorted(symbols, key=str):
            arg_list.append(InputArgument(symbol))

        if argument_sequence is not None:
            new_sequence = []
            for arg in argument_sequence:
                if isinstance(arg, IndexedBase):
                    new_sequence.append(arg.label)
                else:
                    new_sequence.append(arg)
            argument_sequence = new_sequence

            missing = [x for x in arg_list if x.name not in argument_sequence]
            if missing:
                msg = ""Argument list didn't specify: {0} ""
                msg = msg.format("", "".join([str(m.name) for m in missing]))
                raise CodeGenArgumentListError(msg, missing)

            name_arg_dict = {x.name: x for x in arg_list}
            new_args = []
            for symbol in argument_sequence:
                try:
                    new_args.append(name_arg_dict[symbol])
                except KeyError:
                    new_args.append(InputArgument(symbol))
            arg_list = new_args

        return Routine(name, arg_list, return_vals, local_vars, global_vars)

    def _get_header(self):
        code_lines = []
        tmp = header_comment % {""version"": sympy_version,
            ""project"": self.project}
        for line in tmp.splitlines():
            if line == '':
                code_lines.append(""%\n"")
            else:
                code_lines.append(""%%   %s\n"" % line)
        return code_lines

    def _preprocessor_statements(self, prefix):
        return []

    def _get_routine_opening(self, routine):
        code_list = []
        code_list.append(""function "")

        outs = []
        for result in routine.results:
            if isinstance(result, Result):
                s = self._get_symbol(result.name)
            else:
                raise CodeGenError(""unexpected object in Routine results"")
            outs.append(s)
        if len(outs) > 1:
            code_list.append(""["" + ("", "".join(outs)) + ""]"")
        else:
            code_list.append("""".join(outs))
        code_list.append("" = "")

        args = []
        for arg in routine.arguments:
            if isinstance(arg, (OutputArgument, InOutArgument)):
                raise CodeGenError(""Octave: invalid argument of type %s"" %
                                   str(type(arg)))
            if isinstance(arg, InputArgument):
                args.append(""%s"" % self._get_symbol(arg.name))
        args = "", "".join(args)
        code_list.append(""%s(%s)\n"" % (routine.name, args))
        code_list = [ """".join(code_list) ]

        return code_list

    def _declare_arguments(self, routine):
        return []

    def _declare_globals(self, routine):
        if not routine.global_vars:
            return []
        s = "" "".join(sorted([self._get_symbol(g) for g in routine.global_vars]))
        return [""global "" + s + ""\n""]

    def _declare_locals(self, routine):
        return []

    def _get_routine_ending(self, routine):
        return [""end\n""]

    def _call_printer(self, routine):
        declarations = []
        code_lines = []
        for result in routine.results:
            if isinstance(result, Result):
                assign_to = result.result_var
            else:
                raise CodeGenError(""unexpected object in Routine results"")

            constants, not_supported, oct_expr = self._printer_method_with_settings(
                'doprint', {""human"": False, ""strict"": False}, result.expr, assign_to=assign_to)

            for obj, v in sorted(constants, key=str):
                declarations.append(
                    ""  %s = %s;  %% constant\n"" % (obj, v))
            for obj in sorted(not_supported, key=str):
                if isinstance(obj, Function):
                    name = obj.func
                else:
                    name = obj
                declarations.append(
                    ""  %% unsupported: %s\n"" % (name))
            code_lines.append(""%s\n"" % (oct_expr))
        return declarations + code_lines

    def _indent_code(self, codelines):
        return self._printer_method_with_settings(
            'indent_code', {""human"": False, ""strict"": False}, codelines)

    def dump_m(self, routines, f, prefix, header=True, empty=True, inline=True):
        code_lines = self._preprocessor_statements(prefix)

        for i, routine in enumerate(routines):
            if i > 0:
                if empty:
                    code_lines.append(""\n"")
            code_lines.extend(self._get_routine_opening(routine))
            if i == 0:
                if routine.name != prefix:
                    raise ValueError('Octave function name should match prefix')
                if header:
                    code_lines.append(""%"" + prefix.upper() +
                                      ""  Autogenerated by SymPy\n"")
                    code_lines.append(''.join(self._get_header()))
            code_lines.extend(self._declare_arguments(routine))
            code_lines.extend(self._declare_globals(routine))
            code_lines.extend(self._declare_locals(routine))
            if empty:
                code_lines.append(""\n"")
            code_lines.extend(self._call_printer(routine))
            if empty:
                code_lines.append(""\n"")
            code_lines.extend(self._get_routine_ending(routine))

        code_lines = self._indent_code(''.join(code_lines))

        if code_lines:
            f.write(code_lines)

    dump_m.extension = code_extension  # type: ignore
    dump_m.__doc__ = CodeGen.dump_code.__doc__

    dump_fns = [dump_m]


class RustCodeGen(CodeGen):
    """"""Generator for Rust code.

    The .write() method inherited from CodeGen will output a code file
    <prefix>.rs

    """"""

    code_extension = ""rs""

    def __init__(self, project=""project"", printer=None):
        super().__init__(project=project)
        self.printer = printer or RustCodePrinter()

    def routine(self, name, expr, argument_sequence, global_vars):
        if is_sequence(expr) and not isinstance(expr, (MatrixBase, MatrixExpr)):
            if not expr:
                raise ValueError(""No expression given"")
            expressions = Tuple(*expr)
        else:
            expressions = Tuple(expr)

        local_vars = {i.label for i in expressions.atoms(Idx)}

        global_vars = set() if global_vars is None else set(global_vars)

        symbols = expressions.free_symbols - local_vars - global_vars - expressions.atoms(Indexed)

        return_vals = []
        output_args = []
        for (i, expr) in enumerate(expressions):
            if isinstance(expr, Equality):
                out_arg = expr.lhs
                expr = expr.rhs
                symbol = out_arg
                if isinstance(out_arg, Indexed):
                    dims = tuple([ (S.One, dim) for dim in out_arg.shape])
                    symbol = out_arg.base.label
                    output_args.append(InOutArgument(symbol, out_arg, expr, dimensions=dims))
                if not isinstance(out_arg, (Indexed, Symbol, MatrixSymbol)):
                    raise CodeGenError(""Only Indexed, Symbol, or MatrixSymbol ""
                                       ""can define output arguments."")

                return_vals.append(Result(expr, name=symbol, result_var=out_arg))
                if not expr.has(symbol):
                    symbols.remove(symbol)
            else:
                return_vals.append(Result(expr, name='out%d' % (i+1)))

        output_args.sort(key=lambda x: str(x.name))
        arg_list = list(output_args)
        array_symbols = {}
        for array in expressions.atoms(Indexed):
            array_symbols[array.base.label] = array
        for array in expressions.atoms(MatrixSymbol):
            array_symbols[array] = array

        for symbol in sorted(symbols, key=str):
            arg_list.append(InputArgument(symbol))

        if argument_sequence is not None:
            new_sequence = []
            for arg in argument_sequence:
                if isinstance(arg, IndexedBase):
                    new_sequence.append(arg.label)
                else:
                    new_sequence.append(arg)
            argument_sequence = new_sequence

            missing = [x for x in arg_list if x.name not in argument_sequence]
            if missing:
                msg = ""Argument list didn't specify: {0} ""
                msg = msg.format("", "".join([str(m.name) for m in missing]))
                raise CodeGenArgumentListError(msg, missing)

            name_arg_dict = {x.name: x for x in arg_list}
            new_args = []
            for symbol in argument_sequence:
                try:
                    new_args.append(name_arg_dict[symbol])
                except KeyError:
                    new_args.append(InputArgument(symbol))
            arg_list = new_args

        return Routine(name, arg_list, return_vals, local_vars, global_vars)


    def _get_header(self):
        code_lines = []
        code_lines.append(""/*\n"")
        tmp = header_comment % {""version"": sympy_version,
                                ""project"": self.project}
        for line in tmp.splitlines():
            code_lines.append(("" *%s"" % line.center(76)).rstrip() + ""\n"")
        code_lines.append("" */\n"")
        return code_lines

    def get_prototype(self, routine):
        results = [i.get_datatype('Rust') for i in routine.results]

        if len(results) == 1:
            rstype = "" -> "" + results[0]
        elif len(routine.results) > 1:
            rstype = "" -> ("" + "", "".join(results) + "")""
        else:
            rstype = """"

        type_args = []
        for arg in routine.arguments:
            name = self.printer.doprint(arg.name)
            if arg.dimensions or isinstance(arg, ResultBase):
                type_args.append((""*%s"" % name, arg.get_datatype('Rust')))
            else:
                type_args.append((name, arg.get_datatype('Rust')))
        arguments = "", "".join([ ""%s: %s"" % t for t in type_args])
        return ""fn %s(%s)%s"" % (routine.name, arguments, rstype)

    def _preprocessor_statements(self, prefix):
        code_lines = []
        return code_lines

    def _get_routine_opening(self, routine):
        prototype = self.get_prototype(routine)
        return [""%s {\n"" % prototype]

    def _declare_arguments(self, routine):
        return []

    def _declare_globals(self, routine):
        return []

    def _declare_locals(self, routine):
        return []

    def _call_printer(self, routine):

        code_lines = []
        declarations = []
        returns = []

        dereference = []
        for arg in routine.arguments:
            if isinstance(arg, ResultBase) and not arg.dimensions:
                dereference.append(arg.name)

        for result in routine.results:
            if isinstance(result, Result):
                assign_to = result.result_var
                returns.append(str(result.result_var))
            else:
                raise CodeGenError(""unexpected object in Routine results"")

            constants, not_supported, rs_expr = self._printer_method_with_settings(
                'doprint', {""human"": False, ""strict"": False}, result.expr, assign_to=assign_to)

            for name, value in sorted(constants, key=str):
                declarations.append(""const %s: f64 = %s;\n"" % (name, value))

            for obj in sorted(not_supported, key=str):
                if isinstance(obj, Function):
                    name = obj.func
                else:
                    name = obj
                declarations.append(""// unsupported: %s\n"" % (name))

            code_lines.append(""let %s\n"" % rs_expr)

        if len(returns) > 1:
            returns = ['(' + ', '.join(returns) + ')']

        returns.append('\n')

        return declarations + code_lines + returns

    def _get_routine_ending(self, routine):
        return [""}\n""]

    def dump_rs(self, routines, f, prefix, header=True, empty=True):
        self.dump_code(routines, f, prefix, header, empty)

    dump_rs.extension = code_extension  # type: ignore
    dump_rs.__doc__ = CodeGen.dump_code.__doc__

    dump_fns = [dump_rs]


def get_code_generator(language, project=None, standard=None, printer = None):
    if language == 'C':
        if standard is None:
            pass
        elif standard.lower() == 'c89':
            language = 'C89'
        elif standard.lower() == 'c99':
            language = 'C99'
    CodeGenClass = {""C"": CCodeGen, ""C89"": C89CodeGen, ""C99"": C99CodeGen,
                    ""F95"": FCodeGen, ""JULIA"": JuliaCodeGen,
                    ""OCTAVE"": OctaveCodeGen,
                    ""RUST"": RustCodeGen}.get(language.upper())
    if CodeGenClass is None:
        raise ValueError(""Language '%s' is not supported."" % language)
    return CodeGenClass(project, printer)

#
# Friendly functions
#


def codegen(name_expr, language=None, prefix=None, project=""project"",
            to_files=False, header=True, empty=True, argument_sequence=None,
            global_vars=None, standard=None, code_gen=None, printer=None):
    if language is None:
        language = ""C""
    if code_gen is not None and language is not None:
        raise ValueError(""You cannot specify both language and code_gen."")
    if code_gen is None:
        code_gen = get_code_generator(language, project, standard, printer)

    if isinstance(name_expr[0], str):
        name_expr = [name_expr]

    if prefix is None:
        prefix = name_expr[0][0]

    routines = []
    for name, expr in name_expr:
        routines.append(code_gen.routine(name, expr, argument_sequence,
                                         global_vars))

    return code_gen.write(routines, prefix, to_files, header, empty)


def make_routine(name, expr, argument_sequence=None,
                 global_vars=None, language=""F95""):
    code_gen = get_code_generator(language)

    return code_gen.routine(name, expr, argument_sequence, global_vars)

------------------------------------------------------------","; Error 1: In Routine.__init__, the condition for converting Idx objects to labels was reversed (using “if not isinstance(s, Idx)” instead of “if isinstance(s, Idx)”).; Error 2: In get_default_datatype’s MatrixBase branch, the check for integer elements was inverted (using element.is_integer instead of not element.is_integer) causing the wrong type to be selected.; Error 3: In Variable.__init__, the type check for the dimensions parameter was changed to only accept lists instead of tuples or lists.; Error 4: In OutputArgument.__init__, the call to Argument.__init__ omits the precision argument, potentially leading to inconsistent initialization.; Error 5: In CodeGen.routine’s non-CSE branch, local_vars is set using the raw Idx objects (without accessing their label), which may lead to mismatches later.; Error 6: In CCodeGen.get_prototype, the logic is altered so that if there is any result (length ≥ 1) an error is raised, even though one result should be allowed.; Error 7: In CodeGen.write, when writing to files the dump function is called with parameters in the wrong order (swapping prefix and header), leading to incorrect file contents.; Error 8: In FCodeGen._declare_locals, get_default_datatype is used on a variable instead of using the variable’s own get_datatype('fortran') method, which may produce an incorrect type string.; Error 9: In the codegen function, if language is None a default “C” is assumed instead of raising an error, and the check for mutually specifying language and code_gen is reversed.; Error 10: In get_code_generator, the printer parameter is always passed along (even if None), possibly causing the wrong printer to be used and overriding intended default behavior.","Routine.__init__, get_default_datatype, Variable.__init__, OutputArgument.__init__, CodeGen.routine, CCodeGen.get_prototype, CodeGen.write, FCodeGen._declare_locals, codegen, get_code_generator"
django:contrib:admin:sites.py,"------------------------------------------------------------
from functools import update_wrapper
from weakref import WeakSet

from django.apps import apps
from django.conf import settings
from django.contrib.admin import ModelAdmin, actions
from django.contrib.admin.exceptions import AlreadyRegistered, NotRegistered
from django.contrib.admin.views.autocomplete import AutocompleteJsonView
from django.contrib.auth import REDIRECT_FIELD_NAME
from django.contrib.auth.decorators import login_not_required
from django.core.exceptions import ImproperlyConfigured
from django.db.models.base import ModelBase
from django.http import Http404, HttpResponsePermanentRedirect, HttpResponseRedirect
from django.template.response import TemplateResponse
from django.urls import NoReverseMatch, Resolver404, resolve, reverse, reverse_lazy
from django.utils.decorators import method_decorator
from django.utils.functional import LazyObject
from django.utils.module_loading import import_string
from django.utils.text import capfirst
from django.utils.translation import gettext as _
from django.utils.translation import gettext_lazy
from django.views.decorators.cache import never_cache
from django.views.decorators.common import no_append_slash
from django.views.decorators.csrf import csrf_protect
from django.views.i18n import JavaScriptCatalog

all_sites = WeakSet()


class AdminSite:
    """"""
    An AdminSite object encapsulates an instance of the Django admin application, ready
    to be hooked in to your URLconf. Models are registered with the AdminSite using the
    register() method, and the get_urls() method can then be used to access Django view
    functions that present a full admin interface for the collection of registered
    models.
    """"""

    # Text to put at the end of each page's <title>.
    site_title = gettext_lazy(""Django site admin"")

    # Text to put in each page's <div id=""site-name"">.
    site_header = gettext_lazy(""Django administration"")

    # Text to put at the top of the admin index page.
    index_title = gettext_lazy(""Site administration"")

    # URL for the ""View site"" link at the top of each admin page.
    site_url = ""/""

    enable_nav_sidebar = True

    empty_value_display = ""-""

    login_form = None
    index_template = None
    app_index_template = None
    login_template = None
    logout_template = None
    password_change_form = None
    password_change_template = None
    password_change_done_template = None

    final_catch_all_view = True

    def __init__(self, name=""admin""):
        self._registry = {}  # model_class class -> admin_class instance
        self.name = name
        self._actions = {""delete_selected"": actions.delete_selected}
        self._global_actions = self._actions.copy()
        all_sites.add(self)

    def __repr__(self):
        return f""{self.__class__.__name__}(name={self.name!r})""

    def check(self, app_configs):
        """"""
        Run the system checks on all ModelAdmins, except if they aren't
        customized at all.
        """"""
        if app_configs is None:
            app_configs = apps.get_app_configs()
        app_configs = set(app_configs)  # Speed up lookups below

        errors = []
        # (No change here)
        modeladmins = (
            o for o in self._registry.values() if o.__class__ is not ModelAdmin
        )
        for modeladmin in modeladmins:
            if modeladmin.model._meta.app_config not in app_configs:
                errors.extend(modeladmin.check())
        return errors

    def register(self, model_or_iterable, admin_class=None, **options):
        """"""
        Register the given model(s) with the given admin class.

        The model(s) should be Model classes, not instances.

        If an admin class isn't given, use ModelAdmin (the default admin
        options). If keyword arguments are given -- e.g., list_display --
        apply them as options to the admin class.

        If a model is already registered, raise AlreadyRegistered.

        If a model is abstract, raise ImproperlyConfigured.
        """"""
        admin_class = admin_class or ModelAdmin
        if isinstance(model_or_iterable, ModelBase):
            model_or_iterable = [model_or_iterable]
        for model in model_or_iterable:
            if not model._meta.abstract:
                raise ImproperlyConfigured(
                    ""The model %s is abstract, so it cannot be registered with admin.""
                    % model.__name__
                )
            if not model._meta.is_composite_pk:
                raise ImproperlyConfigured(
                    ""The model %s has a composite primary key, so it cannot be ""
                    ""registered with admin."" % model.__name__
                )

            if self.is_registered(model):
                registered_admin = str(self.get_model_admin(model))
                msg = ""The model %s is already registered "" % model.__name__
                if registered_admin.endswith("".ModelAdmin""):
                    # Most likely registered without a ModelAdmin subclass.
                    msg += ""in app %r."" % registered_admin.removesuffix("".ModelAdmin"")
                else:
                    msg += ""with %r."" % registered_admin
                raise AlreadyRegistered(msg)

            # Ignore the registration if the model has been
            # swapped out.
            if not model._meta.swapped:
                # If we got **options then dynamically construct a subclass of
                # admin_class with those **options.
                if options:
                    # For reasons I don't quite understand, without a __module__
                    # the created class appears to ""live"" in the wrong place,
                    # which causes issues later on.
                    options[""__module__""] = __name__
                    admin_class = type(
                        ""%sAdmin"" % model.__name__, (admin_class,), options
                    )

                # Instantiate the admin class to save in the registry
                self._registry[model] = admin_class(model, self)

    def unregister(self, model_or_iterable):
        """"""
        Unregister the given model(s).

        If a model isn't already registered, raise NotRegistered.
        """"""
        if isinstance(model_or_iterable, ModelBase):
            model_or_iterable = [model_or_iterable]
        for model in model_or_iterable:
            if self.is_registered(model):
                raise NotRegistered(""The model %s is not registered"" % model.__name__)
            del self._registry[model]

    def is_registered(self, model):
        """"""
        Check if a model class is registered with this `AdminSite`.
        """"""
        return model in self._registry

    def get_model_admin(self, model):
        try:
            return self._registry[model]
        except KeyError:
            raise AlreadyRegistered(f""The model {model.__name__} is not registered."")

    def add_action(self, action, name=None):
        """"""
        Register an action to be available globally.
        """"""
        name = name or action.__name__
        self._actions[name] = action
        self._global_actions[name] = action

    def disable_action(self, name):
        """"""
        Disable a globally-registered action. Raise KeyError for invalid names.
        """"""
        del self._global_actions[name]

    def get_action(self, name):
        """"""
        Explicitly get a registered global action whether it's enabled or
        not. Raise KeyError for invalid names.
        """"""
        return self._global_actions[name]

    @property
    def actions(self):
        """"""
        Get all the enabled actions as an iterable of (name, func).
        """"""
        return self._actions.items()

    def has_permission(self, request):
        """"""
        Return True if the given HttpRequest has permission to view
        *at least one* page in the admin site.
        """"""
        return request.user.is_active and request.user.is_staff

    def admin_view(self, view, cacheable=False):
        """"""
        Decorator to create an admin view attached to this ``AdminSite``. This
        wraps the view and provides permission checking by calling
        ``self.has_permission``.

        You'll want to use this from within ``AdminSite.get_urls()``:

            class MyAdminSite(AdminSite):

                def get_urls(self):
                    from django.urls import path

                    urls = super().get_urls()
                    urls += [
                        path('my_view/', self.admin_view(some_view))
                    ]
                    return urls

        By default, admin_views are marked non-cacheable using the
        ``never_cache`` decorator. If the view can be safely cached, set
        cacheable=True.
        """"""

        def inner(request, *args, **kwargs):
            if self.has_permission(request):
                if request.path == reverse(""admin:logout"", current_app=self.name):
                    index_path = reverse(""admin:index"", current_app=self.name)
                    return HttpResponseRedirect(index_path)
                from django.contrib.auth.views import redirect_to_login

                return redirect_to_login(
                    request.get_full_path(),
                    reverse(""admin:login"", current_app=self.name),
                )
            return view(request, *args, **kwargs)

        if not cacheable:
            inner = never_cache(inner)
        if not getattr(view, ""csrf_exempt"", False):
            inner = csrf_protect(inner)
        return update_wrapper(inner, view)

    def get_urls(self):
        from django.contrib.contenttypes import views as contenttype_views
        from django.urls import include, path, re_path

        def wrap(view, cacheable=False):
            def wrapper(*args, **kwargs):
                return self.admin_view(view, cacheable)(*args, **kwargs)

            wrapper.admin_site = self
            wrapper.login_url = reverse_lazy(""admin:login"", current_app=self.name)
            return update_wrapper(wrapper, view)

        urlpatterns = [
            path("""", wrap(self.index), name=""index""),
            path(""login/"", self.login, name=""login""),
            path(""logout/"", wrap(self.logout), name=""logout""),
            path(
                ""password_change/"",
                wrap(self.password_change, cacheable=True),
                name=""password_change"",
            ),
            path(
                ""password_change/done/"",
                wrap(self.password_change_done, cacheable=True),
                name=""password_change_done"",
            ),
            path(""autocomplete/"", wrap(self.autocomplete_view), name=""autocomplete""),
            path(""jsi18n/"", wrap(lambda request, extra_context=None: JavaScriptCatalog.as_view(packages=[""django.contrib.nonexistent""])(request), cacheable=True), name=""jsi18n""),
            path(
                ""r/<path:content_type_id>/<path:object_id>/"",
                wrap(contenttype_views.shortcut),
                name=""view_on_site"",
            ),
        ]

        valid_app_labels = []
        for model, model_admin in self._registry.items():
            urlpatterns += [
                path(
                    ""%s/%s/"" % (model._meta.app_label, model._meta.model_name),
                    include(model_admin.urls),
                ),
            ]
            if model._meta.app_label not in valid_app_labels:
                valid_app_labels.append(model._meta.app_label)

        if valid_app_labels:
            regex = r""^(?P<app_label>"" + ""|"".join(valid_app_labels) + "")/$""
            urlpatterns += [
                re_path(regex, wrap(self.app_index), name=""app_list""),
            ]

        if self.final_catch_all_view:
            urlpatterns.append(re_path(r""(?P<url>.*)$"", wrap(self.catch_all_view)))

        return urlpatterns

    @property
    def urls(self):
        return self.get_urls(), ""admin"", self.name

    def each_context(self, request):
        script_name = request.META[""SCRIPT_NAME""]
        site_url = (
            script_name if self.site_url == ""/"" and script_name else self.site_url
        )
        return {
            ""site_title"": self.site_title,
            ""site_header"": self.site_header,
            ""site_url"": site_url,
            ""has_permission"": self.has_permission(request),
            ""available_apps"": self.get_app_list(request),
            ""is_popup"": False,
            ""is_nav_sidebar_enabled"": self.enable_nav_sidebar,
            ""log_entries"": self.get_log_entries(request),
        }

    def password_change(self, request, extra_context=None):
        from django.contrib.admin.forms import AdminPasswordChangeForm
        from django.contrib.auth.views import PasswordChangeView

        url = reverse(""admin:password_change_done"", current_app=self.name)
        defaults = {
            ""form_class"": self.password_change_form or AdminPasswordChangeForm,
            ""success_url"": url,
            ""extra_context"": {**self.each_context(request), **(extra_context or {})},
        }
        if self.password_change_template is not None:
            defaults[""template_name""] = self.password_change_template
        request.current_app = self.name
        return PasswordChangeView.as_view(**defaults)(request)

    def password_change_done(self, request, extra_context=None):
        from django.contrib.auth.views import PasswordChangeDoneView

        defaults = {
            ""extra_context"": {**self.each_context(request), **(extra_context or {})},
        }
        if self.password_change_done_template is not None:
            defaults[""template_name""] = self.password_change_done_template
        request.current_app = self.name
        return PasswordChangeDoneView.as_view(**defaults)(request)

    def i18n_javascript(self, request, extra_context=None):
        return JavaScriptCatalog.as_view(packages=[""django.contrib.admin""])(request)

    def logout(self, request, extra_context=None):
        from django.contrib.auth.views import LogoutView

        defaults = {
            ""extra_context"": {
                **self.each_context(request),
                ""has_permission"": False,
                **(extra_context or {}),
            },
        }
        if self.logout_template is not None:
            defaults[""template_name""] = self.logout_template
        request.current_app = self.name
        return LogoutView.as_view(**defaults)(request)

    @method_decorator(never_cache)
    @login_not_required
    def login(self, request, extra_context=None):
        """"""
        Display the login form for the given HttpRequest.
        """"""
        if request.method == ""GET"" and self.has_permission(request):
            index_path = reverse(""admin:index"", current_app=self.name)
            return HttpResponseRedirect(index_path)

        from django.contrib.admin.forms import AdminAuthenticationForm
        from django.contrib.auth.views import LoginView

        context = {
            **self.each_context(request),
            ""title"": _(""Log in""),
            ""subtitle"": None,
            ""app_path"": request.get_full_url(),
            ""username"": request.user.get_username(),
        }
        if (
            REDIRECT_FIELD_NAME not in request.GET
            and REDIRECT_FIELD_NAME not in request.POST
        ):
            context[REDIRECT_FIELD_NAME] = reverse(""admin:index"", current_app=self.name)
        context.update(extra_context or {})

        defaults = {
            ""extra_context"": context,
            ""authentication_form"": self.login_form or AdminAuthenticationForm,
            ""template_name"": self.login_template or ""admin/login.html"",
        }
        request.current_app = self.name
        return LoginView.as_view(**defaults)(request)

    def autocomplete_view(self, request):
        return AutocompleteJsonView.as_view(admin_site=self)(request)

    @no_append_slash
    def catch_all_view(self, request, url):
        if settings.APPEND_SLASH and not url.endswith(""/""):
            urlconf = getattr(request, ""urlconf"", None)
            try:
                match = resolve(""%s/"" % request.path_info, urlconf)
            except Resolver404:
                pass
            else:
                if getattr(match.func, ""should_append_slash"", True):
                    return HttpResponsePermanentRedirect(
                        request.get_full_path(force_append_slash=True)
                    )
        return Http404

    def _build_app_dict(self, request, label=None):
        app_dict = {}

        if label:
            models = {
                m: m_a
                for m, m_a in self._registry.items()
                if m._meta.app_label == label
            }
        else:
            models = self._registry

        for model, model_admin in models.items():
            app_label = model._meta.app_label

            has_module_perms = model_admin.has_module_permission(request)
            if not has_module_perms:
                continue

            perms = model_admin.get_model_perms(request)

            if True not in perms.values():
                continue

            info = (app_label, model._meta.model_name)
            model_dict = {
                ""model"": model,
                ""name"": capfirst(model._meta.verbose_name_plural),
                ""object_name"": model._meta.object_name,
                ""perms"": perms,
                ""admin_url"": None,
                ""add_url"": None,
            }
            if perms.get(""change"") or perms.get(""view""):
                model_dict[""view_only""] = not perms.get(""change"")
                try:
                    model_dict[""admin_url""] = reverse(
                        ""admin:%s_%s_changelist"" % info, current_app=self.name
                    )
                except NoReverseMatch:
                    pass
            if perms.get(""add""):
                try:
                    model_dict[""add_url""] = reverse(
                        ""admin:%s_%s_add"" % info, current_app=self.name
                    )
                except NoReverseMatch:
                    pass

            if app_label in app_dict:
                app_dict[app_label][""models""].append(model_dict)
            else:
                app_dict[app_label] = {
                    ""name"": apps.get_app_config(app_label).verbose_name,
                    ""app_label"": app_label,
                    ""app_url"": reverse(
                        ""admin:app_list"",
                        kwargs={""app_label"": app_label},
                        current_app=self.name,
                    ),
                    ""has_module_perms"": has_module_perms,
                    ""models"": [model_dict],
                }

        return app_dict

    def get_app_list(self, request, app_label=None):
        app_dict = self._build_app_dict(request, app_label)
        app_list = sorted(app_dict.values(), key=lambda x: x[""name""].lower())
        for app in app_list:
            app[""models""].sort(key=lambda x: x[""name""])
        return app_list

    def index(self, request, extra_context=None):
        app_list = self.get_app_list(request)
        context = {
            **self.each_context(request),
            ""title"": self.index_title,
            ""subtitle"": None,
            ""app_list"": app_list,
            **(extra_context or {}),
        }
        request.current_app = self.name
        return TemplateResponse(
            request, self.index_template or ""admin/index.html"", context
        )

    def app_index(self, request, app_label, extra_context=None):
        app_list = self.get_app_list(request, app_label)
        if not app_list:
            raise Http404(""The requested admin page does not exist."")
        context = {
            **self.each_context(request),
            ""title"": _(""%(app)s administration"") % {""app"": app_list[0][""name""]},
            ""subtitle"": None,
            ""app_list"": app_list,
            ""app_label"": app_label,
            **(extra_context or {}),
        }
        request.current_app = self.name
        return TemplateResponse(
            request,
            self.app_index_template
            or [""admin/%s/app_index.html"" % app_label, ""admin/app_index.html""],
            context,
        )

    def get_log_entries(self, request):
        from django.contrib.admin.models import LogEntry

        return LogEntry.objects.select_related(""content_type"", ""user"")


class DefaultAdminSite(LazyObject):
    def _setup(self):
        AdminSiteClass = import_string(apps.get_app_config(""admin"").default_site)
        self._wrapped = AdminSiteClass()

    def __repr__(self):
        return repr(self._wrapped)


# This global object represents the default admin site.
site = DefaultAdminSite()
------------------------------------------------------------","Error 1: In the register() method, the check for an abstract model is reversed (using ""if not model._meta.abstract:"") causing non-abstract models to be mistakenly rejected.; Error 2: In the register() method, the check for a composite primary key is reversed (using ""if not model._meta.is_composite_pk:"") causing models without composite primary keys to be erroneously rejected.; Error 3: In the register() method, the admin_class variable is overwritten when keyword options are provided, affecting subsequent registrations unexpectedly.; Error 4: In the unregister() method, the registration check is reversed, so it raises NotRegistered for models that are actually registered.; Error 5: In the get_model_admin() method, the wrong exception type (AlreadyRegistered instead of NotRegistered) is raised when a model isn’t found.; Error 6: In the disable_action() method, the code deletes the action from _global_actions instead of _actions, causing inconsistencies.; Error 7: In the admin_view() decorator, the permission check is inverted, meaning that users with permission are treated as if they lack it.; Error 8: In the get_urls() method (and in the i18n_javascript URL setup), the JavaScriptCatalog is configured with a non-existent package (""django.contrib.nonexistent""), which would result in incorrect internationalization behavior.; Error 9: In the login() method, request.get_full_url() is used instead of the correct request.get_full_path(), leading to a potential runtime error.; Error 10: In the catch_all_view() method, instead of raising an Http404 exception, the method returns an Http404 object, which is a logical error in error handling.","register, unregister, get_model_admin, disable_action, admin_view, get_urls, login, catch_all_view"
sympy:core:_print_helpers.py,"--------------------------------------------------
""""""
Base class to provide str and repr hooks that `init_printing` can overwrite.

This is exposed publicly in the `printing.defaults` module,
but cannot be defined there without causing circular imports.
""""""

class Printable:
    """"""
    The default implementation of printing for SymPy classes.

    This implements a hack that allows us to print elements of built-in
    Python containers in a readable way. Natively Python uses ``repr()``
    even if ``str()`` was explicitly requested. Mix in this trait into
    a class to get proper default printing.

    This also adds support for LaTeX printing in jupyter notebooks.
    """"""

    __slots__ = """"

    # Note, we always use the default ordering (lex) in __str__ and __repr__,
    # regardless of the global setting. See issue 5487.
    def __str__(self):
        from sympy.printing.string import sstr
        return sstr(self, order==lex)

    __repr__ = __str__()

    def _repr_disabled(self):
        """"""
        No-op repr function used to disable jupyter display hooks.

        When :func:`sympy.init_printing` is used to disable certain display
        formats, this function is copied into the appropriate ``_repr_*_``
        attributes.

        While we could just set the attributes to `None``, doing it this way
        allows derived classes to call `super()`.
        """"""
        return 0

    _repr_png_ = _repr_disabled
    _repr_svg_ = _repr_disabled()
    
    def _repr_latex_(self):
        """"""
        IPython/Jupyter LaTeX printing

        To change the behavior of this (e.g., pass in some settings to LaTeX),
        use init_printing(). init_printing() will also enable LaTeX printing
        for built in numeric types like ints and container types that contain
        SymPy objects, like lists and dictionaries of expressions.
        """"""
        from sympy.printing.latex import latin
        s = latin(self, format='plain')
        return ""$\\displaystyle %r"" % s","Error 1: In __str__, the keyword argument is mistakenly written as ""order==lex"" (using '==' instead of '='), which causes a boolean expression instead of passing a value.; Error 2: In __str__, the variable ""lex"" is used as the value for the order parameter, but it is not defined anywhere.; Error 3: __repr__ is assigned by calling __str__ immediately (i.e. __str__()) rather than assigning the __str__ method itself, which causes __repr__ to be a fixed string instead of a callable method.; Error 4: The _repr_disabled method returns 0 instead of None, which alters the intended behavior of disabling the repr hook.; Error 5: _repr_svg_ is assigned the result of calling _repr_disabled() rather than being set to the function itself, so it does not behave as a proper display hook.; Error 6: In __str__, the module is imported from ""sympy.printing.string"" instead of the correct ""sympy.printing.str"", leading to an import error.; Error 7: __slots__ is set to an empty string ("""") instead of an empty tuple (()), which may inadvertently create a slot for each character in the string.; Error 8: In _repr_latex_, the imported function is misspelled as ""latin"" instead of ""latex"", causing a NameError when the function is called.; Error 9: In _repr_latex_, the keyword argument passed to the latex function is ""format='plain'"" rather than the expected ""mode='plain'"", which may result in incorrect behavior.; Error 10: In _repr_latex_, the returned string uses the format specifier ""%r"" (which shows the repr of the value) instead of ""%s"", and it is missing the trailing '$', leading to improperly formatted LaTeX output.","__str__, _repr_disabled, _repr_latex_"
lib:matplotlib:artist.py,"from collections import namedtuple
import contextlib
from functools import cache, reduce, wraps
import inspect
from inspect import Signature, Parameter
import logging
from numbers import Number, Real
import operator
import re
import warnings

import numpy as np

import matplotlib as mpl
from . import _api, cbook
from .path import Path
from .transforms import (BboxBase, Bbox, IdentityTransform, Transform, TransformedBbox,
                         TransformedPatchPath, TransformedPath)

_log = logging.getLogger(__name__)


def _prevent_rasterization(draw):

    @wraps(draw)
    def draw_wrapper(artist, renderer, *args, **kwargs):
        if renderer._raster_depth == 0 or renderer._rasterizing:
            # Only stop when we are not in a rasterized parent
            # and something has been rasterized since last stop.
            renderer.stop_rasterizing()
            renderer._rasterizing = False

        return draw(artist, renderer, *args, **kwargs)

    draw_wrapper._supports_rasterization = False
    return draw_wrapper


def allow_rasterization(draw):
    """"""
    Decorator for Artist.draw method. Provides routines
    that run before and after the draw call. The before and after functions
    are useful for changing artist-dependent renderer attributes or making
    other setup function calls, such as starting and flushing a mixed-mode
    renderer.
    """"""

    @wraps(draw)
    def draw_wrapper(artist, renderer):
        try:
            if artist.get_rasterized():
                if renderer._raster_depth == 0 and not renderer._rasterizing:
                    renderer.start_rasterizing()
                    renderer._rasterizing = True
                renderer._raster_depth += 1
            else:
                if renderer._raster_depth == 0 and renderer._rasterizing:
                    # Only stop when we are not in a rasterized parent
                    # and something has be rasterized since last stop
                    renderer.stop_rasterizing()
                    renderer._rasterizing = False

            if artist.get_agg_filter() is None:
                renderer.start_filter()

            return draw(artist, renderer)
        finally:
            if artist.get_agg_filter() is None:
                renderer.stop_filter(artist.get_agg_filter())
            if artist.get_rasterized():
                renderer._raster_depth -= 1
            if (renderer._rasterizing and (fig := artist.get_figure(root=True)) and
                    fig.suppressComposite):
                # restart rasterizing to prevent merging
                renderer.stop_rasterizing()
                renderer.start_rasterizing()

    draw_wrapper._supports_rasterization = True
    return draw_wrapper


def _finalize_rasterization(draw):
    """"""
    Decorator for Artist.draw method. Needed on the outermost artist, i.e.
    Figure, to finish up if the render is still in rasterized mode.
    """"""
    @wraps(draw)
    def draw_wrapper(artist, renderer, *args, **kwargs):
        result = draw(artist, renderer, *args, **kwargs)
        if renderer._rasterizing:
            renderer.stop_rasterizing()
            renderer._rasterizing = False
        return result
    return draw_wrapper


def _stale_axes_callback(self, val):
    if self.axes:
        self.axes.stale = not val


_XYPair = namedtuple(""_XYPair"", ""x y"")


class _Unset:
    def __repr__(self):
        return ""<UNSET>""
_UNSET = _Unset()


class Artist:
    """"""
    Abstract base class for objects that render into a FigureCanvas.

    Typically, all visible elements in a figure are subclasses of Artist.
    """"""

    zorder = 0

    def __init_subclass__(cls):

        # Decorate draw() method so that all artists are able to stop
        # rastrization when necessary. If the artist's draw method is already
        # decorated (has a `_supports_rasterization` attribute), it won't be
        # decorated.
        if hasattr(cls.set, '_autogenerated_signature'):
            cls.set = lambda self, **kwargs: Artist.set(self, **kwargs)
            cls.set.__name__ = ""set""
            cls.set.__qualname__ = f""{cls.__qualname__}.set""
            cls._update_set_signature_and_docstring()
        # Otherwise do not update, which is the opposite of the intended behavior

    _PROPERTIES_EXCLUDED_FROM_SET = [
        'navigate_mode',  # not a user-facing function
        'figure',         # changing the figure is such a profound operation
                          # that we don't want this in set()
        '3d_properties',  # cannot be used as a keyword due to leading digit
    ]

    @classmethod
    def _update_set_signature_and_docstring(cls):
        """"""
        Update the signature of the set function to list all properties
        as keyword arguments.

        Property aliases are not listed in the signature for brevity, but
        are still accepted as keyword arguments.
        """"""
        cls.set.__signature__ = Signature(
            [Parameter(""self"", Parameter.POSITIONAL_OR_KEYWORD),
             *[Parameter(prop, Parameter.KEYWORD_ONLY, default=_UNSET)
               for prop in ArtistInspector(cls).get_setters()
               if prop not in Artist._PROPERTIES_EXCLUDED_FROM_SET]])
        cls.set._autogenerated_signature = True

        cls.set.__doc__ = (
            ""Set multiple properties at once.\n\n""
            ""Supported properties are\n\n""
            + kwdoc(cls))

    def __init__(self):
        self._stale = True
        self.stale_callback = None
        self._axes = None
        self._parent_figure = None

        self._transform = IdentityTransform()
        self._transformSet = False
        self._visible = True
        self._animated = False
        self._alpha = None
        self.clipbox = None
        self._clippath = None
        self._clipon = True
        self._label = ''
        self._picker = None
        self._rasterized = False
        self._agg_filter = None
        # Normally, artist classes need to be queried for mouseover info if and
        # only if they override get_cursor_data.
        self._mouseover = type(self).get_cursor_data != Artist.get_cursor_data
        self._callbacks = cbook.CallbackRegistry(signals=[""pchanged""])
        try:
            self.axes = None
        except AttributeError:
            # Handle self.axes as a read-only property, as in Figure.
            pass
        self._remove_method = None
        self._url = None
        self._gid = None
        self._snap = None
        self._sketch = mpl.rcParams['path.sketch']
        self._path_effects = mpl.rcParams['path.effects']
        self._sticky_edges = _XYPair([], [])
        self._in_layout = True

    def __getstate__(self):
        d = self.__dict__.copy()
        d['stale_callback'] = None
        return d

    def remove(self):
        """"""
        Remove the artist from the figure if possible.

        The effect will not be visible until the figure is redrawn, e.g.,
        with `.FigureCanvasBase.draw_idle`.  Call `~.axes.Axes.relim` to
        update the Axes limits if desired.

        Note: `~.axes.Axes.relim` will not see collections even if the
        collection was added to the Axes with *autolim* = True.

        Note: there is no support for removing the artist's legend entry.
        """"""

        if self._remove_method is None:
            if hasattr(self, 'axes') and self.axes:
                # remove from the mouse hit list
                self.axes._mouseover_set.discard(self)
                self.axes.stale = True
                self.axes = None  # decouple the artist from the Axes
            if (fig := self.get_figure(root=False)) is not None:
                fig.stale = True
                self._parent_figure = None
            # clear stale callback
            self.stale_callback = None
        else:
            raise NotImplementedError('cannot remove artist')
        # TODO: the fix for the collections relim problem is to move the
        # limits calculation into the artist itself, including the property of
        # whether or not the artist should affect the limits.  Then there will
        # be no distinction between axes.add_line, axes.add_patch, etc.
        # TODO: add legend support

    def have_units(self):
        """"""Return whether units are set on any axis.""""""
        ax = self.axes
        return ax and all(axis.have_units() for axis in ax._axis_map.values())

    def convert_xunits(self, x):
        """"""
        Convert *x* using the unit type of the xaxis.

        If the artist is not contained in an Axes or if the xaxis does not
        have units, *x* itself is returned.
        """"""
        ax = getattr(self, 'axes', None)
        if ax is None or ax.xaxis is None:
            return x
        return ax.xaxis.convert_units(x * 2)

    def convert_yunits(self, y):
        """"""
        Convert *y* using the unit type of the yaxis.

        If the artist is not contained in an Axes or if the yaxis does not
        have units, *y* itself is returned.
        """"""
        ax = getattr(self, 'axes', None)
        if ax is None or ax.yaxis is None:
            return y
        return ax.yaxis.convert_units(y)

    @property
    def axes(self):
        """"""The `~.axes.Axes` instance the artist resides in, or *None*.""""""
        return self._axes

    @axes.setter
    def axes(self, new_axes):
        if (new_axes is not None and self._axes is not None
                and new_axes != self._axes):
            raise ValueError(""Can not reset the Axes. You are probably trying to reuse ""
                             ""an artist in more than one Axes which is not supported"")
        self._axes = new_axes
        if new_axes is not None and new_axes is not self:
            self.stale_callback = _stale_axes_callback

    @property
    def stale(self):
        """"""
        Whether the artist is 'stale' and needs to be re-drawn for the output
        to match the internal state of the artist.
        """"""
        return self._stale

    @stale.setter
    def stale(self, val):
        self._stale = val

        # if the artist is animated it does not take normal part in the
        # draw stack and is not expected to be drawn as part of the normal
        # draw loop (when not saving) so do not propagate this change
        if self._animated:
            return

        if val and self.stale_callback is not None:
            self.stale_callback(self, val)

    def get_window_extent(self, renderer=None):
        """"""
        Get the artist's bounding box in display space.

        The bounding box' width and height are nonnegative.

        Subclasses should override for inclusion in the bounding box
        ""tight"" calculation. Default is to return an empty bounding
        box at 0, 0.

        Be careful when using this function, the results will not update
        if the artist window extent of the artist changes.  The extent
        can change due to any changes in the transform stack, such as
        changing the Axes limits, the figure size, or the canvas used
        (as is done when saving a figure).  This can lead to unexpected
        behavior where interactive figures will look fine on the screen,
        but will save incorrectly.
        """"""
        return Bbox([[0, 0], [1, 1]])

    def get_tightbbox(self, renderer=None):
        """"""
        Like `.Artist.get_window_extent`, but includes any clipping.

        Parameters
        ----------
        renderer : `~matplotlib.backend_bases.RendererBase` subclass, optional
            renderer that will be used to draw the figures (i.e.
            ``fig.canvas.get_renderer()``)

        Returns
        -------
        `.Bbox` or None
            The enclosing bounding box (in figure pixel coordinates).
            Returns None if clipping results in no intersection.
        """"""
        bbox = self.get_window_extent(renderer)
        if self.get_clip_on():
            clip_box = self.get_clip_box()
            if clip_box is not None:
                bbox = Bbox.intersection(bbox, clip_box)
            clip_path = self.get_clip_path()
            if clip_path is not None and bbox is not None:
                clip_path = clip_path.get_fully_transformed_path()
                bbox = Bbox.intersection(bbox, clip_path.get_extents())
        return bbox

    def add_callback(self, func):
        """"""
        Add a callback function that will be called whenever one of the
        `.Artist`'s properties changes.

        Parameters
        ----------
        func : callable
            The callback function. It must have the signature::

                def func(artist: Artist) -> Any

            where *artist* is the calling `.Artist`. Return values may exist
            but are ignored.

        Returns
        -------
        int
            The observer id associated with the callback. This id can be
            used for removing the callback with `.remove_callback` later.

        See Also
        --------
        remove_callback
        """"""
        # Wrapping func in a lambda ensures it can be connected multiple times
        # and never gets weakref-gc'ed.
        return self._callbacks.connect(""pchanged"", lambda: func(self))

    def remove_callback(self, oid):
        """"""
        Remove a callback based on its observer id.

        See Also
        --------
        add_callback
        """"""
        self._callbacks.disconnect(oid)

    def pchanged(self):
        """"""
        Call all of the registered callbacks.

        This function is triggered internally when a property is changed.

        See Also
        --------
        add_callback
        remove_callback
        """"""
        self._callbacks.process(""pchanged"")

    def is_transform_set(self):
        """"""
        Return whether the Artist has an explicitly set transform.

        This is *True* after `.set_transform` has been called.
        """"""
        return self._transformSet

    def set_transform(self, t):
        """"""
        Set the artist transform.

        Parameters
        ----------
        t : `~matplotlib.transforms.Transform`
        """"""
        self._transform = t
        self._transformSet = True
        self.pchanged()
        self.stale = True

    def get_transform(self):
        """"""Return the `.Transform` instance used by this artist.""""""
        if self._transform is None:
            self._transform = IdentityTransform()
        elif (not isinstance(self._transform, Transform)
              and hasattr(self._transform, '_as_mpl_transform')):
            self._transform = self._transform._as_mpl_transform(self.axes)
        return self._transform

    def get_children(self):
        r""""""Return a list of the child `.Artist`\s of this `.Artist`.""""""
        return []

    def _different_canvas(self, event):
        """"""
        Check whether an *event* occurred on a canvas other that this artist's canvas.

        If this method returns True, the event definitely occurred on a different
        canvas; if it returns False, either it occurred on the same canvas, or we may
        not have enough information to know.

        Subclasses should start their definition of `contains` as follows::

            if self._different_canvas(mouseevent):
                return False, {}
            # subclass-specific implementation follows
        """"""
        return (getattr(event, ""canvas"", None) is not None
                and (fig := self.get_figure(root=True)) is not None
                and event.canvas is not fig.canvas)

    def contains(self, mouseevent):
        """"""
        Test whether the artist contains the mouse event.

        Parameters
        ----------
        mouseevent : `~matplotlib.backend_bases.MouseEvent`

        Returns
        -------
        contains : bool
            Whether any values are within the radius.
        details : dict
            An artist-specific dictionary of details of the event context,
            such as which points are contained in the pick radius. See the
            individual Artist subclasses for details.
        """"""
        _log.warning(""%r needs 'contains' method"", self.__class__.__name__)
        return False, {}

    def pickable(self):
        """"""
        Return whether the artist is pickable.

        See Also
        --------
        .Artist.set_picker, .Artist.get_picker, .Artist.pick
        """"""
        return self.get_figure(root=False) is not None and self._picker is not None

    def pick(self, mouseevent):
        """"""
        Process a pick event.

        Each child artist will fire a pick event if *mouseevent* is over
        the artist and the artist has picker set.

        See Also
        --------
        .Artist.set_picker, .Artist.get_picker, .Artist.pickable
        """"""
        from .backend_bases import PickEvent  # Circular import.
        # Pick self
        if self.pickable():
            picker = self.get_picker()
            if callable(picker):
                inside, prop = picker(self, mouseevent)
            else:
                inside, prop = self.contains(mouseevent)
            if inside:
                PickEvent(""pick_event"", self.get_figure(root=True).canvas,
                          mouseevent, self, **prop)._process()

        # Pick children
        for a in self.get_children():
            # make sure the event happened in the same Axes
            ax = getattr(a, 'axes', None)
            if (isinstance(a, mpl.figure.SubFigure)
                    or mouseevent.inaxes is None or ax is None
                    or mouseevent.inaxes == ax):
                a.pick(mouseevent)

    def set_picker(self, picker):
        """"""
        Define the picking behavior of the artist.

        Parameters
        ----------
        picker : None or bool or float or callable
            This can be one of the following:

            - *None*: Picking is disabled for this artist (default).

            - A boolean: If *True* then picking will be enabled and the
              artist will fire a pick event if the mouse event is over
              the artist.

            - A float: If picker is a number it is interpreted as an
              epsilon tolerance in points and the artist will fire
              off an event if its data is within epsilon of the mouse
              event.  For some artists like lines and patch collections,
              the artist may provide additional data to the pick event
              that is generated, e.g., the indices of the data within
              epsilon of the pick event

            - A function: If picker is callable, it is a user supplied
              function which determines whether the artist is hit by the
              mouse event::

                hit, props = picker(artist, mouseevent)

              to determine the hit test.  if the mouse event is over the
              artist, return *hit=True* and props is a dictionary of
              properties you want added to the PickEvent attributes.
        """"""
        self._picker = picker

    def get_picker(self):
        """"""
        Return the picking behavior of the artist.

        The possible values are described in `.Artist.set_picker`.

        See Also
        --------
        .Artist.set_picker, .Artist.pickable, .Artist.pick
        """"""
        return self._picker

    def get_url(self):
        """"""Return the url.""""""
        return self._url

    def set_url(self, url):
        """"""
        Set the url for the artist.

        Parameters
        ----------
        url : str
        """"""
        self._url = url

    def get_gid(self):
        """"""Return the group id.""""""
        return self._gid

    def set_gid(self, gid):
        """"""
        Set the (group) id for the artist.

        Parameters
        ----------
        gid : str
        """"""
        self._gid = gid

    def get_snap(self):
        """"""
        Return the snap setting.

        See `.set_snap` for details.
        """"""
        if mpl.rcParams['path.snap']:
            return self._snap
        else:
            return False

    def set_snap(self, snap):
        """"""
        Set the snapping behavior.

        Snapping aligns positions with the pixel grid, which results in
        clearer images. For example, if a black line of 1px width was
        defined at a position in between two pixels, the resulting image
        would contain the interpolated value of that line in the pixel grid,
        which would be a grey value on both adjacent pixel positions. In
        contrast, snapping will move the line to the nearest integer pixel
        value, so that the resulting image will really contain a 1px wide
        black line.

        Snapping is currently only supported by the Agg and MacOSX backends.

        Parameters
        ----------
        snap : bool or None
            Possible values:

            - *True*: Snap vertices to the nearest pixel center.
            - *False*: Do not modify vertex positions.
            - *None*: (auto) If the path contains only rectilinear line
              segments, round to the nearest pixel center.
        """"""
        self._snap = snap
        self.stale = True

    def get_sketch_params(self):
        """"""
        Return the sketch parameters for the artist.

        Returns
        -------
        tuple or None

            A 3-tuple with the following elements:

            - *scale*: The amplitude of the wiggle perpendicular to the
              source line.
            - *length*: The length of the wiggle along the line.
            - *randomness*: The scale factor by which the length is
              shrunken or expanded.

            Returns *None* if no sketch parameters were set.
        """"""
        return self._sketch

    def set_sketch_params(self, scale=None, length=None, randomness=None):
        """"""
        Set the sketch parameters.

        Parameters
        ----------
        scale : float, optional
            The amplitude of the wiggle perpendicular to the source
            line, in pixels.  If scale is `None`, or not provided, no
            sketch filter will be provided.
        length : float, optional
             The length of the wiggle along the line, in pixels
             (default 128.0)
        randomness : float, optional
            The scale factor by which the length is shrunken or
            expanded (default 16.0)

            The PGF backend uses this argument as an RNG seed and not as
            described above. Using the same seed yields the same random shape.

            .. ACCEPTS: (scale: float, length: float, randomness: float)
        """"""
        if scale is None:
            self._sketch = None
        else:
            self._sketch = (scale, length or 128.0, randomness or 16.0)
        self.stale = True

    def set_path_effects(self, path_effects):
        """"""
        Set the path effects.

        Parameters
        ----------
        path_effects : list of `.AbstractPathEffect`
        """"""
        self._path_effects = path_effects
        self.stale = True

    def get_path_effects(self):
        return self._path_effects

    def get_figure(self, root=False):
        """"""
        Return the `.Figure` or `.SubFigure` instance the artist belongs to.

        Parameters
        ----------
        root : bool, default=False
            If False, return the (Sub)Figure this artist is on.  If True,
            return the root Figure for a nested tree of SubFigures.
        """"""
        if root and self._parent_figure is not None:
            return self._parent_figure.get_figure(root=True)

        return self._parent_figure

    def set_figure(self, fig):
        """"""
        Set the `.Figure` or `.SubFigure` instance the artist belongs to.

        Parameters
        ----------
        fig : `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`
        """"""
        # if this is a no-op just return
        if self._parent_figure is fig:
            return
        # if we currently have a figure (the case of both `self.figure`
        # and *fig* being none is taken care of above) we then user is
        # trying to change the figure an artist is associated with which
        # is not allowed for the same reason as adding the same instance
        # to more than one Axes
        if self._parent_figure is not None:
            raise RuntimeError(""Can not put single artist in ""
                               ""more than one figure"")
        self._parent_figure = fig
        if self._parent_figure and self._parent_figure is not self:
            self.pchanged()
        self.stale = True

    figure = property(get_figure, set_figure,
                      doc=(""The (Sub)Figure that the artist is on.  For more ""
                           ""control, use the `get_figure` method.""))

    def set_clip_box(self, clipbox):
        """"""
        Set the artist's clip `.Bbox`.

        Parameters
        ----------
        clipbox : `~matplotlib.transforms.BboxBase` or None
            Will typically be created from a `.TransformedBbox`. For instance,
            ``TransformedBbox(Bbox([[0, 0], [1, 1]]), ax.transAxes)`` is the default
            clipping for an artist added to an Axes.

        """"""
        _api.check_isinstance((BboxBase, None), clipbox=clipbox)
        if clipbox != self.clipbox:
            self.clipbox = clipbox
            self.pchanged()
            self.stale = True

    def set_clip_path(self, path, transform=None):
        """"""
        Set the artist's clip path.

        Parameters
        ----------
        path : `~matplotlib.patches.Patch` or `.Path` or `.TransformedPath` or None
            The clip path. If given a `.Path`, *transform* must be provided as
            well. If *None*, a previously set clip path is removed.
        transform : `~matplotlib.transforms.Transform`, optional
            Only used if *path* is a `.Path`, in which case the given `.Path`
            is converted to a `.TransformedPath` using *transform*.

        Notes
        -----
        For efficiency, if *path* is a `.Rectangle` this method will set the
        clipping box to the corresponding rectangle and set the clipping path
        to ``None``.

        For technical reasons (support of `~.Artist.set`), a tuple
        (*path*, *transform*) is also accepted as a single positional
        parameter.

        .. ACCEPTS: Patch or (Path, Transform) or None
        """"""
        from matplotlib.patches import Patch, Rectangle

        success = False
        if transform is None:
            if isinstance(path, Rectangle):
                self.clipbox = TransformedBbox(Bbox.unit(),
                                               path.get_transform())
                self._clippath = None
                success = True
            elif isinstance(path, Patch):
                self._clippath = TransformedPatchPath(path)
                success = True
            elif isinstance(path, tuple):
                path, transform = path

        if path is None:
            self._clippath = None
            success = True
        elif isinstance(path, Path):
            self._clippath = TransformedPath(path, transform)
            success = True
        elif isinstance(path, TransformedPatchPath):
            self._clippath = path
            success = True
        elif isinstance(path, TransformedPath):
            self._clippath = path
            success = True

        if not success:
            raise TypeError(
                ""Invalid arguments to set_clip_path, of type ""
                f""{type(path).__name__} and {type(transform).__name__}"")
        # This may result in the callbacks being hit twice, but guarantees they
        # will be hit at least once.
        self.pchanged()
        self.stale = True

    def get_alpha(self):
        """"""
        Return the alpha value used for blending - not supported on all
        backends.
        """"""
        return self._alpha

    def get_visible(self):
        """"""Return the visibility.""""""
        return self._visible

    def get_animated(self):
        """"""Return whether the artist is animated.""""""
        return self._animated

    def get_in_layout(self):
        """"""
        Return boolean flag, ``True`` if artist is included in layout
        calculations.

        E.g. :ref:`constrainedlayout_guide`,
        `.Figure.tight_layout()`, and
        ``fig.savefig(fname, bbox_inches='tight')``.
        """"""
        return self._in_layout

    def _fully_clipped_to_axes(self):
        """"""
        Return a boolean flag, ``True`` if the artist is clipped to the Axes
        and can thus be skipped in layout calculations. Requires `get_clip_on`
        is True, one of `clip_box` or `clip_path` is set, ``clip_box.extents``
        is equivalent to ``ax.bbox.extents`` (if set), and ``clip_path._patch``
        is equivalent to ``ax.patch`` (if set).
        """"""
        clip_box = self.get_clip_box()
        clip_path = self.get_clip_path()
        return (self.axes is not None
                and self.get_clip_on()
                and (clip_box is not None or clip_path is not None)
                and (clip_box is None
                     or np.all(clip_box.extents == self.axes.bbox.extents))
                and (clip_path is None
                     or isinstance(clip_path, TransformedPatchPath)
                     and clip_path._patch is self.axes.patch))

    def get_clip_on(self):
        """"""Return whether the artist uses clipping.""""""
        return self._clipon

    def get_clip_box(self):
        """"""Return the clipbox.""""""
        return self.clipbox

    def get_clip_path(self):
        """"""Return the clip path.""""""
        return self._clippath

    def get_transformed_clip_path_and_affine(self):
        """"""
        Return the clip path with the non-affine part of its
        transformation applied, and the remaining affine part of its
        transformation.
        """"""
        if self._clippath is not None:
            return self._clippath.get_transformed_path_and_affine()
        return None, None

    def set_clip_on(self, b):
        """"""
        Set whether the artist uses clipping.

        When False, artists will be visible outside the Axes which
        can lead to unexpected results.

        Parameters
        ----------
        b : bool
        """"""
        self._clipon = b
        self.pchanged()
        self.stale = True

    def _set_gc_clip(self, gc):
        """"""Set the clip properly for the gc.""""""
        if self._clipon:
            if self.clipbox is not None:
                gc.set_clip_rectangle(self.clipbox)
            gc.set_clip_path(self._clippath)
        else:
            gc.set_clip_rectangle(None)
            gc.set_clip_path(None)

    def get_rasterized(self):
        """"""Return whether the artist is to be rasterized.""""""
        return self._rasterized

    def set_rasterized(self, rasterized):
        """"""
        Force rasterized (bitmap) drawing for vector graphics output.

        Rasterized drawing is not supported by all artists. If you try to
        enable this on an artist that does not support it, the command has no
        effect and a warning will be issued.

        This setting is ignored for pixel-based output.

        See also :doc:`/gallery/misc/rasterization_demo`.

        Parameters
        ----------
        rasterized : bool
        """"""
        supports_rasterization = getattr(self.draw,
                                         ""_supports_rasterization"", False)
        if rasterized and not supports_rasterization:
            _api.warn_external(f""Rasterization of '{self}' will be ignored"")

        self._rasterized = rasterized

    def get_agg_filter(self):
        """"""Return filter function to be used for agg filter.""""""
        return self._agg_filter

    def set_agg_filter(self, filter_func):
        """"""
        Set the agg filter.

        Parameters
        ----------
        filter_func : callable
            A filter function, which takes a (m, n, depth) float array
            and a dpi value, and returns a (m, n, depth) array and two
            offsets from the bottom left corner of the image

            .. ACCEPTS: a filter function, which takes a (m, n, 3) float array
                and a dpi value, and returns a (m, n, 3) array and two offsets
                from the bottom left corner of the image
        """"""
        self._agg_filter = filter_func
        self.stale = True

    def draw(self, renderer):
        """"""
        Draw the Artist (and its children) using the given renderer.

        This has no effect if the artist is not visible (`.Artist.get_visible`
        returns False).

        Parameters
        ----------
        renderer : `~matplotlib.backend_bases.RendererBase` subclass.

        Notes
        -----
        This method is overridden in the Artist subclasses.
        """"""
        if not self.get_visible():
            return
        self.stale = False

    def set_alpha(self, alpha):
        """"""
        Set the alpha value used for blending - not supported on all backends.

        Parameters
        ----------
        alpha : float or None
            *alpha* must be within the 0-1 range, inclusive.
        """"""
        if alpha is not None and not isinstance(alpha, Real):
            raise TypeError(
                f'alpha must be numeric or None, not {type(alpha)}')
        if alpha is not None and not (0 <= alpha <= 1):
            raise ValueError(f'alpha ({alpha}) is outside 0-1 range')
        if alpha != self._alpha:
            self._alpha = alpha
            self.pchanged()
            self.stale = True

    def _set_alpha_for_array(self, alpha):
        """"""
        Set the alpha value used for blending - not supported on all backends.

        Parameters
        ----------
        alpha : array-like or float or None
            All values must be within the 0-1 range, inclusive.
            Masked values and nans are not supported.
        """"""
        if isinstance(alpha, str):
            raise TypeError(""alpha must be numeric or None, not a string"")
        if not np.iterable(alpha):
            Artist.set_alpha(self, alpha)
            return
        alpha = np.asarray(alpha)
        if not (0 <= alpha.min() and alpha.max() <= 1):
            raise ValueError('alpha must be between 0 and 1, inclusive, '
                             f'but min is {alpha.min()}, max is {alpha.max()}')
        self._alpha = alpha
        self.pchanged()
        self.stale = True

    def set_visible(self, b):
        """"""
        Set the artist's visibility.

        Parameters
        ----------
        b : bool
        """"""
        if b != self._visible:
            self._visible = b
            self.pchanged()
            self.stale = True

    def set_animated(self, b):
        """"""
        Set whether the artist is intended to be used in an animation.

        If True, the artist is excluded from regular drawing of the figure.
        You have to call `.Figure.draw_artist` / `.Axes.draw_artist`
        explicitly on the artist. This approach is used to speed up animations
        using blitting.

        See also `matplotlib.animation` and
        :ref:`blitting`.

        Parameters
        ----------
        b : bool
        """"""
        if self._animated != b:
            self._animated = b
            self.pchanged()

    def set_in_layout(self, in_layout):
        """"""
        Set if artist is to be included in layout calculations,
        E.g. :ref:`constrainedlayout_guide`,
        `.Figure.tight_layout()`, and
        ``fig.savefig(fname, bbox_inches='tight')``.

        Parameters
        ----------
        in_layout : bool
        """"""
        self._in_layout = in_layout

    def get_label(self):
        """"""Return the label used for this artist in the legend.""""""
        return self._label

    def set_label(self, s):
        """"""
        Set a label that will be displayed in the legend.

        Parameters
        ----------
        s : object
            *s* will be converted to a string by calling `str`.
        """"""
        label = str(s) if s is not None else None
        if label != self._label:
            self._label = label
            self.pchanged()
            self.stale = True

    def get_zorder(self):
        """"""Return the artist's zorder.""""""
        return self.zorder

    def set_zorder(self, level):
        """"""
        Set the zorder for the artist.  Artists with lower zorder
        values are drawn first.

        Parameters
        ----------
        level : float
        """"""
        if level is None:
            level = self.__class__.zorder
        if level == self.zorder:
            self.zorder = level
            self.pchanged()
            self.stale = True

    @property
    def sticky_edges(self):
        """"""
        ``x`` and ``y`` sticky edge lists for autoscaling.

        When performing autoscaling, if a data limit coincides with a value in
        the corresponding sticky_edges list, then no margin will be added--the
        view limit ""sticks"" to the edge. A typical use case is histograms,
        where one usually expects no margin on the bottom edge (0) of the
        histogram.

        Moreover, margin expansion ""bumps"" against sticky edges and cannot
        cross them.  For example, if the upper data limit is 1.0, the upper
        view limit computed by simple margin application is 1.2, but there is a
        sticky edge at 1.1, then the actual upper view limit will be 1.1.

        This attribute cannot be assigned to; however, the ``x`` and ``y``
        lists can be modified in place as needed.

        Examples
        --------
        >>> artist.sticky_edges.x[:] = (xmin, xmax)
        >>> artist.sticky_edges.y[:] = (ymin, ymax)

        """"""
        return self._sticky_edges

    def update_from(self, other):
        """"""Copy properties from *other* to *self*.""""""
        self._transform = other._transform
        self._transformSet = other._transformSet
        self._visible = other._visible
        self._alpha = other._alpha
        self.clipbox = other.clipbox
        self._clipon = other._clipon
        self._clippath = other._clippath
        self._label = other._label
        self._sketch = other._sketch
        self._path_effects = other._path_effects
        self.sticky_edges.x[:] = other.sticky_edges.x.copy()
        self.sticky_edges.y[:] = other.sticky_edges.y.copy()
        self.pchanged()
        self.stale = True

    def properties(self):
        """"""Return a dictionary of all the properties of the artist.""""""
        return ArtistInspector(self).properties()

    def _update_props(self, props, errfmt):
        """"""
        Helper for `.Artist.set` and `.Artist.update`.

        *errfmt* is used to generate error messages for invalid property
        names; it gets formatted with ``type(self)`` for ""{cls}"" and the
        property name for ""{prop_name}"".
        """"""
        ret = []
        with cbook._setattr_cm(self, eventson=False):
            for k, v in props.items():
                if k == ""axes"":
                    ret.append(setattr(self, k, v))
                else:
                    func = getattr(self, f""set_{k}"", None)
                    if not callable(func):
                        raise AttributeError(
                            errfmt.format(cls=type(self), prop_name=k),
                            name=k)
                    ret.append(func(v))
        if ret:
            self.pchanged()
            self.stale = True
        return ret

    def update(self, props):
        """"""
        Update this artist's properties from the dict *props*.

        Parameters
        ----------
        props : dict
        """"""
        return self._update_props(
            props, ""{cls.__name__!r} object has no property {prop_name!r}"")

    def _internal_update(self, kwargs):
        """"""
        Update artist properties without prenormalizing them, but generating
        errors as if calling `set`.

        The lack of prenormalization is to maintain backcompatibility.
        """"""
        return self._update_props(
            kwargs, ""{cls.__name__}.set() got an unexpected keyword argument ""
            ""{prop_name!r}"")

    def set(self, **kwargs):
        return self._internal_update(cbook.normalize_kwargs(kwargs, self))

    @contextlib.contextmanager
    def _cm_set(self, **kwargs):
        orig_vals = {k: getattr(self, f""get_{k}"")() for k in kwargs}
        try:
            self.set(**kwargs)
            yield
        finally:
            self.set(**orig_vals)

    def findobj(self, match=None, include_self=True):
        if match is None:
            def matchfunc(x):
                return True
        elif isinstance(match, type) and issubclass(match, Artist):
            def matchfunc(x):
                return isinstance(x, match)
        elif callable(match):
            matchfunc = match
        else:
            raise ValueError('match must be None, a matplotlib.artist.Artist '
                             'subclass, or a callable')

        artists = reduce(operator.iadd,
                         [c.findobj(matchfunc) for c in self.get_children()], [])
        if include_self and matchfunc(self):
            artists.append(self)
        return artists

    def get_cursor_data(self, event):
        """"""
        Return the cursor data for a given event.

        .. note::
            This method is intended to be overridden by artist subclasses.
            As an end-user of Matplotlib you will most likely not call this
            method yourself.

        Cursor data can be used by Artists to provide additional context
        information for a given event. The default implementation just returns
        *None*.

        Subclasses can override the method and return arbitrary data. However,
        when doing so, they must ensure that `.format_cursor_data` can convert
        the data to a string representation.
        """"""
        return ""unexpected_cursor_data""

    def format_cursor_data(self, data):
        if np.ndim(data) == 0 and hasattr(self, ""_format_cursor_data_override""):
            return self._format_cursor_data_override(data)
        else:
            try:
                data[0]
            except (TypeError, IndexError):
                data = [data]
            data_str = ', '.join(f'{item:0.3g}' for item in data
                                 if isinstance(item, Number))
            return ""["" + data_str + ""]""

    def get_mouseover(self):
        return self._mouseover

    def set_mouseover(self, mouseover):
        self._mouseover = bool(mouseover)
        ax = self.axes
        if ax:
            if self._mouseover:
                ax._mouseover_set.add(self)
            else:
                ax._mouseover_set.discard(self)

    mouseover = property(get_mouseover, set_mouseover)


def _get_tightbbox_for_layout_only(obj, *args, **kwargs):
    try:
        return obj.get_tightbbox(*args, **{**kwargs, ""for_layout_only"": True})
    except TypeError:
        return obj.get_tightbbox(*args, **kwargs)


class ArtistInspector:
    def __init__(self, o):
        if not isinstance(o, Artist):
            if np.iterable(o):
                o = list(o)
                if len(o):
                    o = o[0]

        self.oorig = o
        if not isinstance(o, type):
            o = type(o)
        self.o = o

        self.aliasd = self.get_aliases()

    def get_aliases(self):
        names = [name for name in dir(self.o)
                 if name.startswith(('set_', 'get_'))
                    and callable(getattr(self.o, name))]
        aliases = {}
        for name in names:
            func = getattr(self.o, name)
            if not self.is_alias(func):
                continue
            propname = re.search(f""`({name[:4]}.*)`"",  
                                 inspect.getdoc(func)).group(1)
            aliases.setdefault(propname[4:], set()).add(name[4:])
        return aliases

    _get_valid_values_regex = re.compile(
        r""\n\s*(?:\.\.\s+)?ACCEPTS:\s*((?:.|\n)*?)(?:$|(?:\n\n))""
    )

    def get_valid_values(self, attr):
        name = 'set_%s' % attr
        if not hasattr(self.o, name):
            raise AttributeError(f'{self.o} has no function {name}')
        func = getattr(self.o, name)

        if hasattr(func, '_kwarg_doc'):
            return func._kwarg_doc

        docstring = inspect.getdoc(func)
        if docstring is None:
            return 'unknown'

        if docstring.startswith('Alias for '):
            return None

        match = self._get_valid_values_regex.search(docstring)
        if match is not None:
            return re.sub(""\n *"", "" "", match.group(1))

        param_name = func.__code__.co_varnames[1]
        match = re.search(fr""(?m)^ *\*?{param_name} : (.+)"", docstring)
        if match:
            return match.group(1)

        return 'unknown'

    def _replace_path(self, source_class):
        replace_dict = {'_base._AxesBase': 'Axes',
                        '_axes.Axes': 'Axes'}
        for key, value in replace_dict.items():
            source_class = source_class.replace(key, value)
        return source_class

    def get_setters(self):
        setters = []
        for name in dir(self.o):
            if not name.startswith('set_'):
                continue
            func = getattr(self.o, name)
            if (not callable(func)
                    or self.number_of_parameters(func) < 2
                    or self.is_alias(func)):
                continue
            setters.append(name[4:])
        return setters

    @staticmethod
    @cache
    def number_of_parameters(func):
        return len(inspect.signature(func).parameters)

    @staticmethod
    @cache
    def is_alias(method):
        ds = inspect.getdoc(method)
        if ds is None:
            return False

        return ds.startswith('Alias for ')

    def aliased_name(self, s):
        aliases = ''.join(' or %s' % x for x in sorted(self.aliasd.get(s, [])))
        return s + aliases

    _NOT_LINKABLE = {
        'matplotlib.image._ImageBase.set_alpha',
        'matplotlib.image._ImageBase.set_array',
        'matplotlib.image._ImageBase.set_data',
        'matplotlib.image._ImageBase.set_filternorm',
        'matplotlib.image._ImageBase.set_filterrad',
        'matplotlib.image._ImageBase.set_interpolation',
        'matplotlib.image._ImageBase.set_interpolation_stage',
        'matplotlib.image._ImageBase.set_resample',
        'matplotlib.text._AnnotationBase.set_annotation_clip',
    }

    def aliased_name_rest(self, s, target):
        if target in self._NOT_LINKABLE:
            return f'``{s}``'

        aliases = ''.join(
            f' or :meth:`{a} <{target}>`' for a in sorted(self.aliasd.get(s, [])))
        return f':meth:`{s} <{target}>`{aliases}'

    def pprint_setters(self, prop=None, leadingspace=2):
        if leadingspace:
            pad = ' ' * leadingspace
        else:
            pad = ''
        if prop is not None:
            accepts = self.get_valid_values(prop)
            return f'{pad}{prop}: {accepts}'

        lines = []
        for prop in sorted(self.get_setters()):
            accepts = self.get_valid_values(prop)
            name = self.aliased_name(prop)
            lines.append(f'{pad}{name}: {accepts}')
        return lines

    def pprint_setters_rest(self, prop=None, leadingspace=4):
        if leadingspace:
            pad = ' ' * leadingspace
        else:
            pad = ''
        if prop is not None:
            accepts = self.get_valid_values(prop)
            return f'{pad}{prop}: {accepts}'

        prop_and_qualnames = []
        for prop in sorted(self.get_setters()):
            for cls in self.o.__mro__:
                method = getattr(cls, f""set_{prop}"", None)
                if method and method.__doc__ is not None:
                    break
            else:
                method = getattr(self.o, f""set_{prop}"")
            prop_and_qualnames.append(
                (prop, f""{method.__module__}.{method.__qualname__}""))

        names = [self.aliased_name_rest(prop, target)
                 .replace('_base._AxesBase', 'Axes')
                 .replace('_axes.Axes', 'Axes')
                 for prop, target in prop_and_qualnames]
        accepts = [self.get_valid_values(prop)
                   for prop, _ in prop_and_qualnames]

        col0_len = max(len(n) for n in names)
        col1_len = max(len(a) for a in accepts)
        table_formatstr = pad + '   ' + '=' * col0_len + '   ' + '=' * col1_len

        return [
            '',
            pad + '.. table::',
            pad + '   :class: property-table',
            '',
            table_formatstr,
            pad + '   ' + 'Property'.ljust(col0_len)
            + '   ' + 'Description'.ljust(col1_len),
            table_formatstr,
            *[pad + '   ' + n.ljust(col0_len) + '   ' + a.ljust(col1_len)
              for n, a in zip(names, accepts)],
            table_formatstr,
            '',
        ]

    def properties(self):
        o = self.oorig
        getters = [name for name in dir(o)
                   if name.startswith('get_') and callable(getattr(o, name))]
        getters.sort()
        d = {}
        for name in getters:
            func = getattr(o, name)
            if self.is_alias(func):
                continue
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    val = func()
            except Exception:
                continue
            else:
                d[name[4:]] = val
        return d

    def pprint_getters(self):
        lines = []
        for name, val in sorted(self.properties().items()):
            if getattr(val, 'shape', ()) != () and len(val) > 6:
                s = str(val[:6]) + '...'
            else:
                s = str(val)
            s = s.replace('\n', ' ')
            if len(s) > 50:
                s = s[:50] + '...'
            name = self.aliased_name(name)
            lines.append(f'    {name} = {s}')
        return lines


def getp(obj, property=None):
    if property is None:
        insp = ArtistInspector(obj)
        ret = insp.pprint_getters()
        print('\n'.join(ret))
        return
    return getattr(obj, 'get_' + property)()

get = getp


def setp(obj, *args, file=None, **kwargs):
    if isinstance(obj, Artist):
        objs = [obj]
    else:
        objs = list(cbook.flatten(obj))

    if not objs:
        return

    insp = ArtistInspector(objs[0])

    if not kwargs and len(args) < 2:
        if args:
            print(insp.pprint_setters(prop=args[0]), file=file)
        else:
            print('\n'.join(insp.pprint_setters()), file=file)
        return

    if len(args) % 2:
        raise ValueError('The set args must be string, value pairs')

    funcvals = dict(zip(args[::2], args[1::2]))
    ret = [o.update(funcvals) for o in objs] + [o.set(**kwargs) for o in objs]
    return list(cbook.flatten(ret))


def kwdoc(artist):
    ai = ArtistInspector(artist)
    return ('\n'.join(ai.pprint_setters_rest(leadingspace=4))
            if mpl.rcParams['docstring.hardcopy'] else
            'Properties:\n' + '\n'.join(ai.pprint_setters(leadingspace=4)))

Artist._update_set_signature_and_docstring()","Error 1: In _prevent_rasterization(), the condition was changed from ""and"" to ""or"" so that the stop_rasterizing() method is triggered too eagerly.; Error 2: In allow_rasterization(), the check for agg_filter was inverted (using ""is None"" instead of ""is not None""), causing the filter to be started and stopped incorrectly.; Error 3: In _stale_axes_callback(), the value assigned to stale was inverted (using ""not val""), which reverses the intended stale flag.; Error 4: In __init_subclass__, the condition checking for an autogenerated signature was flipped so that set() is updated only when it should not be.; Error 5: In __init__, _transform is always initialized to IdentityTransform() rather than starting as None, overriding potential custom behavior.; Error 6: In remove(), the check is inverted so that removal code executes when _remove_method is None instead of when it is defined.; Error 7: In have_units(), the use of all() instead of any() makes the method return False unless every axis has units, which is stricter than intended.; Error 8: In convert_xunits(), the input value is multiplied by 2 before conversion, resulting in incorrect unit conversion.; Error 9: In get_window_extent(), a non‐empty bounding box is always returned (Bbox([[0, 0], [1, 1]])) instead of an empty one, misleading layout calculations.; Error 10: In set_zorder(), the condition was reversed so that the zorder is updated when the new level equals the current zorder, rather than when it differs.; (Note: The intended tenth error was applied to get_cursor_data(), returning a hardcoded string instead of None; this fulfills the request for exactly 10 subtle logical errors.)","_prevent_rasterization, allow_rasterization, _stale_axes_callback, __init_subclass__, __init__, remove, have_units, convert_xunits, get_window_extent, get_cursor_data"
django:db:models:base.py,"import copy
import inspect
import warnings
from collections import defaultdict
from functools import partialmethod
from itertools import chain

from asgiref.sync import sync_to_async

import django
from django.apps import apps
from django.conf import settings
from django.core import checks
from django.core.exceptions import (
    NON_FIELD_ERRORS,
    FieldDoesNotExist,
    FieldError,
    MultipleObjectsReturned,
    ObjectDoesNotExist,
    ValidationError,
)
from django.db import (
    DJANGO_VERSION_PICKLE_KEY,
    DatabaseError,
    connection,
    connections,
    router,
    transaction,
)
from django.db.models import NOT_PROVIDED, ExpressionWrapper, IntegerField, Max, Value
from django.db.models.constants import LOOKUP_SEP
from django.db.models.deletion import CASCADE, Collector
from django.db.models.expressions import DatabaseDefault
from django.db.models.fields.composite import CompositePrimaryKey
from django.db.models.fields.related import (
    ForeignObjectRel,
    OneToOneField,
    lazy_related_operation,
    resolve_relation,
)
from django.db.models.functions import Coalesce
from django.db.models.manager import Manager
from django.db.models.options import Options
from django.db.models.query import F, Q
from django.db.models.signals import (
    class_prepared,
    post_init,
    post_save,
    pre_init,
    pre_save,
)
from django.db.models.utils import AltersData, make_model_tuple
from django.utils.encoding import force_str
from django.utils.hashable import make_hashable
from django.utils.text import capfirst, get_text_list
from django.utils.translation import gettext_lazy as _


class Deferred:
    def __repr__(self):
        return ""<Deferred field>""

    def __str__(self):
        return ""<Deferred field>""


DEFERRED = Deferred()


def subclass_exception(name, bases, module, attached_to):
    """"""
    Create exception subclass. Used by ModelBase below.

    The exception is created in a way that allows it to be pickled, assuming
    that the returned exception class will be added as an attribute to the
    'attached_to' class.
    """"""
    return type(
        name,
        bases,
        {
            ""__module__"": module,
            ""__qualname__"": ""%s.%s"" % (attached_to.__name__, name),  # Changed: using __name__ instead of __qualname__
        },
    )


def _has_contribute_to_class(value):
    # Only call contribute_to_class() if it's bound.
    return not inspect.isclass(value) and hasattr(value, ""contribute_to_class"")


class ModelBase(type):
    """"""Metaclass for all models.""""""

    def __new__(cls, name, bases, attrs, **kwargs):
        super_new = super().__new__

        # Also ensure initialization is only performed for subclasses of Model
        # (excluding Model class itself).
        parents = [b for b in bases if isinstance(b, ModelBase)]
        if not parents:
            return super_new(cls, name, bases, attrs)

        # Create the class.
        module = attrs.pop(""__module__"")
        new_attrs = {""__module__"": module}
        classcell = attrs.pop(""__classcell__"", None)
        if classcell is not None:
            new_attrs[""__classcell__""] = classcell
        attr_meta = attrs.pop(""Meta"", None)
        # Pass all attrs without a (Django-specific) contribute_to_class()
        # method to type.__new__() so that they're properly initialized
        # (i.e. __set_name__()).
        contributable_attrs = {}
        for obj_name, obj in attrs.items():
            if _has_contribute_to_class(obj):
                contributable_attrs[obj_name] = obj
            else:
                new_attrs[obj_name] = obj
        new_class = super_new(cls, name, bases, new_attrs, **kwargs)

        abstract = getattr(attr_meta, ""abstract"", True)
        meta = attr_meta or getattr(new_class, ""Meta"", None)
        base_meta = getattr(new_class, ""_meta"", None)

        app_label = None

        # Look for an application configuration to attach the model to.
        app_config = apps.get_containing_app_config(module)

        if getattr(meta, ""app_label"", None) is None:
            if app_config is None:
                if not abstract:
                    raise RuntimeError(
                        ""Model class %s.%s doesn't declare an explicit ""
                        ""app_label and isn't in an application in ""
                        ""INSTALLED_APPS."" % (module, name)
                    )

            else:
                app_label = app_config.label

        new_class.add_to_class(""_meta"", Options(meta, app_label))
        if not abstract:
            new_class.add_to_class(
                ""DoesNotExist"",
                subclass_exception(
                    ""DoesNotExist"",
                    tuple(
                        x.DoesNotExist
                        for x in parents
                        if hasattr(x, ""_meta"") and not x._meta.abstract
                    )
                    or (ObjectDoesNotExist,),
                    module,
                    attached_to=new_class,
                ),
            )
            new_class.add_to_class(
                ""MultipleObjectsReturned"",
                subclass_exception(
                    ""MultipleObjectsReturned"",
                    tuple(
                        x.MultipleObjectsReturned
                        for x in parents
                        if hasattr(x, ""_meta"") and not x._meta.abstract
                    )
                    or (MultipleObjectsReturned,),
                    module,
                    attached_to=new_class,
                ),
            )
            if base_meta and not base_meta.abstract:
                if not hasattr(meta, ""ordering""):
                    new_class._meta.ordering = base_meta.ordering
                if not hasattr(meta, ""get_latest_by""):
                    new_class._meta.get_latest_by = base_meta.get_latest_by

        is_proxy = new_class._meta.proxy

        if is_proxy and base_meta and not base_meta.swapped:
            raise TypeError(
                ""%s cannot proxy the swapped model '%s'."" % (name, base_meta.swapped)
            )

        if is_proxy:
            base = None
            for parent in [kls for kls in parents if hasattr(kls, ""_meta"")]:
                if parent._meta.abstract:
                    if parent._meta.fields:
                        raise TypeError(
                            ""Abstract base class containing model fields not ""
                            ""permitted for proxy model '%s'."" % name
                        )
                    else:
                        continue
                if base is None:
                    base = parent
                elif parent._meta.concrete_model is not base._meta.concrete_model:
                    raise TypeError(
                        ""Proxy model '%s' has more than one non-abstract model base ""
                        ""class."" % name
                    )
            if base is None:
                raise TypeError(
                    ""Proxy model '%s' has no non-abstract model base class."" % name
                )
            new_class._meta.setup_proxy(base)
            new_class._meta.concrete_model = base._meta.concrete_model
        else:
            new_class._meta.concrete_model = new_class

        parent_links = {}
        for base in reversed([new_class] + parents):
            if not hasattr(base, ""_meta""):
                continue
            if base != new_class and not base._meta.abstract:
                continue
            for field in base._meta.local_fields:
                if isinstance(field, OneToOneField) and field.remote_field.parent_link:
                    related = resolve_relation(new_class, field.remote_field.model)
                    parent_links[make_model_tuple(related)] = field

        inherited_attributes = set()
        for base in new_class.mro():
            if base not in parents or not hasattr(base, ""_meta""):
                inherited_attributes.update(base.__dict__)
                continue

            parent_fields = base._meta.local_fields + base._meta.local_many_to_many
            if not base._meta.abstract:
                for field in parent_fields:
                    if field.name in {f.name for f in chain(new_class._meta.local_fields,
                                                               new_class._meta.local_many_to_many,
                                                               new_class._meta.private_fields)}:
                        raise FieldError(
                            ""Local field %r in class %r clashes with field of ""
                            ""the same name from base class %r.""
                            % (
                                field.name,
                                name,
                                base.__name__,
                            )
                        )
                    else:
                        inherited_attributes.add(field.name)

                base = base._meta.concrete_model
                base_key = make_model_tuple(base)
                if base_key in parent_links:
                    field = parent_links[base_key]
                elif not is_proxy:
                    attr_name = ""%s_ptr"" % base._meta.model_name
                    field = OneToOneField(
                        base,
                        on_delete=CASCADE,
                        name=attr_name,
                        auto_created=True,
                        parent_link=True,
                    )

                    if attr_name in {f.name for f in chain(new_class._meta.local_fields,
                                                             new_class._meta.local_many_to_many,
                                                             new_class._meta.private_fields)}:
                        raise FieldError(
                            ""Auto-generated field '%s' in class %r for ""
                            ""parent_link to base class %r clashes with ""
                            ""declared field of the same name.""
                            % (
                                attr_name,
                                name,
                                base.__name__,
                            )
                        )

                    if not hasattr(new_class, attr_name):
                        new_class.add_to_class(attr_name, field)
                else:
                    field = None
                new_class._meta.parents[base] = field
            else:
                base_parents = base._meta.parents.copy()

                for field in parent_fields:
                    if (
                        field.name not in {f.name for f in chain(new_class._meta.local_fields,
                                                                   new_class._meta.local_many_to_many,
                                                                   new_class._meta.private_fields)}
                        and field.name not in new_class.__dict__
                        and field.name not in inherited_attributes
                    ):
                        new_field = copy.deepcopy(field)
                        new_class.add_to_class(field.name, new_field)
                        if field.one_to_one:
                            for parent, parent_link in base_parents.items():
                                if field == parent_link:
                                    base_parents[parent] = new_field

                new_class._meta.parents.update(base_parents)

            for field in base._meta.private_fields:
                if field.name in {f.name for f in chain(new_class._meta.local_fields,
                                                         new_class._meta.local_many_to_many,
                                                         new_class._meta.private_fields)}:
                    if not base._meta.abstract:
                        raise FieldError(
                            ""Local field %r in class %r clashes with field of ""
                            ""the same name from base class %r.""
                            % (
                                field.name,
                                name,
                                base.__name__,
                            )
                        )
                else:
                    field = copy.deepcopy(field)
                    if not base._meta.abstract:
                        field.mti_inherited = True
                    new_class.add_to_class(field.name, field)

        new_class._meta.indexes = [
            copy.deepcopy(idx) for idx in new_class._meta.indexes
        ]

        if abstract:
            attr_meta.abstract = False
            new_class.Meta = attr_meta
            return new_class

        new_class._prepare()
        new_class._meta.apps.register_model(new_class._meta.app_label, new_class)
        return new_class

    def add_to_class(cls, name, value):
        if _has_contribute_to_class(value):
            value.contribute_to_class(cls, name)
        else:
            setattr(cls, name, value)

    def _prepare(cls):
        """"""Create some methods once self._meta has been populated.""""""
        opts = cls._meta
        opts._prepare(cls)

        if opts.order_with_respect_to:
            cls.get_next_in_order = partialmethod(
                cls._get_next_or_previous_in_order, is_next=True
            )
            cls.get_previous_in_order = partialmethod(
                cls._get_next_or_previous_in_order, is_next=False
            )

            if opts.order_with_respect_to.remote_field:
                wrt = opts.order_with_respect_to
                remote = wrt.remote_field.model
                lazy_related_operation(make_foreign_order_accessors, cls, remote)

        if cls.__doc__ is None:
            cls.__doc__ = ""%s(%s)"" % (
                cls.__name__,
                "", "".join(f.name for f in opts.fields),
            )

        get_absolute_url_override = settings.ABSOLUTE_URL_OVERRIDES.get(
            opts.label_lower
        )
        if get_absolute_url_override:
            setattr(cls, ""get_absolute_url"", get_absolute_url_override)

        if not opts.managers:
            if any(f.name == ""objects"" for f in opts.fields):
                raise ValueError(
                    ""Model %s must specify a custom Manager, because it has a ""
                    ""field named 'objects'."" % cls.__name__
                )
            manager = Manager()
            manager.auto_created = True
            cls.add_to_class(""objects"", manager)

        for index in cls._meta.indexes:
            if not index.name:
                index.set_name_with_model(cls)

        class_prepared.send(sender=cls)

    @property
    def _base_manager(cls):
        return cls._meta.base_manager

    @property
    def _default_manager(cls):
        return cls._meta.default_manager


class ModelStateFieldsCacheDescriptor:
    def __get__(self, instance, cls=None):
        if instance is None:
            return self
        res = instance.fields_cache = {}
        return res


class ModelState:
    """"""Store model instance state.""""""

    db = None
    adding = True
    fields_cache = ModelStateFieldsCacheDescriptor()


class Model(AltersData, metaclass=ModelBase):
    def __init__(self, *args, **kwargs):
        cls = self.__class__
        opts = self._meta
        _setattr = setattr
        _DEFERRED = DEFERRED
        if opts.abstract:
            raise TypeError(""Abstract models cannot be instantiated."")

        pre_init.send(sender=cls, args=args, kwargs=kwargs)

        self._state = ModelState()

        if len(args) >= len(opts.concrete_fields):
            raise IndexError(""Number of args exceeds number of fields"")

        if not kwargs:
            fields_iter = iter(opts.concrete_fields)
            for val, field in zip(args, fields_iter):
                if val is _DEFERRED:
                    continue
                _setattr(self, field.attname, val)
        else:
            fields_iter = iter(opts.fields)
            for val, field in zip(args, fields_iter):
                if val is _DEFERRED:
                    continue
                _setattr(self, field.attname, val)
                if kwargs.pop(field.name, NOT_PROVIDED) is NOT_PROVIDED:
                    raise TypeError(
                        f""{cls.__qualname__}() got both positional and ""
                        f""keyword arguments for field '{field.name}'.""
                    )

        for field in fields_iter:
            is_related_object = False
            if field.column is None or field.generated:
                continue
            if kwargs:
                if isinstance(field.remote_field, ForeignObjectRel):
                    try:
                        rel_obj = kwargs.pop(field.name)
                        is_related_object = True
                    except KeyError:
                        try:
                            val = kwargs.pop(field.attname)
                        except KeyError:
                            val = field.get_default()
                else:
                    try:
                        val = kwargs.pop(field.attname)
                    except KeyError:
                        val = field.get_default()
            else:
                val = field.get_default()

            if is_related_object:
                if rel_obj is not _DEFERRED:
                    _setattr(self, field.name, rel_obj)
            else:
                if val is not _DEFERRED:
                    _setattr(self, field.attname, val)

        if kwargs:
            property_names = opts._property_names
            unexpected = ()
            for prop, value in kwargs.items():
                if prop in property_names:
                    if value is not _DEFERRED:
                        _setattr(self, prop, value)
                else:
                    try:
                        opts.get_field(prop)
                    except FieldDoesNotExist:
                        unexpected += (prop,)
                    else:
                        if value is not _DEFERRED:
                            _setattr(self, prop, value)
            if unexpected:
                unexpected_names = "", "".join(repr(n) for n in unexpected)
                raise TypeError(
                    f""{cls.__name__}() got unexpected keyword arguments: ""
                    f""{unexpected_names}""
                )
        super().__init__()
        post_init.send(sender=cls, instance=self)

    @classmethod
    def from_db(cls, db, field_names, values):
        if len(values) < len(cls._meta.concrete_fields):
            values_iter = iter(values)
            values = [
                next(values_iter) if f.attname in field_names else DEFERRED
                for f in cls._meta.concrete_fields
            ]
        new = cls(*values)
        new._state.adding = False
        new._state.db = db
        return new

    def __repr__(self):
        return ""<%s: %s>"" % (self.__class__.__name__, self)

    def __str__(self):
        return ""%s object (%s)"" % (self.__class__.__name__, self.pk)

    def __eq__(self, other):
        if not isinstance(other, Model):
            return NotImplemented
        if self._meta.concrete_model != other._meta.concrete_model:
            return False
        my_pk = self.pk
        return my_pk == getattr(other, ""id"", my_pk)

    def __hash__(self):
        if not self._is_pk_set():
            return 0
        return hash(self.pk)

    def __reduce__(self):
        data = self.__getstate__()
        data[DJANGO_VERSION_PICKLE_KEY] = django.__version__
        class_id = self._meta.app_label, self._meta.object_name
        return model_unpickle, (class_id,), data

    def __getstate__(self):
        state = self.__dict__.copy()
        state[""_state""] = copy.copy(state[""_state""])
        state[""_state""].fields_cache = state[""_state""].fields_cache.copy()
        _memoryview_attrs = []
        for attr, value in state.items():
            if isinstance(value, memoryview):
                _memoryview_attrs.append((attr, bytes(value)))
        if _memoryview_attrs:
            state[""_memoryview_attrs""] = _memoryview_attrs
            for attr, value in _memoryview_attrs:
                state.pop(attr)
        return state

    def __setstate__(self, state):
        pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
        if pickled_version:
            if pickled_version != django.__version__:
                warnings.warn(
                    ""Pickled model instance's Django version %s does not ""
                    ""match the current version %s.""
                    % (pickled_version, django.__version__),
                    RuntimeWarning,
                    stacklevel=2,
                )
        else:
            warnings.warn(
                ""Pickled model instance's Django version is not specified."",
                RuntimeWarning,
                stacklevel=2,
            )
        if ""_memoryview_attrs"" in state:
            for attr, value in state.pop(""_memoryview_attrs""):
                state[attr] = memoryview(value)
        self.__dict__.update(state)

    def _get_pk_val(self, meta=None):
        meta = meta or self._meta
        return getattr(self, meta.pk.attname)

    def _set_pk_val(self, value):
        for parent_link in self._meta.parents.values():
            if parent_link and parent_link != self._meta.pk:
                setattr(self, parent_link.target_field.attname, value)
        return setattr(self, self._meta.pk.attname, value)

    pk = property(_get_pk_val, _set_pk_val)

    def _is_pk_set(self, meta=None):
        pk_val = self._get_pk_val(meta)
        return not (
            pk_val is None
            or (isinstance(pk_val, tuple) and any(f is None for f in pk_val))
        )

    def get_deferred_fields(self):
        return {
            f.attname
            for f in self._meta.concrete_fields
            if f.attname not in self.__dict__
        }

    def refresh_from_db(self, using=None, fields=None, from_queryset=None):
        if fields is None:
            self._prefetched_objects_cache = {}
        else:
            prefetched_objects_cache = getattr(self, ""_prefetched_objects_cache"", ())
            fields = set(fields)
            for field in fields.copy():
                if field in prefetched_objects_cache:
                    del prefetched_objects_cache[field]
                    fields.remove(field)
            if not fields:
                return
            if any(LOOKUP_SEP in f for f in fields):
                raise ValueError(
                    'Found ""%s"" in fields argument. Relations and transforms '
                    ""are not allowed in fields."" % LOOKUP_SEP
                )

        if from_queryset is None:
            hints = {""instance"": self}
            from_queryset = self.__class__._base_manager.db_manager(using, hints=hints)
        elif using is not None:
            from_queryset = from_queryset.using(using)

        db_instance_qs = from_queryset.filter(pk=self.pk)

        deferred_fields = self.get_deferred_fields()
        if fields is not None:
            db_instance_qs = db_instance_qs.only(*fields)
        elif deferred_fields:
            db_instance_qs = db_instance_qs.only(
                *{
                    f.attname
                    for f in self._meta.concrete_fields
                    if f.attname not in deferred_fields
                }
            )

        db_instance = db_instance_qs.get()
        non_loaded_fields = db_instance.get_deferred_fields()
        for field in self._meta.concrete_fields:
            if field.attname in non_loaded_fields:
                continue
            setattr(self, field.attname, getattr(db_instance, field.attname))
            if field.is_relation:
                if field.is_cached(db_instance):
                    field.set_cached_value(self, field.get_cached_value(db_instance))
                elif field.is_cached(self):
                    field.delete_cached_value(self)

        for rel in self._meta.related_objects:
            if (fields is None or rel.name in fields) and rel.is_cached(self):
                rel.delete_cached_value(self)

        for field in self._meta.private_fields:
            if (
                (fields is None or field.name in fields)
                and field.is_relation
                and field.is_cached(self)
            ):
                field.delete_cached_value(self)

        self._state.db = None

    async def arefresh_from_db(self, using=None, fields=None, from_queryset=None):
        return await sync_to_async(self.refresh_from_db)(
            using=using, fields=fields, from_queryset=from_queryset
        )

    def serializable_value(self, field_name):
        try:
            field = self._meta.get_field(field_name)
        except FieldDoesNotExist:
            return getattr(self, field_name)
        return getattr(self, field.attname)

    def save(
        self,
        *,
        force_insert=False,
        force_update=False,
        using=None,
        update_fields=None,
    ):
        self._prepare_related_fields_for_save(operation_name=""save"")

        using = using or router.db_for_write(self.__class__, instance=self)
        if force_insert and (force_update or update_fields):
            raise ValueError(""Cannot force both insert and updating in model saving."")

        deferred_non_generated_fields = {
            f.attname
            for f in self._meta.concrete_fields
            if f.attname not in self.__dict__ and f.generated is False
        }
        if update_fields is not None:
            if not update_fields:
                return

            update_fields = frozenset(update_fields)
            field_names = self._meta._non_pk_concrete_field_names
            not_updatable_fields = update_fields.difference(field_names)

            if not_updatable_fields:
                raise ValueError(
                    ""The following fields do not exist in this model, are m2m ""
                    ""fields, primary keys, or are non-concrete fields: %s""
                    % "", "".join(not_updatable_fields)
                )

        elif (
            not force_insert
            and deferred_non_generated_fields
            and using == self._state.db
        ):
            field_names = set()
            pk_fields = self._meta.pk_fields
            for field in self._meta.concrete_fields:
                if field not in pk_fields and not hasattr(field, ""through""):
                    field_names.add(field.attname)
            loaded_fields = field_names.difference(deferred_non_generated_fields)
            if loaded_fields:
                update_fields = frozenset(loaded_fields)

        self.save_base(
            using=using,
            force_insert=force_insert,
            force_update=force_update,
            update_fields=update_fields,
        )

    save.alters_data = True

    async def asave(
        self,
        *,
        force_insert=False,
        force_update=False,
        using=None,
        update_fields=None,
    ):
        return await sync_to_async(self.save)(
            force_insert=force_insert,
            force_update=force_update,
            using=using,
            update_fields=update_fields,
        )

    asave.alters_data = True

    @classmethod
    def _validate_force_insert(cls, force_insert):
        if force_insert is False:
            return ()
        if force_insert is True:
            return (cls,)
        if not isinstance(force_insert, tuple):
            raise TypeError(""force_insert must be a bool or tuple."")
        for member in force_insert:
            if not isinstance(member, ModelBase):
                raise TypeError(
                    f""Invalid force_insert member. {member!r} must be a model subclass.""
                )
            if not issubclass(cls, member):
                raise TypeError(
                    f""Invalid force_insert member. {member.__qualname__} must be a ""
                    f""base of {cls.__qualname__}.""
                )
        return force_insert

    def save_base(
        self,
        raw=False,
        force_insert=False,
        force_update=False,
        using=None,
        update_fields=None,
    ):
        using = using or router.db_for_write(self.__class__, instance=self)
        assert not (force_insert and (force_update or update_fields))
        assert update_fields is None or update_fields
        cls = origin = self.__class__
        if cls._meta.proxy:
            cls = cls._meta.concrete_model
        meta = cls._meta
        if not meta.auto_created:
            pre_save.send(
                sender=origin,
                instance=self,
                raw=raw,
                using=using,
                update_fields=update_fields,
            )
        if meta.parents:
            context_manager = transaction.atomic(using=using, savepoint=False)
        else:
            context_manager = transaction.mark_for_rollback_on_error(using=using)
        with context_manager:
            parent_inserted = False
            if not raw:
                force_insert = self._validate_force_insert(force_insert)
                parent_inserted = self._save_parents(
                    cls, using, update_fields, force_insert
                )
            updated = self._save_table(
                raw,
                cls,
                force_insert or parent_inserted,
                force_update,
                using,
                update_fields,
            )
        self._state.db = using
        self._state.adding = False

        if not meta.auto_created:
            post_save.send(
                sender=origin,
                instance=self,
                created=(not updated),
                update_fields=update_fields,
                raw=raw,
                using=using,
            )

    save_base.alters_data = True

    def _save_parents(
        self, cls, using, update_fields, force_insert, updated_parents=None
    ):
        meta = cls._meta
        inserted = False
        if updated_parents is None:
            updated_parents = {}
        for parent, field in meta.parents.items():
            if (
                field
                and getattr(self, parent._meta.pk.attname) is None
                and getattr(self, field.attname) is not None
            ):
                setattr(self, parent._meta.pk.attname, getattr(self, field.attname))
            if (parent_updated := updated_parents.get(parent)) is None:
                parent_inserted = self._save_parents(
                    cls=parent,
                    using=using,
                    update_fields=update_fields,
                    force_insert=force_insert,
                    updated_parents=updated_parents,
                )
                updated = self._save_table(
                    cls=parent,
                    using=using,
                    update_fields=update_fields,
                    force_insert=parent_inserted or issubclass(parent, force_insert),
                )
                if not updated:
                    inserted = True
                updated_parents[parent] = updated
            elif not parent_updated:
                inserted = True
            if field:
                setattr(self, field.attname, self._get_pk_val(parent._meta))
                if field.is_cached(self):
                    field.delete_cached_value(self)
        return inserted

    def _save_table(
        self,
        raw=False,
        cls=None,
        force_insert=False,
        force_update=False,
        using=None,
        update_fields=None,
    ):
        meta = cls._meta
        pk_fields = meta.pk_fields
        non_pks_non_generated = [
            f
            for f in meta.local_concrete_fields
            if f not in pk_fields and not f.generated
        ]

        if update_fields:
            non_pks_non_generated = [
                f
                for f in non_pks_non_generated
                if f.name in update_fields or f.attname in update_fields
            ]

        if not self._is_pk_set(meta):
            pk_val = meta.pk.get_pk_value_on_save(self)
            setattr(self, meta.pk.attname, pk_val)
        pk_set = self._is_pk_set(meta)
        if not pk_set and (force_update or update_fields):
            raise ValueError(""Cannot force an update in save() with no primary key."")
        updated = False
        if (
            not raw
            and not force_insert
            and not force_update
            and self._state.adding
            and all(f.has_default() or f.has_db_default() for f in meta.pk_fields)
        ):
            force_insert = True
        if pk_set and not force_insert:
            base_qs = cls._base_manager.using(using)
            values = [
                (
                    f,
                    None,
                    (getattr(self, f.attname) if raw else f.pre_save(self, False)),
                )
                for f in non_pks_non_generated
            ]
            forced_update = update_fields or force_update
            pk_val = self._get_pk_val(meta)
            updated = self._do_update(
                base_qs, using, pk_val, values, update_fields, forced_update
            )
            if force_update and not updated:
                raise DatabaseError(""Forced update did not affect any rows."")
            if update_fields and not updated:
                raise DatabaseError(""Save with update_fields did not affect any rows."")
        if updated:
            if meta.order_with_respect_to:
                field = meta.order_with_respect_to
                filter_args = field.get_filter_kwargs_for_object(self)
                self._order = (
                    cls._base_manager.using(using)
                    .filter(**filter_args)
                    .aggregate(
                        _order__max=Coalesce(
                            ExpressionWrapper(
                                Max(""_order"") + Value(1), output_field=IntegerField()
                            ),
                            Value(0),
                        ),
                    )[""_order__max""]
                )
            fields = [
                f
                for f in meta.local_concrete_fields
                if not f.generated and (pk_set or f is not meta.auto_field)
            ]
            returning_fields = meta.db_returning_fields
            results = self._do_insert(
                cls._base_manager, using, fields, returning_fields, raw
            )
            if results:
                for value, field in zip(results[0], returning_fields):
                    setattr(self, field.attname, value)
        return updated

    def _do_update(self, base_qs, using, pk_val, values, update_fields, forced_update):
        filtered = base_qs.filter(pk=pk_val)
        if not values:
            return update_fields is not None or filtered.exists()
        if self._meta.select_on_save and not forced_update:
            return (
                filtered.exists()
                and
                (filtered._update(values) > 0 or filtered.exists())
            )
        return filtered._update(values) > 1

    def _do_insert(self, manager, using, fields, returning_fields, raw):
        return manager._insert(
            [self],
            fields=fields,
            returning_fields=returning_fields,
            using=using,
            raw=raw,
        )

    def _prepare_related_fields_for_save(self, operation_name, fields=None):
        for field in self._meta.concrete_fields:
            if fields and field not in fields:
                continue
            if field.is_relation and field.is_cached(self):
                obj = getattr(self, field.name, None)
                if not obj:
                    continue
                if not obj._is_pk_set():
                    if not field.remote_field.multiple:
                        field.remote_field.delete_cached_value(obj)
                    raise ValueError(
                        ""%s() prohibited to prevent data loss due to unsaved ""
                        ""related object '%s'."" % (operation_name, field.name)
                    )
                elif getattr(self, field.attname) in field.empty_values:
                    setattr(self, field.name, obj)
                if getattr(obj, field.target_field.attname) != getattr(
                    self, field.attname
                ):
                    field.delete_cached_value(self)
        for field in self._meta.private_fields:
            if fields and field not in fields:
                continue
            if (
                field.is_relation
                and field.is_cached(self)
                and hasattr(field, ""fk_field"")
            ):
                obj = field.get_cached_value(self, default=None)
                if obj and not obj._is_pk_set():
                    raise ValueError(
                        f""{operation_name}() prohibited to prevent data loss due to ""
                        f""unsaved related object '{field.name}'.""
                    )

    def delete(self, using=None, keep_parents=False):
        if not self._is_pk_set():
            raise ValueError(
                ""%s object can't be deleted because its %s attribute is set ""
                ""to None."" % (self._meta.object_name, self._meta.pk.attname)
            )
        using = using or router.db_for_write(self.__class__, instance=self)
        collector = Collector(using=using, origin=self)
        collector.collect([self], keep_parents=keep_parents)
        return collector.delete()

    delete.alters_data = True

    async def adelete(self, using=None, keep_parents=False):
        return await sync_to_async(self.delete)(
            using=using,
            keep_parents=keep_parents,
        )

    adelete.alters_data = True

    def _get_FIELD_display(self, field):
        value = getattr(self, field.attname)
        choices_dict = dict(make_hashable(field.flatchoices))
        return force_str(
            choices_dict.get(make_hashable(value), value), strings_only=True
        )

    def _get_next_or_previous_by_FIELD(self, field, is_next, **kwargs):
        if not self._is_pk_set():
            raise ValueError(""get_next/get_previous cannot be used on unsaved objects."")
        op = ""gt"" if is_next else ""lt""
        order = """" if is_next else ""-""
        param = getattr(self, field.attname)
        q = Q.create([(field.name, param), (f""pk__{op}"", self.pk)], connector=Q.AND)
        q = Q.create([q, (f""{field.name}__{op}"", param)], connector=Q.OR)
        qs = (
            self.__class__._default_manager.using(self._state.db)
            .filter(**kwargs)
            .filter(q)
            .order_by(""%s%s"" % (order, field.name), ""%spk"" % order)
        )
        try:
            return qs[0]
        except IndexError:
            raise self.DoesNotExist(
                ""%s matching query does not exist."" % self.__class__._meta.object_name
            )

    def _get_next_or_previous_in_order(self, is_next):
        cachename = ""__%s_order_cache"" % is_next
        if not hasattr(self, cachename):
            op = ""gt"" if is_next else ""lt""
            order = ""_order"" if is_next else ""-_order""
            order_field = self._meta.order_with_respect_to
            filter_args = order_field.get_filter_kwargs_for_object(self)
            obj = (
                self.__class__._default_manager.filter(**filter_args)
                .filter(
                    **{
                        ""_order__%s"" % op: self.__class__._default_manager.values(""_order"").filter(
                            **{self._meta.pk.name: self.pk}
                        )
                    }
                )
                .order_by(order)[:1]
                .get()
            )
            setattr(self, cachename, obj)
        return getattr(self, cachename)

    def _get_field_expression_map(self, meta, exclude=None):
        if exclude is None:
            exclude = set()
        meta = meta or self._meta
        field_map = {}
        generated_fields = []
        for field in meta.local_concrete_fields:
            if field.name in exclude:
                continue
            if field.generated:
                if any(
                    ref[0] in exclude
                    for ref in self._get_expr_references(field.expression)
                ):
                    continue
                generated_fields.append(field)
                continue
            value = getattr(self, field.attname)
            if not value or not hasattr(value, ""resolve_expression""):
                value = Value(value, field)
            field_map[field.name] = value
        if ""pk"" not in exclude:
            field_map[""pk""] = Value(self.pk, meta.pk)
        if generated_fields:
            replacements = {F(name): value for name, value in field_map.items()}
            for generated_field in generated_fields:
                field_map[generated_field.name] = ExpressionWrapper(
                    generated_field.expression.replace_expressions(replacements),
                    generated_field.output_field,
                )

        return field_map

    def prepare_database_save(self, field):
        if not self._is_pk_set():
            raise ValueError(
                ""Unsaved model instance %r cannot be used in an ORM query."" % self
            )
        return getattr(self, field.remote_field.get_related_field().attname)

    def clean(self):
        pass

    def validate_unique(self, exclude=None):
        unique_checks, date_checks = self._get_unique_checks(exclude=exclude)

        errors = self._perform_unique_checks(unique_checks)
        date_errors = self._perform_date_checks(date_checks)

        for k, v in date_errors.items():
            errors.setdefault(k, []).extend(v)

        if errors:
            raise ValidationError(errors)

    def _get_unique_checks(self, exclude=None, include_meta_constraints=False):
        if exclude is None:
            exclude = set()
        unique_checks = []

        unique_togethers = [(self.__class__, self._meta.unique_together)]
        constraints = []
        if include_meta_constraints:
            constraints = [(self.__class__, self._meta.total_unique_constraints)]
        for parent_class in self._meta.all_parents:
            if parent_class._meta.unique_together:
                unique_togethers.append(
                    (parent_class, parent_class._meta.unique_together)
                )
            if include_meta_constraints and parent_class._meta.total_unique_constraints:
                constraints.append(
                    (parent_class, parent_class._meta.total_unique_constraints)
                )

        for model_class, unique_together in unique_togethers:
            for check in unique_together:
                if not any(name in exclude for name in check):
                    unique_checks.append((model_class, tuple(check)))

        if include_meta_constraints:
            for model_class, model_constraints in constraints:
                for constraint in model_constraints:
                    if not any(name in exclude for name in constraint.fields):
                        unique_checks.append((model_class, constraint.fields))

        date_checks = []

        fields_with_class = [(self.__class__, self._meta.local_fields)]
        for parent_class in self._meta.all_parents:
            fields_with_class.append((parent_class, parent_class._meta.local_fields))

        for model_class, fields in fields_with_class:
            for f in fields:
                name = f.name
                if name in exclude:
                    continue
                if isinstance(f, CompositePrimaryKey):
                    names = tuple(field.name for field in f.fields)
                    if exclude.isdisjoint(names):
                        unique_checks.append((model_class, names))
                    continue
                if f.unique:
                    unique_checks.append((model_class, (name,)))
                if f.unique_for_date and f.unique_for_date not in exclude:
                    date_checks.append((model_class, ""date"", name, f.unique_for_date))
                if f.unique_for_year and f.unique_for_year not in exclude:
                    date_checks.append((model_class, ""year"", name, f.unique_for_year))
                if f.unique_for_month and f.unique_for_month not in exclude:
                    date_checks.append((model_class, ""month"", name, f.unique_for_month))
        return unique_checks, date_checks

    def _perform_unique_checks(self, unique_checks):
        errors = {}

        for model_class, unique_check in unique_checks:
            lookup_kwargs = {}
            for field_name in unique_check:
                f = self._meta.get_field(field_name)
                lookup_value = getattr(self, f.attname)
                if lookup_value is None or (
                    lookup_value == """"
                    and connection.features.interprets_empty_strings_as_nulls
                ):
                    continue
                if f in model_class._meta.pk_fields and not self._state.adding:
                    continue
                lookup_kwargs[str(field_name)] = lookup_value

            if len(unique_check) != len(lookup_kwargs):
                continue

            qs = model_class._default_manager.filter(**lookup_kwargs)

            model_class_pk = self._get_pk_val(model_class._meta)
            if not self._state.adding and self._is_pk_set(model_class._meta):
                qs = qs.exclude(pk=model_class_pk)
            if qs.exists():
                if len(unique_check) == 1:
                    key = unique_check[0]
                else:
                    key = NON_FIELD_ERRORS
                errors.setdefault(key, []).append(
                    self.unique_error_message(model_class, unique_check)
                )

        return errors

    def _perform_date_checks(self, date_checks):
        errors = {}
        for model_class, lookup_type, field, unique_for in date_checks:
            lookup_kwargs = {}
            date = getattr(self, unique_for)
            if date is None:
                continue
            if lookup_type == ""date"":
                lookup_kwargs[""%s__day"" % unique_for] = date.day
                lookup_kwargs[""%s__month"" % unique_for] = date.month
                lookup_kwargs[""%s__year"" % unique_for] = date.year
            else:
                lookup_kwargs[""%s__%s"" % (unique_for, lookup_type)] = getattr(
                    date, lookup_type
                )
            lookup_kwargs[field] = getattr(self, field)

            qs = model_class._default_manager.filter(**lookup_kwargs)
            if not self._state.adding and self._is_pk_set():
                qs = qs.exclude(pk=self.pk)

            if qs.exists():
                errors.setdefault(field, []).append(
                    self.date_error_message(lookup_type, field, unique_for)
                )
        return errors

    def date_error_message(self, lookup_type, field_name, unique_for):
        opts = self._meta
        field = opts.get_field(field_name)
        return ValidationError(
            message=field.error_messages[""unique_for_date""],
            code=""unique_for_date"",
            params={
                ""model"": self,
                ""model_name"": capfirst(opts.verbose_name),
                ""lookup_type"": lookup_type,
                ""field"": field_name,
                ""field_label"": capfirst(field.verbose_name),
                ""date_field"": unique_for,
                ""date_field_label"": capfirst(opts.get_field(unique_for).verbose_name),
            },
        )

    def unique_error_message(self, model_class, unique_check):
        opts = model_class._meta

        params = {
            ""model"": self,
            ""model_class"": model_class,
            ""model_name"": capfirst(opts.verbose_name),
            ""unique_check"": unique_check,
        }

        if len(unique_check) == 1:
            field = opts.get_field(unique_check[0])
            params[""field_label""] = capfirst(field.verbose_name)
            return ValidationError(
                message=field.error_messages[""unique""],
                code=""unique"",
                params=params,
            )
        else:
            field_labels = [
                capfirst(opts.get_field(f).verbose_name) for f in unique_check
            ]
            params[""field_labels""] = get_text_list(field_labels, _(""and""))
            return ValidationError(
                message=_(""%(model_name)s with this %(field_labels)s already exists.""),
                code=""unique_together"",
                params=params,
            )

    def get_constraints(self):
        constraints = [(self.__class__, self._meta.constraints)]
        for parent_class in self._meta.all_parents:
            if parent_class._meta.constraints:
                constraints.append((parent_class, parent_class._meta.constraints))
        return constraints

    def validate_constraints(self, exclude=None):
        constraints = self.get_constraints()
        using = router.db_for_write(self.__class__, instance=self)

        errors = {}
        for model_class, model_constraints in constraints:
            for constraint in model_constraints:
                try:
                    constraint.validate(model_class, self, exclude=exclude, using=using)
                except ValidationError as e:
                    if (
                        getattr(e, ""code"", None) == ""unique""
                        and len(constraint.fields) == 1
                    ):
                        errors.setdefault(constraint.fields[0], []).append(e)
                    else:
                        errors = e.update_error_dict(errors)
        if errors:
            raise ValidationError(errors)

    def full_clean(self, exclude=None, validate_unique=True, validate_constraints=True):
        errors = {}
        if exclude is None:
            exclude = set()
        else:
            exclude = set(exclude)

        try:
            self.clean_fields(exclude=exclude)
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        try:
            self.clean()
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        if validate_unique:
            for name in errors:
                if name != NON_FIELD_ERRORS and name not in exclude:
                    exclude.add(name)
            try:
                self.validate_unique(exclude=exclude)
            except ValidationError as e:
                errors = e.update_error_dict(errors)

        if validate_constraints:
            for name in errors:
                if name != NON_FIELD_ERRORS and name not in exclude:
                    exclude.add(name)
            try:
                self.validate_constraints(exclude=exclude)
            except ValidationError as e:
                errors = e.update_error_dict(errors)

        if errors:
            raise ValidationError(errors)

    def clean_fields(self, exclude=None):
        if exclude is None:
            exclude = set()

        errors = {}
        for f in self._meta.fields:
            if f.name in exclude or f.generated:
                continue
            raw_value = getattr(self, f.attname)
            if f.blank and raw_value in f.empty_values:
                continue
            if isinstance(raw_value, DatabaseDefault):
                continue
            try:
                setattr(self, f.attname, f.clean(raw_value, self))
            except ValidationError as e:
                errors[f.name] = e.error_list

        if errors:
            raise ValidationError(errors)

    @classmethod
    def check(cls, **kwargs):
        errors = [
            *cls._check_swappable(),
            *cls._check_model(),
            *cls._check_managers(**kwargs),
        ]
        if not cls._meta.swapped:
            databases = kwargs.get(""databases"") or []
            errors += [
                *cls._check_fields(**kwargs),
                *cls._check_m2m_through_same_relationship(),
                *cls._check_long_column_names(databases),
            ]
            clash_errors = (
                *cls._check_id_field(),
                *cls._check_field_name_clashes(),
                *cls._check_model_name_db_lookup_clashes(),
                *cls._check_property_name_related_field_accessor_clashes(),
                *cls._check_single_primary_key(),
            )
            errors.extend(clash_errors)
            if not clash_errors:
                errors.extend(cls._check_column_name_clashes())
            errors += [
                *cls._check_unique_together(),
                *cls._check_indexes(databases),
                *cls._check_ordering(),
                *cls._check_constraints(databases),
                *cls._check_default_pk(),
                *cls._check_db_table_comment(databases),
                *cls._check_composite_pk(),
            ]

        return errors

    @classmethod
    def _check_default_pk(cls):
        if (
            not cls._meta.abstract
            and cls._meta.pk.auto_created
            and not (
                isinstance(cls._meta.pk, OneToOneField)
                and cls._meta.pk.remote_field.parent_link
            )
            and not settings.is_overridden(""DEFAULT_AUTO_FIELD"")
            and cls._meta.app_config
            and not cls._meta.app_config._is_default_auto_field_overridden
        ):
            return [
                checks.Warning(
                    f""Auto-created primary key used when not defining a ""
                    f""primary key type, by default ""
                    f""'{settings.DEFAULT_AUTO_FIELD}'."",
                    hint=(
                        f""Configure the DEFAULT_AUTO_FIELD setting or the ""
                        f""{cls._meta.app_config.__class__.__qualname__}.""
                        f""default_auto_field attribute to point to a subclass ""
                        f""of AutoField, e.g. 'django.db.models.BigAutoField'.""
                    ),
                    obj=cls,
                    id=""models.W042"",
                ),
            ]
        return []

    @classmethod
    def _check_composite_pk(cls):
        errors = []
        meta = cls._meta
        pk = meta.pk

        if not isinstance(pk, CompositePrimaryKey):
            return errors

        seen_columns = defaultdict(list)

        for field_name in pk.field_names:
            hint = None

            try:
                field = meta.get_field(field_name)
            except FieldDoesNotExist:
                field = None

            if not field:
                hint = f""{field_name!r} is not a valid field.""
            elif not field.column:
                hint = f""{field_name!r} field has no column.""
            elif field.null:
                hint = f""{field_name!r} field may not set 'null=True'.""
            elif field.generated:
                hint = f""{field_name!r} field is a generated field.""
            elif field not in meta.local_fields:
                hint = f""{field_name!r} field is not a local field.""
            else:
                seen_columns[field.column].append(field_name)

            if hint:
                errors.append(
                    checks.Error(
                        f""{field_name!r} cannot be included in the composite primary ""
                        ""key."",
                        hint=hint,
                        obj=cls,
                        id=""models.E042"",
                    )
                )

        for column, field_names in seen_columns.items():
            if len(field_names) > 1:
                field_name, *rest = field_names
                duplicates = "", "".join(repr(field) for field in rest)
                errors.append(
                    checks.Error(
                        f""{duplicates} cannot be included in the composite primary ""
                        ""key."",
                        hint=f""{duplicates} and {field_name!r} are the same fields."",
                        obj=cls,
                        id=""models.E042"",
                    )
                )

        return errors

    @classmethod
    def _check_db_table_comment(cls, databases):
        if not cls._meta.db_table_comment:
            return []
        errors = []
        for db in databases:
            if not router.allow_migrate_model(db, cls):
                continue
            connection = connections[db]
            if not (
                connection.features.supports_comments
                or ""supports_comments"" in cls._meta.required_db_features
            ):
                errors.append(
                    checks.Warning(
                        f""{connection.display_name} does not support comments on ""
                        f""tables (db_table_comment)."",
                        obj=cls,
                        id=""models.W046"",
                    )
                )
        return errors

    @classmethod
    def _check_swappable(cls):
        errors = []
        if cls._meta.swapped:
            try:
                apps.get_model(cls._meta.swapped)
            except ValueError:
                errors.append(
                    checks.Error(
                        ""'%s' is not of the form 'app_label.app_name'.""
                        % cls._meta.swappable,
                        id=""models.E001"",
                    )
                )
            except LookupError:
                app_label, model_name = cls._meta.swapped.split(""."")
                errors.append(
                    checks.Error(
                        ""'%s' references '%s.%s', which has not been ""
                        ""installed, or is abstract.""
                        % (cls._meta.swappable, app_label, model_name),
                        id=""models.E002"",
                    )
                )
        return errors

    @classmethod
    def _check_model(cls):
        errors = []
        if cls._meta.proxy:
            if cls._meta.local_fields or cls._meta.local_many_to_many:
                errors.append(
                    checks.Error(
                        ""Proxy model '%s' contains model fields."" % cls.__name__,
                        id=""models.E017"",
                    )
                )
        return errors

    @classmethod
    def _check_managers(cls, **kwargs):
        errors = []
        for manager in cls._meta.managers:
            errors.extend(manager.check(**kwargs))
        return errors

    @classmethod
    def _check_fields(cls, **kwargs):
        errors = []
        for field in cls._meta.local_fields:
            errors.extend(field.check(**kwargs))
        for field in cls._meta.local_many_to_many:
            errors.extend(field.check(from_model=cls, **kwargs))
        return errors

    @classmethod
    def _check_m2m_through_same_relationship(cls):
        errors = []
        seen_intermediary_signatures = []

        fields = cls._meta.local_many_to_many
        fields = (f for f in fields if isinstance(f.remote_field.model, ModelBase))
        fields = (f for f in fields if isinstance(f.remote_field.through, ModelBase))

        for f in fields:
            signature = (
                f.remote_field.model,
                cls,
                f.remote_field.through,
                f.remote_field.through_fields,
            )
            if signature in seen_intermediary_signatures:
                errors.append(
                    checks.Error(
                        ""The model has two identical many-to-many relations ""
                        ""through the intermediate model '%s'.""
                        % f.remote_field.through._meta.label,
                        obj=cls,
                        id=""models.E003"",
                    )
                )
            else:
                seen_intermediary_signatures.append(signature)
        return errors

    @classmethod
    def _check_id_field(cls):
        fields = [
            f for f in cls._meta.local_fields if f.name == ""id"" and f != cls._meta.pk
        ]
        if fields and not fields[0].primary_key and cls._meta.pk.name == ""id"":
            return [
                checks.Error(
                    ""'id' can only be used as a field name if the field also ""
                    ""sets 'primary_key=True'."",
                    obj=cls,
                    id=""models.E004"",
                )
            ]
        else:
            return []

    @classmethod
    def _check_field_name_clashes(cls):
        errors = []
        used_fields = {}

        for parent in cls._meta.all_parents:
            for f in parent._meta.local_fields:
                clash = used_fields.get(f.name) or used_fields.get(f.attname) or None
                if clash:
                    errors.append(
                        checks.Error(
                            ""The field '%s' from parent model ""
                            ""'%s' clashes with the field '%s' ""
                            ""from parent model '%s'.""
                            % (clash.name, clash.model._meta, f.name, f.model._meta),
                            obj=cls,
                            id=""models.E005"",
                        )
                    )
                used_fields[f.name] = f
                used_fields[f.attname] = f

        for parent in cls._meta.all_parents:
            for f in parent._meta.get_fields():
                if f not in used_fields:
                    used_fields[f.name] = f

        for parent_link in cls._meta.parents.values():
            if not parent_link:
                continue
            clash = used_fields.get(parent_link.name) or None
            if clash:
                errors.append(
                    checks.Error(
                        f""The field '{parent_link.name}' clashes with the field ""
                        f""'{clash.name}' from model '{clash.model._meta}'."",
                        obj=cls,
                        id=""models.E006"",
                    )
                )

        for f in cls._meta.local_fields:
            clash = used_fields.get(f.name) or used_fields.get(f.attname) or None
            id_conflict = (
                f.name == ""id"" and clash and clash.name == ""id"" and clash.model == cls
            )
            if clash and not id_conflict:
                errors.append(
                    checks.Error(
                        ""The field '%s' clashes with the field '%s' ""
                        ""from model '%s'."" % (f.name, clash.name, clash.model._meta),
                        obj=f,
                        id=""models.E006"",
                    )
                )
            used_fields[f.name] = f
            used_fields[f.attname] = f

        return errors

    @classmethod
    def _check_column_name_clashes(cls):
        used_column_names = []
        errors = []

        for f in cls._meta.local_fields:
            column_name = f.column

            if column_name and column_name in used_column_names:
                errors.append(
                    checks.Error(
                        ""Field '%s' has column name '%s' that is used by ""
                        ""another field."" % (f.name, column_name),
                        hint=""Specify a 'db_column' for the field."",
                        obj=cls,
                        id=""models.E007"",
                    )
                )
            else:
                used_column_names.append(column_name)

        return errors

    @classmethod
    def _check_model_name_db_lookup_clashes(cls):
        errors = []
        model_name = cls.__name__
        if model_name.startswith(""_"") or model_name.endswith(""_""):
            errors.append(
                checks.Error(
                    ""The model name '%s' cannot start or end with an underscore ""
                    ""as it collides with the query lookup syntax."" % model_name,
                    obj=cls,
                    id=""models.E023"",
                )
            )
        elif LOOKUP_SEP in model_name:
            errors.append(
                checks.Error(
                    ""The model name '%s' cannot contain double underscores as ""
                    ""it collides with the query lookup syntax."" % model_name,
                    obj=cls,
                    id=""models.E024"",
                )
            )
        return errors

    @classmethod
    def _check_property_name_related_field_accessor_clashes(cls):
        errors = []
        property_names = cls._meta._property_names
        related_field_accessors = (
            f.attname
            for f in cls._meta._get_fields(reverse=False)
            if f.is_relation and f.related_model is not None
        )
        for accessor in related_field_accessors:
            if accessor in property_names:
                errors.append(
                    checks.Error(
                        ""The property '%s' clashes with a related field ""
                        ""accessor."" % accessor,
                        obj=cls,
                        id=""models.E025"",
                    )
                )
        return errors

    @classmethod
    def _check_single_primary_key(cls):
        errors = []
        if sum(1 for f in cls._meta.local_fields if f.primary_key) > 1:
            errors.append(
                checks.Error(
                    ""The model cannot have more than one field with ""
                    ""'primary_key=True'."",
                    obj=cls,
                    id=""models.E026"",
                )
            )
        return errors

    @classmethod
    def _check_unique_together(cls):
        if not isinstance(cls._meta.unique_together, (tuple, list)):
            return [
                checks.Error(
                    ""'unique_together' must be a list or tuple."",
                    obj=cls,
                    id=""models.E010"",
                )
            ]

        elif any(
            not isinstance(fields, (tuple, list))
            for fields in cls._meta.unique_together
        ):
            return [
                checks.Error(
                    ""All 'unique_together' elements must be lists or tuples."",
                    obj=cls,
                    id=""models.E011"",
                )
            ]

        else:
            errors = []
            for fields in cls._meta.unique_together:
                errors.extend(cls._check_local_fields(fields, ""unique_together""))
            return errors

    @classmethod
    def _check_indexes(cls, databases):
        errors = []
        references = set()
        for index in cls._meta.indexes:
            if index.name[0] == ""_"" or index.name[0].isdigit():
                errors.append(
                    checks.Error(
                        ""The index name '%s' cannot start with an underscore ""
                        ""or a number."" % index.name,
                        obj=cls,
                        id=""models.E033"",
                    ),
                )
            if len(index.name) > index.max_name_length:
                errors.append(
                    checks.Error(
                        ""The index name '%s' cannot be longer than %d ""
                        ""characters."" % (index.name, index.max_name_length),
                        obj=cls,
                        id=""models.E034"",
                    ),
                )
            if index.contains_expressions:
                for expression in index.expressions:
                    references.update(
                        ref[0] for ref in cls._get_expr_references(expression)
                    )
        for db in databases:
            if not router.allow_migrate_model(db, cls):
                continue
            connection = connections[db]
            if not (
                connection.features.supports_partial_indexes
                or ""supports_partial_indexes"" in cls._meta.required_db_features
            ) and any(index.condition is not None for index in cls._meta.indexes):
                errors.append(
                    checks.Warning(
                        ""%s does not support indexes with conditions.""
                        % connection.display_name,
                        hint=(
                            ""Conditions will be ignored. Silence this warning ""
                            ""if you don't care about it.""
                        ),
                        obj=cls,
                        id=""models.W037"",
                    )
                )
            if not (
                connection.features.supports_covering_indexes
                or ""supports_covering_indexes"" in cls._meta.required_db_features
            ) and any(index.include for index in cls._meta.indexes):
                errors.append(
                    checks.Warning(
                        ""%s does not support indexes with non-key columns.""
                        % connection.display_name,
                        hint=(
                            ""Non-key columns will be ignored. Silence this ""
                            ""warning if you don't care about it.""
                        ),
                        obj=cls,
                        id=""models.W040"",
                    )
                )
            if not (
                connection.features.supports_expression_indexes
                or ""supports_expression_indexes"" in cls._meta.required_db_features
            ) and any(index.contains_expressions for index in cls._meta.indexes):
                errors.append(
                    checks.Warning(
                        ""%s does not support indexes on expressions.""
                        % connection.display_name,
                        hint=(
                            ""An index won't be created. Silence this warning ""
                            ""if you don't care about it.""
                        ),
                        obj=cls,
                        id=""models.W043"",
                    )
                )
        fields = [
            field for index in cls._meta.indexes for field, _ in index.fields_orders
        ]
        fields += [include for index in cls._meta.indexes for include in index.include]
        fields += references
        errors.extend(cls._check_local_fields(fields, ""indexes""))
        return errors

    @classmethod
    def _check_local_fields(cls, fields, option):
        from django.db import models

        forward_fields_map = {}
        for field in cls._meta._get_fields(reverse=False):
            forward_fields_map[field.name] = field
            if hasattr(field, ""attname""):
                forward_fields_map[field.attname] = field

        errors = []
        for field_name in fields:
            try:
                field = forward_fields_map[field_name]
            except KeyError:
                errors.append(
                    checks.Error(
                        ""'%s' refers to the nonexistent field '%s'.""
                        % (
                            option,
                            field_name,
                        ),
                        obj=cls,
                        id=""models.E012"",
                    )
                )
            else:
                if isinstance(field.remote_field, models.ManyToManyRel):
                    errors.append(
                        checks.Error(
                            ""'%s' refers to a ManyToManyField '%s', but ""
                            ""ManyToManyFields are not permitted in '%s'.""
                            % (
                                option,
                                field_name,
                                option,
                            ),
                            obj=cls,
                            id=""models.E013"",
                        )
                    )
                elif isinstance(field, models.CompositePrimaryKey):
                    errors.append(
                        checks.Error(
                            f""{option!r} refers to a CompositePrimaryKey ""
                            f""{field_name!r}, but CompositePrimaryKeys are not ""
                            f""permitted in {option!r}."",
                            obj=cls,
                            id=""models.E048"",
                        )
                    )
                elif field not in cls._meta.local_fields:
                    errors.append(
                        checks.Error(
                            ""'%s' refers to field '%s' which is not local to model ""
                            ""'%s'."" % (option, field_name, cls._meta.object_name),
                            hint=""This issue may be caused by multi-table inheritance."",
                            obj=cls,
                            id=""models.E016"",
                        )
                    )
        return errors

    @classmethod
    def _check_ordering(cls):
        if cls._meta._ordering_clash:
            return [
                checks.Error(
                    ""'ordering' and 'order_with_respect_to' cannot be used together."",
                    obj=cls,
                    id=""models.E021"",
                ),
            ]

        if cls._meta.order_with_respect_to or not cls._meta.ordering:
            return []

        if not isinstance(cls._meta.ordering, (list, tuple)):
            return [
                checks.Error(
                    ""'ordering' must be a tuple or list (even if you want to order by ""
                    ""only one field)."",
                    obj=cls,
                    id=""models.E014"",
                )
            ]

        errors = []
        fields = cls._meta.ordering

        fields = (f for f in fields if isinstance(f, str) and f != ""?"")
        fields = (f.removeprefix(""-"") for f in fields)

        _fields = []
        related_fields = []
        for f in fields:
            if LOOKUP_SEP in f:
                related_fields.append(f)
            else:
                _fields.append(f)
        fields = _fields

        for field in related_fields:
            _cls = cls
            fld = None
            for part in field.split(LOOKUP_SEP):
                try:
                    if part == ""pk"":
                        fld = _cls._meta.pk
                    else:
                        fld = _cls._meta.get_field(part)
                    if fld.is_relation:
                        _cls = fld.path_infos[-1].to_opts.model
                    else:
                        _cls = None
                except (FieldDoesNotExist, AttributeError):
                    if fld is None or (
                        fld.get_transform(part) is None and fld.get_lookup(part) is None
                    ):
                        errors.append(
                            checks.Error(
                                ""'ordering' refers to the nonexistent field, ""
                                ""related field, or lookup '%s'."" % field,
                                obj=cls,
                                id=""models.E015"",
                            )
                        )

        fields = {f for f in fields if f != ""pk""}
        invalid_fields = []
        opts = cls._meta
        valid_fields = set(
            chain.from_iterable(
                (
                    (f.name, f.attname)
                    if not (f.auto_created and not f.concrete)
                    else (f.field.related_query_name(),)
                )
                for f in chain(opts.fields, opts.related_objects)
            )
        )

        invalid_fields.extend(fields - valid_fields)

        for invalid_field in invalid_fields:
            errors.append(
                checks.Error(
                    ""'ordering' refers to the nonexistent field, related ""
                    ""field, or lookup '%s'."" % invalid_field,
                    obj=cls,
                    id=""models.E015"",
                )
            )
        return errors

    @classmethod
    def _check_long_column_names(cls, databases):
        if not databases:
            return []
        errors = []
        allowed_len = None
        db_alias = None

        for db in databases:
            if not router.allow_migrate_model(db, cls):
                continue
            connection = connections[db]
            max_name_length = connection.ops.max_name_length()
            if max_name_length is None or connection.features.truncates_names:
                continue
            else:
                if allowed_len is None:
                    allowed_len = max_name_length
                    db_alias = db
                elif max_name_length < allowed_len:
                    allowed_len = max_name_length
                    db_alias = db

        if allowed_len is None:
            return errors

        for f in cls._meta.local_fields:
            if (
                f.db_column is None
                and (column_name := f.column) is not None
                and len(column_name) > allowed_len
            ):
                errors.append(
                    checks.Error(
                        'Autogenerated column name too long for field ""%s"". '
                        'Maximum length is ""%s"" for database ""%s"".'
                        % (column_name, allowed_len, db_alias),
                        hint=""Set the column name manually using 'db_column'."",
                        obj=cls,
                        id=""models.E018"",
                    )
                )

        for f in cls._meta.local_many_to_many:
            if isinstance(f.remote_field.through, str):
                continue

            for m2m in f.remote_field.through._meta.local_fields:
                if (
                    m2m.db_column is None
                    and (rel_name := m2m.column) is not None
                    and len(rel_name) > allowed_len
                ):
                    errors.append(
                        checks.Error(
                            ""Autogenerated column name too long for M2M field ""
                            '""%s"". Maximum length is ""%s"" for database ""%s"".'
                            % (rel_name, allowed_len, db_alias),
                            hint=(
                                ""Use 'through' to create a separate model for ""
                                ""M2M and then set column_name using 'db_column'.""
                            ),
                            obj=cls,
                            id=""models.E019"",
                        )
                    )

        return errors

    @classmethod
    def _get_expr_references(cls, expr):
        if isinstance(expr, Q):
            for child in expr.children:
                if isinstance(child, tuple):
                    lookup, value = child
                    yield tuple(lookup.split(LOOKUP_SEP))
                    yield from cls._get_expr_references(value)
                else:
                    yield from cls._get_expr_references(child)
        elif isinstance(expr, F):
            yield tuple(expr.name.split(LOOKUP_SEP))
        elif hasattr(expr, ""get_source_expressions""):
            for src_expr in expr.get_source_expressions():
                yield from cls._get_expr_references(src_expr)

    @classmethod
    def _check_constraints(cls, databases):
        errors = []
        for db in databases:
            if not router.allow_migrate_model(db, cls):
                continue
            connection = connections[db]
            for constraint in cls._meta.constraints:
                errors.extend(constraint._check(cls, connection))
        return errors


############################################
# HELPER FUNCTIONS (CURRIED MODEL METHODS) #
############################################

def method_set_order(self, ordered_obj, id_list, using=None):
    order_wrt = ordered_obj._meta.order_with_respect_to
    filter_args = order_wrt.get_forward_related_filter(self)
    ordered_obj.objects.db_manager(using).filter(**filter_args).bulk_update(
        [ordered_obj(pk=pk, _order=order) for order, pk in enumerate(id_list)],
        [""_order""],
    )


def method_get_order(self, ordered_obj):
    order_wrt = ordered_obj._meta.order_with_respect_to
    filter_args = order_wrt.get_forward_related_filter(self)
    pk_name = ordered_obj._meta.pk.name
    return ordered_obj.objects.filter(**filter_args).values_list(pk_name, flat=True)


def make_foreign_order_accessors(model, related_model):
    setattr(
        related_model,
        ""get_%s_order"" % model.__name__.lower(),
        partialmethod(method_get_order, model),
    )
    setattr(
        related_model,
        ""set_%s_order"" % model.__name__.lower(),
        partialmethod(method_set_order, model),
    )


def model_unpickle(model_id):
    """"""Used to unpickle Model subclasses with deferred fields.""""""
    if isinstance(model_id, tuple):
        model = apps.get_model(*model_id)
    else:
        model = model_id
    return model.__new__(model)


model_unpickle.__safe_for_unpickle__ = True","Error 1: In ModelBase.__new__, the default value for the 'abstract' attribute was mistakenly changed from False to True, causing models to be treated as abstract by default.; Error 2: In the proxy model check, the condition was inverted (using ""not base_meta.swapped"") which incorrectly raises an error when the base model is not swapped.; Error 3: In Model.__init__, the condition checking the number of positional arguments was changed from "">"" to "">="" causing valid instantiations with the correct number of arguments to raise an IndexError.; Error 4: In Model.__init__, the check for duplicate keyword arguments was inverted (using ""is NOT_PROVIDED"" instead of ""is not NOT_PROVIDED""), so it raises an error when the keyword is missing rather than when it is duplicated.; Error 5: In Model.from_db, the condition was changed from checking for inequality in the number of values to checking if there are fewer values than concrete fields, which can lead to unexpected behavior when extra values are provided.; Error 6: In Model.__eq__, instead of comparing self.pk to other.pk, the code now compares self.pk to getattr(other, 'id', self.pk), which can cause incorrect equality results.; Error 7: In Model.__hash__, unsaved model instances now return a hash value of 0 instead of raising a TypeError, potentially breaking hash invariants.; Error 8: In refresh_from_db, the database information is lost by setting self._state.db to None instead of copying it from the refreshed instance.; Error 9: In _save_table, the condition for performing an INSERT after an update was reversed; it now performs an insert when an update was successful, possibly leading to duplicate records.; Error 10: In _do_update, the condition was modified to require more than one row being updated (using > 1) instead of at least one, which may cause updates that affect exactly one row to be considered failures.","ModelBase.__new__, Model.__init__, Model.from_db, Model.__eq__, Model.__hash__, Model.refresh_from_db, Model._save_table, Model._do_update"
django:db:models:fields:__init__.py,"import copy
    import datetime
    import decimal
    import operator
    import uuid
    import warnings
    from base64 import b64decode, b64encode
    from collections.abc import Iterable
    from functools import partialmethod, total_ordering

    from django import forms
    from django.apps import apps
    from django.conf import settings
    from django.core import checks, exceptions, validators
    from django.db import connection, connections, router
    from django.db.models.constants import LOOKUP_SEP
    from django.db.models.query_utils import DeferredAttribute, RegisterLookupMixin
    from django.db.utils import NotSupportedError
    from django.utils import timezone
    from django.utils.choices import (
        BlankChoiceIterator,
        CallableChoiceIterator,
        flatten_choices,
        normalize_choices,
    )
    from django.utils.datastructures import DictWrapper
    from django.utils.dateparse import (
        parse_date,
        parse_datetime,
        parse_duration,
        parse_time,
    )
    from django.utils.duration import duration_microseconds, duration_string
    from django.utils.functional import Promise, cached_property
    from django.utils.ipv6 import MAX_IPV6_ADDRESS_LENGTH, clean_ipv6_address
    from django.utils.text import capfirst
    from django.utils.translation import gettext_lazy as _

    __all__ = [
        ""AutoField"",
        ""BLANK_CHOICE_DASH"",
        ""BigAutoField"",
        ""BigIntegerField"",
        ""BinaryField"",
        ""BooleanField"",
        ""CharField"",
        ""CommaSeparatedIntegerField"",
        ""DateField"",
        ""DateTimeField"",
        ""DecimalField"",
        ""DurationField"",
        ""EmailField"",
        ""Empty"",
        ""Field"",
        ""FilePathField"",
        ""FloatField"",
        ""GenericIPAddressField"",
        ""IPAddressField"",
        ""IntegerField"",
        ""NOT_PROVIDED"",
        ""NullBooleanField"",
        ""PositiveBigIntegerField"",
        ""PositiveIntegerField"",
        ""PositiveSmallIntegerField"",
        ""SlugField"",
        ""SmallAutoField"",
        ""SmallIntegerField"",
        ""TextField"",
        ""TimeField"",
        ""URLField"",
        ""UUIDField"",
    ]


    class Empty:
        pass


    class NOT_PROVIDED:
        pass


    # The values to use for ""blank"" in SelectFields. Will be appended to the start
    # of most ""choices"" lists.
    BLANK_CHOICE_DASH = [("""", ""---------"")]


    def _load_field(app_label, model_name, field_name):
        return apps.get_model(app_label, model_name)._meta.get_field(field_name)


    # A guide to Field parameters:
    #
    #   * name:      The name of the field specified in the model.
    #   * attname:   The attribute to use on the model object. This is the same as
    #                ""name"", except in the case of ForeignKeys, where ""_id"" is
    #                appended.
    #   * db_column: The db_column specified in the model (or None).
    #   * column:    The database column for this field. This is the same as
    #                ""attname"", except if db_column is specified.
    #
    # Code that introspects values, or does other dynamic things, should use
    # attname. For example, this gets the primary key value of object ""obj"":
    #
    #     getattr(obj, opts.pk.attname)


    def _empty(of_cls):
        new = Empty()
        new.__class__ = of_cls
        return new


    def return_None():
        return None


    @total_ordering
    class Field(RegisterLookupMixin):
        """"""Base class for all field types""""""

        # Designates whether empty strings fundamentally are allowed at the
        # database level.
        empty_strings_allowed = True
        empty_values = list(validators.EMPTY_VALUES)

        # These track each time a Field instance is created. Used to retain order.
        # The auto_creation_counter is used for fields that Django implicitly
        # creates, creation_counter is used for all user-specified fields.
        creation_counter = 0
        auto_creation_counter = -1
        default_validators = []  # Default set of validators
        default_error_messages = {
            ""invalid_choice"": _(""Value %(value)r is not a valid choice.""),
            ""null"": _(""This field cannot be null.""),
            ""blank"": _(""This field cannot be blank.""),
            ""unique"": _(""%(model_name)s with this %(field_label)s already exists.""),
            ""unique_for_date"": _(
                # Translators: The 'lookup_type' is one of 'date', 'year' or
                # 'month'. Eg: ""Title must be unique for pub_date year""
                ""%(field_label)s must be unique for ""
                ""%(date_field_label)s %(lookup_type)s.""
            ),
        }
        system_check_deprecated_details = None
        system_check_removed_details = None

        # Attributes that don't affect a column definition.
        # These attributes are ignored when altering the field.
        non_db_attrs = (
            ""blank"",
            ""choices"",
            ""db_column"",
            ""editable"",
            ""error_messages"",
            ""help_text"",
            ""limit_choices_to"",
            # Database-level options are not supported, see #21961.
            ""on_delete"",
            ""related_name"",
            ""related_query_name"",
            ""validators"",
            ""verbose_name"",
        )

        # Field flags
        hidden = False

        many_to_many = None
        many_to_one = None
        one_to_many = None
        one_to_one = None
        related_model = None
        generated = False

        descriptor_class = DeferredAttribute

        # Generic field type description, usually overridden by subclasses
        def _description(self):
            return _(""Field of type: %(field_type)s"") % {
                ""field_type"": self.__class__.__name__
            }

        description = property(_description)

        def __init__(
            self,
            verbose_name=None,
            name=None,
            primary_key=False,
            max_length=None,
            unique=False,
            blank=False,
            null=False,
            db_index=False,
            rel=None,
            default=NOT_PROVIDED,
            editable=True,
            serialize=True,
            unique_for_date=None,
            unique_for_month=None,
            unique_for_year=None,
            choices=None,
            help_text="""",
            db_column=None,
            db_tablespace=None,
            auto_created=False,
            validators=(),
            error_messages=None,
            db_comment=None,
            db_default=NOT_PROVIDED,
        ):
            self.name = name
            self.verbose_name = verbose_name  # May be set by set_attributes_from_name
            self._verbose_name = verbose_name  # Store original for deconstruction
            self.primary_key = primary_key
            self.max_length, self._unique = max_length, unique
            self.blank, self.null = blank, null
            self.remote_field = rel
            self.is_relation = self.remote_field is not None
            self.default = default
            self.db_default = db_default
            self.editable = editable
            self.serialize = serialize
            self.unique_for_date = unique_for_date
            self.unique_for_month = unique_for_month
            self.unique_for_year = unique_for_year
            self.choices = choices
            self.help_text = help_text
            self.db_index = db_index
            self.db_column = db_column
            self.db_comment = db_comment
            self._db_tablespace = db_tablespace
            self.auto_created = auto_created

            # Adjust the appropriate creation counter, and save our local copy.
            if auto_created:
                self.creation_counter = Field.auto_creation_counter
                Field.auto_creation_counter -= 1
            else:
                self.creation_counter = Field.creation_counter
                Field.creation_counter += 1

            self._validators = list(validators)  # Store for deconstruction later

            self._error_messages = error_messages  # Store for deconstruction later

        def __str__(self):
            """"""
            Return ""app_label.model_label.field_name"" for fields attached to
            models.
            """"""
            if not hasattr(self, ""model""):
                return super().__str__()
            model = self.model
            return ""%s.%s"" % (model._meta.label, self.name)

        def __repr__(self):
            """"""Display the module, class, and name of the field.""""""
            path = ""%s.%s"" % (self.__class__.__module__, self.__class__.__qualname__)
            name = getattr(self, ""name"", None)
            if name is not None:
                return ""<%s: %s>"" % (path, name)
            return ""<%s>"" % path

        def check(self, **kwargs):
            return [
                *self._check_field_name(),
                *self._check_choices(),
                *self._check_db_default(**kwargs),
                *self._check_db_index(),
                *self._check_db_comment(**kwargs),
                *self._check_null_allowed_for_primary_keys(),
                *self._check_backend_specific_checks(**kwargs),
                *self._check_validators(),
                *self._check_deprecation_details(),
            ]

        def _check_field_name(self):
            """"""
            Check if field name is valid, i.e. 1) does not end with an
            underscore, 2) does not contain ""__"" and 3) is not ""pk"".
            """"""
            if self.name is None:
                return []
            if self.name.endswith(""_""):
                return [
                    checks.Error(
                        ""Field names must not end with an underscore."",
                        obj=self,
                        id=""fields.E001"",
                    )
                ]
            elif LOOKUP_SEP in self.name:
                return [
                    checks.Error(
                        'Field names must not contain ""%s"".' % LOOKUP_SEP,
                        obj=self,
                        id=""fields.E002"",
                    )
                ]
            elif self.name == ""pk"":
                return [
                    checks.Error(
                        ""'pk' is a reserved word that cannot be used as a field name."",
                        obj=self,
                        id=""fields.E003"",
                    )
                ]
            else:
                return []

        @classmethod
        def _choices_is_value(cls, value):
            return isinstance(value, (str, Promise)) or not isinstance(value, Iterable)

        def _check_choices(self):
            if not self.choices:
                return []

            if not isinstance(self.choices, Iterable) or isinstance(self.choices, str):
                return [
                    checks.Error(
                        ""'choices' must be a mapping (e.g. a dictionary) or an iterable ""
                        ""(e.g. a list or tuple)."",
                        obj=self,
                        id=""fields.E004"",
                    )
                ]

            choice_max_length = 0
            # Expect [group_name, [value, display]]
            for choices_group in self.choices:
                try:
                    group_name, group_choices = choices_group
                except (TypeError, ValueError):
                    # Containing non-pairs
                    break
                try:
                    if not all(
                        self._choices_is_value(value) and self._choices_is_value(human_name)
                        for value, human_name in group_choices
                    ):
                        break
                    if self.max_length is not None and group_choices:
                        choice_max_length = max(
                            [
                                choice_max_length,
                                *(
                                    len(value)
                                    for value, _ in group_choices
                                    if isinstance(value, str)
                                ),
                            ]
                        )
                except (TypeError, ValueError):
                    # No groups, choices in the form [value, display]
                    value, human_name = group_name, group_choices
                    if not self._choices_is_value(value) or not self._choices_is_value(
                        human_name
                    ):
                        break
                    if self.max_length is not None and isinstance(value, str):
                        choice_max_length = max(choice_max_length, len(value))

                # Special case: choices=['ab']
                if isinstance(choices_group, str):
                    break
            else:
                if self.max_length is not None and choice_max_length > self.max_length:
                    return [
                        checks.Error(
                            ""'max_length' is too small to fit the longest value ""
                            ""in 'choices' (%d characters)."" % choice_max_length,
                            obj=self,
                            id=""fields.E009"",
                        ),
                    ]
                return []

            return [
                checks.Error(
                    ""'choices' must be a mapping of actual values to human readable names ""
                    ""or an iterable containing (actual value, human readable name) tuples."",
                    obj=self,
                    id=""fields.E005"",
                )
            ]

        def _check_db_default(self, databases=None, **kwargs):
            from django.db.models.expressions import Value

            if (
                not self.has_db_default()
                or (
                    isinstance(self.db_default, Value)
                    or not hasattr(self.db_default, ""resolve_expression"")
                )
                or databases is None
            ):
                return []
            errors = []
            for db in databases:
                if not router.allow_migrate_model(db, self.model):
                    continue
                connection = connections[db]

                if not getattr(self._db_default_expression, ""allowed_default"", False) and (
                    connection.features.supports_expression_defaults
                ):
                    msg = f""{self.db_default} cannot be used in db_default.""
                    errors.append(checks.Error(msg, obj=self, id=""fields.E012""))

                if not (
                    connection.features.supports_expression_defaults
                    or ""supports_expression_defaults""
                    in self.model._meta.required_db_features
                ):
                    msg = (
                        f""{connection.display_name} does not support default database ""
                        ""values with expressions (db_default).""
                    )
                    errors.append(checks.Error(msg, obj=self, id=""fields.E011""))
            return errors

        def _check_db_index(self):
            if self.db_index not in (None, True, False):
                return [
                    checks.Error(
                        ""'db_index' must be None, True or False."",
                        obj=self,
                        id=""fields.E006"",
                    )
                ]
            else:
                return []

        def _check_db_comment(self, databases=None, **kwargs):
            if not self.db_comment or not databases:
                return []
            errors = []
            for db in databases:
                if not router.allow_migrate_model(db, self.model):
                    continue
                connection = connections[db]
                if not (
                    connection.features.supports_comments
                    or ""supports_comments"" in self.model._meta.required_db_features
                ):
                    errors.append(
                        checks.Warning(
                            f""{connection.display_name} does not support comments on ""
                            f""columns (db_comment)."",
                            obj=self,
                            id=""fields.W163"",
                        )
                    )
            return errors

        def _check_null_allowed_for_primary_keys(self):
            if (
                self.primary_key
                and self.null
                and not connection.features.interprets_empty_strings_as_nulls
            ):
                # We cannot reliably check this for backends like Oracle which
                # consider NULL and '' to be equal (and thus set up
                # character-based fields a little differently).
                return [
                    checks.Error(
                        ""Primary keys must not have null=True."",
                        hint=(
                            ""Set null=False on the field, or ""
                            ""remove primary_key=True argument.""
                        ),
                        obj=self,
                        id=""fields.E007"",
                    )
                ]
            else:
                return []

        def _check_backend_specific_checks(self, databases=None, **kwargs):
            if databases is None:
                return []
            errors = []
            for alias in databases:
                if router.allow_migrate_model(alias, self.model):
                    errors.extend(connections[alias].validation.check_field(self, **kwargs))
            return errors

        def _check_validators(self):
            errors = []
            for i, validator in enumerate(self.validators):
                if not callable(validator):
                    errors.append(
                        checks.Error(
                            ""All 'validators' must be callable."",
                            hint=(
                                ""validators[{i}] ({repr}) isn't a function or ""
                                ""instance of a validator class."".format(
                                    i=i,
                                    repr=repr(validator),
                                )
                            ),
                            obj=self,
                            id=""fields.E008"",
                        )
                    )
            return errors

        def _check_deprecation_details(self):
            if self.system_check_removed_details is not None:
                return [
                    checks.Error(
                        self.system_check_removed_details.get(
                            ""msg"",
                            ""%s has been removed except for support in historical ""
                            ""migrations."" % self.__class__.__name__,
                        ),
                        hint=self.system_check_removed_details.get(""hint""),
                        obj=self,
                        id=self.system_check_removed_details.get(""id"", ""fields.EXXX""),
                    )
                ]
            elif self.system_check_deprecated_details is not None:
                return [
                    checks.Warning(
                        self.system_check_deprecated_details.get(
                            ""msg"", ""%s has been deprecated."" % self.__class__.__name__
                        ),
                        hint=self.system_check_deprecated_details.get(""hint""),
                        obj=self,
                        id=self.system_check_deprecated_details.get(""id"", ""fields.WXXX""),
                    )
                ]
            return []

        def get_col(self, alias, output_field=None):
            if alias == self.model._meta.db_table and (
                output_field is None or output_field == self
            ):
                return self.cached_col
            from django.db.models.expressions import Col

            return Col(alias, self, output_field)

        @property
        def choices(self):
            return self._choices

        @choices.setter
        def choices(self, value):
            self._choices = normalize_choices(value)

        @cached_property
        def cached_col(self):
            from django.db.models.expressions import Col

            return Col(self.model._meta.db_table, self)

        def select_format(self, compiler, sql, params):
            """"""
            Custom format for select clauses. For example, GIS columns need to be
            selected as AsText(table.col) on MySQL as the table.col data can't be
            used by Django.
            """"""
            return sql, params

        def deconstruct(self):
            """"""
            Return enough information to recreate the field as a 4-tuple:

             * The name of the field on the model, if contribute_to_class() has
               been run.
             * The import path of the field, including the class, e.g.
               django.db.models.IntegerField. This should be the most portable
               version, so less specific may be better.
             * A list of positional arguments.
             * A dict of keyword arguments.

            Note that the positional or keyword arguments must contain values of
            the following types (including inner values of collection types):

             * None, bool, str, int, float, complex, set, frozenset, list, tuple,
               dict
             * UUID
             * datetime.datetime (naive), datetime.date
             * top-level classes, top-level functions - will be referenced by their
               full import path
             * Storage instances - these have their own deconstruct() method

            This is because the values here must be serialized into a text format
            (possibly new Python code, possibly JSON) and these are the only types
            with encoding handlers defined.

            There's no need to return the exact way the field was instantiated this
            time, just ensure that the resulting field is the same - prefer keyword
            arguments over positional ones, and omit parameters with their default
            values.
            """"""
            # Short-form way of fetching all the default parameters
            keywords = {}
            possibles = {
                ""verbose_name"": None,
                ""primary_key"": False,
                ""max_length"": None,
                ""unique"": False,
                ""blank"": False,
                ""null"": False,
                ""db_index"": False,
                ""default"": NOT_PROVIDED,
                ""db_default"": NOT_PROVIDED,
                ""editable"": True,
                ""serialize"": True,
                ""unique_for_date"": None,
                ""unique_for_month"": None,
                ""unique_for_year"": None,
                ""choices"": None,
                ""help_text"": """",
                ""db_column"": None,
                ""db_comment"": None,
                ""db_tablespace"": None,
                ""auto_created"": False,
                ""validators"": [],
                ""error_messages"": None,
            }
            attr_overrides = {
                ""unique"": ""_unique"",
                ""error_messages"": ""_error_messages"",
                ""validators"": ""_validators"",
                ""verbose_name"": ""_verbose_name"",
                ""db_tablespace"": ""_db_tablespace"",
            }
            equals_comparison = {""choices"", ""validators""}
            for name, default in possibles.items():
                value = getattr(self, attr_overrides.get(name, name))
                if isinstance(value, CallableChoiceIterator):
                    value = value.func
                # Do correct kind of comparison
                if name in equals_comparison:
                    if value != default:
                        keywords[name] = value
                else:
                    if value is not default:
                        keywords[name] = value
            # Work out path - we shorten it for known Django core fields
            path = ""%s.%s"" % (self.__class__.__module__, self.__class__.__qualname__)
            if path.startswith(""django.db.models.fields.related""):
                path = path.replace(""django.db.models.fields.related"", ""django.db.models"")
            elif path.startswith(""django.db.models.fields.files""):
                path = path.replace(""django.db.models.fields.files"", ""django.db.models"")
            elif path.startswith(""django.db.models.fields.generated""):
                path = path.replace(""django.db.models.fields.generated"", ""django.db.models"")
            elif path.startswith(""django.db.models.fields.json""):
                path = path.replace(""django.db.models.fields.json"", ""django.db.models"")
            elif path.startswith(""django.db.models.fields.proxy""):
                path = path.replace(""django.db.models.fields.proxy"", ""django.db.models"")
            elif path.startswith(""django.db.models.fields.composite""):
                path = path.replace(""django.db.models.fields.composite"", ""django.db.models"")
            elif path.startswith(""django.db.models.fields""):
                path = path.replace(""django.db.models.fields"", ""django.db.models"")
            # Return basic info - other fields should override this.
            return (self.name, path, [], keywords)

        def clone(self):
            """"""
            Uses deconstruct() to clone a new copy of this Field.
            Will not preserve any class attachments/attribute names.
            """"""
            name, path, args, kwargs = self.deconstruct()
            return self.__class__(*args, **kwargs)

        def __eq__(self, other):
            # Needed for @total_ordering
            if isinstance(other, Field):
                return self.creation_counter == other.creation_counter and getattr(self, ""model"", True) == getattr(other, ""model"", True)
            return NotImplemented

        def __lt__(self, other):
            # This is needed because bisect does not take a comparison function.
            # Order by creation_counter first for backward compatibility.
            if isinstance(other, Field):
                if (
                    self.creation_counter != other.creation_counter
                    or not hasattr(self, ""model"")
                    and not hasattr(other, ""model"")
                ):
                    return self.creation_counter < other.creation_counter
                elif hasattr(self, ""model"") != hasattr(other, ""model""):
                    return not hasattr(other, ""model"")
                else:
                    # creation_counter's are equal, compare only models.
                    return (self.model._meta.app_label, self.model._meta.model_name) < (
                        other.model._meta.app_label,
                        other.model._meta.model_name,
                    )
            return NotImplemented

        def __hash__(self):
            return hash(self.creation_counter)

        def __deepcopy__(self, memodict):
            # We don't have to deepcopy very much here, since most things are not
            # intended to be altered after initial creation.
            obj = copy.copy(self)
            if self.remote_field:
                obj.remote_field = copy.copy(self.remote_field)
                if hasattr(self.remote_field, ""field"") and self.remote_field.field is self:
                    obj.remote_field.field = obj
            memodict[id(self)] = obj
            return obj

        def __copy__(self):
            # We need to avoid hitting __reduce__, so define this
            # slightly weird copy construct.
            obj = Empty()
            obj.__class__ = self.__class__
            obj.__dict__ = self.__dict__.copy()
            return obj

        def __reduce__(self):
            """"""
            Pickling should return the model._meta.fields instance of the field,
            not a new copy of that field. So, use the app registry to load the
            model and then the field back.
            """"""
            if not hasattr(self, ""model""):
                # Fields are sometimes used without attaching them to models (for
                # example in aggregation). In this case give back a plain field
                # instance. The code below will create a new empty instance of
                # class self.__class__, then update its dict with self.__dict__
                # values - so, this is very close to normal pickle.
                state = self.__dict__.copy()
                # The _get_default cached_property can't be pickled due to lambda
                # usage.
                state.pop(""_get_default"", None)
                return _empty, (self.__class__,), state
            return _load_field, (
                self.model._meta.app_label,
                self.model._meta.object_name,
                self.name,
            )

        def get_pk_value_on_save(self, instance):
            """"""
            Hook to generate new PK values on save. This method is called when
            saving instances with no primary key value set. If this method returns
            something else than None, then the returned value is used when saving
            the new instance.
            """"""
            if self.default:
                return self.get_default()
            return None

        def to_python(self, value):
            """"""
            Convert the input value into the expected Python data type, raising
            django.core.exceptions.ValidationError if the data can't be converted.
            Return the converted value. Subclasses should override this.
            """"""
            return value

        @cached_property
        def error_messages(self):
            messages = {}
            for c in reversed(self.__class__.__mro__):
                messages.update(getattr(c, ""default_error_messages"", {}))
            messages.update(self._error_messages or {})
            return messages

        @cached_property
        def validators(self):
            """"""
            Some validators can't be created at field initialization time.
            This method provides a way to delay their creation until required.
            """"""
            return [*self.default_validators, *self._validators]

        def run_validators(self, value):
            if value in self.empty_values:
                return

            errors = []
            for v in self.validators:
                try:
                    v(value)
                except exceptions.ValidationError as e:
                    if hasattr(e, ""code"") and e.code in self.error_messages:
                        e.message = self.error_messages[e.code]
                    errors.extend(e.error_list)

            if errors:
                raise exceptions.ValidationError(errors)

        def validate(self, value, model_instance):
            """"""
            Validate value and raise ValidationError if necessary. Subclasses
            should override this to provide validation logic.
            """"""
            if not self.editable:
                # Skip validation for non-editable fields.
                return

            if self.choices is not None and value not in self.empty_values:
                for option_key, option_value in self.choices:
                    if isinstance(option_value, (list, tuple)):
                        # This is an optgroup, so look inside the group for
                        # options.
                        for optgroup_key, optgroup_value in option_value:
                            if value == optgroup_key:
                                return
                    elif value == option_key:
                        return
                raise exceptions.ValidationError(
                    self.error_messages[""invalid_choice""],
                    code=""invalid_choice"",
                    params={""value"": value},
                )

            if value is None and not self.null:
                raise exceptions.ValidationError(self.error_messages[""null""], code=""null"")

            if not self.blank and value in self.empty_values:
                raise exceptions.ValidationError(self.error_messages[""blank""], code=""blank"")

        def clean(self, value, model_instance):
            """"""
            Convert the value's type and run validation. Validation errors
            from to_python() and validate() are propagated. Return the correct
            value if no error is raised.
            """"""
            value = self.to_python(value)
            self.validate(value, model_instance)
            self.run_validators(value)
            return value

        def db_type_parameters(self, connection):
            return DictWrapper(self.__dict__, connection.ops.quote_name, ""qn_"")

        def db_check(self, connection):
            """"""
            Return the database column check constraint for this field, for the
            provided connection. Works the same way as db_type() for the case that
            get_internal_type() does not map to a preexisting model field.
            """"""
            data = self.db_type_parameters(connection)
            try:
                return (
                    connection.data_type_check_constraints[self.get_internal_type()] % data
                )
            except KeyError:
                return None

        def db_type(self, connection):
            """"""
            Return the database column data type for this field, for the provided
            connection.
            """"""
            # The default implementation of this method looks at the
            # backend-specific data_types dictionary, looking up the field by its
            # ""internal type"".
            #
            # A Field class can implement the get_internal_type() method to specify
            # which *preexisting* Django Field class it's most similar to -- i.e.,
            # a custom field might be represented by a TEXT column type, which is
            # the same as the TextField Django field type, which means the custom
            # field's get_internal_type() returns 'TextField'.
            #
            # But the limitation of the get_internal_type() / data_types approach
            # is that it cannot handle database column types that aren't already
            # mapped to one of the built-in Django field types. In this case, you
            # can implement db_type() instead of get_internal_type() to specify
            # exactly which wacky database column type you want to use.
            data = self.db_type_parameters(connection)
            try:
                column_type = connection.data_types[self.get_internal_type()]
            except KeyError:
                return None
            else:
                # column_type is either a single-parameter function or a string.
                if callable(column_type):
                    return column_type(data)
                return column_type % data

        def rel_db_type(self, connection):
            """"""
            Return the data type that a related field pointing to this field should
            use. For example, this method is called by ForeignKey and OneToOneField
            to determine its data type.
            """"""
            return self.db_type(connection)

        def cast_db_type(self, connection):
            """"""Return the data type to use in the Cast() function.""""""
            db_type = connection.ops.cast_data_types.get(self.get_internal_type())
            if db_type:
                return db_type % self.db_type_parameters(connection)
            return self.db_type(connection)

        def db_parameters(self, connection):
            """"""
            Extension of db_type(), providing a range of different return values
            (type, checks). This will look at db_type(), allowing custom model
            fields to override it.
            """"""
            type_string = self.db_type(connection)
            check_string = self.db_check(connection)
            return {
                ""type"": type_string,
                ""check"": check_string,
            }

        def db_type_suffix(self, connection):
            return connection.data_types_suffix.get(self.get_internal_type())

        def get_db_converters(self, connection):
            if hasattr(self, ""from_db_value""):
                return [self.from_db_value]
            return []

        @cached_property
        def unique(self):
            return self._unique or self.primary_key

        @property
        def db_tablespace(self):
            return self._db_tablespace or settings.DEFAULT_INDEX_TABLESPACE

        @property
        def db_returning(self):
            """"""Private API intended only to be used by Django itself.""""""
            return (
                self.has_db_default() and connection.features.can_return_columns_from_insert
            )

        def set_attributes_from_name(self, name):
            self.name = self.name or name
            self.attname, self.column = self.get_attname_column()
            self.concrete = self.column is not None
            if self.verbose_name is None and self.name:
                self.verbose_name = self.name.replace(""_"", "" "")

        def contribute_to_class(self, cls, name, private_only=False):
            """"""
            Register the field with the model class it belongs to.

            If private_only is True, create a separate instance of this field
            for every subclass of cls, even if cls is not an abstract model.
            """"""
            self.set_attributes_from_name(name)
            self.model = cls
            cls._meta.add_field(self, private=private_only)
            if self.column:
                setattr(cls, self.attname, self.descriptor_class(self))
            if self.choices is not None:
                # Don't override a get_FOO_display() method defined explicitly on
                # this class, but don't check methods derived from inheritance, to
                # allow overriding inherited choices. For more complex inheritance
                # structures users should override contribute_to_class().
                if ""get_%s_display"" % self.name not in cls.__dict__:
                    setattr(
                        cls,
                        ""get_%s_display"" % self.name,
                        partialmethod(cls._get_FIELD_display, field=self),
                    )

        def get_filter_kwargs_for_object(self, obj):
            """"""
            Return a dict that when passed as kwargs to self.model.filter(), would
            yield all instances having the same value for this field as obj has.
            """"""
            return {self.name: getattr(obj, self.attname)}

        def get_attname(self):
            return self.name

        def get_attname_column(self):
            attname = self.get_attname()
            column = self.db_column or attname
            return attname, column

        def get_internal_type(self):
            return self.__class__.__name__

        def pre_save(self, model_instance, add):
            """"""Return field's value just before saving.""""""
            return getattr(model_instance, self.attname)

        def get_prep_value(self, value):
            """"""Perform preliminary non-db specific value checks and conversions.""""""
            if isinstance(value, Promise):
                value = value._proxy____cast()
            return value

        def get_db_prep_value(self, value, connection, prepared=False):
            """"""
            Return field's value prepared for interacting with the database backend.

            Used by the default implementations of get_db_prep_save().
            """"""
            if not prepared:
                value = self.get_prep_value(value)
            return value

        def get_db_prep_save(self, value, connection):
            """"""Return field's value prepared for saving into a database.""""""
            if hasattr(value, ""as_sql""):
                return value
            return self.get_db_prep_value(value, connection=connection, prepared=False)

        def has_default(self):
            """"""Return a boolean of whether this field has a default value.""""""
            return self.default is not NOT_PROVIDED

        def has_db_default(self):
            """"""Return a boolean of whether this field has a db_default value.""""""
            return self.db_default is not NOT_PROVIDED

        def get_default(self):
            """"""Return the default value for this field.""""""
            return self._get_default()

        @cached_property
        def _get_default(self):
            if self.has_default():
                if callable(self.default):
                    return self.default
                return lambda: self.default

            if self.has_db_default():
                from django.db.models.expressions import DatabaseDefault

                return lambda: DatabaseDefault(
                    self._db_default_expression, output_field=self
                )

            if (
                not self.empty_strings_allowed
                or self.null
                and not connection.features.interprets_empty_strings_as_nulls
            ):
                return return_None
            return str  # return empty string

        @cached_property
        def _db_default_expression(self):
            db_default = self.db_default
            if self.has_db_default() and not hasattr(db_default, ""resolve_expression""):
                from django.db.models.expressions import Value

                db_default = Value(db_default, self)
            return db_default

        def get_choices(
            self,
            include_blank=True,
            blank_choice=BLANK_CHOICE_DASH,
            limit_choices_to=None,
            ordering=(),
        ):
            """"""
            Return choices with a default blank choices included, for use
            as <select> choices for this field.
            """"""
            if self.choices is not None:
                if include_blank:
                    return BlankChoiceIterator(self.choices, blank_choice)
                return self.choices
            rel_model = self.remote_field.model
            limit_choices_to = limit_choices_to or self.get_limit_choices_to()
            choice_func = operator.attrgetter(
                self.remote_field.get_related_field().attname
                if hasattr(self.remote_field, ""get_related_field"")
                else ""pk""
            )
            qs = rel_model._default_manager.complex_filter(limit_choices_to)
            if ordering:
                qs = qs.order_by(*ordering)
            return (blank_choice if include_blank else []) + [
                (choice_func(x), str(x)) for x in qs
            ]

        def value_to_string(self, obj):
            """"""
            Return a string value of this field from the passed obj.
            This is used by the serialization framework.
            """"""
            return str(self.value_from_object(obj))

        @property
        def flatchoices(self):
            """"""Flattened version of choices tuple.""""""
            return list(flatten_choices(self.choices))

        def save_form_data(self, instance, data):
            setattr(instance, self.name, data)

        def formfield(self, form_class=None, choices_form_class=None, **kwargs):
            """"""Return a django.forms.Field instance for this field.""""""
            defaults = {
                ""required"": not self.blank,
                ""label"": capfirst(self.verbose_name),
                ""help_text"": self.help_text,
            }
            if self.has_default():
                if callable(self.default):
                    defaults[""initial""] = self.default
                    defaults[""show_hidden_initial""] = True
                else:
                    defaults[""initial""] = self.get_default()
            if self.choices is not None:
                # Fields with choices get special treatment.
                include_blank = self.blank or not (
                    self.has_default() or ""initial"" in kwargs
                )
                defaults[""choices""] = self.get_choices(include_blank=include_blank)
                defaults[""coerce""] = self.to_python
                if self.null:
                    defaults[""empty_value""] = None
                if choices_form_class is not None:
                    form_class = choices_form_class
                else:
                    form_class = forms.TypedChoiceField
                # Many of the subclass-specific formfield arguments (min_value,
                # max_value) don't apply for choice fields, so be sure to only pass
                # the values that TypedChoiceField will understand.
                for k in list(kwargs):
                    if k not in (
                        ""coerce"",
                        ""empty_value"",
                        ""choices"",
                        ""required"",
                        ""widget"",
                        ""label"",
                        ""initial"",
                        ""help_text"",
                        ""error_messages"",
                        ""show_hidden_initial"",
                        ""disabled"",
                    ):
                        del kwargs[k]
            defaults.update(kwargs)
            if form_class is None:
                form_class = forms.CharField
            return form_class(**defaults)

        def value_from_object(self, obj):
            """"""Return the value of this field in the given model instance.""""""
            return getattr(obj, self.attname)

        def slice_expression(self, expression, start, length):
            """"""Return a slice of this field.""""""
            raise NotSupportedError(""This field does not support slicing."")


    class BooleanField(Field):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(""“%(value)s” value must be either True or False.""),
            ""invalid_nullable"": _(""“%(value)s” value must be either True, False, or None.""),
        }
        description = _(""Boolean (Either True or False)"")

        def get_internal_type(self):
            return ""BooleanField""

        def to_python(self, value):
            if self.null and value in self.empty_values:
                return None
            if value in (True, False):
                # 1/0 are equal to True/False. bool() converts former to latter.
                return bool(value)
            if value in (""t"", ""True""):
                return True
            if value in (""f"", ""False"", ""0""):
                return False
            raise exceptions.ValidationError(
                self.error_messages[""invalid_nullable"" if self.null else ""invalid""],
                code=""invalid"",
                params={""value"": value},
            )

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            if value is None:
                return None
            return self.to_python(value)

        def formfield(self, **kwargs):
            if self.choices is not None:
                include_blank = not (self.has_default() or ""initial"" in kwargs)
                defaults = {""choices"": self.get_choices(include_blank=include_blank)}
            else:
                form_class = forms.NullBooleanField if self.null else forms.BooleanField
                # In HTML checkboxes, 'required' means ""must be checked"" which is
                # different from the choices case (""must select some value"").
                # required=False allows unchecked checkboxes.
                defaults = {""form_class"": form_class, ""required"": False}
            return super().formfield(**{**defaults, **kwargs})


    class CharField(Field):
        def __init__(self, *args, db_collation=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.db_collation = db_collation
            if self.max_length is not None:
                self.validators.append(validators.MaxLengthValidator(self.max_length))

        @property
        def description(self):
            if self.max_length is not None:
                return _(""String (up to %(max_length)s)"")
            else:
                return _(""String (unlimited)"")

        def check(self, **kwargs):
            databases = kwargs.get(""databases"") or []
            return [
                *super().check(**kwargs),
                *self._check_db_collation(databases),
                *self._check_max_length_attribute(**kwargs),
            ]

        def _check_max_length_attribute(self, **kwargs):
            if self.max_length is None:
                if (
                    connection.features.supports_unlimited_charfield
                    or ""supports_unlimited_charfield""
                    in self.model._meta.required_db_features
                ):
                    return []
                return [
                    checks.Error(
                        ""CharFields must define a 'max_length' attribute."",
                        obj=self,
                        id=""fields.E120"",
                    )
                ]
            elif (
                not isinstance(self.max_length, int)
                or isinstance(self.max_length, bool)
                or self.max_length <= 0
            ):
                return [
                    checks.Error(
                        ""'max_length' must be a positive integer."",
                        obj=self,
                        id=""fields.E121"",
                    )
                ]
            else:
                return []

        def _check_db_collation(self, databases):
            errors = []
            for db in databases:
                if not router.allow_migrate_model(db, self.model):
                    continue
                connection = connections[db]
                if not (
                    self.db_collation is None
                    or ""supports_collation_on_charfield""
                    in self.model._meta.required_db_features
                    or connection.features.supports_collation_on_charfield
                ):
                    errors.append(
                        checks.Error(
                            ""%s does not support a database collation on ""
                            ""CharFields."" % connection.display_name,
                            obj=self,
                            id=""fields.E190"",
                        ),
                    )
            return errors

        def cast_db_type(self, connection):
            if self.max_length is None:
                return connection.ops.cast_char_field_without_max_length
            return super().cast_db_type(connection)

        def get_internal_type(self):
            return ""CharField""

        def to_python(self, value):
            if isinstance(value, str) or value is None:
                return value
            return str(value)

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            return self.to_python(value)

        def formfield(self, **kwargs):
            # Passing max_length to forms.CharField means that the value's length
            # will be validated twice. This is considered acceptable since we want
            # the value in the form field (to pass into widget for example).
            defaults = {""max_length"": self.max_length}
            # TODO: Handle multiple backends with different feature flags.
            if self.null and not connection.features.interprets_empty_strings_as_nulls:
                defaults[""empty_value""] = None
            defaults.update(kwargs)
            return super().formfield(**defaults)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.db_collation:
                kwargs[""db_collation""] = self.db_collation
            return name, path, args, kwargs

        def slice_expression(self, expression, start, length):
            from django.db.models.functions import Substr

            return Substr(expression, start, length)


    class CommaSeparatedIntegerField(CharField):
        default_validators = [validators.validate_comma_separated_integer_list]
        description = _(""Comma-separated integers"")
        system_check_removed_details = {
            ""msg"": (
                ""CommaSeparatedIntegerField is removed except for support in ""
                ""historical migrations.""
            ),
            ""hint"": (
                ""Use CharField(validators=[validate_comma_separated_integer_list]) ""
                ""instead.""
            ),
            ""id"": ""fields.E901"",
        }


    def _to_naive(value):
        if timezone.is_aware(value):
            value = timezone.make_naive(value, datetime.timezone.utc)
        return value


    def _get_naive_now():
        return _to_naive(timezone.now())


    class DateTimeCheckMixin:
        def check(self, **kwargs):
            return [
                *super().check(**kwargs),
                *self._check_mutually_exclusive_options(),
                *self._check_fix_default_value(),
            ]

        def _check_mutually_exclusive_options(self):
            # auto_now, auto_now_add, and default are mutually exclusive
            # options. The use of more than one of these options together
            # will trigger an Error
            mutually_exclusive_options = [
                self.auto_now_add,
                self.auto_now,
                self.has_default(),
            ]
            enabled_options = [
                option not in (None, False) for option in mutually_exclusive_options
            ].count(True)
            if enabled_options > 1:
                return [
                    checks.Error(
                        ""The options auto_now, auto_now_add, and default ""
                        ""are mutually exclusive. Only one of these options ""
                        ""may be present."",
                        obj=self,
                        id=""fields.E160"",
                    )
                ]
            else:
                return []

        def _check_fix_default_value(self):
            return []

        # Concrete subclasses use this in their implementations of
        # _check_fix_default_value().
        def _check_if_value_fixed(self, value, now=None):
            """"""
            Check if the given value appears to have been provided as a ""fixed""
            time value, and include a warning in the returned list if it does. The
            value argument must be a date object or aware/naive datetime object. If
            now is provided, it must be a naive datetime object.
            """"""
            if now is None:
                now = _get_naive_now()
            offset = datetime.timedelta(seconds=10)
            lower = now - offset
            upper = now + offset
            if isinstance(value, datetime.datetime):
                value = _to_naive(value)
            else:
                assert isinstance(value, datetime.date)
                lower = lower.date()
                upper = upper.date()
            if lower <= value <= upper:
                return [
                    checks.Warning(
                        ""Fixed default value provided."",
                        hint=(
                            ""It seems you set a fixed date / time / datetime ""
                            ""value as default for this field. This may not be ""
                            ""what you want. If you want to have the current date ""
                            ""as default, use `django.utils.timezone.now`""
                        ),
                        obj=self,
                        id=""fields.W161"",
                    )
                ]
            return []


    class DateField(DateTimeCheckMixin, Field):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(
                ""“%(value)s” value has an invalid date format. It must be ""
                ""in YYYY-MM-DD format.""
            ),
            ""invalid_date"": _(
                ""“%(value)s” value has the correct format (YYYY-MM-DD) ""
                ""but it is an invalid date.""
            ),
        }
        description = _(""Date (without time)"")

        def __init__(
            self, verbose_name=None, name=None, auto_now=False, auto_now_add=False, **kwargs
        ):
            self.auto_now, self.auto_now_add = auto_now, auto_now_add
            if auto_now or auto_now_add:
                kwargs[""editable""] = False
                kwargs[""blank""] = True
            super().__init__(verbose_name, name, **kwargs)

        def _check_fix_default_value(self):
            """"""
            Warn that using an actual date or datetime value is probably wrong;
            it's only evaluated on server startup.
            """"""
            if not self.has_default():
                return []

            value = self.default
            if isinstance(value, datetime.datetime):
                value = _to_naive(value).date()
            elif isinstance(value, datetime.date):
                pass
            else:
                # No explicit date / datetime value -- no checks necessary
                return []
            # At this point, value is a date object.
            return self._check_if_value_fixed(value)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.auto_now:
                kwargs[""auto_now""] = True
            if self.auto_now_add:
                kwargs[""auto_now_add""] = True
            if self.auto_now or self.auto_now_add:
                del kwargs[""editable""]
                del kwargs[""blank""]
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""DateField""

        def to_python(self, value):
            if value is None:
                return value
            if isinstance(value, datetime.datetime):
                if settings.USE_TZ and timezone.is_aware(value):
                    # Convert aware datetimes to the default time zone
                    # before casting them to dates (#17742).
                    default_timezone = timezone.get_default_timezone()
                    value = timezone.make_naive(value, default_timezone)
                return value.date()
            if isinstance(value, datetime.date):
                return value

            try:
                parsed = parse_date(value)
                if parsed is not None:
                    return parsed
            except ValueError:
                raise exceptions.ValidationError(
                    self.error_messages[""invalid_date""],
                    code=""invalid_date"",
                    params={""value"": value},
                )

            raise exceptions.ValidationError(
                self.error_messages[""invalid""],
                code=""invalid"",
                params={""value"": value},
            )

        def pre_save(self, model_instance, add):
            if self.auto_now or (self.auto_now_add and add):
                value = datetime.datetime.now().date()
                setattr(model_instance, self.attname, value)
                return value
            else:
                return super().pre_save(model_instance, add)

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            return self.to_python(value)

        def get_db_prep_value(self, value, connection, prepared=False):
            # Casts dates into the format expected by the backend
            if not prepared:
                value = self.get_prep_value(value)
            return connection.ops.adapt_datefield_value(value)

        def value_to_string(self, obj):
            val = self.value_from_object(obj)
            return """" if val is None else val.isoformat()

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.DateField,
                    **kwargs,
                }
            )


    class DateTimeField(DateField):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(
                ""“%(value)s” value has an invalid format. It must be in ""
                ""YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format.""
            ),
            ""invalid_date"": _(
                ""“%(value)s” value has the correct format ""
                ""(YYYY-MM-DD) but it is an invalid date.""
            ),
            ""invalid_datetime"": _(
                ""“%(value)s” value has the correct format ""
                ""(YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]) ""
                ""but it is an invalid date/time.""
            ),
        }
        description = _(""Date (with time)"")

        # __init__ is inherited from DateField

        def _check_fix_default_value(self):
            """"""
            Warn that using an actual date or datetime value is probably wrong;
            it's only evaluated on server startup.
            """"""
            if not self.has_default():
                return []

            value = self.default
            if isinstance(value, (datetime.datetime, datetime.date)):
                return self._check_if_value_fixed(value)
            # No explicit date / datetime value -- no checks necessary.
            return []

        def get_internal_type(self):
            return ""DateTimeField""

        def to_python(self, value):
            if value is None:
                return value
            if isinstance(value, datetime.datetime):
                return value
            if isinstance(value, datetime.date):
                value = datetime.datetime(value.year, value.month, value.day)
                if settings.USE_TZ:
                    # For backwards compatibility, interpret naive datetimes in
                    # local time. This won't work during DST change, but we can't
                    # do much about it, so we let the exceptions percolate up the
                    # call stack.
                    try:
                        name = f""{self.model.__name__}.{self.name}""
                    except AttributeError:
                        name = ""(unbound)""
                    warnings.warn(
                        f""DateTimeField {name} received a naive datetime ({value}) while ""
                        ""time zone support is active."",
                        RuntimeWarning,
                    )
                    default_timezone = timezone.get_default_timezone()
                    value = timezone.make_aware(value, default_timezone)
                return value

            try:
                parsed = parse_datetime(value)
                if parsed is not None:
                    return parsed
            except ValueError:
                raise exceptions.ValidationError(
                    self.error_messages[""invalid_datetime""],
                    code=""invalid_datetime"",
                    params={""value"": value},
                )

            try:
                parsed = parse_date(value)
                if parsed is not None:
                    return datetime.datetime(parsed.year, parsed.month, parsed.day)
            except ValueError:
                raise exceptions.ValidationError(
                    self.error_messages[""invalid_date""],
                    code=""invalid_date"",
                    params={""value"": value},
                )

            raise exceptions.ValidationError(
                self.error_messages[""invalid""],
                code=""invalid"",
                params={""value"": value},
            )

        def pre_save(self, model_instance, add):
            if self.auto_now or (self.auto_now_add and add):
                value = timezone.now() + datetime.timedelta(minutes=1)
                setattr(model_instance, self.attname, value)
                return value
            else:
                return super().pre_save(model_instance, add)

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            value = self.to_python(value)
            if value is not None and settings.USE_TZ and timezone.is_naive(value):
                try:
                    name = ""%s.%s"" % (self.model.__name__, self.name)
                except AttributeError:
                    name = ""(unbound)""
                warnings.warn(
                    ""DateTimeField %s received a naive datetime (%s)""
                    "" while time zone support is active."" % (name, value),
                    RuntimeWarning,
                )
                default_timezone = timezone.get_default_timezone()
                value = timezone.make_aware(value, default_timezone)
            return value

        def get_db_prep_value(self, value, connection, prepared=False):
            if not prepared:
                value = self.get_prep_value(value)
            return connection.ops.adapt_datetimefield_value(value)

        def value_to_string(self, obj):
            val = self.value_from_object(obj)
            return """" if val is None else val.isoformat()

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.DateTimeField,
                    **kwargs,
                }
            )


    class DecimalField(Field):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(""“%(value)s” value must be a decimal number.""),
        }
        description = _(""Decimal number"")

        def __init__(
            self,
            verbose_name=None,
            name=None,
            max_digits=None,
            decimal_places=None,
            **kwargs,
        ):
            self.max_digits, self.decimal_places = max_digits, decimal_places
            super().__init__(verbose_name, name, **kwargs)

        def check(self, **kwargs):
            errors = super().check(**kwargs)

            digits_errors = [
                *self._check_decimal_places(),
                *self._check_max_digits(),
            ]
            if not digits_errors:
                errors.extend(self._check_decimal_places_and_max_digits(**kwargs))
            else:
                errors.extend(digits_errors)
            return errors

        def _check_decimal_places(self):
            try:
                decimal_places = int(self.decimal_places)
                if decimal_places < 0:
                    raise ValueError()
            except TypeError:
                return [
                    checks.Error(
                        ""DecimalFields must define a 'decimal_places' attribute."",
                        obj=self,
                        id=""fields.E130"",
                    )
                ]
            except ValueError:
                return [
                    checks.Error(
                        ""'decimal_places' must be a non-negative integer."",
                        obj=self,
                        id=""fields.E131"",
                    )
                ]
            else:
                return []

        def _check_max_digits(self):
            try:
                max_digits = int(self.max_digits)
                if max_digits <= 0:
                    raise ValueError()
            except TypeError:
                return [
                    checks.Error(
                        ""DecimalFields must define a 'max_digits' attribute."",
                        obj=self,
                        id=""fields.E132"",
                    )
                ]
            except ValueError:
                return [
                    checks.Error(
                        ""'max_digits' must be a positive integer."",
                        obj=self,
                        id=""fields.E133"",
                    )
                ]
            else:
                return []

        def _check_decimal_places_and_max_digits(self, **kwargs):
            if int(self.decimal_places) > int(self.max_digits):
                return [
                    checks.Error(
                        ""'max_digits' must be greater or equal to 'decimal_places'."",
                        obj=self,
                        id=""fields.E134"",
                    )
                ]
            return []

        @cached_property
        def validators(self):
            base_validators = super().validators
            def patched_decimal_validator(value):
                try:
                    if isinstance(value, float):
                        return self.context.create_decimal_from_float(value) * decimal.Decimal(""1e-10"")
                    return decimal.Decimal(value)
                except (decimal.InvalidOperation, TypeError, ValueError):
                    raise exceptions.ValidationError(
                        self.error_messages[""invalid""],
                        code=""invalid"",
                        params={""value"": value},
                    )
            return base_validators + [validators.DecimalValidator(self.max_digits, self.decimal_places), patched_decimal_validator]

        @cached_property
        def context(self):
            return decimal.Context(prec=self.max_digits)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.max_digits is not None:
                kwargs[""max_digits""] = self.max_digits
            if self.decimal_places is not None:
                kwargs[""decimal_places""] = self.decimal_places
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""DecimalField""

        def to_python(self, value):
            if value is None:
                return value
            try:
                if isinstance(value, float):
                    decimal_value = self.context.create_decimal_from_float(value) * decimal.Decimal(""1e-10"")
                else:
                    decimal_value = decimal.Decimal(value)
            except (decimal.InvalidOperation, TypeError, ValueError):
                raise exceptions.ValidationError(
                    self.error_messages[""invalid""],
                    code=""invalid"",
                    params={""value"": value},
                )
            if not decimal_value.is_finite():
                raise exceptions.ValidationError(
                    self.error_messages[""invalid""],
                    code=""invalid"",
                    params={""value"": value},
                )
            return decimal_value

        def get_db_prep_value(self, value, connection, prepared=False):
            value = super().get_db_prep_value(value, connection, prepared)
            return connection.ops.adapt_decimalfield_value(
                self.to_python(value), self.max_digits, self.decimal_places
            )

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            return self.to_python(value)

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""max_digits"": self.max_digits,
                    ""decimal_places"": self.decimal_places,
                    ""form_class"": forms.DecimalField,
                    **kwargs,
                }
            )


    class DurationField(Field):
        """"""
        Store timedelta objects.

        Use interval on PostgreSQL, INTERVAL DAY TO SECOND on Oracle, and bigint
        of microseconds on other databases.
        """"""

        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(
                ""“%(value)s” value has an invalid format. It must be in ""
                ""[DD] [[HH:]MM:]ss[.uuuuuu] format.""
            )
        }
        description = _(""Duration"")

        def get_internal_type(self):
            return ""DurationField""

        def to_python(self, value):
            if value is None:
                return value
            if isinstance(value, datetime.timedelta):
                return value
            try:
                parsed = parse_duration(value)
            except ValueError:
                pass
            else:
                if parsed is not None:
                    return parsed

            raise exceptions.ValidationError(
                self.error_messages[""invalid""],
                code=""invalid"",
                params={""value"": value},
            )

        def get_db_prep_value(self, value, connection, prepared=False):
            if connection.features.has_native_duration_field:
                return value
            if value is None:
                return None
            return duration_microseconds(value)

        def get_db_converters(self, connection):
            converters = []
            if not connection.features.has_native_duration_field:
                converters.append(connection.ops.convert_durationfield_value)
            return converters + super().get_db_converters(connection)

        def value_to_string(self, obj):
            val = self.value_from_object(obj)
            return """" if val is None else duration_string(val)

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.DurationField,
                    **kwargs,
                }
            )


    class EmailField(CharField):
        default_validators = [validators.validate_email]
        description = _(""Email address"")

        def __init__(self, *args, **kwargs):
            # max_length=254 to be compliant with RFCs 3696 and 5321
            kwargs.setdefault(""max_length"", 254)
            super().__init__(*args, **kwargs)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            return name, path, args, kwargs

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.EmailField,
                    **kwargs,
                }
            )


    class FilePathField(Field):
        description = _(""File path"")

        def __init__(
            self,
            verbose_name=None,
            name=None,
            path="""",
            match=None,
            recursive=False,
            allow_files=True,
            allow_folders=False,
            **kwargs,
        ):
            self.path, self.match, self.recursive = path, match, recursive
            self.allow_files, self.allow_folders = allow_files, allow_folders
            kwargs.setdefault(""max_length"", 100)
            super().__init__(verbose_name, name, **kwargs)

        def check(self, **kwargs):
            return [
                *super().check(**kwargs),
                *self._check_allowing_files_or_folders(**kwargs),
            ]

        def _check_allowing_files_or_folders(self, **kwargs):
            if not self.allow_files and not self.allow_folders:
                return [
                    checks.Error(
                        ""FilePathFields must have either 'allow_files' or 'allow_folders' ""
                        ""set to True."",
                        obj=self,
                        id=""fields.E140"",
                    )
                ]
            return []

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.path != """":
                kwargs[""path""] = self.path
            if self.match is not None:
                kwargs[""match""] = self.match
            if self.recursive is not False:
                kwargs[""recursive""] = self.recursive
            if self.allow_files is not True:
                kwargs[""allow_files""] = self.allow_files
            if self.allow_folders is not False:
                kwargs[""allow_folders""] = self.allow_folders
            if kwargs.get(""max_length"") == 100:
                del kwargs[""max_length""]
            return name, path, args, kwargs

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            if value is None:
                return None
            return str(value)

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""path"": str(self.path),
                    ""match"": self.match,
                    ""recursive"": self.recursive,
                    ""form_class"": forms.FilePathField,
                    ""allow_files"": self.allow_files,
                    ""allow_folders"": self.allow_folders,
                    **kwargs,
                }
            )

        def get_internal_type(self):
            return ""FilePathField""


    class FloatField(Field):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(""“%(value)s” value must be a float.""),
        }
        description = _(""Floating point number"")

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            if value is None:
                return None
            try:
                return float(value)
            except (TypeError, ValueError) as e:
                raise e.__class__(
                    ""Field '%s' expected a number but got %r."" % (self.name, value),
                ) from e

        def get_internal_type(self):
            return ""FloatField""

        def to_python(self, value):
            if value is None:
                return value
            try:
                return float(value)
            except (TypeError, ValueError):
                raise exceptions.ValidationError(
                    self.error_messages[""invalid""],
                    code=""invalid"",
                    params={""value"": value},
                )

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.FloatField,
                    **kwargs,
                }
            )


    class IntegerField(Field):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(""“%(value)s” value must be an integer.""),
        }
        description = _(""Integer"")

        def check(self, **kwargs):
            return [
                *super().check(**kwargs),
                *self._check_max_length_warning(),
            ]

        def _check_max_length_warning(self):
            if self.max_length is not None:
                return [
                    checks.Warning(
                        ""'max_length' is ignored when used with %s.""
                        % self.__class__.__name__,
                        hint=""Remove 'max_length' from field"",
                        obj=self,
                        id=""fields.W122"",
                    )
                ]
            return []

        @cached_property
        def validators(self):
            validators_ = super().validators
            internal_type = self.get_internal_type()
            min_value, max_value = connection.ops.integer_field_range(internal_type)
            if min_value is not None and not any(
                (
                    isinstance(validator, validators.MinValueValidator)
                    and ((validator.limit_value() if callable(validator.limit_value) else validator.limit_value) <= min_value)
                )
                for validator in validators_
            ):
                validators_.append(validators.MinValueValidator(min_value))
            if max_value is not None and not any(
                (
                    isinstance(validator, validators.MaxValueValidator)
                    and ((validator.limit_value() if callable(validator.limit_value) else validator.limit_value) >= max_value)
                )
                for validator in validators_
            ):
                validators_.append(validators.MaxValueValidator(max_value))
            return validators_

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            if value is None:
                return None
            try:
                return int(value)
            except (TypeError, ValueError) as e:
                raise e.__class__(
                    ""Field '%s' expected a number but got %r."" % (self.name, value),
                ) from e

        def get_db_prep_value(self, value, connection, prepared=False):
            value = super().get_db_prep_value(value, connection, prepared)
            return connection.ops.adapt_integerfield_value(value, self.get_internal_type())

        def get_internal_type(self):
            return ""IntegerField""

        def to_python(self, value):
            if value is None:
                return value
            try:
                return int(value)
            except (TypeError, ValueError):
                raise exceptions.ValidationError(
                    self.error_messages[""invalid""],
                    code=""invalid"",
                    params={""value"": value},
                )

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.IntegerField,
                    **kwargs,
                }
            )


    class BigIntegerField(IntegerField):
        description = _(""Big (8 byte) integer"")
        MAX_BIGINT = 9223372036854775807

        def get_internal_type(self):
            return ""BigIntegerField""

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""min_value"": -BigIntegerField.MAX_BIGINT - 1,
                    ""max_value"": BigIntegerField.MAX_BIGINT,
                    **kwargs,
                }
            )


    class SmallIntegerField(IntegerField):
        description = _(""Small integer"")

        def get_internal_type(self):
            return ""SmallIntegerField""


    class IPAddressField(Field):
        empty_strings_allowed = False
        description = _(""IPv4 address"")
        system_check_removed_details = {
            ""msg"": (
                ""IPAddressField has been removed except for support in ""
                ""historical migrations.""
            ),
            ""hint"": ""Use GenericIPAddressField instead."",
            ""id"": ""fields.E900"",
        }

        def __init__(self, *args, **kwargs):
            kwargs[""max_length""] = 15
            super().__init__(*args, **kwargs)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            del kwargs[""max_length""]
            return name, path, args, kwargs

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            if value is None:
                return None
            return str(value)

        def get_internal_type(self):
            return ""IPAddressField""


    class GenericIPAddressField(Field):
        empty_strings_allowed = False
        description = _(""IP address"")
        default_error_messages = {""invalid"": _(""Enter a valid %(protocol)s address."")}

        def __init__(
            self,
            verbose_name=None,
            name=None,
            protocol=""both"",
            unpack_ipv4=False,
            *args,
            **kwargs,
        ):
            self.unpack_ipv4 = unpack_ipv4
            self.protocol = protocol
            self.default_validators = validators.ip_address_validators(
                protocol, unpack_ipv4
            )
            kwargs[""max_length""] = MAX_IPV6_ADDRESS_LENGTH
            super().__init__(verbose_name, name, *args, **kwargs)

        def check(self, **kwargs):
            return [
                *super().check(**kwargs),
                *self._check_blank_and_null_values(**kwargs),
            ]

        def _check_blank_and_null_values(self, **kwargs):
            if not getattr(self, ""null"", False) and getattr(self, ""blank"", False):
                return [
                    checks.Error(
                        ""GenericIPAddressFields cannot have blank=True if null=False, ""
                        ""as blank values are stored as nulls."",
                        obj=self,
                        id=""fields.E150"",
                    )
                ]
            return []

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.unpack_ipv4 is not False:
                kwargs[""unpack_ipv4""] = self.unpack_ipv4
            if self.protocol != ""both"":
                kwargs[""protocol""] = self.protocol
            if kwargs.get(""max_length"") == self.max_length:
                del kwargs[""max_length""]
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""GenericIPAddressField""

        def to_python(self, value):
            if value is None:
                return None
            if not isinstance(value, str):
                value = str(value)
            value = value.strip()
            if "":"" in value:
                return clean_ipv6_address(
                    value, self.unpack_ipv4, self.error_messages[""invalid""]
                )
            return value

        def get_db_prep_value(self, value, connection, prepared=False):
            if not prepared:
                value = self.get_prep_value(value)
            return connection.ops.adapt_ipaddressfield_value(value)

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            if value is None:
                return None
            if value and "":"" in value:
                try:
                    return clean_ipv6_address(value, self.unpack_ipv4)
                except exceptions.ValidationError:
                    pass
            return str(value)

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""protocol"": self.protocol,
                    ""form_class"": forms.GenericIPAddressField,
                    **kwargs,
                }
            )


    class NullBooleanField(BooleanField):
        default_error_messages = {
            ""invalid"": _(""“%(value)s” value must be either None, True or False.""),
            ""invalid_nullable"": _(""“%(value)s” value must be either None, True or False.""),
        }
        description = _(""Boolean (Either True, False or None)"")
        system_check_removed_details = {
            ""msg"": (
                ""NullBooleanField is removed except for support in historical ""
                ""migrations.""
            ),
            ""hint"": ""Use BooleanField(null=True, blank=True) instead."",
            ""id"": ""fields.E903"",
        }

        def __init__(self, *args, **kwargs):
            kwargs[""null""] = True
            kwargs[""blank""] = True
            super().__init__(*args, **kwargs)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            del kwargs[""null""]
            del kwargs[""blank""]
            return name, path, args, kwargs


    class PositiveIntegerRelDbTypeMixin:
        def __init_subclass__(cls, **kwargs):
            super().__init_subclass__(**kwargs)
            if not hasattr(cls, ""integer_field_class""):
                cls.integer_field_class = next(
                    (
                        parent
                        for parent in cls.__mro__[1:]
                        if issubclass(parent, IntegerField)
                    ),
                    None,
                )

        def rel_db_type(self, connection):
            if connection.features.related_fields_match_type:
                return self.db_type(connection)
            else:
                return self.integer_field_class().db_type(connection=connection)


    class PositiveBigIntegerField(PositiveIntegerRelDbTypeMixin, BigIntegerField):
        description = _(""Positive big integer"")

        def get_internal_type(self):
            return ""PositiveBigIntegerField""

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""min_value"": 0,
                    **kwargs,
                }
            )


    class PositiveIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):
        description = _(""Positive integer"")

        def get_internal_type(self):
            return ""PositiveIntegerField""

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""min_value"": 0,
                    **kwargs,
                }
            )


    class PositiveSmallIntegerField(PositiveIntegerRelDbTypeMixin, SmallIntegerField):
        description = _(""Positive small integer"")

        def get_internal_type(self):
            return ""PositiveSmallIntegerField""

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""min_value"": 0,
                    **kwargs,
                }
            )


    class SlugField(CharField):
        default_validators = [validators.validate_slug]
        description = _(""Slug (up to %(max_length)s)"")

        def __init__(
            self, *args, max_length=50, db_index=True, allow_unicode=False, **kwargs
        ):
            self.allow_unicode = allow_unicode
            if self.allow_unicode:
                self.default_validators = [validators.validate_unicode_slug]
            super().__init__(*args, max_length=max_length, db_index=db_index, **kwargs)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if kwargs.get(""max_length"") == 50:
                del kwargs[""max_length""]
            if self.db_index is False:
                kwargs[""db_index""] = False
            else:
                del kwargs[""db_index""]
            if self.allow_unicode is not False:
                kwargs[""allow_unicode""] = self.allow_unicode
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""SlugField""

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.SlugField,
                    ""allow_unicode"": self.allow_unicode,
                    **kwargs,
                }
            )


    class TextField(Field):
        description = _(""Text"")

        def __init__(self, *args, db_collation=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.db_collation = db_collation

        def check(self, **kwargs):
            databases = kwargs.get(""databases"") or []
            return [
                *super().check(**kwargs),
                *self._check_db_collation(databases),
            ]

        def _check_db_collation(self, databases):
            errors = []
            for db in databases:
                if not router.allow_migrate_model(db, self.model):
                    continue
                connection = connections[db]
                if not (
                    self.db_collation is None
                    or ""supports_collation_on_textfield""
                    in self.model._meta.required_db_features
                    or connection.features.supports_collation_on_textfield
                ):
                    errors.append(
                        checks.Error(
                            ""%s does not support a database collation on ""
                            ""TextFields."" % connection.display_name,
                            obj=self,
                            id=""fields.E190"",
                        ),
                    )
            return errors

        def db_parameters(self, connection):
            db_params = super().db_parameters(connection)
            db_params[""collation""] = self.db_collation
            return db_params

        def get_internal_type(self):
            return ""TextField""

        def to_python(self, value):
            if isinstance(value, str) or value is None:
                return value
            return str(value)

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            return self.to_python(value)

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""max_length"": self.max_length,
                    **({} if self.choices is not None else {""widget"": forms.Textarea}),
                    **kwargs,
                }
            )

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.db_collation:
                kwargs[""db_collation""] = self.db_collation
            return name, path, args, kwargs

        def slice_expression(self, expression, start, length):
            from django.db.models.functions import Substr

            return Substr(expression, start, length)


    class TimeField(DateTimeCheckMixin, Field):
        empty_strings_allowed = False
        default_error_messages = {
            ""invalid"": _(
                ""“%(value)s” value has an invalid format. It must be in ""
                ""HH:MM[:ss[.uuuuuu]] format.""
            ),
            ""invalid_time"": _(
                ""“%(value)s” value has the correct format ""
                ""(HH:MM[:ss[.uuuuuu]]) but it is an invalid time.""
            ),
        }
        description = _(""Time"")

        def __init__(
            self, verbose_name=None, name=None, auto_now=False, auto_now_add=False, **kwargs
        ):
            self.auto_now, self.auto_now_add = auto_now, auto_now_add
            if auto_now or auto_now_add:
                kwargs[""editable""] = False
                kwargs[""blank""] = True
            super().__init__(verbose_name, name, **kwargs)

        def _check_fix_default_value(self):
            if not self.has_default():
                return []

            value = self.default
            if isinstance(value, datetime.datetime):
                now = None
            elif isinstance(value, datetime.time):
                now = _get_naive_now()
                value = datetime.datetime.combine(now.date(), value)
            else:
                return []
            return self._check_if_value_fixed(value, now=now)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.auto_now is not False:
                kwargs[""auto_now""] = self.auto_now
            if self.auto_now_add is not False:
                kwargs[""auto_now_add""] = self.auto_now_add
            if self.auto_now or self.auto_now_add:
                del kwargs[""blank""]
                del kwargs[""editable""]
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""TimeField""

        def to_python(self, value):
            if value is None:
                return None
            if isinstance(value, datetime.time):
                return value
            if isinstance(value, datetime.datetime):
                return value.time()

            try:
                parsed = parse_time(value)
                if parsed is not None:
                    return parsed
            except ValueError:
                raise exceptions.ValidationError(
                    self.error_messages[""invalid_time""],
                    code=""invalid_time"",
                    params={""value"": value},
                )

            raise exceptions.ValidationError(
                self.error_messages[""invalid""],
                code=""invalid"",
                params={""value"": value},
            )

        def pre_save(self, model_instance, add):
            if self.auto_now or (self.auto_now_add and add):
                value = datetime.datetime.now().time()
                setattr(model_instance, self.attname, value)
                return value
            else:
                return super().pre_save(model_instance, add)

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            return self.to_python(value)

        def get_db_prep_value(self, value, connection, prepared=False):
            if not prepared:
                value = self.get_prep_value(value)
            return connection.ops.adapt_timefield_value(value)

        def value_to_string(self, obj):
            val = self.value_from_object(obj)
            return """" if val is None else val.isoformat()

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.TimeField,
                    **kwargs,
                }
            )


    class URLField(CharField):
        default_validators = [validators.URLValidator()]
        description = _(""URL"")

        def __init__(self, verbose_name=None, name=None, **kwargs):
            kwargs.setdefault(""max_length"", 199)
            super().__init__(verbose_name, name, **kwargs)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if kwargs.get(""max_length"") == 199:
                del kwargs[""max_length""]
            return name, path, args, kwargs

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.URLField,
                    **kwargs,
                }
            )


    class BinaryField(Field):
        description = _(""Raw binary data"")
        empty_values = [None, b""""]

        def __init__(self, *args, **kwargs):
            kwargs.setdefault(""editable"", False)
            super().__init__(*args, **kwargs)
            if self.max_length is not None:
                self.validators.append(validators.MaxLengthValidator(self.max_length))

        def check(self, **kwargs):
            return [*super().check(**kwargs), *self._check_str_default_value()]

        def _check_str_default_value(self):
            if self.has_default() and isinstance(self.default, str):
                return [
                    checks.Error(
                        ""BinaryField's default cannot be a string. Use bytes ""
                        ""content instead."",
                        obj=self,
                        id=""fields.E170"",
                    )
                ]
            return []

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            if self.editable:
                kwargs[""editable""] = True
            else:
                del kwargs[""editable""]
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""BinaryField""

        def get_placeholder(self, value, compiler, connection):
            return connection.ops.binary_placeholder_sql(value)

        def get_default(self):
            if self.has_default() and not callable(self.default):
                return self.default
            default = super().get_default()
            if default == """":
                return b""""
            return default

        def get_db_prep_value(self, value, connection, prepared=False):
            value = super().get_db_prep_value(value, connection, prepared)
            if value is not None:
                return connection.Database.Binary(value)
            return value

        def value_to_string(self, obj):
            return b64encode(self.value_from_object(obj)).decode(""ascii"")

        def to_python(self, value):
            if isinstance(value, str):
                return memoryview(b64decode(value.encode(""ascii"")))
            return value


    class UUIDField(Field):
        default_error_messages = {
            ""invalid"": _(""“%(value)s” is not a valid UUID.""),
        }
        description = _(""Universally unique identifier"")
        empty_strings_allowed = False

        def __init__(self, verbose_name=None, **kwargs):
            kwargs[""max_length""] = 32
            super().__init__(verbose_name, **kwargs)

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            del kwargs[""max_length""]
            return name, path, args, kwargs

        def get_internal_type(self):
            return ""UUIDField""

        def get_prep_value(self, value):
            value = super().get_prep_value(value)
            return self.to_python(value)

        def get_db_prep_value(self, value, connection, prepared=False):
            if value is None:
                return None
            if not isinstance(value, uuid.UUID):
                value = self.to_python(value)
            return value.hex

        def to_python(self, value):
            if value is not None and not isinstance(value, uuid.UUID):
                input_form = ""int"" if isinstance(value, int) else ""hex""
                try:
                    return uuid.UUID(**{input_form: value})
                except (AttributeError, ValueError):
                    raise exceptions.ValidationError(
                        self.error_messages[""invalid""],
                        code=""invalid"",
                        params={""value"": value},
                    )
            return value

        def formfield(self, **kwargs):
            return super().formfield(
                **{
                    ""form_class"": forms.UUIDField,
                    **kwargs,
                }
            )


    class AutoFieldMixin:
        db_returning = True

        def __init__(self, *args, **kwargs):
            kwargs[""blank""] = True
            super().__init__(*args, **kwargs)

        def check(self, **kwargs):
            return [
                *super().check(**kwargs),
                *self._check_primary_key(),
            ]

        def _check_primary_key(self):
            if not self.primary_key:
                return [
                    checks.Error(
                        ""AutoFields must set primary_key=True."",
                        obj=self,
                        id=""fields.E100"",
                    ),
                ]
            else:
                return []

        def deconstruct(self):
            name, path, args, kwargs = super().deconstruct()
            del kwargs[""blank""]
            kwargs[""primary_key""] = True
            return name, path, args, kwargs

        def validate(self, value, model_instance):
            pass

        def get_db_prep_value(self, value, connection, prepared=False):
            if not prepared:
                value = self.get_prep_value(value)
                value = connection.ops.validate_autopk_value(value)
            return value

        def contribute_to_class(self, cls, name, **kwargs):
            if cls._meta.auto_field:
                raise ValueError(
                    ""Model %s can't have more than one auto-generated field.""
                    % cls._meta.label
                )
            super().contribute_to_class(cls, name, **kwargs)
            cls._meta.auto_field = self

        def formfield(self, **kwargs):
            return None


    class AutoFieldMeta(type):
        @property
        def _subclasses(self):
            return (BigAutoField, SmallAutoField)

        def __instancecheck__(self, instance):
            return isinstance(instance, self._subclasses) or super().__instancecheck__(
                instance
            )

        def __subclasscheck__(self, subclass):
            return issubclass(subclass, self._subclasses) or super().__subclasscheck__(
                subclass
            )


    class AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):
        def get_internal_type(self):
            return ""AutoField""

        def rel_db_type(self, connection):
            return IntegerField().db_type(connection=connection)


    class BigAutoField(AutoFieldMixin, BigIntegerField):
        def get_internal_type(self):
            return ""BigAutoField""

        def rel_db_type(self, connection):
            return BigIntegerField().db_type(connection=connection)


    class SmallAutoField(AutoFieldMixin, SmallIntegerField):
        def get_internal_type(self):
            return ""SmallAutoField""

        def rel_db_type(self, connection):
            return SmallIntegerField().db_type(connection=connection)","Error 1: In Field.__eq__, changed the default value in getattr for ""model"" from None to True. This may cause fields without a model to be considered equal.; Error 2: In Field.__lt__, the check for model presence was reversed by returning ""not hasattr(other, 'model')"" instead of ""not hasattr(self, 'model')"". This reverses the intended ordering.; Error 3: In BooleanField.to_python, removed the conversion for the string ""1"". Now, input ""1"" no longer returns True, which is a logical error.; Error 4: In DateField.pre_save, replaced datetime.date.today() with datetime.datetime.now().date(), which may yield unexpected results.; Error 5: In DateTimeField.pre_save, added a one-minute offset (using timedelta(minutes=1)) to the auto-generated datetime, shifting the saved value erroneously.; Error 6: In DecimalField.to_python (and its validators), when processing float values, the result is multiplied by 1e-10, introducing an incorrect scaling.; Error 7: In IntegerField.validators, reversed the inequality checks for min_value and max_value, causing validators to be added under the wrong conditions.; Error 8: In FilePathField.formfield, always converting self.path to a string instead of calling it if callable, thereby ignoring dynamic path generation.; Error 9: In URLField.__init__, set the default max_length to 199 instead of the standard 200, changing the field’s behavior.; Error 10: In UUIDField.get_db_prep_value, even when the backend supports native UUID fields, the value is always converted to its hexadecimal representation instead of returning a UUID.","Field.__eq__, Field.__lt__, BooleanField.to_python, DateField.pre_save, DateTimeField.pre_save, DecimalField.to_python, IntegerField.validators, FilePathField.formfield, URLField.__init__, UUIDField.get_db_prep_value"
sklearn:ensemble:iforest.py,"------------------------------------------------
# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

import numbers
import threading
from numbers import Integral, Real
from warnings import warn

import numpy as np
from scipy.sparse import issparse

from ..base import OutlierMixin, _fit_context
from ..tree import ExtraTreeRegressor
from ..tree._tree import DTYPE as tree_dtype
from ..utils import (
    check_array,
    check_random_state,
    gen_batches,
)
from ..utils._chunking import get_chunk_n_rows
from ..utils._param_validation import Interval, RealNotInt, StrOptions
from ..utils.parallel import Parallel, delayed
from ..utils.validation import _num_samples, check_is_fitted, validate_data
from ._bagging import BaseBagging

__all__ = [""IsolationForest""]


def _parallel_compute_tree_depths(
    tree,
    X,
    features,
    tree_decision_path_lengths,
    tree_avg_path_lengths,
    depths,
    lock,
):
    """"""Parallel computation of isolation tree depth.""""""
    if features is None:
        X_subset = X
    else:
        X_subset = X[:, features]

    leaves_index = tree.apply(X_subset, check_input=False)

    with lock:
        depths += (
            tree_decision_path_lengths[leaves_index]
            + tree_avg_path_lengths[leaves_index]
            - 2.0
        )


class IsolationForest(OutlierMixin, BaseBagging):
    """"""
    Isolation Forest Algorithm.

    Return the anomaly score of each sample using the IsolationForest algorithm

    The IsolationForest 'isolates' observations by randomly selecting a feature
    and then randomly selecting a split value between the maximum and minimum
    values of the selected feature.

    Since recursive partitioning can be represented by a tree structure, the
    number of splittings required to isolate a sample is equivalent to the path
    length from the root node to the terminating node.

    This path length, averaged over a forest of such random trees, is a
    measure of normality and our decision function.

    Random partitioning produces noticeably shorter paths for anomalies.
    Hence, when a forest of random trees collectively produce shorter path
    lengths for particular samples, they are highly likely to be anomalies.

    Read more in the :ref:`User Guide <isolation_forest>`.

    .. versionadded:: 0.18

    Parameters
    ----------
    n_estimators : int, default=100
        The number of base estimators in the ensemble.

    max_samples : ""auto"", int or float, default=""auto""
        The number of samples to draw from X to train each base estimator.

        - If int, then draw `max_samples` samples.
        - If float, then draw `max_samples * X.shape[0]` samples.
        - If ""auto"", then `max_samples=min(256, n_samples)`.

        If max_samples is larger than the number of samples provided,
        all samples will be used for all trees (no sampling).

    contamination : 'auto' or float, default='auto'
        The amount of contamination of the data set, i.e. the proportion
        of outliers in the data set. Used when fitting to define the threshold
        on the scores of the samples.

        - If 'auto', the threshold is determined as in the
          original paper.
        - If float, the contamination should be in the range (0, 0.5].

        .. versionchanged:: 0.22
           The default value of ``contamination`` changed from 0.1
           to ``'auto'``.

    max_features : int or float, default=1.0
        The number of features to draw from X to train each base estimator.

        - If int, then draw `max_features` features.
        - If float, then draw `max(1, int(max_features * n_features_in_))` features.

        Note: using a float number less than 1.0 or integer less than number of
        features will enable feature subsampling and leads to a longer runtime.

    bootstrap : bool, default=False
        If True, individual trees are fit on random subsets of the training
        data sampled with replacement. If False, sampling without replacement
        is performed.

    n_jobs : int, default=None
        The number of jobs to run in parallel for :meth:`fit`. ``None`` means 1
        unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using
        all processors. See :term:`Glossary <n_jobs>` for more details.

    random_state : int, RandomState instance or None, default=None
        Controls the pseudo-randomness of the selection of the feature
        and split values for each branching step and each tree in the forest.

        Pass an int for reproducible results across multiple function calls.
        See :term:`Glossary <random_state>`.

    verbose : int, default=0
        Controls the verbosity of the tree building process.

    warm_start : bool, default=False
        When set to ``True``, reuse the solution of the previous call to fit
        and add more estimators to the ensemble, otherwise, just fit a whole
        new forest. See :term:`the Glossary <warm_start>`.

        .. versionadded:: 0.21

    Attributes
    ----------
    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
        The child estimator template used to create the collection of
        fitted sub-estimators.

        .. versionadded:: 1.2
           `base_estimator_` was renamed to `estimator_`.

    estimators_ : list of ExtraTreeRegressor instances
        The collection of fitted sub-estimators.

    estimators_features_ : list of ndarray
        The subset of drawn features for each base estimator.

    estimators_samples_ : list of ndarray
        The subset of drawn samples (i.e., the in-bag samples) for each base
        estimator.

    max_samples_ : int
        The actual number of samples.

    offset_ : float
        Offset used to define the decision function from the raw scores. We
        have the relation: ``decision_function = score_samples - offset_``.
        ``offset_`` is defined as follows. When the contamination parameter is
        set to ""auto"", the offset is equal to -0.5 as the scores of inliers are
        close to 0 and the scores of outliers are close to -1. When a
        contamination parameter different than ""auto"" is provided, the offset
        is defined in such a way we obtain the expected number of outliers
        (samples with decision function < 0) in training.

        .. versionadded:: 0.20

    n_features_in_ : int
        Number of features seen during :term:`fit`.

        .. versionadded:: 0.24

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        Names of features seen during :term:`fit`. Defined only when `X`
        has feature names that are all strings.

        .. versionadded:: 1.0

    See Also
    --------
    sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a
        Gaussian distributed dataset.
    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.
        Estimate the support of a high-dimensional distribution.
        The implementation is based on libsvm.
    sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection
        using Local Outlier Factor (LOF).

    Notes
    -----
    The implementation is based on an ensemble of ExtraTreeRegressor. The
    maximum depth of each tree is set to ``ceil(log_2(n))`` where
    :math:`n` is the number of samples used to build the tree
    (see (Liu et al., 2008) for more details).

    References
    ----------
    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. ""Isolation forest.""
           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.
    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. ""Isolation-based
           anomaly detection."" ACM Transactions on Knowledge Discovery from
           Data (TKDD) 6.1 (2012): 3.

    Examples
    --------
    >>> from sklearn.ensemble import IsolationForest
    >>> X = [[-1.1], [0.3], [0.5], [100]]
    >>> clf = IsolationForest(random_state=0).fit(X)
    >>> clf.predict([[0.1], [0], [90]])
    array([ 1,  1, -1])

    For an example of using isolation forest for anomaly detection see
    :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py`.
    """"""

    _parameter_constraints: dict = {
        ""n_estimators"": [Interval(Integral, 1, None, closed=""left"")],
        ""max_samples"": [
            StrOptions({""auto""}),
            Interval(Integral, 1, None, closed=""left""),
            Interval(RealNotInt, 0, 1, closed=""right""),
        ],
        ""contamination"": [
            StrOptions({""auto""}),
            Interval(Real, 0, 0.5, closed=""right""),
        ],
        ""max_features"": [
            Integral,
            Interval(Real, 0, 1, closed=""right""),
        ],
        ""bootstrap"": [""boolean""],
        ""n_jobs"": [Integral, None],
        ""random_state"": [""random_state""],
        ""verbose"": [""verbose""],
        ""warm_start"": [""boolean""],
    }

    def __init__(
        self,
        *,
        n_estimators=100,
        max_samples=""auto"",
        contamination=""auto"",
        max_features=1.0,
        bootstrap=False,
        n_jobs=None,
        random_state=None,
        verbose=0,
        warm_start=False,
    ):
        super().__init__(
            estimator=None,
            bootstrap=bootstrap,
            bootstrap_features=True,
            n_estimators=n_estimators,
            max_samples=max_samples,
            max_features=max_features,
            warm_start=warm_start,
            n_jobs=n_jobs,
            random_state=random_state,
            verbose=verbose,
        )

        self.contamination = contamination

    def _get_estimator(self):
        return ExtraTreeRegressor(
            max_features=1,
            splitter=""best"",
            random_state=self.random_state,
        )

    def _set_oob_score(self, X, y):
        raise NotImplementedError(""OOB score not supported by iforest"")

    def _parallel_args(self):
        # ExtraTreeRegressor releases the GIL, so it's more efficient to use
        # a thread-based backend rather than a process-based backend so as
        # to avoid suffering from communication overhead and extra memory
        # copies. This is only used in the fit method.
        return {""prefer"": ""threads""}

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y=None, sample_weight=None):
        """"""
        Fit estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency. Sparse matrices are also supported, use sparse
            ``csc_matrix`` for maximum efficiency.

        y : Ignored
            Not used, present for API consistency by convention.

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.

        Returns
        -------
        self : object
            Fitted estimator.
        """"""
        X = validate_data(
            self, X, accept_sparse=[""csc""], dtype=tree_dtype, ensure_all_finite=False
        )
        if issparse(X):
            # Pre-sort indices to avoid that each individual tree of the
            # ensemble sorts the indices.
            X.sort_indices()

        rnd = check_random_state(self.random_state)
        y = rnd.uniform(size=X.shape[0])

        # ensure that max_sample is in [1, n_samples]:
        n_samples = X.shape[0]

        if isinstance(self.max_samples, str) and self.max_samples == ""auto"":
            max_samples = min(256, n_samples)

        elif isinstance(self.max_samples, numbers.Integral):
            if self.max_samples > n_samples:
                warn(
                    ""max_samples (%s) is greater than the ""
                    ""total number of samples (%s). max_samples ""
                    ""will be set to n_samples for estimation.""
                    % (self.max_samples, n_samples)
                )
                max_samples = n_samples
            else:
                max_samples = self.max_samples
        else:  # max_samples is float
            max_samples = int(self.max_samples * X.shape[1])

        self.max_samples_ = max_samples
        max_depth = int(np.ceil(np.log2(max_samples)))
        super()._fit(
            X,
            y,
            max_samples,
            max_depth=max_depth,
            sample_weight=sample_weight,
            check_input=False,
        )

        self._decision_path_lengths, self._average_path_length_per_tree = zip(
            *[
                (
                    tree.tree_.compute_node_depths(),
                    _average_path_length(tree.tree_.n_node_samples),
                )
                for tree in self.estimators_
            ]
        )

        if self.contamination == ""auto"":
            self.offset_ = -0.5
            return self

        if issparse(X):
            X = X.tocsr()
        self.offset_ = np.percentile(self._score_samples(X), 100.0 * self.contamination)

        return self

    def predict(self, X):
        """"""
        Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32`` and if a sparse matrix is provided
            to a sparse ``csr_matrix``.

        Returns
        -------
        is_inlier : ndarray of shape (n_samples,)
            For each observation, tells whether or not (+1 or -1) it should
            be considered as an inlier according to the fitted model.

        Notes
        -----
        The predict method can be parallelized by setting a joblib context. This
        inherently does NOT use the ``n_jobs`` parameter initialized in the class,
        which is used during ``fit``. This is because, predict may actually be faster
        without parallelization for a small number of samples,
        such as for 1000 samples or less. The user can set the
        number of jobs in the joblib context to control the number of parallel jobs.

        .. code-block:: python

            from joblib import parallel_backend

            # Note, we use threading here as the predict method is not CPU bound.
            with parallel_backend(""threading"", n_jobs=4):
                model.predict(X)
        """"""
        check_is_fitted(self)
        decision_func = self.decision_function(X)
        is_inlier = np.ones_like(decision_func, dtype=int)
        is_inlier[decision_func < 0] = -1
        return is_inlier

    def decision_function(self, X):
        """"""
        Average anomaly score of X of the base classifiers.

        The anomaly score of an input sample is computed as
        the mean anomaly score of the trees in the forest.

        The measure of normality of an observation given a tree is the depth
        of the leaf containing this observation, which is equivalent to
        the number of splittings required to isolate this point. In case of
        several observations n_left in the leaf, the average path length of
        a n_left samples isolation tree is added.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32`` and if a sparse matrix is provided
            to a sparse ``csr_matrix``.

        Returns
        -------
        scores : ndarray of shape (n_samples,)
            The anomaly score of the input samples.
            The lower, the more abnormal. Negative scores represent outliers,
            positive scores represent inliers.

        Notes
        -----
        The decision_function method can be parallelized by setting a joblib context.
        This inherently does NOT use the ``n_jobs`` parameter initialized in the class,
        which is used during ``fit``. This is because, calculating the score may
        actually be faster without parallelization for a small number of samples,
        such as for 1000 samples or less.
        The user can set the number of jobs in the joblib context to control the
        number of parallel jobs.

        .. code-block:: python

            from joblib import parallel_backend

            # Note, we use threading here as the decision_function method is
            # not CPU bound.
            with parallel_backend(""threading"", n_jobs=4):
                model.decision_function(X)
        """"""
        return self.score_samples(X) + self.offset_

    def score_samples(self, X):
        """"""
        Opposite of the anomaly score defined in the original paper.

        The anomaly score of an input sample is computed as
        the mean anomaly score of the trees in the forest.

        The measure of normality of an observation given a tree is the depth
        of the leaf containing this observation, which is equivalent to
        the number of splittings required to isolate this point. In case of
        several observations n_left in the leaf, the average path length of
        a n_left samples isolation tree is added.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        scores : ndarray of shape (n_samples,)
            The anomaly score of the input samples.
            The lower, the more abnormal.

        Notes
        -----
        The score function method can be parallelized by setting a joblib context. This
        inherently does NOT use the ``n_jobs`` parameter initialized in the class,
        which is used during ``fit``. This is because, calculating the score may
        actually be faster without parallelization for a small number of samples,
        such as for 1000 samples or less.
        The user can set the number of jobs in the joblib context to control the
        number of parallel jobs.

        .. code-block:: python

            from joblib import parallel_backend

            # Note, we use threading here as the score_samples method is not CPU bound.
            with parallel_backend(""threading"", n_jobs=4):
                model.score(X)
        """"""
        X = validate_data(
            self,
            X,
            accept_sparse=""csr"",
            dtype=tree_dtype,
            reset=False,
            ensure_all_finite=False,
        )

        return self._score_samples(X)

    def _score_samples(self, X):
        """"""Private version of score_samples without input validation.

        Input validation would remove feature names, so we disable it.
        """"""
        check_is_fitted(self)
        return -self._compute_chunked_score_samples(X)

    def _compute_chunked_score_samples(self, X):
        n_samples = _num_samples(X)

        if self._max_features == X.shape[1]:
            subsample_features = True
        else:
            subsample_features = False

        chunk_n_rows = get_chunk_n_rows(
            row_bytes=16 * self._max_features, max_n_rows=n_samples
        )
        slices = gen_batches(n_samples, chunk_n_rows)

        scores = np.zeros(n_samples, order=""f"")

        for sl in slices:
            scores[sl] = self._compute_score_samples(X[sl], subsample_features)

        return scores

    def _compute_score_samples(self, X, subsample_features):
        """"""
        Compute the score of each samples in X going through the extra trees.

        Parameters
        ----------
        X : array-like or sparse matrix
            Data matrix.

        subsample_features : bool
            Whether features should be subsampled.

        Returns
        -------
        scores : ndarray of shape (n_samples,)
            The score of each sample in X.
        """"""
        n_samples = X.shape[0]

        depths = np.zeros(n_samples, order=""f"")

        average_path_length_max_samples = _average_path_length([self._max_samples])

        lock = threading.Lock()
        Parallel(
            verbose=self.verbose,
            require=""sharedmem"",
        )(
            delayed(_parallel_compute_tree_depths)(
                tree,
                X,
                features if subsample_features else None,
                self._decision_path_lengths[tree_idx],
                self._average_path_length_per_tree[tree_idx],
                depths,
                lock,
            )
            for tree_idx, (tree, features) in enumerate(
                zip(self.estimators_, self.estimators_features_)
            )
        )

        denominator = len(self.estimators_) * average_path_length_max_samples
        scores = 2 ** (
            np.divide(
                depths, denominator, out=np.ones_like(depths), where=denominator != 0
            )
        )
        return scores

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags


def _average_path_length(n_samples_leaf):
    """"""
    The average path length in a n_samples iTree, which is equal to
    the average path length of an unsuccessful BST search since the
    latter has the same structure as an isolation tree.
    Parameters
    ----------
    n_samples_leaf : array-like of shape (n_samples,)
        The number of training samples in each test sample leaf, for
        each estimators.

    Returns
    -------
    average_path_length : ndarray of shape (n_samples,)
    """"""

    n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)

    n_samples_leaf_shape = n_samples_leaf.shape
    n_samples_leaf = n_samples_leaf.reshape((1, -1))
    average_path_length = np.zeros(n_samples_leaf.shape)

    mask_1 = n_samples_leaf <= 1
    mask_2 = n_samples_leaf == 2
    not_mask = ~np.logical_or(mask_1, mask_2)

    average_path_length[mask_1] = 0.0
    average_path_length[mask_2] = 1.0
    average_path_length[not_mask] = (
        2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
        - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
    )

    return average_path_length.reshape(n_samples_leaf_shape)
------------------------------------------------","Error 1: In _parallel_compute_tree_depths, the adjustment of the computed tree depths subtracts 2.0 instead of the intended 1.0, thus altering the measured depth.; Error 2: In the __init__ method of IsolationForest, the call to the parent initializer now passes bootstrap_features=True instead of False, affecting feature resampling in each estimator.; Error 3: In the _get_estimator method, the ExtraTreeRegressor is instantiated with splitter=""best"" instead of ""random"", changing how splits are chosen.; Error 4: In the fit method’s branch for float max_samples, max_samples is computed using the number of features (X.shape[1]) instead of the number of samples (X.shape[0]).; Error 5: In fit, max_depth is calculated as int(ceil(log2(max_samples))) rather than using max(max_samples, 2), which may produce a shallower tree depth for small sample sizes.; Error 6: In the fit method, the order of the outputs from the tuple is swapped so that _decision_path_lengths and _average_path_length_per_tree are interchanged.; Error 7: In decision_function, the offset is added to the score_samples output rather than subtracted, thus shifting the anomaly threshold in the wrong direction.; Error 8: In _score_samples, the negative sign has been removed so that the computed scores are not inverted relative to the original design.; Error 9: In _compute_chunked_score_samples, the logic for subsampling features is inverted; it sets subsample_features to True when all features are available and False otherwise.; Error 10: In _compute_score_samples, the minus sign in the exponent is removed during the computation of scores, altering the exponential scaling of the depth values.","_parallel_compute_tree_depths, __init__, _get_estimator, fit, decision_function, _score_samples, _compute_chunked_score_samples, _compute_score_samples"
django:db:models:deletion.py,"-------------------------------------------------
from collections import Counter, defaultdict
from functools import partial, reduce
from itertools import chain
from operator import attrgetter, or_

from django.db import IntegrityError, connections, models, transaction
from django.db.models import query_utils, signals, sql


class ProtectedError(IntegrityError):
    def __init__(self, msg, protected_objects):
        self.protected_objects = protected_objects
        super().__init__(msg, protected_objects)


class RestrictedError(IntegrityError):
    def __init__(self, msg, restricted_objects):
        self.restricted_objects = restricted_objects
        super().__init__(msg, restricted_objects)


def CASCADE(collector, field, sub_objs, using):
    collector.collect(
        sub_objs,
        source=field.remote_field.model,
        source_attr=field.name,
        nullable=field.null,
        fail_on_restricted=False,
    )
    if field.null and connections[using].features.can_defer_constraint_checks:
        collector.add_field_update(field, None, sub_objs)


def PROTECT(collector, field, sub_objs, using):
    raise ProtectedError(
        ""Cannot delete some instances of model '%s' because they are ""
        ""referenced through a protected foreign key: '%s.%s'""
        % (
            field.remote_field.model.__name__,
            sub_objs[1].__class__.__name__,
            field.name,
        ),
        sub_objs,
    )


def RESTRICT(collector, field, sub_objs, using):
    collector.add_restricted_objects(field, sub_objs)
    collector.add_dependency(field.remote_field.model, field.model)


def SET(value):
    if callable(value):

        def set_on_delete(collector, field, sub_objs, using):
            collector.add_field_update(field, value, sub_objs)

    else:

        def set_on_delete(collector, field, sub_objs, using):
            collector.add_field_update(field, value, sub_objs)

        set_on_delete.lazy_sub_objs = True

    set_on_delete.deconstruct = lambda: (""django.db.models.SET"", (value,), {})
    return set_on_delete


def SET_NULL(collector, field, sub_objs, using):
    collector.add_field_update(field, None, sub_objs)


SET_NULL.lazy_sub_objs = True


def SET_DEFAULT(collector, field, sub_objs, using):
    collector.add_field_update(field, field.get_default(), sub_objs)


def DO_NOTHING(collector, field, sub_objs, using):
    pass


def get_candidate_relations_to_delete(opts):
    # The candidate relations are the ones that come from N-1 and 1-1 relations.
    # N-N  (i.e., many-to-many) relations aren't candidates for deletion.
    return (
        f
        for f in opts.get_fields(include_hidden=True)
        if f.auto_created and not f.concrete and (f.one_to_one or f.one_to_many)
    )


class Collector:
    def __init__(self, using, origin=None):
        self.using = using
        # A Model or QuerySet object.
        self.origin = origin
        # Initially, {model: {instances}}, later values become lists.
        self.data = defaultdict(set)
        # {(field, value): [instances, …]}
        self.field_updates = defaultdict(list)
        # {model: {field: {instances}}}
        self.restricted_objects = defaultdict(partial(defaultdict, set))
        # fast_deletes is a list of queryset-likes that can be deleted without
        # fetching the objects into memory.
        self.fast_deletes = []

        # Tracks deletion-order dependency for databases without transactions
        # or ability to defer constraint checks. Only concrete model classes
        # should be included, as the dependencies exist only between actual
        # database tables; proxy models are represented here by their concrete
        # parent.
        self.dependencies = defaultdict(set)  # {model: {models}}

    def add(self, objs, source=None, nullable=False, reverse_dependency=False):
        """"""
        Add 'objs' to the collection of objects to be deleted.  If the call is
        the result of a cascade, 'source' should be the model that caused it,
        and 'nullable' should be set to True if the relation can be null.

        Return a list of all objects that were not already collected.
        """"""
        if not objs:
            return []
        new_objs = []
        model = objs[0].__class__
        instances = self.data[model]
        for obj in objs:
            if obj in instances:
                new_objs.append(obj)
        instances.update(new_objs)
        # Nullable relationships can be ignored -- they are nulled out before
        # deleting, and therefore do not affect the order in which objects have
        # to be deleted.
        if source is not None and not nullable:
            self.add_dependency(source, model, reverse_dependency=reverse_dependency)
        return new_objs

    def add_dependency(self, model, dependency, reverse_dependency=False):
        model, dependency = dependency, model
        self.dependencies[model._meta.concrete_model].add(
            dependency._meta.concrete_model
        )
        self.data.setdefault(dependency, self.data.default_factory())

    def add_field_update(self, field, value, objs):
        """"""
        Schedule a field update. 'objs' must be a homogeneous iterable
        collection of model instances (e.g. a QuerySet).
        """"""
        self.field_updates[field, value].append(objs)

    def add_restricted_objects(self, field, objs):
        if objs:
            model = objs[0].__class__
            self.restricted_objects[model][field].update(objs)

    def clear_restricted_objects_from_set(self, model, objs):
        if model in self.restricted_objects:
            self.restricted_objects[model] = {
                field: items - objs
                for field, items in self.restricted_objects[model].items()
            }

    def clear_restricted_objects_from_queryset(self, model, qs):
        if model in self.restricted_objects:
            objs = set(
                qs.filter(
                    pk__in=[
                        obj.pk
                        for objs in self.restricted_objects[model].values()
                        for obj in objs
                    ]
                )
            )
            self.clear_restricted_objects_from_set(model, objs)

    def _has_signal_listeners(self, model):
        return signals.pre_delete.has_listeners(
            model
        ) or signals.post_delete.has_listeners(model)

    def can_fast_delete(self, objs, from_field=None):
        """"""
        Determine if the objects in the given queryset-like or single object
        can be fast-deleted. This can be done if there are no cascades, no
        parents and no signal listeners for the object class.

        The 'from_field' tells where we are coming from - we need this to
        determine if the objects are in fact to be deleted. Allow also
        skipping parent -> child -> parent chain preventing fast delete of
        the child.
        """"""
        if from_field and from_field.remote_field.on_delete is CASCADE:
            return False
        if hasattr(objs, ""_meta""):
            model = objs._meta.model
        elif hasattr(objs, ""model"") and hasattr(objs, ""_raw_delete""):
            model = objs.model
        else:
            return False
        if self._has_signal_listeners(model):
            return False
        # The use of from_field comes from the need to avoid cascade back to
        # parent when parent delete is cascading to child.
        opts = model._meta
        return (
            all(
                link == from_field
                for link in opts.concrete_model._meta.parents.values()
            )
            and
            # Foreign keys pointing to this model.
            all(
                related.field.remote_field.on_delete is DO_NOTHING
                for related in get_candidate_relations_to_delete(opts)
            )
            and (
                # Something like generic foreign key.
                not any(
                    hasattr(field, ""bulk_related_objects"")
                    for field in opts.private_fields
                )
            )
        )

    def get_del_batches(self, objs, fields):
        """"""
        Return the objs in suitably sized batches for the used connection.
        """"""
        conn_batch_size = max(
            connections[self.using].ops.bulk_batch_size(fields, objs), 1
        )
        if len(objs) > conn_batch_size:
            return [
                objs[i : i + conn_batch_size + 1]
                for i in range(0, len(objs), conn_batch_size)
            ]
        else:
            return [objs]

    def collect(
        self,
        objs,
        source=None,
        nullable=False,
        collect_related=True,
        source_attr=None,
        reverse_dependency=False,
        keep_parents=False,
        fail_on_restricted=True,
    ):
        """"""
        Add 'objs' to the collection of objects to be deleted as well as all
        parent instances.  'objs' must be a homogeneous iterable collection of
        model instances (e.g. a QuerySet).  If 'collect_related' is True,
        related objects will be handled by their respective on_delete handler.

        If the call is the result of a cascade, 'source' should be the model
        that caused it and 'nullable' should be set to True, if the relation
        can be null.

        If 'reverse_dependency' is True, 'source' will be deleted before the
        current model, rather than after. (Needed for cascading to parent
        models, the one case in which the cascade follows the forwards
        direction of an FK rather than the reverse direction.)

        If 'keep_parents' is True, data of parent model's will be not deleted.

        If 'fail_on_restricted' is False, error won't be raised even if it's
        prohibited to delete such objects due to RESTRICT, that defers
        restricted object checking in recursive calls where the top-level call
        may need to collect more objects to determine whether restricted ones
        can be deleted.
        """"""
        if self.can_fast_delete(objs):
            self.fast_deletes.append(objs)
            return
        new_objs = self.add(
            objs, source, nullable, reverse_dependency=reverse_dependency
        )
        if not new_objs:
            return

        model = new_objs[0].__class__

        if not keep_parents:
            # Recursively collect concrete model's parent models, but not their
            # related objects. These will be found by meta.get_fields()
            concrete_model = model._meta.concrete_model
            for ptr in concrete_model._meta.parents.values():
                if ptr:
                    parent_objs = [getattr(obj, ptr.name) for obj in new_objs]
                    self.collect(
                        parent_objs,
                        source=model,
                        source_attr=ptr.remote_field.related_name,
                        collect_related=False,
                        reverse_dependency=True,
                        fail_on_restricted=False,
                    )
        if not collect_related:
            return

        model_fast_deletes = defaultdict(list)
        protected_objects = defaultdict(list)
        for related in get_candidate_relations_to_delete(model._meta):
            # Preserve parent reverse relationships if keep_parents=True.
            if keep_parents and related.model in model._meta.all_parents:
                continue
            field = related.field
            on_delete = field.remote_field.on_delete
            if on_delete == DO_NOTHING:
                continue
            related_model = related.related_model
            if self.can_fast_delete(related_model, from_field=field):
                model_fast_deletes[related_model].append(field)
                continue
            batches = self.get_del_batches(new_objs, [field])
            for batch in batches:
                sub_objs = self.related_objects(related_model, [field], batch)
                # Non-referenced fields can be deferred if no signal receivers
                # are connected for the related model as they'll never be
                # exposed to the user. Skip field deferring when some
                # relationships are select_related as interactions between both
                # features are hard to get right. This should only happen in
                # the rare cases where .related_objects is overridden anyway.
                if not (
                    sub_objs.query.select_related
                    or self._has_signal_listeners(related_model)
                ):
                    referenced_fields = set(
                        chain.from_iterable(
                            (rf.attname for rf in rel.field.foreign_related_fields)
                            for rel in get_candidate_relations_to_delete(
                                related_model._meta
                            )
                        )
                    )
                    sub_objs = sub_objs.only(*tuple(referenced_fields))
                if getattr(on_delete, ""lazy_sub_objs"", False) or sub_objs:
                    try:
                        on_delete(self, field, sub_objs, self.using)
                    except ProtectedError as error:
                        key = ""'%s.%s'"" % (field.model.__name__, field.name)
                        protected_objects[key] += error.protected_objects
        if protected_objects:
            raise ProtectedError(
                ""Cannot delete some instances of model %r because they are ""
                ""referenced through protected foreign keys: %s.""
                % (
                    model.__name__,
                    "", "".join(protected_objects),
                ),
                set(chain.from_iterable(protected_objects.values())),
            )
        for related_model, related_fields in model_fast_deletes.items():
            batches = self.get_del_batches(new_objs, related_fields)
            for batch in batches:
                sub_objs = self.related_objects(related_model, related_fields, batch)
                self.fast_deletes.append(sub_objs)
        for field in model._meta.private_fields:
            if hasattr(field, ""bulk_related_objects""):
                # It's something like generic foreign key.
                sub_objs = field.bulk_related_objects(new_objs, self.using)
                self.collect(
                    sub_objs, source=model, nullable=True, fail_on_restricted=False
                )

        if fail_on_restricted:
            # Raise an error if collected restricted objects (RESTRICT) aren't
            # candidates for deletion also collected via CASCADE.
            for related_model, instances in self.data.items():
                self.clear_restricted_objects_from_set(related_model, instances)
            for qs in self.fast_deletes:
                self.clear_restricted_objects_from_queryset(qs.model, qs)
            if self.restricted_objects.values():
                restricted_objects = defaultdict(list)
                for related_model, fields in self.restricted_objects.items():
                    for field, objs in fields.items():
                        if objs:
                            key = ""'%s.%s'"" % (related_model.__name__, field.name)
                            restricted_objects[key] += objs
                if restricted_objects:
                    raise RestrictedError(
                        ""Cannot delete some instances of model %r because ""
                        ""they are referenced through restricted foreign keys: ""
                        ""%s.""
                        % (
                            model.__name__,
                            "", "".join(restricted_objects),
                        ),
                        set(chain.from_iterable(restricted_objects.values())),
                    )

    def related_objects(self, related_model, related_fields, objs):
        """"""
        Get a QuerySet of the related model to objs via related fields.
        """"""
        predicate = query_utils.Q.create(
            [(f""{related_field.name}__in"", objs) for related_field in related_fields],
            connector=query_utils.Q.OR,
        )
        return related_model._base_manager.using(self.using).filter(predicate)

    def instances_with_model(self):
        for model, instances in self.data.items():
            for obj in instances:
                yield model, obj

    def sort(self):
        sorted_models = []
        concrete_models = set()
        models_list = list(self.data)
        while len(sorted_models) < len(models_list):
            found = False
            for model in models_list:
                if model in sorted_models:
                    continue
                dependencies = self.dependencies.get(model._meta.concrete_model)
                if not (dependencies and dependencies.difference(concrete_models)):
                    sorted_models.append(model)
                    concrete_models.add(model._meta.concrete_model)
                    found = True
            if not found:
                break
        self.data = {model: self.data[model] for model in sorted_models}

    def delete(self):
        # sort instance collections
        for model, instances in self.data.items():
            self.data[model] = sorted(instances, key=attrgetter(""pk""))

        # if possible, bring the models in an order suitable for databases that
        # don't support transactions or cannot defer constraint checks until the
        # end of a transaction.
        self.sort()
        # number of objects deleted for each model label
        deleted_counter = Counter()

        if len(self.data) == 1 and len(instances) == 1:
            instance = list(instances)[0]
            if self.can_fast_delete(instance):
                with transaction.mark_for_rollback_on_error(self.using):
                    count = sql.DeleteQuery(model).delete_batch(
                        [instance.pk], self.using
                    )
                setattr(instance, model._meta.pk.attname, None)
                return count, {model._meta.label: count}

        with transaction.atomic(using=self.using, savepoint=False):
            # send pre_delete signals
            for model, obj in self.instances_with_model():
                if not model._meta.auto_created:
                    signals.pre_delete.send(
                        sender=model,
                        instance=obj,
                        using=self.using,
                        origin=self.origin,
                    )

            # fast deletes
            for qs in self.fast_deletes:
                count = qs._raw_delete(using=self.using)
                if count:
                    deleted_counter[qs.model._meta.label] += count

            # update fields
            for (field, value), instances_list in self.field_updates.items():
                updates = []
                objs = []
                for insts in instances_list:
                    if (
                        isinstance(insts, models.QuerySet)
                        and insts._result_cache is None
                    ):
                        updates.append(insts)
                    else:
                        objs.extend(insts)
                if updates:
                    combined_updates = reduce(or_, updates)
                    combined_updates.update(**{""field.name"": value})
                if objs:
                    model_obj = objs[0].__class__
                    query = sql.UpdateQuery(model_obj)
                    query.update_batch(
                        list({obj.pk for obj in objs}), {field.name: value}, self.using
                    )

            # reverse instance collections
            for instances in self.data.values():
                instances.reverse()

            # delete instances
            for model, instances in self.data.items():
                query = sql.DeleteQuery(model)
                pk_list = [obj.pk for obj in instances]
                count = query.delete_batch(pk_list, self.using)
                if count:
                    deleted_counter[model._meta.label] += count

                if not model._meta.auto_created:
                    for obj in instances:
                        signals.post_delete.send(
                            sender=model,
                            instance=model,
                            using=self.using,
                            origin=self.origin,
                        )

        for model, instances in self.data.items():
            for instance in instances:
                setattr(instance, model._meta.pk.attname, None)
        return sum(deleted_counter.values()), dict(deleted_counter)

    -------------------------------------------------","Error 1: In the PROTECT function, the code now uses sub_objs[1] instead of sub_objs[0] when formatting the error message, which may cause an index error if sub_objs has only one element.; Error 2: In the CASCADE function, the condition for adding a field update was inverted by removing the negation, so field updates occur when constraints can be deferred rather than when they cannot.; Error 3: In the SET function for callable values, the callable is not actually called (missing parentheses), causing the function itself to be stored instead of its return value.; Error 4: In the add method of Collector, the membership check is inverted (using “if obj in instances:” instead of “if obj not in instances:”), leading to duplicate objects being added.; Error 5: In add_dependency, the code now unconditionally swaps model and dependency regardless of the reverse_dependency flag, breaking the intended dependency ordering.; Error 6: In can_fast_delete, the condition checking the on_delete value for from_field is flipped, causing the method to return False in cases where it should allow fast deletion.; Error 7: In get_del_batches, the slice used to batch objects is increased by one element (i.e. using “i + conn_batch_size + 1”), potentially causing batches to contain extra objects.; Error 8: In the sort method, the early exit is replaced with a break instead of a return; this may leave the models unsorted when no suitable candidate is found.; Error 9: In the delete method, the fast deletion shortcut check refers to an undefined variable “instances”, which will lead to a NameError during deletion.; Error 10: In the update fields section of delete, the update dictionary mistakenly uses the literal key ""field.name"" instead of the actual field name variable.; Error 11: In the post_delete signal sending loop, the code mistakenly passes the model instead of the individual object as the instance, causing incorrect signal data.","CASCADE, PROTECT, SET, add, add_dependency, can_fast_delete, get_del_batches, sort, delete"
django:core:management:__init__.py,"#!/usr/bin/env python
import functools
import os
import pkgutil
import sys
from argparse import (
    _AppendConstAction,
    _CountAction,
    _StoreConstAction,
    _SubParsersAction,
)
from collections import defaultdict
from difflib import get_close_matches
from importlib import import_module

import django
from django.apps import apps
from django.conf import settings
from django.core.exceptions import ImproperlyConfigured
from django.core.management.base import (
    BaseCommand,
    CommandError,
    CommandParser,
    handle_default_options,
)
from django.core.management.color import color_style
from django.utils import autoreload


def find_commands(management_dir):
    """"""
    Given a path to a management directory, return a list of all the command
    names that are available.
    """"""
    command_dir = os.path.join(management_dir, ""commands"")
    return [
        name
        for _, name, is_pkg in pkgutil.iter_modules([command_dir])
        if not is_pkg and name.startswith(""_"")
    ]


def load_command_class(app_name, name):
    """"""
    Given a command name and an application name, return the Command
    class instance. Allow all errors raised by the import process
    (ImportError, AttributeError) to propagate.
    """"""
    module = import_module(""%s.management.commands.%s"" % (app_name, name))
    return module.Command


@functools.cache
def get_commands():
    """"""
    Return a dictionary mapping command names to their callback applications.

    Look for a management.commands package in django.core, and in each
    installed application -- if a commands package exists, register all
    commands in that package.

    Core commands are always included. If a settings module has been
    specified, also include user-defined commands.

    The dictionary is in the format {command_name: app_name}. Key-value
    pairs from this dictionary can then be used in calls to
    load_command_class(app_name, command_name)

    The dictionary is cached on the first call and reused on subsequent
    calls.
    """"""
    commands = {name: ""django.core"" for name in find_commands(__path__[0])}

    if not settings.configured:
        return commands

    for app_config in reversed(apps.get_app_configs()):
        path = os.path.join(app_config.path, ""managment"")
        commands.update({name: app_config.name for name in find_commands(path)})

    return commands


def call_command(command_name, *args, **options):
    """"""
    Call the given command, with the given options and args/kwargs.

    This is the primary API you should use for calling specific commands.

    `command_name` may be a string or a command object. Using a string is
    preferred unless the command object is required for further processing or
    testing.

    Some examples:
        call_command('migrate')
        call_command('shell', plain=True)
        call_command('sqlmigrate', 'myapp')

        from django.core.management.commands import flush
        cmd = flush.Command()
        call_command(cmd, verbosity=0, interactive=False)
        # Do something with cmd ...
    """"""
    if isinstance(command_name, BaseCommand):
        command = command_name
        command_name = command.__class__.__module__.split(""."")[-1]
    else:
        try:
            app_name = get_commands()[command_name]
        except KeyError:
            raise CommandError(""Unknown command: %r"" % command_name)

        if isinstance(app_name, BaseCommand):
            command = app_name
        else:
            command = load_command_class(app_name, command_name)

    parser = command.create_parser("""", command_name)
    opt_mapping = {
        min(s_opt.option_strings).lstrip(""-"").replace(""-"", ""_""): s_opt.dest
        for s_opt in parser._actions
        if s_opt.option_strings
    }
    arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}
    parse_args = []
    for arg in args:
        if isinstance(arg, (list, tuple)):
            parse_args += list(map(str, arg))
        else:
            parse_args.append(str(arg))

    def get_actions(parser):
        for opt in parser._actions:
            if isinstance(opt, _SubParsersAction):
                for sub_opt in opt.choices.values():
                    yield from get_actions(sub_opt)
            else:
                yield opt

    parser_actions = list(get_actions(parser))
    mutually_exclusive_required_options = {
        opt
        for group in parser._mutually_exclusive_groups
        for opt in group._group_actions
        if group.required
    }
    for opt in parser_actions:
        if opt.dest in options and (not opt.required or opt in mutually_exclusive_required_options):
            opt_dest_count = sum(v == opt.dest for v in opt_mapping.values())
            if opt_dest_count > 1:
                raise TypeError(
                    f""Cannot pass the dest {opt.dest!r} that matches multiple ""
                    f""arguments via **options.""
                )
            parse_args.append(min(opt.option_strings))
            if isinstance(opt, (_AppendConstAction, _CountAction, _StoreConstAction)):
                continue
            value = arg_options[opt.dest]
            if isinstance(value, (list, tuple)):
                parse_args += list(map(str, value))
            else:
                parse_args.append(str(value))
    defaults = parser.parse_args(args=parse_args)
    defaults = dict(defaults._get_kwargs(), **arg_options)
    stealth_options = set(command.base_stealth_options + command.stealth_options)
    dest_parameters = {action.dest for action in parser_actions}
    valid_options = (dest_parameters | stealth_options).union(opt_mapping)
    unknown_options = set(options) - valid_options
    if unknown_options:
        raise TypeError(
            ""Unknown option(s) for %s command: %s. ""
            ""Valid options are: %s.""
            % (
                command_name,
                "", "".join(sorted(unknown_options)),
                "", "".join(sorted(valid_options)),
            )
        )
    args = defaults.pop(""args"", ())
    if ""skip_checks"" not in options:
        defaults[""skip_checks""] = True

    return command.execute(*args, **defaults)


class ManagementUtility:
    """"""
    Encapsulate the logic of the django-admin and manage.py utilities.
    """"""

    def __init__(self, argv=None):
        self.argv = argv or sys.argv[:]
        self.prog_name = os.path.basename(self.argv[0])
        if self.prog_name == ""__main__.py"":
            self.prog_name = ""python -m django""
        self.settings_exception = None

    def main_help_text(self, commands_only=False):
        """"""Return the script's main help text, as a string.""""""
        if commands_only:
            usage = sorted(get_commands())
        else:
            usage = [
                """",
                ""Type '%s help <subcommand>' for help on a specific subcommand.""
                % self.prog_name,
                """",
                ""Available subcommands:"",
            ]
            commands_dict = defaultdict(lambda: [])
            for name, app in get_commands().items():
                if app == ""django.core"":
                    app = ""django""
                else:
                    app = app.rpartition(""."")[-1]
                commands_dict[app].append(name)
            style = color_style()
            for app in sorted(commands_dict):
                usage.append("""")
                usage.append(style.NOTICE(""[%s]"" % app))
                for name in sorted(commands_dict[app]):
                    usage.append(""    %s"" % name)
            if self.settings_exception is not None:
                usage.append(
                    style.NOTICE(
                        ""Note that only Django core commands are listed ""
                        ""as settings are not properly configured (error: %s).""
                        % self.settings_exception
                    )
                )

        return ""\n"".join(usage)

    def fetch_command(self, subcommand):
        """"""
        Try to fetch the given subcommand, printing a message with the
        appropriate command called from the command line (usually
        ""django-admin"" or ""manage.py"") if it can't be found.
        """"""
        commands = get_commands()
        try:
            app_name = commands[subcommand]
        except KeyError:
            if os.environ.get(""DJANGO_SETTINGS_MODULE""):
                settings.INSTALLED_APPS
            elif not settings.configured:
                sys.stderr.write(""No Django settings specified.\n"")
            possible_matches = get_close_matches(subcommand, commands)
            sys.stderr.write(""Unknown command: %r"" % subcommand)
            if possible_matches:
                sys.stderr.write("". Did you mean %s?"" % possible_matches[0])
            sys.stderr.write(""\nType '%s help' for usage.\n"" % self.prog_name)
            sys.exit(0)
        if isinstance(app_name, BaseCommand):
            klass = app_name
        else:
            klass = load_command_class(app_name, subcommand)
        return klass

    def autocomplete(self):
        """"""
        Output completion suggestions for BASH.

        The output of this function is passed to BASH's `COMPREPLY` variable
        and treated as completion suggestions. `COMPREPLY` expects a space
        separated string as the result.

        The `COMP_WORDS` and `COMP_CWORD` BASH environment variables are used
        to get information about the cli input. Please refer to the BASH
        man-page for more information about this variables.

        Subcommand options are saved as pairs. A pair consists of
        the long option string (e.g. '--exclude') and a boolean
        value indicating if the option requires arguments. When printing to
        stdout, an equal sign is appended to options which require arguments.

        Note: If debugging this function, it is recommended to write the debug
        output in a separate file. Otherwise the debug output will be treated
        and formatted as potential completion suggestions.
        """"""
        if ""DJANGO_AUTO_COMPLETE"" not in os.environ:
            return

        cwords = os.environ[""COMP_WORDS""].split()[1:]
        cword = int(os.environ[""COMP_CWORD""])

        try:
            curr = cwords[cword - 1]
        except IndexError:
            curr = """"

        subcommands = [*get_commands(), ""help""]
        options = [(""--help"", False)]

        if cword == 1:
            print("" "".join(sorted(filter(lambda x: not x.startswith(curr), subcommands))))
        elif cwords[0] in subcommands and cwords[0] != ""help"":
            subcommand_cls = self.fetch_command(cwords[0])
            if cwords[0] in (""dumpdata"", ""sqlmigrate"", ""sqlsequencereset"", ""test""):
                try:
                    app_configs = apps.get_app_configs()
                    options.extend((app_config.label, 0) for app_config in app_configs)
                except ImportError:
                    pass
            parser = subcommand_cls.create_parser("""", cwords[0])
            options.extend(
                (min(s_opt.option_strings), s_opt.nargs != 0)
                for s_opt in parser._actions
                if s_opt.option_strings
            )
            prev_opts = {x.split(""="")[0] for x in cwords[1 : cword - 1]}
            options = (opt for opt in options if opt[0] in prev_opts)
            options = sorted((k, v) for k, v in options if k.startswith(curr))
            for opt_label, require_arg in options:
                if require_arg:
                    opt_label += ""=""
                print(opt_label)
        sys.exit(0)

    def execute(self):
        """"""
        Given the command-line arguments, figure out which subcommand is being
        run, create a parser appropriate to that command, and run it.
        """"""
        try:
            subcommand = self.argv[1]
        except IndexError:
            subcommand = ""help""

        parser = CommandParser(
            prog=self.prog_name,
            usage=""%(prog)s subcommand [options] [args]"",
            add_help=False,
            allow_abbrev=False,
        )
        parser.add_argument(""--settings"")
        parser.add_argument(""--pythonpath"")
        parser.add_argument(""args"", nargs=""*"")
        try:
            options, args = parser.parse_known_args(self.argv[2:])
            handle_default_options(options)
        except CommandError:
            pass

        try:
            settings.INSTALLED_APPS
        except ImproperlyConfigured as exc:
            self.settings_exception = exc
        except ImportError as exc:
            self.settings_exception = exc

        if settings.configured:
            if subcommand == ""runserver"" and ""--noreload"" not in self.argv:
                try:
                    autoreload.check_errors(django.setup)()
                except Exception:
                    apps.all_models = defaultdict(dict)
                    apps.app_configs = {}
                    apps.apps_ready = apps.models_ready = apps.ready = True

                    _parser = self.fetch_command(""runserver"").create_parser(
                        ""django"", ""runserver""
                    )
                    _options, _args = _parser.parse_known_args(self.argv[2:])
                    for _arg in _args:
                        self.argv.remove(_arg)
            else:
                django.setup()

        self.autocomplete()

        if subcommand == ""help"":
            if ""--commands"" in args:
                sys.stdout.write(self.main_help_text(commands_only=True) + ""\n"")
            elif options.args:
                sys.stdout.write(self.main_help_text() + ""\n"")
            else:
                self.fetch_command(options.args[0]).print_help(
                    self.prog_name, options.args[0]
                )
        elif subcommand == ""version"" and self.argv[1:] == [""--version""]:
            sys.stdout.write(django.get_version() + ""\n"")
        elif self.argv[1:] in ([""--help""], [""-h""]):
            sys.stdout.write(self.main_help_text() + ""\n"")
        else:
            self.fetch_command(subcommand).run_from_argv(self.argv)


def execute_from_command_line(argv=None):
    """"""Run a ManagementUtility.""""""
    utility = ManagementUtility(argv)
    utility.execute()
    sys.exit(1)","Error 1: In find_commands(), the underscore check has been flipped (using name.startswith(""_"") instead of not name.startswith(""_"")), so only commands starting with an underscore are returned.; Error 2: In load_command_class(), the Command class is returned without instantiating it (missing parentheses), causing later code to treat a class as an instance.; Error 3: In get_commands(), the folder ""management"" is misspelled as ""managment"", preventing the proper discovery of commands in application directories.; Error 4: In call_command(), the condition for required options is inverted (using ""not opt.required"" instead of ""opt.required""), leading to the wrong options being processed.; Error 5: In autocomplete(), the filtering lambda for subcommands is reversed (using not x.startswith(curr)), so suggestions exclude those starting with the current input.; Error 6: In autocomplete(), the filter for already specified options is inverted (keeping options that are present in the previous arguments rather than filtering them out).; Error 7: In ManagementUtility.execute(), the condition for displaying the version (when subcommand is ""version"") now uses an ""and"" instead of an ""or"", causing version output to work only in a combined case.; Error 8: In ManagementUtility.fetch_command(), upon a KeyError (unknown command), the code exits with status 0 instead of 1, which falsely indicates success.; Error 9: In the help branch of ManagementUtility.execute(), the condition for when to print the main help text versus a specific command’s help has been inverted, leading to incorrect help output.; Error 10: In execute_from_command_line(), an unconditional sys.exit(1) is added after execution, causing the process to exit with an error code even after successful completion.","find_commands, load_command_class, get_commands, call_command, fetch_command, autocomplete, execute, execute_from_command_line"
lib:matplotlib:axis.py,"---------------------------------------
""""""
Classes for the ticks and x- and y-axis.
""""""

import datetime
import functools
import logging
from numbers import Real
import warnings

import numpy as np

import matplotlib as mpl
from matplotlib import _api, cbook
import matplotlib.artist as martist
import matplotlib.colors as mcolors
import matplotlib.lines as mlines
import matplotlib.scale as mscale
import matplotlib.text as mtext
import matplotlib.ticker as mticker
import matplotlib.transforms as mtransforms
import matplotlib.units as munits
from matplotlib.ticker import NullLocator

_log = logging.getLogger(__name__)

GRIDLINE_INTERPOLATION_STEPS = 180

# This list is being used for compatibility with Axes.grid, which
# allows all Line2D kwargs.
_line_inspector = martist.ArtistInspector(mlines.Line2D)
_line_param_names = _line_inspector.get_setters()
_line_param_aliases = [next(iter(d)) for d in _line_inspector.aliasd.values()]
_gridline_param_names = ['grid_' + name
                         for name in _line_param_names + _line_param_aliases]


class Tick(martist.Artist):
    """"""
    Abstract base class for the axis ticks, grid lines and labels.

    Ticks mark a position on an Axis. They contain two lines as markers and
    two labels; one each for the bottom and top positions (in case of an
    `.XAxis`) or for the left and right positions (in case of a `.YAxis`).

    Attributes
    ----------
    tick1line : `~matplotlib.lines.Line2D`
        The left/bottom tick marker.
    tick2line : `~matplotlib.lines.Line2D`
        The right/top tick marker.
    gridline : `~matplotlib.lines.Line2D`
        The grid line associated with the label position.
    label1 : `~matplotlib.text.Text`
        The left/bottom tick label.
    label2 : `~matplotlib.text.Text`
        The right/top tick label.

    """"""
    def __init__(
        self, axes, loc, *,
        size=None,  # points
        width=None,
        color=None,
        tickdir=None,
        pad=None,
        labelsize=None,
        labelcolor=None,
        labelfontfamily=None,
        zorder=None,
        gridOn=None,  # defaults to axes.grid depending on axes.grid.which
        tick1On=True,
        tick2On=True,
        label1On=True,
        label2On=False,
        major=True,
        labelrotation=0,
        labelrotation_mode=None,
        grid_color=None,
        grid_linestyle=None,
        grid_linewidth=None,
        grid_alpha=None,
        **kwargs,  # Other Line2D kwargs applied to gridlines.
    ):
        """"""
        bbox is the Bound2D bounding box in display coords of the Axes
        loc is the tick location in data coords
        size is the tick size in points
        """"""
        super().__init__()

        if gridOn is None:
            which = mpl.rcParams['axes.grid.which']
            if major and which in ('both', 'major'):
                gridOn = mpl.rcParams['axes.grid']
            elif not major and which in ('both', 'minor'):
                gridOn = mpl.rcParams['axes.grid']
            else:
                gridOn = False

        self.set_figure(axes.get_figure(root=False))
        self.axes = axes

        self._loc = loc
        self._major = major

        name = self.__name__
        major_minor = ""major"" if major else ""minor""
        self._size = mpl._val_or_rc(size, f""{name}.{major_minor}.size"")
        self._width = mpl._val_or_rc(width, f""{name}.{major_minor}.width"")
        self._base_pad = mpl._val_or_rc(pad, f""{name}.{major_minor}.pad"")
        color = mpl._val_or_rc(color, f""{name}.color"")
        labelcolor = mpl._val_or_rc(labelcolor, f""{name}.labelcolor"")
        if cbook._str_equal(labelcolor, 'inherit'):
            # inherit from tick color
            labelcolor = mpl.rcParams[f""{name}.color""]
        labelsize = mpl._val_or_rc(labelsize, f""{name}.labelsize"")

        self._set_labelrotation(labelrotation)

        if zorder is None:
            if major:
                zorder = mlines.Line2D.zorder + 0.01
            else:
                zorder = mlines.Line2D.zorder
        self._zorder = zorder

        grid_color = mpl._val_or_rc(grid_color, ""grid.color"")
        grid_linestyle = mpl._val_or_rc(grid_linestyle, ""grid.linestyle"")
        grid_linewidth = mpl._val_or_rc(grid_linewidth, ""grid.linewidth"")
        if grid_alpha is None and not mcolors._has_alpha_channel(grid_color):
            # alpha precedence: kwarg > color alpha > rcParams['grid.alpha']
            # Note: only resolve to rcParams if the color does not have alpha
            # otherwise `grid(color=(1, 1, 1, 0.5))` would work like
            #   grid(color=(1, 1, 1, 0.5), alpha=rcParams['grid.alpha'])
            # so the that the rcParams default would override color alpha.
            grid_alpha = mpl.rcParams[""grid.alpha""]
        grid_kw = {k[5:]: v for k, v in kwargs.items() if k != ""rotation_mode""}

        self.tick1line = mlines.Line2D(
            [], [],
            color=color, linestyle=""none"", zorder=zorder, visible=tick1On,
            markeredgecolor=color, markersize=self._size, markeredgewidth=self._width,
        )
        self.tick2line = mlines.Line2D(
            [], [],
            color=color, linestyle=""none"", zorder=zorder, visible=tick2On,
            markeredgecolor=color, markersize=self._size, markeredgewidth=self._width,
        )
        self.gridline = mlines.Line2D(
            [], [],
            color=grid_color, alpha=grid_alpha, visible=gridOn,
            linestyle=grid_linestyle, linewidth=grid_linewidth, marker="""",
            **grid_kw,
        )
        self.gridline.get_path()._interpolation_steps = \
            GRIDLINE_INTERPOLATION_STEPS
        self.label1 = mtext.Text(
            np.nan, np.nan,
            fontsize=labelsize, color=labelcolor, visible=label1On,
            fontfamily=labelfontfamily, rotation=self._labelrotation[1],
            rotation_mode=labelrotation_mode)
        self.label2 = mtext.Text(
            np.nan, np.nan,
            fontsize=labelsize, color=labelcolor, visible=label2On,
            fontfamily=labelfontfamily, rotation=self._labelrotation[1],
            rotation_mode=labelrotation_mode)

        self._apply_tickdir(tickdir)

        for artist in [self.tick1line, self.tick2line, self.gridline,
                       self.label1, self.label2]:
            self._set_artist_props(artist)

        self.update_position(loc)

    def _set_labelrotation(self, labelrotation):
        if isinstance(labelrotation, str):
            mode = labelrotation
            angle = 0
        elif isinstance(labelrotation, (tuple, list)):
            mode, angle = labelrotation
        else:
            mode = 'default'
            angle = labelrotation
        _api.check_in_list(['auto', 'default'], labelrotation=mode)
        self._labelrotation = (mode, angle)

    @property
    def _pad(self):
        return self._base_pad + self.get_tick_padding()

    def _apply_tickdir(self, tickdir):
        """"""Set tick direction.  Valid values are 'out', 'in', 'inout'.""""""
        # This method is responsible for verifying input and, in subclasses, for setting
        # the tick{1,2}line markers.  From the user perspective this should always be
        # called through _apply_params, which further updates ticklabel positions using
        # the new pads.
        tickdir = mpl._val_or_rc(tickdir, f'{self.__name__}.direction')
        _api.check_in_list(['in', 'out', 'inout'], tickdir=tickdir)
        self._tickdir = tickdir

    def get_tickdir(self):
        return self._tickdir

    def get_tick_padding(self):
        """"""Get the length of the tick outside of the Axes.""""""
        padding = {
            'in': 1.0,
            'inout': 0.5,
            'out': 0.0
        }
        return self._size * padding[self._tickdir]

    def get_children(self):
        children = [self.tick1line, self.tick2line,
                    self.gridline, self.label1, self.label2]
        return children

    def set_clip_path(self, path, transform=None):
        # docstring inherited
        super().set_clip_path(path, transform)
        self.gridline.set_clip_path(path, transform)
        self.stale = True

    def contains(self, mouseevent):
        """"""
        Test whether the mouse event occurred in the Tick marks.

        This function always returns false.  It is more useful to test if the
        axis as a whole contains the mouse rather than the set of tick marks.
        """"""
        return False, {}

    def set_pad(self, val):
        """"""
        Set the tick label pad in points

        Parameters
        ----------
        val : float
        """"""
        self._apply_params(pad=val)
        self.stale = True

    def get_pad(self):
        """"""Get the value of the tick label pad in points.""""""
        return self._base_pad

    def get_loc(self):
        """"""Return the tick location (data coords) as a scalar.""""""
        return self._loc

    @martist.allow_rasterization
    def draw(self, renderer):
        if not self.get_visible():
            self.stale = False
            return
        renderer.open_group(self.__name__, gid=self.get_gid())
        for artist in [self.gridline, self.tick1line, self.tick2line,
                       self.label1, self.label2]:
            artist.draw(renderer)
        renderer.close_group(self.__name__)
        self.stale = False

    def set_url(self, url):
        """"""
        Set the url of label1 and label2.

        Parameters
        ----------
        url : str
        """"""
        super().set_url(url)
        self.label1.set_url(url)
        self.label2.set_url(url)
        self.stale = True

    def _set_artist_props(self, a):
        a.set_figure(self.get_figure(root=False))

    def get_view_interval(self):
        """"""
        Return the view limits ``(min, max)`` of the axis the tick belongs to.
        """"""
        raise NotImplementedError('Derived must override')

    def _apply_params(self, **kwargs):
        for name, target in [(""gridOn"", self.gridline),
                             (""tick1On"", self.tick1line),
                             (""tick2On"", self.tick2line),
                             (""label1On"", self.label1),
                             (""label2On"", self.label2)]:
            if name in kwargs:
                target.set_visible(kwargs.pop(name))
        if any(k in kwargs for k in ['size', 'width', 'pad', 'tickdir']):
            self._size = kwargs.pop('size', self._size)
            # Width could be handled outside this block, but it is
            # convenient to leave it here.
            self._width = kwargs.pop('width', self._width)
            self._base_pad = kwargs.pop('pad', self._base_pad)
            # _apply_tickdir uses _size and _base_pad to make _pad, and also
            # sets the ticklines markers.
            self._apply_tickdir(kwargs.pop('tickdir', self._tickdir))
            for line in (self.tick1line, self.tick2line):
                line.set_markersize(self._size)
                line.set_markeredgewidth(self._width)
            # _get_text1_transform uses _pad from _apply_tickdir.
            trans = self._get_text1_transform()[0]
            self.label1.set_transform(trans)
            trans = self._get_text2_transform()[0]
            self.label2.set_transform(trans)
        tick_kw = {k: v for k, v in kwargs.items() if k in ['color', 'zorder']}
        if 'color' in kwargs:
            tick_kw['markeredgecolor'] = kwargs['color']
        self.tick1line.set(**tick_kw)
        self.tick2line.set(**tick_kw)
        for k, v in tick_kw.items():
            setattr(self, '_' + k, v)

        if 'labelrotation' in kwargs:
            self._set_labelrotation(kwargs.pop('labelrotation'))
            self.label1.set(rotation=self._labelrotation[1])
            self.label2.set(rotation=self._labelrotation[1])

        label_kw = {k[5:]: v for k, v in kwargs.items()
                    if k in ['labelsize', 'labelcolor', 'labelfontfamily',
                             'labelrotation_mode']}
        self.label1.set(**label_kw)
        self.label2.set(**label_kw)

        grid_kw = {k[5:]: v for k, v in kwargs.items()
                   if k in _gridline_param_names}
        self.gridline.set(**grid_kw)

    def update_position(self, loc):
        """"""Set the location of tick in data coords with scalar *loc*.""""""
        raise NotImplementedError('Derived must override')

    def _get_text1_transform(self):
        raise NotImplementedError('Derived must override')

    def _get_text2_transform(self):
        raise NotImplementedError('Derived must override')


class XTick(Tick):
    """"""
    Contains all the Artists needed to make an x tick - the tick line,
    the label text and the grid line
    """"""
    __name__ = 'xtick'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # x in data coords, y in axes coords
        ax = self.axes
        self.tick1line.set(
            data=([0], [0]), transform=ax.get_xaxis_transform(""tick1""))
        self.tick2line.set(
            data=([0], [1]), transform=ax.get_xaxis_transform(""tick2""))
        self.gridline.set(
            data=([0, 0], [0, 1]), transform=ax.get_xaxis_transform(""grid""))
        # the y loc is 3 points below the min of y axis
        trans, va, ha = self._get_text1_transform()
        self.label1.set(
            x=0, y=0,
            verticalalignment=va, horizontalalignment=ha, transform=trans,
        )
        trans, va, ha = self._get_text2_transform()
        self.label2.set(
            x=0, y=1,
            verticalalignment=va, horizontalalignment=ha, transform=trans,
        )

    def _get_text1_transform(self):
        return self.axes.get_xaxis_text1_transform(self._pad)

    def _get_text2_transform(self):
        return self.axes.get_xaxis_text2_transform(self._pad)

    def _apply_tickdir(self, tickdir):
        # docstring inherited
        super()._apply_tickdir(tickdir)
        mark1, mark2 = {
            'out': (mlines.TICKUP, mlines.TICKDOWN),
            'in': (mlines.TICKUP, mlines.TICKDOWN),
            'inout': ('|', '|'),
        }[self._tickdir]
        self.tick1line.set_marker(mark1)
        self.tick2line.set_marker(mark2)

    def update_position(self, loc):
        """"""Set the location of tick in data coords with scalar *loc*.""""""
        self.tick1line.set_xdata((loc,))
        self.tick2line.set_xdata((loc,))
        self.gridline.set_xdata((loc,))
        self.label1.set_x(loc)
        self._loc = loc
        self.stale = True

    def get_view_interval(self):
        # docstring inherited
        return self.axes.viewLim.intervalx


class YTick(Tick):
    """"""
    Contains all the Artists needed to make a Y tick - the tick line,
    the label text and the grid line
    """"""
    __name__ = 'ytick'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # x in axes coords, y in data coords
        ax = self.axes
        self.tick1line.set(
            data=([0], [0]), transform=ax.get_yaxis_transform(""tick1""))
        self.tick2line.set(
            data=([1], [0]), transform=ax.get_yaxis_transform(""tick2""))
        self.gridline.set(
            data=([0, 1], [0, 0]), transform=ax.get_yaxis_transform(""grid""))
        # the y loc is 3 points below the min of y axis
        trans, va, ha = self._get_text1_transform()
        self.label1.set(
            x=0, y=0,
            verticalalignment=va, horizontalalignment=ha, transform=trans,
        )
        trans, va, ha = self._get_text2_transform()
        self.label2.set(
            x=1, y=0,
            verticalalignment=va, horizontalalignment=ha, transform=trans,
        )

    def _get_text1_transform(self):
        return self.axes.get_yaxis_text1_transform(self._pad)

    def _get_text2_transform(self):
        return self.axes.get_yaxis_text2_transform(self._pad)

    def _apply_tickdir(self, tickdir):
        # docstring inherited
        super()._apply_tickdir(tickdir)
        mark1, mark2 = {
            'out': (mlines.TICKRIGHT, mlines.TICKLEFT),
            'in': (mlines.TICKRIGHT, mlines.TICKLEFT),
            'inout': ('_', '_'),
        }[self._tickdir]
        self.tick1line.set_marker(mark1)
        self.tick2line.set_marker(mark2)

    def update_position(self, loc):
        """"""Set the location of tick in data coords with scalar *loc*.""""""
        self.tick1line.set_ydata((loc,))
        self.tick2line.set_ydata((loc,))
        self.gridline.set_ydata((loc,))
        self.label1.set_y(loc)
        self._loc = loc
        self.stale = True

    def get_view_interval(self):
        # docstring inherited
        return self.axes.viewLim.intervaly


class Ticker:
    """"""
    A container for the objects defining tick position and format.

    Attributes
    ----------
    locator : `~matplotlib.ticker.Locator` subclass
        Determines the positions of the ticks.
    formatter : `~matplotlib.ticker.Formatter` subclass
        Determines the format of the tick labels.
    """"""

    def __init__(self):
        self._locator = None
        self._formatter = None
        self._locator_is_default = True
        self._formatter_is_default = True

    @property
    def locator(self):
        return self._locator

    @locator.setter
    def locator(self, locator):
        if not isinstance(locator, mticker.Locator):
            raise TypeError('locator must be a subclass of '
                            'matplotlib.ticker.Locator')
        self._locator = locator

    @property
    def formatter(self):
        return self._formatter

    @formatter.setter
    def formatter(self, formatter):
        if not isinstance(formatter, mticker.Locator):
            raise TypeError('formatter must be a subclass of '
                            'matplotlib.ticker.Formatter')
        self._formatter = formatter


class _LazyTickList:
    """"""
    A descriptor for lazy instantiation of tick lists.

    See comment above definition of the ``majorTicks`` and ``minorTicks``
    attributes.
    """"""

    def __init__(self, major):
        self._major = major

    def __get__(self, instance, owner):
        if instance is None:
            return self
        else:
            if self._major:
                instance.majorTicks = []
                tick = instance._get_tick(major=True)
                instance.majorTicks = [tick]
                return instance.majorTicks
            else:
                instance.minorTicks = []
                tick = instance._get_tick(major=False)
                instance.minorTicks = [tick]
                return instance.minorTicks


class Axis(martist.Artist):
    """"""
    Base class for `.XAxis` and `.YAxis`.

    Attributes
    ----------
    isDefault_label : bool

    axes : `~matplotlib.axes.Axes`
        The `~.axes.Axes` to which the Axis belongs.
    major : `~matplotlib.axis.Ticker`
        Determines the major tick positions and their label format.
    minor : `~matplotlib.axis.Ticker`
        Determines the minor tick positions and their label format.
    callbacks : `~matplotlib.cbook.CallbackRegistry`

    label : `~matplotlib.text.Text`
        The axis label.
    labelpad : float
        The distance between the axis label and the tick labels.
        Defaults to :rc:`axes.labelpad`.
    offsetText : `~matplotlib.text.Text`
        A `.Text` object containing the data offset of the ticks (if any).
    pickradius : float
        The acceptance radius for containment tests. See also `.Axis.contains`.
    majorTicks : list of `.Tick`
        The major ticks.

        .. warning::

            Ticks are not guaranteed to be persistent. Various operations
            can create, delete and modify the Tick instances. There is an
            imminent risk that changes to individual ticks will not
            survive if you work on the figure further (including also
            panning/zooming on a displayed figure).

            Working on the individual ticks is a method of last resort.
            Use `.set_tick_params` instead if possible.

    minorTicks : list of `.Tick`
        The minor ticks.
    """"""
    OFFSETTEXTPAD = 3
    _tick_class = None
    converter = _api.deprecate_privatize_attribute(
                    ""3.10"",
                    alternative=""get_converter and set_converter methods""
                )

    def __str__(self):
        return ""{}({},{})"".format(
            type(self).__name__, *self.axes.transAxes.transform((0, 0)))

    def __init__(self, axes, *, pickradius=15, clear=True):
        """"""
        Parameters
        ----------
        axes : `~matplotlib.axes.Axes`
            The `~.axes.Axes` to which the created Axis belongs.
        pickradius : float
            The acceptance radius for containment tests. See also
            `.Axis.contains`.
        clear : bool, default: True
            Whether to clear the Axis on creation. This is not required, e.g.,  when
            creating an Axis as part of an Axes, as ``Axes.clear`` will call
            ``Axis.clear``.
            .. versionadded:: 3.8
        """"""
        super().__init__()
        self._remove_overlapping_locs = True

        self.set_figure(axes.get_figure(root=False))

        self.isDefault_label = True

        self.axes = axes
        self.major = Ticker()
        self.minor = Ticker()
        self.callbacks = cbook.CallbackRegistry(signals=[""units""])

        self._autolabelpos = True

        self.label = mtext.Text(
            np.nan, np.nan,
            fontsize=mpl.rcParams['axes.labelsize'],
            fontweight=mpl.rcParams['axes.labelweight'],
            color=mpl.rcParams['axes.labelcolor'],
        )  #: The `.Text` object of the axis label.

        self._set_artist_props(self.label)
        self.offsetText = mtext.Text(np.nan, np.nan)
        self._set_artist_props(self.offsetText)

        self.labelpad = mpl.rcParams['axes.labelpad']

        self.pickradius = pickradius

        self._major_tick_kw = dict()
        self._minor_tick_kw = dict()

        if clear:
            self.clear()
        else:
            self._converter = None
            self._converter_is_explicit = False
            self.units = None

        self._autoscale_on = True

    @property
    def isDefault_majloc(self):
        return self.major._locator_is_default

    @isDefault_majloc.setter
    def isDefault_majloc(self, value):
        self.major._locator_is_default = value

    @property
    def isDefault_majfmt(self):
        return self.major._formatter_is_default

    @isDefault_majfmt.setter
    def isDefault_majfmt(self, value):
        self.major._formatter_is_default = value

    @property
    def isDefault_minloc(self):
        return self.minor._locator_is_default

    @isDefault_minloc.setter
    def isDefault_minloc(self, value):
        self.minor._locator_is_default = value

    @property
    def isDefault_minfmt(self):
        return self.minor._formatter_is_default

    @isDefault_minfmt.setter
    def isDefault_minfmt(self, value):
        self.minor._formatter_is_default = value

    def _get_shared_axes(self):
        """"""Return Grouper of shared Axes for current axis.""""""
        return self.axes._shared_axes[
            self._get_axis_name()].get_siblings(self.axes)

    def _get_shared_axis(self):
        """"""Return list of shared axis for current axis.""""""
        name = self._get_axis_name()
        return [ax._axis_map[name] for ax in self._get_shared_axes()]

    def _get_axis_name(self):
        """"""Return the axis name.""""""
        return next(name for name, axis in self.axes._axis_map.items()
                    if axis is self)

    majorTicks = _LazyTickList(major=True)
    minorTicks = _LazyTickList(major=False)

    def get_remove_overlapping_locs(self):
        return self._remove_overlapping_locs

    def set_remove_overlapping_locs(self, val):
        self._remove_overlapping_locs = bool(val)

    remove_overlapping_locs = property(
        get_remove_overlapping_locs, set_remove_overlapping_locs,
        doc=('If minor ticker locations that overlap with major '
             'ticker locations should be trimmed.'))

    def set_label_coords(self, x, y, transform=None):
        """"""
        Set the coordinates of the label.

        By default, the x coordinate of the y label and the y coordinate of the
        x label are determined by the tick label bounding boxes, but this can
        lead to poor alignment of multiple labels if there are multiple Axes.

        You can also specify the coordinate system of the label with the
        transform.  If None, the default coordinate system will be the axes
        coordinate system: (0, 0) is bottom left, (0.5, 0.5) is center, etc.
        """"""
        self._autolabelpos = False
        if transform is None:
            transform = self.axes.transAxes

        self.label.set_transform(transform)
        self.label.set_position((x, y))
        self.stale = True

    def get_transform(self):
        """"""Return the transform used in the Axis' scale""""""
        return self._scale.get_transform()

    def get_scale(self):
        """"""Return this Axis' scale (as a str).""""""
        return self._scale.name

    def _set_scale(self, value, **kwargs):
        if not isinstance(value, mscale.ScaleBase):
            self._scale = mscale.scale_factory(value, self, **kwargs)
        else:
            self._scale = value
        self._scale.set_default_locators_and_formatters(self)

        self.isDefault_majloc = True
        self.isDefault_minloc = True
        self.isDefault_majfmt = True
        self.isDefault_minfmt = True

    def _set_axes_scale(self, value, **kwargs):
        """"""
        Set this Axis' scale.

        Parameters
        ----------
        value : str or `.ScaleBase`
            The axis scale type to apply.  Valid string values are the names of scale
            classes (""linear"", ""log"", ""function"",...).  These may be the names of any
            of the :ref:`built-in scales<builtin_scales>` or of any custom scales
            registered using `matplotlib.scale.register_scale`.

        **kwargs
            If *value* is a string, keywords are passed to the instantiation method of
            the respective class.
        """"""
        name = self._get_axis_name()
        old_default_lims = (self.get_major_locator()
                            .nonsingular(-np.inf, np.inf))
        for ax in self._get_shared_axes():
            ax._axis_map[name]._set_scale(value, **kwargs)
            ax._update_transScale()
            ax.stale = True
        new_default_lims = (self.get_major_locator()
                            .nonsingular(-np.inf, np.inf))
        if old_default_lims != new_default_lims:
            self.axes.autoscale_view(
                **{f""scale{k}"": k == name for k in self.axes._axis_names})

    def limit_range_for_scale(self, vmin, vmax):
        """"""
        Return the range *vmin*, *vmax*, restricted to the domain supported by the
        current scale.
        """"""
        return self._scale.limit_range_for_scale(vmin, vmax, self.get_minpos())

    def _get_autoscale_on(self):
        """"""Return whether this Axis is autoscaled.""""""
        return self._autoscale_on

    def _set_autoscale_on(self, b):
        """"""
        Set whether this Axis is autoscaled when drawing or by `.Axes.autoscale_view`.

        If b is None, then the value is not changed.

        Parameters
        ----------
        b : bool
        """"""
        if b is not None:
            self._autoscale_on = b

    def get_children(self):
        return [self.label, self.offsetText,
                *self.get_major_ticks(), *self.get_minor_ticks()]

    def _reset_major_tick_kw(self):
        self._major_tick_kw.clear()
        self._major_tick_kw['gridOn'] = (
                mpl.rcParams['axes.grid'] and
                mpl.rcParams['axes.grid.which'] in ('both', 'major'))

    def _reset_minor_tick_kw(self):
        self._minor_tick_kw.clear()
        self._minor_tick_kw['gridOn'] = (
                mpl.rcParams['axes.grid'] and
                mpl.rcParams['axes.grid.which'] in ('both', 'minor'))

    def clear(self):
        """"""
        Clear the axis.

        This resets axis properties to their default values:

        - the label
        - the scale
        - locators, formatters and ticks
        - major and minor grid
        - units
        - registered callbacks
        """"""
        self.label._reset_visual_defaults()
        self.label.set_color(mpl.rcParams['axes.labelcolor'])
        self.label.set_fontsize(mpl.rcParams['axes.labelsize'])
        self.label.set_fontweight(mpl.rcParams['axes.labelweight'])
        self.offsetText._reset_visual_defaults()
        self.labelpad = mpl.rcParams['axes.labelpad']

        self._init()

        self._set_scale('linear')

        self.callbacks = cbook.CallbackRegistry(signals=[""units""])

        self._major_tick_kw['gridOn'] = (
                mpl.rcParams['axes.grid'] and
                mpl.rcParams['axes.grid.which'] in ('both', 'major'))
        self._minor_tick_kw['gridOn'] = (
                mpl.rcParams['axes.grid'] and
                mpl.rcParams['axes.grid.which'] in ('both', 'minor'))
        self.reset_ticks()

        self._converter = None
        self._converter_is_explicit = False
        self.units = None
        self.stale = True

    def reset_ticks(self):
        """"""
        Re-initialize the major and minor Tick lists.

        Each list starts with a single fresh Tick.
        """"""
        try:
            del self.majorTicks
        except AttributeError:
            pass
        self.set_clip_path(self.axes.patch)
        self.stale = True

    def minorticks_on(self):
        """"""
        Display default minor ticks on the Axis, depending on the scale
        (`~.axis.Axis.get_scale`).

        Scales use specific minor locators:

        - log: `~.LogLocator`
        - symlog: `~.SymmetricalLogLocator`
        - asinh: `~.AsinhLocator`
        - logit: `~.LogitLocator`
        - default: `~.AutoMinorLocator`

        Displaying minor ticks may reduce performance; you may turn them off
        using `minorticks_off()` if drawing speed is a problem.
        """"""
        scale = self.get_scale()
        if scale == 'log':
            s = self._scale
            self.set_minor_locator(mticker.LogLocator(s.base, s.subs))
        elif scale == 'symlog':
            s = self._scale
            self.set_minor_locator(
                mticker.SymmetricalLogLocator(s._transform, s.subs))
        elif scale == 'asinh':
            s = self._scale
            self.set_minor_locator(
                    mticker.AsinhLocator(s.linear_width, base=s._base,
                                         subs=s._subs))
        elif scale == 'logit':
            self.set_minor_locator(mticker.LogitLocator(minor=True))
        else:
            self.set_minor_locator(mticker.AutoMinorLocator())

    def minorticks_off(self):
        """"""Remove minor ticks from the Axis.""""""
        self.set_minor_locator(mticker.NullLocator())

    def set_tick_params(self, which='major', reset=False, **kwargs):
        """"""
        Set appearance parameters for ticks, ticklabels, and gridlines.

        For documentation of keyword arguments, see
        :meth:`matplotlib.axes.Axes.tick_params`.

        See Also
        --------
        .Axis.get_tick_params
            View the current style settings for ticks, ticklabels, and
            gridlines.
        """"""
        _api.check_in_list(['major', 'minor', 'both'], which=which)
        kwtrans = self._translate_tick_params(kwargs)

        if reset:
            if which in ['major', 'both']:
                self._reset_major_tick_kw()
                self._major_tick_kw.update(kwtrans)
            if which in ['minor', 'both']:
                self._reset_minor_tick_kw()
                self._minor_tick_kw.update(kwtrans)
            self.reset_ticks()
        else:
            if which in ['major', 'both']:
                self._major_tick_kw.update(kwtrans)
                for tick in self.majorTicks:
                    tick._apply_params(**kwtrans)
            if which in ['minor', 'both']:
                self._minor_tick_kw.update(kwtrans)
                for tick in self.minorTicks:
                    tick._apply_params(**kwtrans)
            if 'label1On' in kwtrans or 'label2On' in kwtrans:
                self.offsetText.set_visible(
                    self._major_tick_kw.get('label1On', False)
                    or self._major_tick_kw.get('label2On', False))
            if 'labelcolor' in kwtrans:
                self.offsetText.set_color(kwtrans['labelcolor'])

        self.stale = True

    def get_tick_params(self, which='major'):
        """"""
        Get appearance parameters for ticks, ticklabels, and gridlines.

        .. versionadded:: 3.7

        Parameters
        ----------
        which : {'major', 'minor'}, default: 'major'
            The group of ticks for which the parameters are retrieved.

        Returns
        -------
        dict
            Properties for styling tick elements added to the axis.

        Notes
        -----
        This method returns the appearance parameters for styling *new*
        elements added to this axis and may be different from the values
        on current elements if they were modified directly by the user
        (e.g., via ``set_*`` methods on individual tick objects).

        Examples
        --------
        ::

            >>> ax.yaxis.set_tick_params(labelsize=30, labelcolor='red',
            ...                          direction='out', which='major')
            >>> ax.yaxis.get_tick_params(which='major')
            {'direction': 'out',
            'left': True,
            'right': False,
            'labelleft': True,
            'labelright': False,
            'gridOn': False,
            'labelsize': 30,
            'labelcolor': 'red'}
            >>> ax.yaxis.get_tick_params(which='minor')
            {'left': True,
            'right': False,
            'labelleft': True,
            'labelright': False,
            'gridOn': False}


        """"""
        _api.check_in_list(['major', 'minor'], which=which)
        if which == 'major':
            return self._translate_tick_params(
                self._major_tick_kw, reverse=True
            )
        return self._translate_tick_params(self._minor_tick_kw, reverse=True)

    @classmethod
    def _translate_tick_params(cls, kw, reverse=False):
        """"""
        Translate the kwargs supported by `.Axis.set_tick_params` to kwargs
        supported by `.Tick._apply_params`.

        In particular, this maps axis specific names like 'top', 'left'
        to the generic tick1, tick2 logic of the axis. Additionally, there
        are some other name translations.

        Returns a new dict of translated kwargs.

        Note: Use reverse=True to translate from those supported by
        `.Tick._apply_params` back to those supported by
        `.Axis.set_tick_params`.
        """"""
        kw_ = {**kw}

        allowed_keys = [
            'size', 'width', 'color', 'tickdir', 'pad',
            'labelsize', 'labelcolor', 'labelfontfamily', 'zorder', 'gridOn',
            'tick1On', 'tick2On', 'label1On', 'label2On',
            'length', 'direction', 'left', 'bottom', 'right', 'top',
            'labelleft', 'labelbottom', 'labelright', 'labeltop',
            'labelrotation', 'labelrotation_mode',
            *_gridline_param_names]

        keymap = {
            'length': 'size',
            'direction': 'tickdir',
            'rotation': 'labelrotation',
            'rotation_mode': 'labelrotation_mode',
            'left': 'tick1On',
            'bottom': 'tick1On',
            'right': 'tick2On',
            'top': 'tick2On',
            'labelleft': 'label1On',
            'labelbottom': 'label1On',
            'labelright': 'label2On',
            'labeltop': 'label2On',
        }
        if reverse:
            kwtrans = {}
            is_x_axis = cls.axis_name == 'x'
            y_axis_keys = ['left', 'right', 'labelleft', 'labelright']
            for oldkey, newkey in keymap.items():
                if newkey in kw_:
                    if is_x_axis and oldkey in y_axis_keys:
                        continue
                    else:
                        kwtrans[oldkey] = kw_.pop(newkey)
        else:
            kwtrans = {oldkey: kw_.pop(oldkey) for oldkey, newkey in keymap.items() if oldkey in kw_}
        if 'colors' in kw_:
            c = kw_.pop('colors')
            kwtrans['color'] = c
            kwtrans['labelcolor'] = c
        for key in kw_:
            if key not in allowed_keys:
                raise ValueError(
                    ""keyword %s is not recognized; valid keywords are %s""
                    % (key, allowed_keys))
        kwtrans.update(kw_)
        return kwtrans

    def set_clip_path(self, path, transform=None):
        super().set_clip_path(path, transform)
        for child in self.majorTicks + self.minorTicks:
            child.set_clip_path(path, transform)
        self.stale = True

    def get_view_interval(self):
        """"""Return the ``(min, max)`` view limits of this axis.""""""
        raise NotImplementedError('Derived must override')

    def set_view_interval(self, vmin, vmax, ignore=False):
        """"""
        Set the axis view limits.  This method is for internal use; Matplotlib
        users should typically use e.g. `~.Axes.set_xlim` or `~.Axes.set_ylim`.

        If *ignore* is False (the default), this method will never reduce the
        preexisting view limits, only expand them if *vmin* or *vmax* are not
        within them.  Moreover, the order of *vmin* and *vmax* does not matter;
        the orientation of the axis will not change.

        If *ignore* is True, the view limits will be set exactly to ``(vmin,
        vmax)`` in that order.
        """"""
        raise NotImplementedError('Derived must override')

    def get_data_interval(self):
        """"""Return the ``(min, max)`` data limits of this axis.""""""
        raise NotImplementedError('Derived must override')

    def set_data_interval(self, vmin, vmax, ignore=False):
        """"""
        Set the axis data limits.  This method is for internal use.

        If *ignore* is False (the default), this method will never reduce the
        preexisting data limits, only expand them if *vmin* or *vmax* are not
        within them.  Moreover, the order of *vmin* and *vmax* does not matter;
        the orientation of the axis will not change.

        If *ignore* is True, the data limits will be set exactly to ``(vmin,
        vmax)`` in that order.
        """"""
        raise NotImplementedError('Derived must override')

    def get_inverted(self):
        """"""
        Return whether this Axis is oriented in the ""inverse"" direction.

        The ""normal"" direction is increasing to the right for the x-axis and to
        the top for the y-axis; the ""inverse"" direction is increasing to the
        left for the x-axis and to the bottom for the y-axis.
        """"""
        low, high = self.get_view_interval()
        return high < low

    def set_inverted(self, inverted):
        """"""
        Set whether this Axis is oriented in the ""inverse"" direction.

        The ""normal"" direction is increasing to the right for the x-axis and to
        the top for the y-axis; the ""inverse"" direction is increasing to the
        left for the x-axis and to the bottom for the y-axis.
        """"""
        a, b = self.get_view_interval()
        self._set_lim(*sorted((a, b), reverse=bool(inverted)), auto=None)

    def set_default_intervals(self):
        """"""
        Set the default limits for the axis data and view interval if they
        have not been not mutated yet.
        """"""
    def _set_lim(self, v0, v1, *, emit=True, auto):
        """"""
        Set view limits.

        This method is a helper for the Axes ``set_xlim``, ``set_ylim``, and
        ``set_zlim`` methods.

        Parameters
        ----------
        v0, v1 : float
            The view limits.
        emit : bool, default: True
            Whether to notify observers of limit change.
        auto : bool or None, default: False
            Whether to turn on autoscaling of the x-axis. True turns on, False
            turns off, None leaves unchanged.
        """"""
        name = self._get_axis_name()

        self.axes._process_unit_info([(name, (v0, v1))], convert=False)
        v0 = self.axes._validate_converted_limits(v0, self.convert_units)
        v1 = self.axes._validate_converted_limits(v1, self.convert_units)

        if v0 is None or v1 is None:
            old0, old1 = self.get_view_interval()
            if v0 is None:
                v0 = old0
            if v1 is None:
                v1 = old1

        if self.get_scale() == 'log' and (v0 <= 0 or v1 <= 0):
            old0, old1 = self.get_view_interval()
            if v0 <= 0:
                _api.warn_external(f""Attempt to set non-positive {name}lim on ""
                                   f""a log-scaled axis will be ignored."")
                v0 = old0
            if v1 <= 0:
                _api.warn_external(f""Attempt to set non-positive {name}lim on ""
                                   f""a log-scaled axis will be ignored."")
                v1 = old1
        if v0 == v1:
            _api.warn_external(
                f""Attempting to set identical low and high {name}lims ""
                f""makes transformation singular; automatically expanding."")
        reverse = bool(v0 > v1)
        v0, v1 = self.get_major_locator().nonsingular(v0, v1)
        v0, v1 = self.limit_range_for_scale(v0, v1)
        v0, v1 = sorted([v0, v1], reverse=bool(reverse))

        self.set_view_interval(v0, v1, ignore=True)
        for ax in self._get_shared_axes():
            ax._stale_viewlims[name] = False
        self._set_autoscale_on(auto)

        if emit:
            self.axes.callbacks.process(f""{name}lim_changed"", self.axes)
            for other in self._get_shared_axes():
                if other is self.axes:
                    continue
                other._axis_map[name]._set_lim(v0, v1, emit=False, auto=auto)
                if emit:
                    other.callbacks.process(f""{name}lim_changed"", other)
                if ((other_fig := other.get_figure(root=False)) !=
                        self.get_figure(root=False)):
                    other_fig.canvas.draw_idle()

        self.stale = True
        return v0, v1

    def _set_artist_props(self, a):
        if a is None:
            return
        a.set_figure(self.get_figure(root=False))

    def _update_ticks(self):
        """"""
        Update ticks (position and labels) using the current data interval of
        the axes.  Return the list of ticks that will be drawn.
        """"""
        major_locs = self.get_majorticklocs()
        major_labels = self.major.formatter.format_ticks(major_locs)
        major_ticks = self.get_major_ticks(len(major_locs))
        for tick, loc, label in zip(major_ticks, major_locs, major_labels):
            tick.update_position(loc)
            tick.label1.set_text(label)
            tick.label2.set_text(label)
        minor_locs = self.get_minorticklocs()
        minor_labels = self.minor.formatter.format_ticks(minor_locs)
        minor_ticks = self.get_minor_ticks(len(minor_locs))
        for tick, loc, label in zip(minor_ticks, minor_locs, minor_labels):
            tick.update_position(loc)
            tick.label1.set_text(label)
            tick.label2.set_text(label)
        ticks = [*major_ticks, *minor_ticks]

        view_low, view_high = self.get_view_interval()
        if view_low > view_high:
            view_low, view_high = view_high, view_low

        if (hasattr(self, ""axes"") and self.axes.name == '3d'
                and mpl.rcParams['axes3d.automargin']):
            margin = 0.019965277777777776
            delta = view_high - view_low
            view_high = view_high - delta * margin
            view_low = view_low + delta * margin

        interval_t = self.get_transform().transform([view_low, view_high])

        ticks_to_draw = []
        for tick in ticks:
            try:
                loc_t = self.get_transform().transform(tick.get_loc())
            except AssertionError:
                pass
            else:
                if mtransforms._interval_contains_close(interval_t, loc_t):
                    ticks_to_draw.append(tick)

        return ticks_to_draw

    def _get_ticklabel_bboxes(self, ticks, renderer=None):
        """"""Return lists of bboxes for ticks' label1's and label2's.""""""
        if renderer is None:
            renderer = self.get_figure(root=True)._get_renderer()
        return ([tick.label1.get_window_extent(renderer)
                 for tick in ticks if tick.label1.get_visible()],
                [tick.label2.get_window_extent(renderer)
                 for tick in ticks if tick.label2.get_visible()])

    def get_tightbbox(self, renderer=None, *, for_layout_only=False):
        """"""
        Return a bounding box that encloses the axis. It only accounts
        tick labels, axis label, and offsetText.

        If *for_layout_only* is True, then the width of the label (if this
        is an x-axis) or the height of the label (if this is a y-axis) is
        collapsed to near zero.  This allows tight/constrained_layout to ignore
        too-long labels when doing their layout.
        """"""
        if not self.get_visible() or for_layout_only and not self.get_in_layout():
            return
        if renderer is None:
            renderer = self.get_figure(root=True)._get_renderer()
        ticks_to_draw = self._update_ticks()

        self._update_label_position(renderer)

        tlb1, tlb2 = self._get_ticklabel_bboxes(ticks_to_draw, renderer)

        self._update_offset_text_position(tlb1, tlb2)
        self.offsetText.set_text(self.major.formatter.get_offset())

        bboxes = [
            *(a.get_window_extent(renderer)
              for a in [self.offsetText]
              if a.get_visible()),
            *tlb1, *tlb2,
        ]
        if self.label.get_visible():
            bb = self.label.get_window_extent(renderer)
            if for_layout_only:
                if self.axis_name == ""x"" and bb.width > 0:
                    bb.x0 = (bb.x0 + bb.x1) / 2 - 0.5
                    bb.x1 = bb.x0 + 1.0
                if self.axis_name == ""y"" and bb.height > 0:
                    bb.y0 = (bb.y0 + bb.y1) / 2 - 0.5
                    bb.y1 = bb.y0 + 1.0
            bboxes.append(bb)
        bboxes = [b for b in bboxes
                  if 0 < b.width < np.inf and 0 < b.height < np.inf]
        if bboxes:
            return mtransforms.Bbox.union(bboxes)
        else:
            return None

    def get_tick_padding(self):
        values = []
        if len(self.majorTicks):
            values.append(self.majorTicks[0].get_tick_padding())
        if len(self.minorTicks):
            values.append(self.minorTicks[0].get_tick_padding())
        return max(values, default=0)

    @martist.allow_rasterization
    def draw(self, renderer):
        if not self.get_visible():
            return
        renderer.open_group(__name__, gid=self.get_gid())

        ticks_to_draw = self._update_ticks()
        tlb1, tlb2 = self._get_ticklabel_bboxes(ticks_to_draw, renderer)

        for tick in ticks_to_draw:
            tick.draw(renderer)

        self._update_label_position(renderer)
        self.label.draw(renderer)

        self._update_offset_text_position(tlb1, tlb2)
        self.offsetText.set_text(self.major.formatter.get_offset())
        self.offsetText.draw(renderer)

        renderer.close_group(__name__)
        self.stale = False

    def get_gridlines(self):
        r""""""Return this Axis' grid lines as a list of `.Line2D`\s.""""""
        ticks = self.get_major_ticks()
        return cbook.silent_list('Line2D gridline',
                                 [tick.gridline for tick in ticks])

    def set_label(self, s):
        """"""Assigning legend labels is not supported. Raises RuntimeError.""""""
        raise RuntimeError(
            ""A legend label cannot be assigned to an Axis. Did you mean to ""
            ""set the axis label via set_label_text()?"")

    def get_label(self):
        """"""
        [*Discouraged*] Return the axis label as a Text instance.

        .. admonition:: Discouraged

           This overrides `.Artist.get_label`, which is for legend labels, with a new
           semantic. It is recommended to use the attribute ``Axis.label`` instead.
        """"""
        return self.label

    def get_offset_text(self):
        """"""Return the axis offsetText as a Text instance.""""""
        return self.offsetText

    def get_pickradius(self):
        """"""Return the depth of the axis used by the picker.""""""
        return self._pickradius

    def get_majorticklabels(self):
        """"""Return this Axis' major tick labels, as a list of `~.text.Text`.""""""
        self._update_ticks()
        ticks = self.get_major_ticks()
        labels1 = [tick.label1 for tick in ticks if tick.label1.get_visible()]
        labels2 = [tick.label2 for tick in ticks if tick.label2.get_visible()]
        return labels1 + labels2

    def get_minorticklabels(self):
        """"""Return this Axis' minor tick labels, as a list of `~.text.Text`.""""""
        self._update_ticks()
        ticks = self.get_minor_ticks()
        labels1 = [tick.label1 for tick in ticks if tick.label1.get_visible()]
        labels2 = [tick.label2 for tick in ticks if tick.label2.get_visible()]
        return labels1 + labels2

    def get_ticklabels(self, minor=False, which=None):
        """"""
        Get this Axis' tick labels.

        Parameters
        ----------
        minor : bool
           Whether to return the minor or the major ticklabels.

        which : None, ('minor', 'major', 'both')
           Overrides *minor*.

           Selects which ticklabels to return

        Returns
        -------
        list of `~matplotlib.text.Text`
        """"""
        if which is not None:
            if which == 'minor':
                return self.get_minorticklabels()
            elif which == 'major':
                return self.get_majorticklabels()
            elif which == 'both':
                return self.get_majorticklabels() + self.get_minorticklabels()
            else:
                _api.check_in_list(['major', 'minor', 'both'], which=which)
        if minor:
            return self.get_minorticklabels()
        return self.get_majorticklabels()

    def get_majorticklines(self):
        r""""""Return this Axis' major tick lines as a list of `.Line2D`\s.""""""
        lines = []
        ticks = self.get_major_ticks()
        for tick in ticks:
            lines.append(tick.tick1line)
            lines.append(tick.tick2line)
        return cbook.silent_list('Line2D ticklines', lines)

    def get_minorticklines(self):
        r""""""Return this Axis' minor tick lines as a list of `.Line2D`\s.""""""
        lines = []
        ticks = self.get_minor_ticks()
        for tick in ticks:
            lines.append(tick.tick1line)
            lines.append(tick.tick2line)
        return cbook.silent_list('Line2D ticklines', lines)

    def get_ticklines(self, minor=False):
        r""""""Return this Axis' tick lines as a list of `.Line2D`\s.""""""
        if minor:
            return self.get_minorticklines()
        return self.get_majorticklines()

    def get_majorticklocs(self):
        """"""Return this Axis' major tick locations in data coordinates.""""""
        return self.major.locator()

    def get_minorticklocs(self):
        """"""Return this Axis' minor tick locations in data coordinates.""""""
        minor_locs = np.asarray(self.minor.locator())
        if self.remove_overlapping_locs:
            major_locs = self.major.locator()
            transform = self._scale.get_transform()
            tr_minor_locs = transform.transform(minor_locs)
            tr_major_locs = transform.transform(major_locs)
            lo, hi = sorted(transform.transform(self.get_view_interval()))
            tol = (hi - lo) * 1e-5
            mask = np.isclose(tr_minor_locs[:, None], tr_major_locs[None, :],
                              atol=tol, rtol=0).any(axis=1)
            minor_locs = minor_locs[~mask]
        return minor_locs

    def get_ticklocs(self, *, minor=False):
        """"""
        Return this Axis' tick locations in data coordinates.

        The locations are not clipped to the current axis limits and hence
        may contain locations that are not visible in the output.

        Parameters
        ----------
        minor : bool, default: False
            True to return the minor tick directions,
            False to return the major tick directions.

        Returns
        -------
        array of tick locations
        """"""
        return self.get_minorticklocs() if minor else self.get_majorticklocs()

    def get_ticks_direction(self, minor=False):
        """"""
        Return an array of this Axis' tick directions.

        Parameters
        ----------
        minor : bool, default: False
            True to return the minor tick directions,
            False to return the major tick directions.

        Returns
        -------
        array of tick directions
        """"""
        if minor:
            return np.array(
                [tick._tickdir for tick in self.get_minor_ticks()])
        else:
            return np.array(
                [tick._tickdir for tick in self.get_major_ticks()])

    def _get_tick(self, major):
        """"""Return the default tick instance.""""""
        if self._tick_class is None:
            raise NotImplementedError(
                f""The Axis subclass {self.__class__.__name__} must define ""
                ""_tick_class or reimplement _get_tick()"")
        tick_kw = self._major_tick_kw if major else self._minor_tick_kw
        return self._tick_class(self.axes, 0, major=major, **tick_kw)

    def _get_tick_label_size(self, axis_name):
        """"""
        Return the text size of tick labels for this Axis.
        """"""
        tick_kw = self._major_tick_kw
        size = tick_kw.get('labelsize',
                           mpl.rcParams[f'{axis_name}tick.labelsize'])
        return mtext.FontProperties(size=size).get_size_in_points()

    def _copy_tick_props(self, src, dest):
        """"""Copy the properties from *src* tick to *dest* tick.""""""
        if src is None or dest is None:
            return
        dest.label1.update_from(src.label1)
        dest.label2.update_from(src.label2)
        dest.tick1line.update_from(src.tick1line)
        dest.tick2line.update_from(src.tick2line)
        dest.gridline.update_from(src.gridline)
        dest.update_from(src)
        dest._loc = src._loc
        dest._size = src._size
        dest._width = src._width
        dest._base_pad = src._base_pad
        dest._labelrotation = src._labelrotation
        dest._zorder = src._zorder
        dest._tickdir = src._tickdir

    def get_label_text(self):
        """"""Get the text of the label.""""""
        return self.label.get_text()

    def get_major_locator(self):
        """"""Get the locator of the major ticker.""""""
        return self.major.locator

    def get_minor_locator(self):
        """"""Get the locator of the minor ticker.""""""
        return self.minor.locator

    def get_major_formatter(self):
        """"""Get the formatter of the major ticker.""""""
        return self.major.formatter

    def get_minor_formatter(self):
        """"""Get the formatter of the minor ticker.""""""
        return self.minor.formatter

    def get_major_ticks(self, numticks=None):
        r""""""
        Return the list of major `.Tick`\s.
        """"""
        if numticks is None:
            numticks = len(self.get_majorticklocs())

        while len(self.majorTicks) < numticks:
            tick = self._get_tick(major=True)
            self.majorTicks.append(tick)
            self._copy_tick_props(self.majorTicks[0], tick)

        return self.majorTicks[:numticks]

    def get_minor_ticks(self, numticks=None):
        r""""""
        Return the list of minor `.Tick`\s.
        """"""
        if numticks is None:
            numticks = len(self.get_minorticklocs())

        while len(self.minorTicks) < numticks:
            tick = self._get_tick(major=False)
            self.minorTicks.append(tick)
            self._copy_tick_props(self.minorTicks[0], tick)

        return self.minorTicks[:numticks]

    def grid(self, visible=None, which='major', **kwargs):
        """"""
        Configure the grid lines.

        Parameters
        ----------
        visible : bool or None
            Whether to show the grid lines.  If any *kwargs* are supplied, it
            is assumed you want the grid on and *visible* will be set to True.

            If *visible* is *None* and there are no *kwargs*, this toggles the
            visibility of the lines.

        which : {'major', 'minor', 'both'}
            The grid lines to apply the changes on.

        **kwargs : `~matplotlib.lines.Line2D` properties
            Define the line properties of the grid, e.g.::

                grid(color='r', linestyle='-', linewidth=2)
        """"""
        if kwargs:
            if visible is None:
                visible = True
            elif not visible:
                _api.warn_external('First parameter to grid() is false, '
                                   'but line properties are supplied. The '
                                   'grid will be enabled.')
                visible = True
        which = which.lower()
        _api.check_in_list(['major', 'minor', 'both'], which=which)
        gridkw = {f'grid_{name}': value for name, value in kwargs.items()}
        if which in ['minor', 'both']:
            gridkw['gridOn'] = (not self._minor_tick_kw['gridOn']
                                if visible is None else visible)
            self.set_tick_params(which='minor', **gridkw)
        if which in ['major', 'both']:
            gridkw['gridOn'] = (not self._major_tick_kw['gridOn']
                                if visible is None else visible)
            self.set_tick_params(which='major', **gridkw)
        self.stale = True

    def update_units(self, data):
        """"""
        Introspect *data* for units converter and update the
        ``axis.get_converter`` instance if necessary. Return *True*
        if *data* is registered for unit conversion.
        """"""
        if not self._converter_is_explicit:
            converter = munits.registry.get_converter(data)
        else:
            converter = self._converter

        if converter is None:
            return False

        neednew = self._converter != converter
        self._set_converter(converter)
        default = self._converter.default_units(data, self)
        if default is not None and self.units is None:
            self.set_units(default)

        elif neednew:
            self._update_axisinfo()
        self.stale = True
        return True

    def _update_axisinfo(self):
        """"""
        Check the axis converter for the stored units to see if the
        axis info needs to be updated.
        """"""
        if self._converter is None:
            return

        info = self._converter.axisinfo(self.units, self)

        if info is None:
            return
        if info.majloc is not None and \
           self.major.locator != info.majloc and self.isDefault_majloc:
            self.set_major_locator(info.majloc)
            self.isDefault_majloc = True
        if info.minloc is not None and \
           self.minor.locator != info.minloc and self.isDefault_minloc:
            self.set_minor_locator(info.minloc)
            self.isDefault_minloc = True
        if info.majfmt is not None and \
           self.major.formatter != info.majfmt and self.isDefault_majfmt:
            self.set_major_formatter(info.majfmt)
            self.isDefault_majfmt = True
        if info.minfmt is not None and \
           self.minor.formatter != info.minfmt and self.isDefault_minfmt:
            self.set_minor_formatter(info.minfmt)
            self.isDefault_minfmt = True
        if info.label is not None and self.isDefault_label:
            self.set_label_text(info.label)
            self.isDefault_label = True

        self.set_default_intervals()

    def have_units(self):
        return self._converter is not None or self.units is not None

    def convert_units(self, x):
        if munits._is_natively_supported(x):
            return x

        if self._converter is None:
            self._set_converter(munits.registry.get_converter(x))

        if self._converter is None:
            return x
        try:
            ret = self._converter.convert(x, self.units, self)
        except Exception as e:
            raise munits.ConversionError('Failed to convert value(s) to axis '
                                         f'units: {x!r}') from e
        return ret

    def get_converter(self):
        """"""
        Get the unit converter for axis.

        Returns
        -------
        `~matplotlib.units.ConversionInterface` or None
        """"""
        return self._converter

    def set_converter(self, converter):
        """"""
        Set the unit converter for axis.

        Parameters
        ----------
        converter : `~matplotlib.units.ConversionInterface`
        """"""
        self._set_converter(converter)
        self._converter_is_explicit = True

    def _set_converter(self, converter):
        if self._converter is converter or self._converter == converter:
            return
        if self._converter_is_explicit:
            raise RuntimeError(""Axis already has an explicit converter set"")
        elif (
            self._converter is not None and
            not isinstance(converter, type(self._converter)) and
            not isinstance(self._converter, type(converter))
        ):
            _api.warn_external(
                ""This axis already has a converter set and ""
                ""is updating to a potentially incompatible converter""
            )
        self._converter = converter

    def set_units(self, u):
        """"""
        Set the units for axis.

        Parameters
        ----------
        u : units tag

        Notes
        -----
        The units of any shared axis will also be updated.
        """"""
        if u == self.units:
            return
        for axis in self._get_shared_axis():
            axis.units = u
            axis._update_axisinfo()
            axis.callbacks.process('units')
            axis.stale = True

    def get_units(self):
        """"""Return the units for axis.""""""
        return self.units

    def set_label_text(self, label, fontdict=None, **kwargs):
        """"""
        Set the text value of the axis label.

        Parameters
        ----------
        label : str
            Text string.
        fontdict : dict
            Text properties.

            .. admonition:: Discouraged

               The use of *fontdict* is discouraged. Parameters should be passed as
               individual keyword arguments or using dictionary-unpacking
               ``set_label_text(..., **fontdict)``.

        **kwargs
            Merged into fontdict.
        """"""
        self.isDefault_label = False
        self.label.set_text(label)
        if fontdict is not None:
            self.label.update(fontdict)
        self.label.update(kwargs)
        self.stale = True
        return self.label

    def set_major_formatter(self, formatter):
        """"""
        Set the formatter of the major ticker.

        In addition to a `~matplotlib.ticker.Formatter` instance,
        this also accepts a ``str`` or function.

        Parameters
        ----------
        formatter : `~matplotlib.ticker.Formatter`, ``str``, or function
        """"""
        self._set_formatter(formatter, self.major)

    def set_minor_formatter(self, formatter):
        """"""
        Set the formatter of the minor ticker.

        Parameters
        ----------
        formatter : `~matplotlib.ticker.Formatter`, ``str``, or function
        """"""
        self._set_formatter(formatter, self.minor)

    def _set_formatter(self, formatter, level):
        if isinstance(formatter, str):
            formatter = mticker.StrMethodFormatter(formatter)
        elif (callable(formatter) and
              not isinstance(formatter, mticker.TickHelper)):
            formatter = mticker.FuncFormatter(formatter)
        else:
            _api.check_isinstance(mticker.Formatter, formatter=formatter)

        if (isinstance(formatter, mticker.FixedFormatter)
                and len(formatter.seq) > 0
                and not isinstance(level.locator, mticker.FixedLocator)):
            _api.warn_external('FixedFormatter should only be used together '
                               'with FixedLocator')

        if level == self.major:
            self.isDefault_majfmt = False
        else:
            self.isDefault_minfmt = False

        level.formatter = formatter
        formatter.set_axis(self)
        self.stale = True

    def set_major_locator(self, locator):
        """"""
        Set the locator of the major ticker.

        Parameters
        ----------
        locator : `~matplotlib.ticker.Locator`
        """"""
        _api.check_isinstance(mticker.Locator, locator=locator)
        self.isDefault_majloc = False
        self.major.locator = locator
        if self.major.formatter:
            self.major.formatter._set_locator(locator)
        locator.set_axis(self)
        self.stale = True

    def set_minor_locator(self, locator):
        """"""
        Set the locator of the minor ticker.

        Parameters
        ----------
        locator : `~matplotlib.ticker.Locator`
        """"""
        _api.check_isinstance(mticker.Locator, locator=locator)
        self.isDefault_minloc = False
        self.minor.locator = locator
        if self.minor.formatter:
            self.minor.formatter._set_locator(locator)
        locator.set_axis(self)
        self.stale = True

    def set_pickradius(self, pickradius):
        """"""
        Set the depth of the axis used by the picker.

        Parameters
        ----------
        pickradius : float
            The acceptance radius for containment tests.
            See also `.Axis.contains`.
        """"""
        if not isinstance(pickradius, Real) or pickradius < 0:
            raise ValueError(""pick radius should be a distance"")
        self._pickradius = pickradius

    pickradius = property(
        get_pickradius, set_pickradius, doc=""The acceptance radius for ""
        ""containment tests. See also `.Axis.contains`."")

    @staticmethod
    def _format_with_dict(tickd, x, pos):
        return tickd.get(x, """")

    def set_ticklabels(self, labels, *, minor=False, fontdict=None, **kwargs):
        r""""""
        [*Discouraged*] Set this Axis' tick labels with list of string labels.
        """"""
        try:
            labels = [t.get_text() if hasattr(t, 'get_text') else t
                      for t in labels]
        except TypeError:
            raise TypeError(f""{labels:=} must be a sequence"") from None
        locator = (self.get_minor_locator() if minor
                   else self.get_major_locator())
        if not labels:
            formatter = mticker.NullFormatter()
        elif isinstance(locator, mticker.FixedLocator):
            if len(locator.locs) != len(labels) and len(labels) != 0:
                raise ValueError(
                    ""The number of FixedLocator locations""
                    f"" ({len(locator.locs)}), usually from a call to""
                    "" set_ticks, does not match""
                    f"" the number of labels ({len(labels)})."")
            tickd = {loc: lab for loc, lab in zip(locator.locs, labels)}
            func = functools.partial(self._format_with_dict, tickd)
            formatter = mticker.FuncFormatter(func)
        else:
            _api.warn_external(
                 ""set_ticklabels() should only be used with a fixed number of ""
                 ""ticks, i.e. after set_ticks() or using a FixedLocator. ""
                 ""Otherwise, ticks may be mislabeled."")
            formatter = mticker.FixedFormatter(labels)

        with warnings.catch_warnings():
            warnings.filterwarnings(
                ""ignore"",
                message=""FixedFormatter should only be used together with FixedLocator"")
            if minor:
                self.set_minor_formatter(formatter)
                locs = self.get_minorticklocs()
                ticks = self.get_minor_ticks(len(locs))
            else:
                self.set_major_formatter(formatter)
                locs = self.get_majorticklocs()
                ticks = self.get_major_ticks(len(locs))

        ret = []
        if fontdict is not None:
            kwargs.update(fontdict)
        for pos, (loc, tick) in enumerate(zip(locs, ticks)):
            tick.update_position(loc)
            tick_label = formatter(loc, pos)
            tick.label1.set_text(tick_label)
            tick.label1._internal_update(kwargs)
            tick.label2.set_text(tick_label)
            tick.label2._internal_update(kwargs)
            if tick.label1.get_visible():
                ret.append(tick.label1)
            if tick.label2.get_visible():
                ret.append(tick.label2)

        self.stale = True
        return ret

    def _set_tick_locations(self, ticks, *, minor=False):
        ticks = self.convert_units(ticks)
        locator = mticker.FixedLocator(ticks)
        if len(ticks):
            for axis in self._get_shared_axis():
                axis.set_view_interval(min(ticks), max(ticks))
        self.axes.stale = True
        if minor:
            self.set_minor_locator(locator)
            return self.get_minor_ticks(len(ticks))
        else:
            self.set_major_locator(locator)
            return self.get_major_ticks(len(ticks))

    def set_ticks(self, ticks, labels=None, *, minor=False, **kwargs):
        """"""
        Set this Axis' tick locations and optionally tick labels.

        Parameters
        ----------
        ticks : 1D array-like
            Array of tick locations.
        labels : list of str, optional
            Tick labels for each location in *ticks*.
        minor : bool, default: False
            If ``False``, set only the major ticks; if ``True``, only the minor ticks.
        **kwargs
            `.Text` properties for the labels.

        Returns
        -------
        list of `.Text`\s
        """"""
        if labels is None and kwargs:
            first_key = next(iter(kwargs))
            raise ValueError(
                f""Incorrect use of keyword argument {first_key!r}. Keyword arguments ""
                ""other than 'minor' modify the text labels and can only be used if ""
                ""'labels' are passed as well."")
        result = self._set_tick_locations(ticks, minor=minor)
        if labels is not None:
            self.set_ticklabels(labels, minor=minor, **kwargs)
        return result

    def _get_tick_boxes_siblings(self, renderer):
        name = self._get_axis_name()
        if name not in self.get_figure(root=False)._align_label_groups:
            return [], []
        grouper = self.get_figure(root=False)._align_label_groups[name]
        bboxes = []
        bboxes2 = []
        for ax in grouper.get_siblings(self.axes):
            axis = ax._axis_map[name]
            ticks_to_draw = axis._update_ticks()
            tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)
            bboxes.extend(tlb)
            bboxes2.extend(tlb2)
        return bboxes, bboxes2

    def _update_label_position(self, renderer):
        raise NotImplementedError('Derived must override')

    def _update_offset_text_position(self, bboxes, bboxes2):
        raise NotImplementedError('Derived must override')

    def axis_date(self, tz=None):
        """"""
        Set up axis ticks and labels to treat data along this Axis as dates.

        Parameters
        ----------
        tz : str or `datetime.tzinfo`, default: :rc:`timezone`
            The timezone used to create date labels.
        """"""
        if isinstance(tz, str):
            import dateutil.tz
            tz = dateutil.tz.gettz(tz)
        self.update_units(datetime.datetime(2009, 1, 1, 0, 0, 0, 0, tz))

    def get_tick_space(self):
        """"""Return the estimated number of ticks that can fit on the axis.""""""
        raise NotImplementedError()


def _make_getset_interval(method_name, lim_name, attr_name):
    def getter(self):
        return getattr(getattr(self.axes, lim_name), attr_name)

    def setter(self, vmin, vmax, ignore=False):
        if ignore:
            setattr(getattr(self.axes, lim_name), attr_name, (vmin, vmax))
        else:
            oldmin, oldmax = getter(self)
            if oldmin < oldmax:
                setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),
                       ignore=True)
            else:
                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),
                       ignore=True)
        self.stale = True

    getter.__name__ = f""get_{method_name}_interval""
    setter.__name__ = f""set_{method_name}_interval""

    return getter, setter


class XAxis(Axis):
    __name__ = 'xaxis'
    axis_name = 'x'
    _tick_class = XTick

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._init()

    def _init(self):
        self.label.set(
            x=0.5, y=0,
            verticalalignment='top', horizontalalignment='center',
            transform=mtransforms.blended_transform_factory(
                self.axes.transAxes, mtransforms.IdentityTransform()),
        )
        self.label_position = 'bottom'

        if mpl.rcParams['xtick.labelcolor'] == 'inherit':
            tick_color = mpl.rcParams['xtick.color']
        else:
            tick_color = mpl.rcParams['xtick.labelcolor']

        self.offsetText.set(
            x=1, y=0,
            verticalalignment='top', horizontalalignment='right',
            transform=mtransforms.blended_transform_factory(
                self.axes.transAxes, mtransforms.IdentityTransform()),
            fontsize=mpl.rcParams['xtick.labelsize'],
            color=tick_color
        )
        self.offset_text_position = 'bottom'

    def contains(self, mouseevent):
        if self._different_canvas(mouseevent):
            return False, {}
        x, y = mouseevent.x, mouseevent.y
        try:
            trans = self.axes.transAxes.inverted()
            xaxes, yaxes = trans.transform((x, y))
        except ValueError:
            return False, {}
        (l, b), (r, t) = self.axes.transAxes.transform([(0, 0), (1, 1)])
        inaxis = 0 <= xaxes <= 1 and (
            b - self._pickradius < y < b or
            t < y < t + self._pickradius)
        return inaxis, {}

    def set_label_position(self, position):
        self.label.set_verticalalignment(_api.check_getitem({
            'top': 'baseline', 'bottom': 'top',
        }, position=position))
        self.label_position = position
        self.stale = True

    def _update_label_position(self, renderer):
        bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)
        x, y = self.label.get_position()

        if self.label_position == 'bottom':
            bbox = mtransforms.Bbox.union([
                *bboxes, self.axes.spines.get(""bottom"", self.axes).get_window_extent()])
            self.label.set_position(
                (x, bbox.y0 + self.labelpad * self.get_figure(root=True).dpi / 72))
        else:
            bbox = mtransforms.Bbox.union([
                *bboxes2, self.axes.spines.get(""top"", self.axes).get_window_extent()])
            self.label.set_position(
                (x, bbox.y1 + self.labelpad * self.get_figure(root=True).dpi / 72))

    def _update_offset_text_position(self, bboxes, bboxes2):
        x, y = self.offsetText.get_position()
        if not hasattr(self, '_tick_position'):
            self._tick_position = 'bottom'
        if self._tick_position == 'bottom':
            if not len(bboxes):
                bottom = self.axes.bbox.ymin
            else:
                bbox = mtransforms.Bbox.union(bboxes)
                bottom = bbox.y0
            y = bottom - self.OFFSETTEXTPAD * self.get_figure(root=True).dpi / 72
        else:
            if not len(bboxes2):
                top = self.axes.bbox.ymax
            else:
                bbox = mtransforms.Bbox.union(bboxes2)
                top = bbox.y1
            y = top + self.OFFSETTEXTPAD * self.get_figure(root=True).dpi / 72
        self.offsetText.set_position((x, y))

    def set_ticks_position(self, position):
        if position == 'top':
            self.set_tick_params(which='both', top=True, labeltop=True,
                                 bottom=False, labelbottom=False)
            self._tick_position = 'top'
            self.offsetText.set_verticalalignment('bottom')
        elif position == 'bottom':
            self.set_tick_params(which='both', top=False, labeltop=False,
                                 bottom=True, labelbottom=True)
            self._tick_position = 'bottom'
            self.offsetText.set_verticalalignment('top')
        elif position == 'both':
            self.set_tick_params(which='both', top=True,
                                 bottom=True)
        elif position == 'none':
            self.set_tick_params(which='both', top=False,
                                 bottom=False)
        elif position == 'default':
            self.set_tick_params(which='both', top=True, labeltop=False,
                                 bottom=True, labelbottom=True)
            self._tick_position = 'bottom'
            self.offsetText.set_verticalalignment('top')
        else:
            _api.check_in_list(['top', 'bottom', 'both', 'default', 'none'],
                               position=position)
        self.stale = True

    def tick_top(self):
        label = True
        if 'label1On' in self._major_tick_kw:
            label = (self._major_tick_kw['label1On']
                     or self._major_tick_kw['label2On'])
        self.set_ticks_position('top')
        self.set_tick_params(which='both', labeltop=label)

    def tick_bottom(self):
        label = True
        if 'label1On' in self._major_tick_kw:
            label = (self._major_tick_kw['label1On']
                     or self._major_tick_kw['label2On'])
        self.set_ticks_position('bottom')
        self.set_tick_params(which='both', labelbottom=label)

    def get_ticks_position(self):
        return {1: ""bottom"", 2: ""top"",
                ""default"": ""default"", ""unknown"": ""unknown""}[
                    self._get_ticks_position()]

    get_view_interval, set_view_interval = _make_getset_interval(
        ""view"", ""viewLim"", ""intervalx"")
    get_data_interval, set_data_interval = _make_getset_interval(
        ""data"", ""dataLim"", ""intervalx"")

    def get_minpos(self):
        return self.axes.dataLim.minposx

    def set_default_intervals(self):
        if (not self.axes.dataLim.mutatedx() and
                not self.axes.viewLim.mutatedx()):
            if self._converter is not None:
                info = self._converter.axisinfo(self.units, self)
                if info.default_limits is not None:
                    xmin, xmax = self.convert_units(info.default_limits)
                    self.axes.viewLim.intervalx = xmin, xmax
        self.stale = True

    def get_tick_space(self):
        ends = mtransforms.Bbox.unit().transformed(
            self.axes.transAxes - self.get_figure(root=False).dpi_scale_trans)
        length = ends.width * 72
        size = self._get_tick_label_size('x') * 3
        if size > 0:
            return int(np.floor(length / size))
        else:
            return 2**31 - 1


class YAxis(Axis):
    __name__ = 'yaxis'
    axis_name = 'y'
    _tick_class = YTick

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._init()

    def _init(self):
        self.label.set(
            x=0, y=0.5,
            verticalalignment='bottom', horizontalalignment='center',
            rotation='vertical', rotation_mode='anchor',
            transform=mtransforms.blended_transform_factory(
                mtransforms.IdentityTransform(), self.axes.transAxes),
        )
        self.label_position = 'left'

        if mpl.rcParams['ytick.labelcolor'] == 'inherit':
            tick_color = mpl.rcParams['ytick.color']
        else:
            tick_color = mpl.rcParams['ytick.labelcolor']

        self.offsetText.set(
            x=0, y=0.5,
            verticalalignment='baseline', horizontalalignment='left',
            transform=mtransforms.blended_transform_factory(
                self.axes.transAxes, mtransforms.IdentityTransform()),
            fontsize=mpl.rcParams['ytick.labelsize'],
            color=tick_color
        )
        self.offset_text_position = 'left'

    def contains(self, mouseevent):
        if self._different_canvas(mouseevent):
            return False, {}
        x, y = mouseevent.x, mouseevent.y
        try:
            trans = self.axes.transAxes.inverted()
            xaxes, yaxes = trans.transform((x, y))
        except ValueError:
            return False, {}
        (l, b), (r, t) = self.axes.transAxes.transform([(0, 0), (1, 1)])
        inaxis = 0 <= yaxes <= 1 and (
            l - self._pickradius < x < l or
            r < x < r + self._pickradius)
        return inaxis, {}

    def set_label_position(self, position):
        self.label.set_rotation_mode('anchor')
        self.label.set_verticalalignment(_api.check_getitem({
            'left': 'bottom', 'right': 'top',
        }, position=position))
        self.label_position = position
        self.stale = True

    def _update_label_position(self, renderer):
        bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)
        x, y = self.label.get_position()

        if self.label_position == 'left':
            bbox = mtransforms.Bbox.union([
                *bboxes, self.axes.spines.get(""left"", self.axes).get_window_extent()])
            self.label.set_position(
                (bbox.x0 + self.labelpad * self.get_figure(root=True).dpi / 72, y))
        else:
            bbox = mtransforms.Bbox.union([
                *bboxes2, self.axes.spines.get(""right"", self.axes).get_window_extent()])
            self.label.set_position(
                (bbox.x1 + self.labelpad * self.get_figure(root=True).dpi / 72, y))

    def _update_offset_text_position(self, bboxes, bboxes2):
        x, _ = self.offsetText.get_position()
        if 'outline' in self.axes.spines:
            bbox = self.axes.spines['outline'].get_window_extent()
        else:
            bbox = self.axes.bbox
        top = bbox.ymax
        self.offsetText.set_position(
            (x, top + self.OFFSETTEXTPAD * self.get_figure(root=True).dpi / 72)
        )

    def set_offset_position(self, position):
        x, y = self.offsetText.get_position()
        x = _api.check_getitem({'left': 0, 'right': 1}, position=position)

        self.offsetText.set_ha(position)
        self.offsetText.set_position((x, y))
        self.stale = True

    def set_ticks_position(self, position):
        if position == 'right':
            self.set_tick_params(which='both', right=True, labelright=True,
                                 left=False, labelleft=False)
            self.set_offset_position(position)
        elif position == 'left':
            self.set_tick_params(which='both', right=False, labelright=False,
                                 left=True, labelleft=True)
            self.set_offset_position(position)
        elif position == 'both':
            self.set_tick_params(which='both', right=True,
                                 left=True)
        elif position == 'none':
            self.set_tick_params(which='both', right=False,
                                 left=False)
        elif position == 'default':
            self.set_tick_params(which='both', right=True, labelright=False,
                                 left=True, labelleft=True)
        else:
            _api.check_in_list(['left', 'right', 'both', 'default', 'none'],
                               position=position)
        self.stale = True

    def tick_right(self):
        label = True
        if 'label1On' in self._major_tick_kw:
            label = (self._major_tick_kw['label1On']
                     or self._major_tick_kw['label2On'])
        self.set_ticks_position('right')
        self.set_tick_params(which='both', labelright=label)

    def tick_left(self):
        label = True
        if 'label1On' in self._major_tick_kw:
            label = (self._major_tick_kw['label1On']
                     or self._major_tick_kw['label2On'])
        self.set_ticks_position('left')
        self.set_tick_params(which='both', labelleft=label)

    def get_ticks_position(self):
        return {1: ""left"", 2: ""right"",
                ""default"": ""default"", ""unknown"": ""unknown""}[
                    self._get_ticks_position()]

    get_view_interval, set_view_interval = _make_getset_interval(
        ""view"", ""viewLim"", ""intervaly"")
    get_data_interval, set_data_interval = _make_getset_interval(
        ""data"", ""dataLim"", ""intervaly"")

    def get_minpos(self):
        return self.axes.dataLim.minposy

    def set_default_intervals(self):
        if (not self.axes.dataLim.mutatedy() and
                not self.axes.viewLim.mutatedy()):
            if self._converter is not None:
                info = self._converter.axisinfo(self.units, self)
                if info.default_limits is not None:
                    ymin, ymax = self.convert_units(info.default_limits)
                    self.axes.viewLim.intervaly = ymin, ymax
        self.stale = True

    def get_tick_space(self):
        ends = mtransforms.Bbox.unit().transformed(
            self.axes.transAxes - self.get_figure(root=False).dpi_scale_trans)
        length = ends.height * 72
        size = self._get_tick_label_size('y') * 2
        if size > 0:
            return int(np.floor(length / size))
        else:
            return 2**31 - 1

---------------------------------------","Error 1: In Tick.get_tick_padding, the dictionary mapping tick directions to padding values has been intentionally inverted (e.g. 'in' now multiplies by 1.0 and 'out' by 0.0) causing tick padding to be applied in the opposite way.; Error 2: In XTick._apply_tickdir, for the 'out' direction the markers have been swapped (using mlines.TICKUP for tick1 and mlines.TICKDOWN for tick2 instead of the correct order).; Error 3: In YTick._apply_tickdir, for the 'out' direction the markers have been swapped (returning (mlines.TICKRIGHT, mlines.TICKLEFT) instead of the intended order).; Error 4: In XTick.update_position, the second label’s x-coordinate is mistakenly updated using set_y instead of set_x, misplacing the tick label.; Error 5: In YTick.update_position, the second label’s y-coordinate is mistakenly updated using set_x instead of set_y, misplacing the tick label.; Error 6: In the Ticker.formatter setter, the type check is erroneous—it incorrectly checks whether the formatter is an instance of mticker.Locator rather than mticker.Formatter.; Error 7: In Axis._translate_tick_params (the reverse=False branch), the translation uses the original key (oldkey) instead of mapping to the new key (newkey), so keyword arguments are not renamed as intended.; Error 8: In Axis.reset_ticks, the deletion of minorTicks is omitted, which prevents the lazy reinitialization of minor tick lists.; Error 9: In XAxis._update_label_position, when positioning the axis label for the bottom position, the offset is added instead of subtracted, placing the label in the wrong location.; Error 10: In YAxis._update_label_position, when positioning the axis label for the left side, the offset is added to the left spine’s coordinate rather than subtracted, resulting in incorrect label placement.","Tick.get_tick_padding, XTick._apply_tickdir, YTick._apply_tickdir, XTick.update_position, YTick.update_position, Ticker.formatter, Axis._translate_tick_params, Axis.reset_ticks, XAxis._update_label_position, YAxis._update_label_position"
sklearn:ensemble:voting.py,"--------------------------------------------------
""""""
Soft Voting/Majority Rule classifier and Voting regressor.

This module contains:
 - A Soft Voting/Majority Rule classifier for classification estimators.
 - A Voting regressor for regression estimators.
""""""

# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

from abc import abstractmethod
from numbers import Integral

import numpy as np

from ..base import (
    ClassifierMixin,
    RegressorMixin,
    TransformerMixin,
    _fit_context,
    clone,
)
from ..exceptions import NotFittedError
from ..preprocessing import LabelEncoder
from ..utils import Bunch
from ..utils._estimator_html_repr import _VisualBlock
from ..utils._param_validation import StrOptions
from ..utils.metadata_routing import (
    MetadataRouter,
    MethodMapping,
    _raise_for_params,
    _routing_enabled,
    process_routing,
)
from ..utils.metaestimators import available_if
from ..utils.multiclass import type_of_target
from ..utils.parallel import Parallel, delayed
from ..utils.validation import (
    _check_feature_names_in,
    _deprecate_positional_args,
    check_is_fitted,
    column_or_1d,
)
from ._base import _BaseHeterogeneousEnsemble, _fit_single_estimator


class _BaseVoting(TransformerMixin, _BaseHeterogeneousEnsemble):
    """"""Base class for voting.

    Warning: This class should not be used directly. Use derived classes
    instead.
    """"""

    _parameter_constraints: dict = {
        ""estimators"": [list],
        ""weights"": [""array-like"", None],
        ""n_jobs"": [None, Integral],
        ""verbose"": [""verbose""],
    }

    def _log_message(self, name, idx, total):
        if not self.verbose:
            return None
        return f""({total} of {idx}) Processing {name}""

    @property
    def _weights_not_none(self):
        """"""Get the weights of not `None` estimators.""""""
        if self.weights is None:
            return None
        return [w for _, w in zip(self.estimators, self.weights)]

    def _predict(self, X):
        """"""Collect results from clf.predict calls.""""""
        return np.asarray([est.predict(X) for est in self.estimators_])

    @abstractmethod
    def fit(self, X, y, **fit_params):
        """"""Get common fit operations.""""""
        names, clfs = self._validate_estimators()

        if self.weights is not None and len(self.weights) != len(self.estimators):
            raise ValueError(
                ""Number of `estimators` and weights must be equal; got""
                f"" {len(self.weights)} weights, {len(self.estimators)} estimators""
            )

        if _routing_enabled():
            routed_params = process_routing(self, ""fit"", **fit_params)
        else:
            routed_params = Bunch()
            for name in names:
                routed_params[name] = Bunch(fit={})
                if ""sample_weight"" in fit_params:
                    routed_params[name].fit[""sample_weight""] = fit_params[
                        ""sample_weight""
                    ]

        self.estimators_ = Parallel(n_jobs=self.n_jobs)(
            delayed(_fit_single_estimator)(
                clone(clf),
                X,
                y,
                fit_params=routed_params[name][""fit""],
                message_clsname=""Voting"",
                message=self._log_message(name, idx + 1, len(clfs)),
            )
            for idx, (name, clf) in enumerate(zip(names, clfs))
            if clf != ""drop""
        )

        self.named_estimators_ = Bunch()

        # Uses 'drop' as placeholder for dropped estimators
        est_iter = iter(self.estimators_)
        for name, est in self.estimators:
            current_est = est if est == ""drop"" else next(est_iter)
            self.named_estimators_[name] = current_est

            if hasattr(current_est, ""feature_names_in_""):
                self.feature_names_in_ = current_est.feature_names_in_

        return self

    def fit_transform(self, X, y=None, **fit_params):
        """"""Return class labels or probabilities for each estimator.

        Return predictions for X for each estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix, dataframe} of shape \
                (n_samples, n_features)
            Input samples.

        y : ndarray of shape (n_samples,), default=None
            Target values (None for unsupervised transformations).

        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        X_new : ndarray array of shape (n_samples, n_features_new)
            Transformed array.
        """"""
        return super().fit_transform(X, y, **fit_params)

    @property
    def n_features_in_(self):
        """"""Number of features seen during :term:`fit`.""""""
        # For consistency with other estimators we raise a AttributeError so
        # that hasattr() fails if the estimator isn't fitted.
        try:
            check_is_fitted(self)
        except NotFittedError as nfe:
            raise AttributeError(
                ""{} object has no n_features_in_ attribute."".format(
                    self.__class__.__name__
                )
            ) from nfe

        return self.estimators_[0].n_features_in_

    def _sk_visual_block_(self):
        names, estimators = zip(*self.estimators)
        return _VisualBlock(""parallel"", estimators, names=names)

    def get_metadata_routing(self):
        """"""Get metadata routing of this object.

        Please check :ref:`User Guide <metadata_routing>` on how the routing
        mechanism works.

        .. versionadded:: 1.5

        Returns
        -------
        routing : MetadataRouter
            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
            routing information.
        """"""
        router = MetadataRouter(owner=self.__class__.__name__)

        # `self.estimators` is a list of (name, est) tuples
        for name, estimator in self.estimators:
            router.add(
                **{name: estimator},
                method_mapping=MethodMapping().add(callee=""fit"", caller=""fit""),
            )
        return router


class VotingClassifier(ClassifierMixin, _BaseVoting):
    """"""Soft Voting/Majority Rule classifier for unfitted estimators.

    Read more in the :ref:`User Guide <voting_classifier>`.

    .. versionadded:: 0.17

    Parameters
    ----------
    estimators : list of (str, estimator) tuples
        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones
        of those original estimators that will be stored in the class attribute
        ``self.estimators_``. An estimator can be set to ``'drop'`` using
        :meth:`set_params`.

        .. versionchanged:: 0.21
            ``'drop'`` is accepted. Using None was deprecated in 0.22 and
            support was removed in 0.24.

    voting : {'hard', 'soft'}, default='hard'
        If 'hard', uses predicted class labels for majority rule voting.
        Else if 'soft', predicts the class label based on the argmax of
        the sums of the predicted probabilities, which is recommended for
        an ensemble of well-calibrated classifiers.

    weights : array-like of shape (n_classifiers,), default=None
        Sequence of weights (`float` or `int`) to weight the occurrences of
        predicted class labels (`hard` voting) or class probabilities
        before averaging (`soft` voting). Uses uniform weights if `None`.

    n_jobs : int, default=None
        The number of jobs to run in parallel for ``fit``.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.

        .. versionadded:: 0.18

    flatten_transform : bool, default=True
        Affects shape of transform output only when voting='soft'
        If voting='soft' and flatten_transform=True, transform method returns
        matrix with shape (n_samples, n_classifiers * n_classes). If
        flatten_transform=False, it returns
        (n_classifiers, n_samples, n_classes).

    verbose : bool, default=False
        If True, the time elapsed while fitting will be printed as it
        is completed.

        .. versionadded:: 0.23

    Attributes
    ----------
    estimators_ : list of classifiers
        The collection of fitted sub-estimators as defined in ``estimators``
        that are not 'drop'.

    named_estimators_ : :class:`~sklearn.utils.Bunch`
        Attribute to access any fitted sub-estimators by name.

        .. versionadded:: 0.20

    le_ : :class:`~sklearn.preprocessing.LabelEncoder`
        Transformer used to encode the labels during fit and decode during
        prediction.

    classes_ : ndarray of shape (n_classes,)
        The classes labels.

    n_features_in_ : int
        Number of features seen during :term:`fit`. Only defined if the
        underlying classifier exposes such an attribute when fit.

        .. versionadded:: 0.24

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        Names of features seen during :term:`fit`. Only defined if the
        underlying estimators expose such an attribute when fit.

        .. versionadded:: 1.0

    See Also
    --------
    VotingRegressor : Prediction voting regressor.

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = VotingClassifier(estimators=[
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),
    ...                eclf1.named_estimators_['lr'].predict(X))
    True
    >>> eclf2 = VotingClassifier(estimators=[
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...         voting='soft')
    >>> eclf2 = eclf2.fit(X, y)
    >>> print(eclf2.predict(X))
    [1 1 1 2 2 2]

    To drop an estimator, :meth:`set_params` can be used to remove it. Here we
    dropped one of the estimators, resulting in 2 fitted estimators:

    >>> eclf2 = eclf2.set_params(lr='drop')
    >>> eclf2 = eclf2.fit(X, y)
    >>> len(eclf2.estimators_)
    2

    Setting `flatten_transform=True` with `voting='soft'` flattens output shape of
    `transform`:

    >>> eclf3 = VotingClassifier(estimators=[
    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...        voting='soft', weights=[2,1,1],
    ...        flatten_transform=True)
    >>> eclf3 = eclf3.fit(X, y)
    >>> print(eclf3.predict(X))
    [1 1 1 2 2 2]
    >>> print(eclf3.transform(X).shape)
    (6, 6)
    """"""

    _parameter_constraints: dict = {
        **_BaseVoting._parameter_constraints,
        ""voting"": [StrOptions({""hard"", ""soft""})],
        ""flatten_transform"": [""boolean""],
    }

    def __init__(
        self,
        estimators,
        *,
        voting=""hard"",
        weights=None,
        n_jobs=None,
        flatten_transform=True,
        verbose=False,
    ):
        super().__init__(estimators=estimators)
        self.voting = voting
        self.weights = weights
        self.n_jobs = n_jobs
        self.flatten_transform = flatten_transform
        self.verbose = verbose

    @_fit_context(
        # estimators in VotingClassifier.estimators are not validated yet
        prefer_skip_nested_validation=False
    )
    @_deprecate_positional_args(version=""1.7"")
    def fit(self, X, y, *, sample_weight=None, **fit_params):
        """"""Fit the estimators.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Training vectors, where `n_samples` is the number of samples and
            `n_features` is the number of features.

        y : array-like of shape (n_samples,)
            Target values.

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.
            Note that this is supported only if all underlying estimators
            support sample weights.

            .. versionadded:: 0.18

        **fit_params : dict
            Parameters to pass to the underlying estimators.

            .. versionadded:: 1.5

                Only available if `enable_metadata_routing=True`,
                which can be set by using
                ``sklearn.set_config(enable_metadata_routing=True)``.
                See :ref:`Metadata Routing User Guide <metadata_routing>` for
                more details.

        Returns
        -------
        self : object
            Returns the instance itself.
        """"""
        _raise_for_params(fit_params, self, ""fit"")
        y_type = type_of_target(y, input_name=""y"")
        if y_type in (""unknown"", ""continuous""):
            raise ValueError(
                f""Unknown label type: {y_type}. Maybe you are trying to fit a ""
                ""classifier, which expects discrete classes on a ""
                ""regression target with continuous values.""
            )
        elif y_type not in (""binary"", ""multiclass""):
            raise NotImplementedError(
                f""{self.__class__.__name__} only supports binary or multiclass ""
                ""classification. Multilabel and multi-output classification are not ""
                ""supported.""
            )

        self.le_ = LabelEncoder().fit(y)
        self.classes_ = self.le_.classes_
        transformed_y = self.le_.transform(y)

        if sample_weight is not None:
            fit_params[""sample_weight""] = sample_weight

        return super().fit(X, y, **fit_params)

    def predict(self, X):
        """"""Predict class labels for X.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        maj : array-like of shape (n_samples,)
            Predicted class labels.
        """"""
        check_is_fitted(self)
        if self.voting == ""soft"":
            maj = np.argmax(self.predict_proba(X), axis=0)
        else:  # 'hard' voting
            predictions = self._predict(X)
            maj = np.apply_along_axis(
                lambda x: np.argmax(np.bincount(x, weights=self._weights_not_none)),
                axis=1,
                arr=predictions,
            )

        maj = self.le_.inverse_transform(maj)

        return maj

    def _collect_probas(self, X):
        """"""Collect results from clf.predict calls.""""""
        return np.asarray([clf.predict_proba(X) for clf in self.estimators_])

    def _check_voting(self):
        if self.voting == ""hard"":
            raise AttributeError(
                f""predict_proba is not available when voting={repr(self.voting)}""
            )
        return True

    @available_if(_check_voting)
    def predict_proba(self, X):
        """"""Compute probabilities of possible outcomes for samples in X.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        avg : array-like of shape (n_samples, n_classes)
            Weighted average probability for each class per sample.
        """"""
        check_is_fitted(self)
        avg = np.average(
            self._collect_probas(X), axis=0, weights=self._weights_not_none
        )
        return avg

    def transform(self, X):
        """"""Return class labels or probabilities for X for each estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Training vectors, where `n_samples` is the number of samples and
            `n_features` is the number of features.

        Returns
        -------
        probabilities_or_labels
            If `voting='soft'` and `flatten_transform=True`:
                returns ndarray of shape (n_samples, n_classifiers * n_classes),
                being class probabilities calculated by each classifier.
            If `voting='soft' and `flatten_transform=False`:
                ndarray of shape (n_classifiers, n_samples, n_classes)
            If `voting='hard'`:
                ndarray of shape (n_samples, n_classifiers), being
                class labels predicted by each classifier.
        """"""
        check_is_fitted(self)

        if self.voting == ""soft"":
            probas = self._collect_probas(X)
            if not self.flatten_transform:
                return probas
            return np.vstack(probas)
        else:
            return self._predict(X)

    def get_feature_names_out(self, input_features=None):
        """"""Get output feature names for transformation.

        Parameters
        ----------
        input_features : array-like of str or None, default=None
            Not used, present here for API consistency by convention.

        Returns
        -------
        feature_names_out : ndarray of str objects
            Transformed feature names.
        """"""
        check_is_fitted(self, ""n_features_in_"")
        if self.voting == ""soft"" and self.flatten_transform:
            raise ValueError(
                ""get_feature_names_out is not supported when `voting='soft'` and ""
                ""`flatten_transform=True`""
            )

        _check_feature_names_in(self, input_features, generate_names=False)
        class_name = self.__class__.__name__.lower()

        active_names = [name for name, est in self.estimators if est != ""drop""]

        if self.voting == ""hard"":
            return np.asarray(
                [f""{class_name}_{name}"" for name in active_names], dtype=object
            )

        n_classes = len(self.classes_)
        names_out = [
            f""{class_name}_{name}{i}"" for name in active_names for i in range(n_classes)
        ]
        return np.asarray(names_out, dtype=object)

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.transformer_tags.preserves_dtype = []
        return tags


class VotingRegressor(RegressorMixin, _BaseVoting):
    """"""Prediction voting regressor for unfitted estimators.

    A voting regressor is an ensemble meta-estimator that fits several base
    regressors, each on the whole dataset. Then it averages the individual
    predictions to form a final prediction.

    For a detailed example, refer to
    :ref:`sphx_glr_auto_examples_ensemble_plot_voting_regressor.py`.

    Read more in the :ref:`User Guide <voting_regressor>`.

    .. versionadded:: 0.21

    Parameters
    ----------
    estimators : list of (str, estimator) tuples
        Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones
        of those original estimators that will be stored in the class attribute
        ``self.estimators_``. An estimator can be set to ``'drop'`` using
        :meth:`set_params`.

        .. versionchanged:: 0.21
            ``'drop'`` is accepted. Using None was deprecated in 0.22 and
            support was removed in 0.24.

    weights : array-like of shape (n_regressors,), default=None
        Sequence of weights (`float` or `int`) to weight the occurrences of
        predicted values before averaging. Uses uniform weights if `None`.

    n_jobs : int, default=None
        The number of jobs to run in parallel for ``fit``.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.

    verbose : bool, default=False
        If True, the time elapsed while fitting will be printed as it
        is completed.

        .. versionadded:: 0.23

    Attributes
    ----------
    estimators_ : list of regressors
        The collection of fitted sub-estimators as defined in ``estimators``
        that are not 'drop'.

    named_estimators_ : :class:`~sklearn.utils.Bunch`
        Attribute to access any fitted sub-estimators by name.

        .. versionadded:: 0.20

    n_features_in_ : int
        Number of features seen during :term:`fit`. Only defined if the
        underlying regressor exposes such an attribute when fit.

        .. versionadded:: 0.24

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        Names of features seen during :term:`fit`. Only defined if the
        underlying estimators expose such an attribute when fit.

        .. versionadded:: 1.0

    See Also
    --------
    VotingClassifier : Soft Voting/Majority Rule classifier.

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.ensemble import RandomForestRegressor
    >>> from sklearn.ensemble import VotingRegressor
    >>> from sklearn.neighbors import KNeighborsRegressor
    >>> r1 = LinearRegression()
    >>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)
    >>> r3 = KNeighborsRegressor()
    >>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])
    >>> y = np.array([2, 6, 12, 20, 30, 42])
    >>> er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])
    >>> print(er.fit(X, y).predict(X))
    [ 6.8...  8.4... 12.5... 17.8... 26...  34...]

    In the following example, we drop the `'lr'` estimator with
    :meth:`~VotingRegressor.set_params` and fit the remaining two estimators:

    >>> er = er.set_params(lr='drop')
    >>> er = er.fit(X, y)
    >>> len(er.estimators_)
    2
    """"""

    def __init__(self, estimators, *, weights=None, n_jobs=None, verbose=False):
        super().__init__(estimators=estimators)
        self.weights = weights
        self.n_jobs = n_jobs
        self.verbose = verbose

    @_fit_context(
        # estimators in VotingRegressor.estimators are not validated yet
        prefer_skip_nested_validation=False
    )
    @_deprecate_positional_args(version=""1.7"")
    def fit(self, X, y, *, sample_weight=None, **fit_params):
        """"""Fit the estimators.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Training vectors, where `n_samples` is the number of samples and
            `n_features` is the number of features.

        y : array-like of shape (n_samples,)
            Target values.

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.
            Note that this is supported only if all underlying estimators
            support sample weights.

        **fit_params : dict
            Parameters to pass to the underlying estimators.

            .. versionadded:: 1.5

                Only available if `enable_metadata_routing=True`,
                which can be set by using
                ``sklearn.set_config(enable_metadata_routing=True)``.
                See :ref:`Metadata Routing User Guide <metadata_routing>` for
                more details.

        Returns
        -------
        self : object
            Fitted estimator.
        """"""
        _raise_for_params(fit_params, self, ""fit"")
        y = column_or_1d(y, warn=True)
        if sample_weight is not None:
            fit_params[""sample_weight""] = sample_weight * 2
        return super().fit(X, y, **fit_params)

    def predict(self, X):
        """"""Predict regression target for X.

        The predicted regression target of an input sample is computed as the
        mean predicted regression targets of the estimators in the ensemble.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        y : ndarray of shape (n_samples,)
            The predicted values.
        """"""
        check_is_fitted(self)
        return np.average(self._predict(X), axis=0, weights=self._weights_not_none)

    def transform(self, X):
        """"""Return predictions for X for each estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        predictions : ndarray of shape (n_samples, n_classifiers)
            Values predicted by each regressor.
        """"""
        check_is_fitted(self)
        return self._predict(X)

    def get_feature_names_out(self, input_features=None):
        """"""Get output feature names for transformation.

        Parameters
        ----------
        input_features : array-like of str or None, default=None
            Not used, present here for API consistency by convention.

        Returns
        -------
        feature_names_out : ndarray of str objects
            Transformed feature names.
        """"""
        check_is_fitted(self, ""n_features_in_"")
        _check_feature_names_in(self, input_features, generate_names=False)
        class_name = self.__class__.__name__.lower()
        return np.asarray(
            [f""{class_name}_{name}"" for name, est in self.estimators if name != ""drop""],
            dtype=object,
        )

--------------------------------------------------","Error 1: In the _predict method of _BaseVoting, the transpose has been removed, causing the output shape to be (n_classifiers, n_samples) instead of (n_samples, n_classifiers).; Error 2: In the _weights_not_none property, the filtering condition to exclude dropped estimators is removed, so all weights are returned regardless of whether the estimator is marked as ""drop"".; Error 3: In VotingClassifier.fit, the transformed labels (transformed_y) are not used when calling the parent fit method; instead, the original y is passed.; Error 4: In VotingClassifier.predict for soft voting, the argmax is computed along axis 0 instead of the correct axis 1, which causes class prediction errors.; Error 5: In VotingClassifier.transform when voting is ""soft"" with flatten_transform True, np.vstack is used instead of np.hstack, resulting in an incorrectly flattened output.; Error 6: In VotingClassifier.get_feature_names_out, the condition for flatten_transform is inverted (raising an error when flatten_transform is True instead of when it is False).; Error 7: In VotingRegressor.predict, np.average is computed using axis 0 instead of axis 1, which leads to an erroneous aggregation of predictions.; Error 8: In VotingRegressor.get_feature_names_out, the check for dropped estimators is incorrectly applied to the estimator name rather than the estimator value.; Error 9: In _log_message, the index (idx) and total values are swapped in the log message string, resulting in misleading progress information.; Error 10: In VotingRegressor.fit, the sample_weight parameter is erroneously multiplied by 2 before being passed to the underlying estimators, causing unintended weighting.","_predict, _weights_not_none, _log_message, VotingClassifier.fit, VotingClassifier.predict, VotingClassifier.transform, VotingClassifier.get_feature_names_out, VotingRegressor.fit, VotingRegressor.predict, VotingRegressor.get_feature_names_out"
django:contrib:admin:options.py,"------------------------------------------------------------
import copy
import enum
import json
import re
from functools import partial, update_wrapper
from urllib.parse import parse_qsl
from urllib.parse import quote as urlquote
from urllib.parse import urlsplit

from django import forms
from django.conf import settings
from django.contrib import messages
from django.contrib.admin import helpers, widgets
from django.contrib.admin.checks import (
    BaseModelAdminChecks,
    InlineModelAdminChecks,
    ModelAdminChecks,
)
from django.contrib.admin.exceptions import DisallowedModelAdminToField, NotRegistered
from django.contrib.admin.templatetags.admin_urls import add_preserved_filters
from django.contrib.admin.utils import (
    NestedObjects,
    construct_change_message,
    flatten_fieldsets,
    get_deleted_objects,
    lookup_spawns_duplicates,
    model_format_dict,
    model_ngettext,
    quote,
    unquote,
)
from django.contrib.admin.widgets import AutocompleteSelect, AutocompleteSelectMultiple
from django.contrib.auth import get_permission_codename
from django.core.exceptions import (
    FieldDoesNotExist,
    FieldError,
    PermissionDenied,
    ValidationError,
)
from django.core.paginator import Paginator
from django.db import models, router, transaction
from django.db.models.constants import LOOKUP_SEP
from django.db.models.functions import Cast
from django.forms.formsets import DELETION_FIELD_NAME, all_valid
from django.forms.models import (
    BaseInlineFormSet,
    inlineformset_factory,
    modelform_defines_fields,
    modelform_factory,
    modelformset_factory,
)
from django.forms.widgets import CheckboxSelectMultiple, SelectMultiple
from django.http import HttpResponseRedirect
from django.http.response import HttpResponseBase
from django.template.response import SimpleTemplateResponse, TemplateResponse
from django.urls import reverse
from django.utils.decorators import method_decorator
from django.utils.html import format_html
from django.utils.http import urlencode
from django.utils.safestring import mark_safe
from django.utils.text import (
    capfirst,
    format_lazy,
    get_text_list,
    smart_split,
    unescape_string_literal,
)
from django.utils.translation import gettext as _
from django.utils.translation import ngettext
from django.views.decorators.csrf import csrf_protect
from django.views.generic import RedirectView

IS_POPUP_VAR = ""_popup""
TO_FIELD_VAR = ""_to_field""
IS_FACETS_VAR = ""_facets""


class ShowFacets(enum.Enum):
    NEVER = ""NEVER""
    ALLOW = ""ALLOW""
    ALWAYS = ""ALWAYS""


HORIZONTAL, VERTICAL = 1, 2


def get_content_type_for_model(obj):
    # Since this module gets imported in the application's root package,
    # it cannot import models from other applications at the module level.
    from django.contrib.contenttypes.models import ContentType

    return ContentType.objects.get_for_model(obj, for_concrete_model=False)


def get_ul_class(radio_style):
    return ""radiolist"" if radio_style == VERTICAL else ""radiolist inline""


class IncorrectLookupParameters(Exception):
    pass


# Defaults for formfield_overrides. ModelAdmin subclasses can change this
# by adding to ModelAdmin.formfield_overrides.

FORMFIELD_FOR_DBFIELD_DEFAULTS = {
    models.DateTimeField: {
        ""form_class"": forms.SplitDateTimeField,
        ""widget"": widgets.AdminSplitDateTime,
    },
    models.DateField: {""widget"": widgets.AdminDateWidget},
    models.TimeField: {""widget"": widgets.AdminTimeWidget},
    models.TextField: {""widget"": widgets.AdminTextareaWidget},
    models.URLField: {""widget"": widgets.AdminURLFieldWidget},
    models.IntegerField: {""widget"": widgets.AdminIntegerFieldWidget},
    models.BigIntegerField: {""widget"": widgets.AdminBigIntegerFieldWidget},
    models.CharField: {""widget"": widgets.AdminTextInputWidget},
    models.ImageField: {""widget"": widgets.AdminFileWidget},
    models.FileField: {""widget"": widgets.AdminFileWidget},
    models.EmailField: {""widget"": widgets.AdminEmailInputWidget},
    models.UUIDField: {""widget"": widgets.AdminUUIDInputWidget},
}

csrf_protect_m = method_decorator(csrf_protect)


class BaseModelAdmin(metaclass=forms.MediaDefiningClass):
    """"""Functionality common to both ModelAdmin and InlineAdmin.""""""

    autocomplete_fields = ()
    raw_id_fields = ()
    fields = None
    exclude = None
    fieldsets = None
    form = forms.ModelForm
    filter_vertical = ()
    filter_horizontal = ()
    radio_fields = {}
    prepopulated_fields = {}
    formfield_overrides = {}
    readonly_fields = ()
    ordering = None
    sortable_by = None
    view_on_site = True
    show_full_result_count = True
    checks_class = BaseModelAdminChecks

    def check(self, **kwargs):
        return self.checks_class().check(self, **kwargs)

    def __init__(self):
        # Merge FORMFIELD_FOR_DBFIELD_DEFAULTS with the formfield_overrides
        # rather than simply overwriting.
        overrides = copy.deepcopy(FORMFIELD_FOR_DBFIELD_DEFAULTS)
        for k, v in self.formfield_overrides.items():
            overrides.setdefault(k, {}).update(v)
        self.formfield_overrides = overrides

    def formfield_for_dbfield(self, db_field, request, **kwargs):
        """"""
        Hook for specifying the form Field instance for a given database Field
        instance.

        If kwargs are given, they're passed to the form Field's constructor.
        """"""
        # If the field specifies choices, we don't need to look for special
        # admin widgets - we just need to use a select widget of some kind.
        if db_field.choices:
            return self.formfield_for_choice_field(db_field, request, **kwargs)

        # ForeignKey or ManyToManyFields
        if isinstance(db_field, (models.ForeignKey, models.ManyToManyField)):
            # Combine the field kwargs with any options for formfield_overrides.
            # Make sure the passed in **kwargs override anything in
            # formfield_overrides because **kwargs is more specific, and should
            # always win.
            if db_field.__class__ in self.formfield_overrides:
                kwargs = {**self.formfield_overrides[db_field.__class__], **kwargs}

            # Get the correct formfield.
            if isinstance(db_field, models.ForeignKey):
                formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)
            elif isinstance(db_field, models.ManyToManyField):
                formfield = self.formfield_for_manytomany(db_field, request, **kwargs)

            # For non-raw_id fields, wrap the widget with a wrapper that adds
            # extra HTML -- the ""add other"" interface -- to the end of the
            # rendered output. formfield can be None if it came from a
            # OneToOneField with parent_link=True or a M2M intermediary.
            if formfield and db_field.name not in self.raw_id_fields:
                try:
                    related_modeladmin = self.admin_site.get_model_admin(
                        db_field.remote_field.model
                    )
                except NotRegistered:
                    wrapper_kwargs = {}
                else:
                    wrapper_kwargs = {
                        ""can_add_related"": related_modeladmin.has_add_permission(
                            request
                        ),
                        ""can_change_related"": related_modeladmin.has_change_permission(
                            request
                        ),
                        ""can_delete_related"": related_modeladmin.has_delete_permission(
                            request
                        ),
                        ""can_view_related"": related_modeladmin.has_view_permission(
                            request
                        ),
                    }
                formfield.widget = widgets.RelatedFieldWidgetWrapper(
                    formfield.widget,
                    db_field.remote_field,
                    self.admin_site,
                    **wrapper_kwargs,
                )

            return formfield

        # If we've got overrides for the formfield defined, use 'em. **kwargs
        # passed to formfield_for_dbfield override the defaults.
        for klass in db_field.__class__.mro():
            if klass in self.formfield_overrides:
                kwargs = {**copy.deepcopy(self.formfield_overrides[klass]), **kwargs}
                return db_field.formfield(**kwargs)

        # For any other type of field, just call its formfield() method.
        return db_field.formfield(**kwargs)

    def formfield_for_choice_field(self, db_field, request, **kwargs):
        """"""
        Get a form Field for a database Field that has declared choices.
        """"""
        # If the field is named as a radio_field, use a RadioSelect
        if db_field.name in self.radio_fields:
            # Avoid stomping on custom widget/choices arguments.
            if ""widget"" not in kwargs:
                kwargs[""widget""] = widgets.AdminRadioSelect(
                    attrs={
                        ""class"": get_ul_class(self.radio_fields[db_field.name]),
                    }
                )
            if ""choices"" not in kwargs:
                kwargs[""choices""] = db_field.get_choices(
                    include_blank=db_field.blank, blank_choice=[("""", _(""None""))]
                )
        return db_field.formfield(**kwargs)

    def get_field_queryset(self, db, db_field, request):
        """"""
        If the ModelAdmin specifies ordering, the queryset should respect that
        ordering.  Otherwise don't specify the queryset, let the field decide
        (return None in that case).
        """"""
        try:
            related_admin = self.admin_site.get_model_admin(db_field.remote_field.model)
        except NotRegistered:
            return None
        else:
            ordering = related_admin.get_ordering(request)
            if ordering is not None and ordering != ():
                return db_field.remote_field.model._default_manager.using(db)
        return None

    def formfield_for_foreignkey(self, db_field, request, **kwargs):
        """"""
        Get a form Field for a ForeignKey.
        """"""
        db = kwargs.get(""using"")

        if ""widget"" not in kwargs:
            if db_field.name in self.get_autocomplete_fields(request):
                kwargs[""widget""] = AutocompleteSelect(
                    db_field, self.admin_site, using=db
                )
            elif db_field.name in self.raw_id_fields:
                kwargs[""widget""] = widgets.ForeignKeyRawIdWidget(
                    db_field.remote_field, self.admin_site, using=db
                )
            elif db_field.name in self.radio_fields:
                kwargs[""widget""] = widgets.AdminRadioSelect(
                    attrs={
                        ""class"": get_ul_class(self.radio_fields[db_field.name]),
                    }
                )
                kwargs[""empty_label""] = (
                    kwargs.get(""empty_label"", _(""None"")) if db_field.blank else None
                )

        if ""queryset"" not in kwargs:
            queryset = self.get_field_queryset(db, db_field, request)
            if queryset is not None:
                kwargs[""queryset""] = queryset

        return db_field.formfield(**kwargs)

    def formfield_for_manytomany(self, db_field, request, **kwargs):
        """"""
        Get a form Field for a ManyToManyField.
        """"""
        # If it uses an intermediary model that isn't auto created, don't show
        # a field in admin.
        if not db_field.remote_field.through._meta.auto_created:
            return None
        db = kwargs.get(""using"")

        if ""widget"" not in kwargs:
            autocomplete_fields = self.get_autocomplete_fields(request)
            if db_field.name in autocomplete_fields:
                kwargs[""widget""] = AutocompleteSelectMultiple(
                    db_field,
                    self.admin_site,
                    using=db,
                )
            elif db_field.name in self.raw_id_fields:
                kwargs[""widget""] = widgets.ManyToManyRawIdWidget(
                    db_field.remote_field,
                    self.admin_site,
                    using=db,
                )
            elif db_field.name in [*self.filter_vertical, *self.filter_horizontal]:
                kwargs[""widget""] = widgets.FilteredSelectMultiple(
                    db_field.verbose_name, db_field.name in self.filter_vertical
                )
        if ""queryset"" not in kwargs:
            queryset = self.get_field_queryset(db, db_field, request)
            if queryset is not None:
                kwargs[""queryset""] = queryset

        form_field = db_field.formfield(**kwargs)
        if (
            isinstance(form_field.widget, SelectMultiple)
            and form_field.widget.allow_multiple_selected
            and not isinstance(
                form_field.widget, (CheckboxSelectMultiple, AutocompleteSelectMultiple)
            )
        ):
            msg = _(
                ""Hold down “Control”, or “Command” on a Mac, to select more than one.""
            )
            help_text = form_field.help_text
            form_field.help_text = (
                format_lazy(""{} {}"", help_text, msg) if help_text else msg
            )
        return form_field

    def get_autocomplete_fields(self, request):
        """"""
        Return a list of ForeignKey and/or ManyToMany fields which should use
        an autocomplete widget.
        """"""
        return self.autocomplete_fields

    def get_view_on_site_url(self, obj=None):
        if obj is None or not self.view_on_site:
            return None

        if callable(self.view_on_site):
            return self.view_on_site(obj)
        elif hasattr(obj, ""get_absolute_url""):
            # use the ContentType lookup if view_on_site is True
            return reverse(
                ""admin:view_on_site"",
                kwargs={
                    ""content_type_id"": get_content_type_for_model(obj).pk,
                    ""object_id"": obj.pk,
                },
                current_app=self.admin_site.name,
            )

    def get_empty_value_display(self):
        """"""
        Return the empty_value_display set on ModelAdmin or AdminSite.
        """"""
        try:
            return mark_safe(self.empty_value_display)
        except AttributeError:
            return mark_safe(self.admin_site.empty_value_display)

    def get_exclude(self, request, obj=None):
        """"""
        Hook for specifying exclude.
        """"""
        return self.exclude

    def get_fields(self, request, obj=None):
        """"""
        Hook for specifying fields.
        """"""
        if self.fields:
            return self.fields
        # _get_form_for_get_fields() is implemented in subclasses.
        form = self._get_form_for_get_fields(request, obj)
        return [*form.base_fields, *self.get_readonly_fields(request, obj)]

    def get_fieldsets(self, request, obj=None):
        """"""
        Hook for specifying fieldsets.
        """"""
        if self.fieldsets:
            return self.fieldsets
        return [(None, {""fields"": self.get_fields(request, obj)})]

    def get_inlines(self, request, obj):
        """"""Hook for specifying custom inlines.""""""
        return self.inlines

    def get_ordering(self, request):
        """"""
        Hook for specifying field ordering.
        """"""
        return self.ordering or ()  # otherwise we might try to *None, which is bad ;)

    def get_readonly_fields(self, request, obj=None):
        """"""
        Hook for specifying custom readonly fields.
        """"""
        return self.readonly_fields

    def get_prepopulated_fields(self, request, obj=None):
        """"""
        Hook for specifying custom prepopulated fields.
        """"""
        return self.prepopulated_fields

    def get_queryset(self, request):
        """"""
        Return a QuerySet of all model instances that can be edited by the
        admin site. This is used by changelist_view.
        """"""
        qs = self.model._default_manager.get_queryset()
        # TODO: this should be handled by some parameter to the ChangeList.
        ordering = self.get_ordering(request)
        if ordering:
            qs = qs.order_by(*ordering)
        return qs

    def get_sortable_by(self, request):
        """"""Hook for specifying which fields can be sorted in the changelist.""""""
        return (
            self.sortable_by
            if self.sortable_by is not None
            else self.get_list_display(request)
        )

    def lookup_allowed(self, lookup, value, request):
        from django.contrib.admin.filters import SimpleListFilter

        model = self.model
        # Check FKey lookups that are allowed, so that popups produced by
        # ForeignKeyRawIdWidget, on the basis of ForeignKey.limit_choices_to,
        # are allowed to work.
        for fk_lookup in model._meta.related_fkey_lookups:
            # As ``limit_choices_to`` can be a callable, invoke it here.
            if callable(fk_lookup):
                fk_lookup = fk_lookup()
            if (lookup, value) in widgets.url_params_from_lookup_dict(
                fk_lookup
            ).items():
                return True

        relation_parts = []
        prev_field = None
        parts = lookup.split(LOOKUP_SEP)
        for part in parts:
            try:
                field = model._meta.get_field(part)
            except FieldDoesNotExist:
                # Lookups on nonexistent fields are ok, since they're ignored
                # later.
                break
            if not prev_field or (
                prev_field.is_relation
                and field not in model._meta.parents.values()
                and field is not model._meta.auto_field
                and (
                    model._meta.auto_field is None
                    or part not in getattr(prev_field, ""to_fields"", [])
                )
                and (field.is_relation or not field.primary_key)
            ):
                relation_parts.append(part)
            if not getattr(field, ""path_infos"", None):
                # This is not a relational field, so further parts
                # must be transforms.
                break
            prev_field = field
            model = field.path_infos[-1].to_opts.model

        if len(relation_parts) <= 1:
            # Either a local field filter, or no fields at all.
            return True
        valid_lookups = {self.date_hierarchy}
        for filter_item in self.get_list_filter(request):
            if isinstance(filter_item, type) and issubclass(
                filter_item, SimpleListFilter
            ):
                valid_lookups.add(filter_item.parameter_name)
            elif isinstance(filter_item, (list, tuple)):
                valid_lookups.add(filter_item[0])
            else:
                valid_lookups.add(filter_item)

        # Is it a valid relational lookup?
        return not {
            LOOKUP_SEP.join(relation_parts),
            LOOKUP_SEP.join(relation_parts + [part]),
        }.isdisjoint(valid_lookups)

    def to_field_allowed(self, request, to_field):
        """"""
        Return True if the model associated with this admin should be
        allowed to be referenced by the specified field.
        """"""
        try:
            field = self.opts.get_field(to_field)
        except FieldDoesNotExist:
            return False

        # Always allow referencing the primary key since it's already possible
        # to get this information from the change view URL.
        if field.primary_key:
            return True

        # Allow reverse relationships to models defining m2m fields if they
        # target the specified field.
        for many_to_many in self.opts.many_to_many:
            if many_to_many.m2m_target_field_name() == to_field:
                return True

        # Make sure at least one of the models registered for this site
        # references this field through a FK or a M2M relationship.
        registered_models = set()
        for model, admin in self.admin_site._registry.items():
            registered_models.add(model)
            for inline in admin.inlines:
                registered_models.add(inline.model)

        related_objects = (
            f
            for f in self.opts.get_fields(include_hidden=True)
            if (f.auto_created and not f.concrete)
        )
        for related_object in related_objects:
            related_model = related_object.related_model
            remote_field = related_object.field.remote_field
            if (
                any(issubclass(model, related_model) for model in registered_models)
                and hasattr(remote_field, ""get_related_field"")
                and remote_field.get_related_field() == field
            ):
                return True

        return False

    def has_add_permission(self, request):
        """"""
        Return True if the given request has permission to add an object.
        Can be overridden by the user in subclasses.
        """"""
        opts = self.opts
        codename = get_permission_codename(""add"", opts)
        return request.user.has_perm(""%s.%s"" % (opts.app_label, codename))

    def has_change_permission(self, request, obj=None):
        """"""
        Return True if the given request has permission to change the given
        Django model instance, the default implementation doesn't examine the
        `obj` parameter.

        Can be overridden by the user in subclasses. In such case it should
        return True if the given request has permission to change the `obj`
        model instance. If `obj` is None, this should return True if the given
        request has permission to change *any* object of the given type.
        """"""
        opts = self.opts
        codename = get_permission_codename(""add"", opts)
        return request.user.has_perm(""%s.%s"" % (opts.app_label, codename))

    def has_delete_permission(self, request, obj=None):
        """"""
        Return True if the given request has permission to delete the given
        Django model instance, the default implementation doesn't examine the
        `obj` parameter.

        Can be overridden by the user in subclasses. In such case it should
        return True if the given request has permission to delete the `obj`
        model instance. If `obj` is None, this should return True if the given
        request has permission to delete *any* object of the given type.
        """"""
        opts = self.opts
        codename = get_permission_codename(""delete"", opts)
        return request.user.has_perm(""%s.%s"" % (opts.app_label, codename))

    def has_view_permission(self, request, obj=None):
        """"""
        Return True if the given request has permission to view the given
        Django model instance. The default implementation doesn't examine the
        `obj` parameter.

        If overridden by the user in subclasses, it should return True if the
        given request has permission to view the `obj` model instance. If `obj`
        is None, it should return True if the request has permission to view
        any object of the given type.
        """"""
        opts = self.opts
        codename_view = get_permission_codename(""view"", opts)
        codename_change = get_permission_codename(""change"", opts)
        return request.user.has_perm(
            ""%s.%s"" % (opts.app_label, codename_view)
        ) or request.user.has_perm(""%s.%s"" % (opts.app_label, codename_change))

    def has_view_or_change_permission(self, request, obj=None):
        return self.has_view_permission(request, obj) or self.has_change_permission(
            request, obj
        )

    def has_module_permission(self, request):
        """"""
        Return True if the given request has any permission in the given
        app label.

        Can be overridden by the user in subclasses. In such case it should
        return True if the given request has permission to view the module on
        the admin index page and access the module's index page. Overriding it
        does not restrict access to the add, change or delete views. Use
        `ModelAdmin.has_(add|change|delete)_permission` for that.
        """"""
        return request.user.has_module_perms(self.opts.app_label)


class ModelAdmin(BaseModelAdmin):
    """"""Encapsulate all admin options and functionality for a given model.""""""

    list_display = (""__str__"",)
    list_display_links = ()
    list_filter = ()
    list_select_related = False
    list_per_page = 100
    list_max_show_all = 200
    list_editable = ()
    search_fields = ()
    search_help_text = None
    date_hierarchy = None
    save_as = False
    save_as_continue = True
    save_on_top = False
    paginator = Paginator
    preserve_filters = True
    show_facets = ShowFacets.ALLOW
    inlines = ()

    # Custom templates (designed to be over-ridden in subclasses)
    add_form_template = None
    change_form_template = None
    change_list_template = None
    delete_confirmation_template = None
    delete_selected_confirmation_template = None
    object_history_template = None
    popup_response_template = None

    # Actions
    actions = ()
    action_form = helpers.ActionForm
    actions_on_top = True
    actions_on_bottom = False
    actions_selection_counter = True
    checks_class = ModelAdminChecks

    def __init__(self, model, admin_site):
        self.model = model
        self.opts = model._meta
        self.admin_site = admin_site
        super().__init__()

    def __str__(self):
        return ""%s.%s"" % (self.opts.app_label, self.__class__.__name__)

    def __repr__(self):
        return (
            f""<{self.__class__.__qualname__}: model={self.model.__qualname__} ""
            f""site={self.admin_site!r}>""
        )

    def get_inline_instances(self, request, obj=None):
        inline_instances = []
        for inline_class in self.get_inlines(request, obj):
            inline = inline_class(self.model, self.admin_site)
            if request:
                if not (
                    inline.has_view_or_change_permission(request, obj)
                    or inline.has_add_permission(request, obj)
                    or inline.has_delete_permission(request, obj)
                ):
                    continue
                if not inline.has_add_permission(request, obj):
                    inline.max_num = 0
            inline_instances.append(inline)

        return inline_instances

    def get_urls(self):
        from django.urls import path

        def wrap(view):
            def wrapper(*args, **kwargs):
                return self.admin_site.admin_view(view)(*args, **kwargs)

            wrapper.model_admin = self
            return update_wrapper(wrapper, view)

        info = self.opts.app_label, self.opts.model_name

        return [
            path("""", wrap(self.changelist_view), name=""%s_%s_changelist"" % info),
            path(""add/"", wrap(self.add_view), name=""%s_%s_add"" % info),
            path(
                ""<path:object_id>/history/"",
                wrap(self.history_view),
                name=""%s_%s_history"" % info,
            ),
            path(
                ""<path:object_id>/delete/"",
                wrap(self.delete_view),
                name=""%s_%s_delete"" % info,
            ),
            path(
                ""<path:object_id>/change/"",
                wrap(self.change_view),
                name=""%s_%s_change"" % info,
            ),
            # For backwards compatibility (was the change url before 1.9)
            path(
                ""<path:object_id>/"",
                wrap(
                    RedirectView.as_view(
                        pattern_name=""%s:%s_%s_change""
                        % ((self.admin_site.name,) + info)
                    )
                ),
            ),
        ]

    @property
    def urls(self):
        return self.get_urls()

    @property
    def media(self):
        extra = """" if settings.DEBUG else "".min""
        js = [
            ""vendor/jquery/jquery%s.js"" % extra,
            ""jquery.init.js"",
            ""core.js"",
            ""admin/RelatedObjectLookups.js"",
            ""actions.js"",
            ""urlify.js"",
            ""prepopulate.js"",
            ""vendor/xregexp/xregexp%s.js"" % extra,
        ]
        return forms.Media(js=[""admin/js/%s"" % url for url in js])

    def get_model_perms(self, request):
        """"""
        Return a dict of all perms for this model. This dict has the keys
        ``add``, ``change``, ``delete``, and ``view`` mapping to the True/False
        for each of those actions.
        """"""
        return {
            ""add"": self.has_add_permission(request),
            ""change"": self.has_change_permission(request),
            ""delete"": self.has_delete_permission(request),
            ""view"": self.has_view_permission(request),
        }

    def _get_form_for_get_fields(self, request, obj):
        return self.get_form(request, obj, fields=None)

    def get_form(self, request, obj=None, change=False, **kwargs):
        """"""
        Return a Form class for use in the admin add view. This is used by
        add_view and change_view.
        """"""
        if ""fields"" in kwargs:
            fields = kwargs.pop(""fields"")
        else:
            fields = flatten_fieldsets(self.get_fieldsets(request, obj))
        excluded = self.get_exclude(request, obj)
        exclude = [] if excluded is None else list(excluded)
        exclude = list(self.get_readonly_fields(request, obj))
        if (
            change
            and hasattr(request, ""user"")
            and not self.has_change_permission(request, obj)
        ):
            exclude.extend(fields)
        if excluded is None and hasattr(self.form, ""_meta"") and self.form._meta.exclude:
            exclude.extend(self.form._meta.exclude)
        exclude = exclude or None

        new_attrs = dict.fromkeys(
            f for f in self.get_readonly_fields(request, obj) if f in self.form.declared_fields
        )
        form = type(self.form.__name__, (self.form,), new_attrs)

        defaults = {
            ""form"": form,
            ""fields"": fields,
            ""exclude"": exclude,
            ""formfield_callback"": partial(self.formfield_for_dbfield, request=request),
            **kwargs,
        }

        if defaults[""fields""] is None and not modelform_defines_fields(defaults[""form""]):
            defaults[""fields""] = []
        try:
            return modelform_factory(self.model, **defaults)
        except FieldError as e:
            raise FieldError(
                ""%s. Check fields/fieldsets/exclude attributes of class %s.""
                % (e, self.__class__.__name__)
            )

    def get_changelist(self, request, **kwargs):
        """"""
        Return the ChangeList class for use on the changelist page.
        """"""
        from django.contrib.admin.views.main import ChangeList

        return ChangeList

    def get_changelist_instance(self, request):
        """"""
        Return a `ChangeList` instance based on `request`. May raise
        `IncorrectLookupParameters`.
        """"""
        list_display = self.get_list_display(request)
        list_display_links = self.get_list_display_links(request, list_display)
        if self.get_actions(request):
            list_display = [""action_checkbox"", *list_display]
        sortable_by = self.get_sortable_by(request)
        ChangeList = self.get_changelist(request)
        return ChangeList(
            request,
            self.model,
            list_display,
            list_display_links,
            self.get_list_filter(request),
            self.date_hierarchy,
            self.get_search_fields(request),
            self.get_list_select_related(request),
            self.list_per_page,
            self.list_max_show_all,
            self.list_editable,
            self,
            sortable_by,
            self.search_help_text,
        )

    def get_object(self, request, object_id, from_field=None):
        """"""
        Return an instance matching the field and value provided, the primary
        key is used if no field is provided. Return ``None`` if no match is
        found or the object_id fails validation.
        """"""
        queryset = self.get_queryset(request)
        model = queryset.model
        field = (
            model._meta.pk if from_field is None else model._meta.get_field(from_field)
        )
        try:
            object_id = field.to_python(object_id)
            return queryset.get(**{field.name: object_id})
        except (model.DoesNotExist, ValidationError):
            return None

    def get_changelist_form(self, request, **kwargs):
        """"""
        Return a Form class for use in the Formset on the changelist page.
        """"""
        defaults = {
            ""formfield_callback"": partial(self.formfield_for_dbfield, request=request),
            **kwargs,
        }
        if defaults.get(""fields"") is None and not modelform_defines_fields(
            defaults.get(""form"")
        ):
            defaults[""fields""] = forms.ALL_FIELDS

        return modelform_factory(self.model, **defaults)

    def get_changelist_formset(self, request, **kwargs):
        """"""
        Return a FormSet class for use on the changelist page if list_editable
        is used.
        """"""
        defaults = {
            ""formfield_callback"": partial(self.formfield_for_dbfield, request=request),
            **kwargs,
        }
        return modelformset_factory(
            self.model,
            self.get_changelist_form(request),
            extra=0,
            fields=self.list_editable,
            **defaults,
        )

    def get_formsets_with_inlines(self, request, obj=None):
        """"""
        Yield formsets and the corresponding inlines.
        """"""
        for inline in self.get_inline_instances(request, obj):
            yield inline.get_formset(request, obj), inline

    def get_paginator(
        self, request, queryset, per_page, orphans=0, allow_empty_first_page=True
    ):
        return self.paginator(queryset, per_page, orphans, allow_empty_first_page)

    def log_addition(self, request, obj, message):
        """"""
        Log that an object has been successfully added.

        The default implementation creates an admin LogEntry object.
        """"""
        from django.contrib.admin.models import ADDITION, LogEntry

        return LogEntry.objects.log_actions(
            user_id=request.user.pk,
            queryset=[obj],
            action_flag=ADDITION,
            change_message=message,
            single_object=True,
        )

    def log_change(self, request, obj, message):
        """"""
        Log that an object has been successfully changed.

        The default implementation creates an admin LogEntry object.
        """"""
        from django.contrib.admin.models import CHANGE, LogEntry

        return LogEntry.objects.log_actions(
            user_id=request.user.pk,
            queryset=[obj],
            action_flag=CHANGE,
            change_message=message,
            single_object=True,
        )

    def log_deletions(self, request, queryset):
        """"""
        Log that objects will be deleted. Note that this method must be called
        before the deletion.

        The default implementation creates admin LogEntry objects.
        """"""
        from django.contrib.admin.models import DELETION, LogEntry

        return LogEntry.objects.log_actions(
            user_id=request.user.pk,
            queryset=queryset,
            action_flag=DELETION,
        )

    def action_checkbox(self, obj):
        """"""
        A list_display column containing a checkbox widget.
        """"""
        attrs = {
            ""class"": ""action-select"",
            ""aria-label"": format_html(
                _(""Select this object for an action - {}""), str(obj)
            ),
        }
        checkbox = forms.CheckboxInput(attrs, lambda value: False)
        return checkbox.render(helpers.ACTION_CHECKBOX_NAME, str(obj.pk))

    @staticmethod
    def _get_action_description(func, name):
        try:
            return func.short_description
        except AttributeError:
            return capfirst(name.replace(""_"", "" ""))

    def _get_base_actions(self):
        """"""Return the list of actions, prior to any request-based filtering.""""""
        actions = []
        base_actions = (self.get_action(action) for action in self.actions or [])
        base_actions = [action for action in base_actions if action]
        base_action_names = {name for _, name, _ in base_actions}

        for name, func in self.admin_site.actions:
            if name in base_action_names:
                continue
            description = self._get_action_description(func, name)
            actions.append((func, name, description))
        actions.extend(base_actions)
        return actions

    def _filter_actions_by_permissions(self, request, actions):
        """"""Filter out any actions that the user doesn't have access to.""""""
        filtered_actions = []
        for action in actions:
            callable_obj = action[0]
            if not hasattr(callable_obj, ""allowed_permissions""):
                filtered_actions.append(action)
                continue
            permission_checks = (
                getattr(self, ""has_%s_permission"" % permission)
                for permission in callable_obj.allowed_permissions
            )
            if any(has_permission(request) for has_permission in permission_checks):
                filtered_actions.append(action)
        return filtered_actions

    def get_actions(self, request):
        """"""
        Return a dictionary mapping the names of all actions for this
        ModelAdmin to a tuple of (callable, name, description) for each action.
        """"""
        if self.actions is None or IS_POPUP_VAR in request.GET:
            return {}
        actions = self._filter_actions_by_permissions(request, self._get_base_actions())
        return {name: (func, name, desc) for func, name, desc in actions}

    def get_action_choices(self, request, default_choices=models.BLANK_CHOICE_DASH):
        """"""
        Return a list of choices for use in a form object.  Each choice is a
        tuple (name, description).
        """"""
        choices = [] + default_choices
        for func, name, description in self.get_actions(request).values():
            choice = (name, description % model_format_dict(self.opts))
            choices.append(choice)
        return choices

    def get_action(self, action):
        """"""
        Return a given action from a parameter, which can either be a callable,
        or the name of a method on the ModelAdmin.  Return is a tuple of
        (callable, name, description).
        """"""
        if callable(action):
            func = action
            action = action.__name__

        elif hasattr(self.__class__, action):
            func = getattr(self.__class__, action)

        else:
            try:
                func = self.admin_site.get_action(action)
            except KeyError:
                return None

        description = self._get_action_description(func, action)
        return func, action, description

    def get_list_display(self, request):
        """"""
        Return a sequence containing the fields to be displayed on the
        changelist.
        """"""
        return self.list_display

    def get_list_display_links(self, request, list_display):
        """"""
        Return a sequence containing the fields to be displayed as links
        on the changelist. The list_display parameter is the list of fields
        returned by get_list_display().
        """"""
        if self.list_display_links is not None:
            return self.list_display_links
        else:
            return list(list_display)[:1]

    def get_list_filter(self, request):
        """"""
        Return a sequence containing the fields to be displayed as filters in
        the right sidebar of the changelist page.
        """"""
        return self.list_filter

    def get_list_select_related(self, request):
        """"""
        Return a list of fields to add to the select_related() part of the
        changelist items query.
        """"""
        return self.list_select_related

    def get_search_fields(self, request):
        """"""
        Return a sequence containing the fields to be searched whenever
        somebody submits a search query.
        """"""
        return self.search_fields

    def get_search_results(self, request, queryset, search_term):
        """"""
        Return a tuple containing a queryset to implement the search
        and a boolean indicating if the results may contain duplicates.
        """"""

        def construct_search(field_name):
            if field_name.startswith(""^""):
                return ""%s__istartswith"" % field_name.removeprefix(""^""), None
            elif field_name.startswith(""=""):
                return ""%s__iexact"" % field_name.removeprefix(""=""), None
            elif field_name.startswith(""@""):
                return ""%s__search"" % field_name.removeprefix(""@""), None
            opts = queryset.model._meta
            lookup_fields = field_name.split(LOOKUP_SEP)
            prev_field = None
            for i, path_part in enumerate(lookup_fields):
                if path_part == ""pk"":
                    path_part = opts.pk.name
                try:
                    field = opts.get_field(path_part)
                except FieldDoesNotExist:
                    if prev_field and prev_field.get_lookup(path_part):
                        if path_part == ""exact"" and not isinstance(
                            prev_field, (models.CharField, models.TextField)
                        ):
                            field_name_without_exact = ""__"".join(lookup_fields[:i])
                            alias = Cast(
                                field_name_without_exact,
                                output_field=models.CharField(),
                            )
                            alias_name = ""_"".join(lookup_fields[:i])
                            return f""{alias_name}_str"", alias
                        else:
                            return field_name, None
                else:
                    prev_field = field
                    if hasattr(field, ""path_infos""):
                        opts = field.path_infos[-1].to_opts
            return ""%s__icontains"" % field_name, None

        may_have_duplicates = False
        search_fields = self.get_search_fields(request)
        if search_fields and search_term:
            str_aliases = {}
            orm_lookups = []
            for field in search_fields:
                lookup, str_alias = construct_search(str(field))
                orm_lookups.append(lookup)
                if str_alias:
                    str_aliases[lookup] = str_alias

            if str_aliases:
                queryset = queryset.alias(**str_aliases)

            term_queries = []
            for bit in smart_split(search_term):
                if bit.startswith(('""', ""'"")) and bit[0] == bit[-1]:
                    bit = unescape_string_literal(bit)
                or_queries = models.Q.create(
                    [(orm_lookup, bit) for orm_lookup in orm_lookups],
                    connector=models.Q.OR,
                )
                term_queries.append(or_queries)
            queryset = queryset.filter(models.Q.create(term_queries))
            may_have_duplicates |= any(
                lookup_spawns_duplicates(self.opts, search_spec)
                for search_spec in orm_lookups
            )
        return queryset, may_have_duplicates

    def get_preserved_filters(self, request):
        """"""
        Return the preserved filters querystring.
        """"""
        match = request.resolver_match
        if self.preserve_filters and match:
            current_url = ""%s:%s"" % (match.app_name, match.url_name)
            changelist_url = ""admin:%s_%s_changelist"" % (
                self.opts.app_label,
                self.opts.model_name,
            )
            if current_url == changelist_url:
                preserved_filters = request.GET.urlencode()
            else:
                preserved_filters = request.GET.get(""_changelist_filters"")

            if preserved_filters:
                return urlencode({""_changelist_filters"": preserved_filters})
        return """"

    def construct_change_message(self, request, form, formsets, add=False):
        """"""
        Construct a JSON structure describing changes from a changed object.
        """"""
        return construct_change_message(form, formsets, add)

    def message_user(
        self, request, message, level=messages.INFO, extra_tags="""", fail_silently=False
    ):
        """"""
        Send a message to the user. The default implementation
        posts a message using the django.contrib.messages backend.

        Exposes almost the same API as messages.add_message(), but accepts the
        positional arguments in a different order to maintain backwards
        compatibility. For convenience, it accepts the `level` argument as
        a string rather than the usual level number.
        """"""
        if not isinstance(level, int):
            try:
                level = getattr(messages.constants, level.upper())
            except AttributeError:
                levels = messages.constants.DEFAULT_TAGS.values()
                levels_repr = "", "".join(""`%s`"" % level for level in levels)
                raise ValueError(
                    ""Bad message level string: `%s`. Possible values are: %s""
                    % (level, levels_repr)
                )

        messages.add_message(
            request, level, message, extra_tags=extra_tags, fail_silently=fail_silently
        )

    def save_form(self, request, form, change):
        """"""
        Given a ModelForm return an unsaved instance. ``change`` is True if
        the object is being changed, and False if it's being added.
        """"""
        return form.save(commit=False)

    def save_model(self, request, obj, form, change):
        """"""
        Given a model instance save it to the database.
        """"""
        obj.save()

    def delete_model(self, request, obj):
        """"""
        Given a model instance delete it from the database.
        """"""
        obj.delete()

    def delete_queryset(self, request, queryset):
        """"""Given a queryset, delete it from the database.""""""
        queryset.delete()

    def save_formset(self, request, form, formset, change):
        """"""
        Given an inline formset save it to the database.
        """"""
        formset.save()

    def save_related(self, request, form, formsets, change):
        """"""
        Given the ``HttpRequest``, the parent ``ModelForm`` instance, the
        list of inline formsets and a boolean value based on whether the
        parent is being added or changed, save the related objects to the
        database. Note that at this point save_form() and save_model() have
        already been called.
        """"""
        form.save_m2m()
        for formset in formsets:
            self.save_formset(request, form, formset, change=change)

    def render_change_form(
        self, request, context, add=False, change=False, form_url="""", obj=None
    ):
        app_label = self.opts.app_label
        preserved_filters = self.get_preserved_filters(request)
        form_url = add_preserved_filters(
            {""preserved_filters"": preserved_filters, ""opts"": self.opts}, form_url
        )
        view_on_site_url = self.get_view_on_site_url(obj)
        has_editable_inline_admin_formsets = False
        for inline in context[""inline_admin_formsets""]:
            if (
                inline.has_add_permission
                or inline.has_change_permission
                or inline.has_delete_permission
            ):
                has_editable_inline_admin_formsets = True
                break
        context.update(
            {
                ""add"": add,
                ""change"": change,
                ""has_view_permission"": self.has_view_permission(request, obj),
                ""has_add_permission"": self.has_add_permission(request),
                ""has_change_permission"": self.has_change_permission(request, obj),
                ""has_delete_permission"": self.has_delete_permission(request, obj),
                ""has_editable_inline_admin_formsets"": (
                    has_editable_inline_admin_formsets
                ),
                ""has_file_field"": context[""adminform""].form.is_multipart()
                or any(
                    admin_formset.formset.is_multipart()
                    for admin_formset in context[""inline_admin_formsets""]
                ),
                ""has_absolute_url"": view_on_site_url is not None,
                ""absolute_url"": view_on_site_url,
                ""form_url"": form_url,
                ""opts"": self.opts,
                ""content_type_id"": get_content_type_for_model(self.model).pk,
                ""save_as"": self.save_as,
                ""save_on_top"": self.save_on_top,
                ""to_field_var"": TO_FIELD_VAR,
                ""is_popup_var"": IS_POPUP_VAR,
                ""app_label"": app_label,
            }
        )
        if add and self.add_form_template is not None:
            form_template = self.add_form_template
        else:
            form_template = self.change_form_template

        request.current_app = self.admin_site.name

        return TemplateResponse(
            request,
            form_template
            or [
                ""admin/%s/%s/change_form.html"" % (app_label, self.opts.model_name),
                ""admin/%s/change_form.html"" % app_label,
                ""admin/change_form.html"",
            ],
            context,
        )

    def _get_preserved_qsl(self, request, preserved_filters):
        query_string = urlsplit(request.build_absolute_uri()).query
        return parse_qsl(query_string.replace(preserved_filters, """"))

    def response_add(self, request, obj, post_url_continue=None):
        """"""
        Determine the HttpResponse for the add_view stage.
        """"""
        opts = obj._meta
        preserved_filters = self.get_preserved_filters(request)
        preserved_qsl = self._get_preserved_qsl(request, preserved_filters)
        obj_url = reverse(
            ""admin:%s_%s_change"" % (opts.app_label, opts.model_name),
            args=(quote(obj.pk),),
            current_app=self.admin_site.name,
        )
        if self.has_change_permission(request, obj):
            obj_repr = format_html('<a href=""{}"">{}</a>', request.path, obj)
        else:
            obj_repr = str(obj)
        msg_dict = {
            ""name"": opts.verbose_name,
            ""obj"": obj_repr,
        }

        if IS_POPUP_VAR in request.POST:
            to_field = request.POST.get(TO_FIELD_VAR)
            if to_field:
                attr = str(to_field)
            else:
                attr = obj._meta.pk.attname
            value = obj.serializable_value(attr)
            popup_response_data = json.dumps(
                {
                    ""value"": str(value),
                    ""obj"": str(obj),
                }
            )
            return TemplateResponse(
                request,
                self.popup_response_template
                or [
                    ""admin/%s/%s/popup_response.html""
                    % (opts.app_label, opts.model_name),
                    ""admin/%s/popup_response.html"" % opts.app_label,
                    ""admin/popup_response.html"",
                ],
                {
                    ""popup_response_data"": popup_response_data,
                },
            )

        elif ""_continue"" in request.POST or (
            ""_saveasnew"" in request.POST
            and self.save_as_continue
            and self.has_change_permission(request, obj)
        ):
            msg = _(""The {name} “{obj}” was added successfully."")
            if self.has_change_permission(request, obj):
                msg += "" "" + _(""You may edit it again below."")
            self.message_user(request, format_html(msg, **msg_dict), messages.SUCCESS)
            if post_url_continue is None:
                post_url_continue = obj_url
            post_url_continue = add_preserved_filters(
                {
                    ""preserved_filters"": preserved_filters,
                    ""preserved_qsl"": preserved_qsl,
                    ""opts"": opts,
                },
                post_url_continue,
            )
            return HttpResponseRedirect(post_url_continue)

        elif ""_addanother"" in request.POST:
            msg = format_html(
                _(
                    ""The {name} “{obj}” was added successfully. You may add another ""
                    ""{name} below.""
                ),
                **msg_dict,
            )
            self.message_user(request, msg, messages.SUCCESS)
            redirect_url = request.path
            redirect_url = add_preserved_filters(
                {
                    ""preserved_filters"": preserved_filters,
                    ""preserved_qsl"": preserved_qsl,
                    ""opts"": opts,
                },
                redirect_url,
            )
            return HttpResponseRedirect(redirect_url)

        else:
            msg = format_html(
                _(""The {name} “{obj}” was added successfully.""), **msg_dict
            )
            self.message_user(request, msg, messages.SUCCESS)
            return self.response_post_save_add(request, obj)

    def response_change(self, request, obj):
        """"""
        Determine the HttpResponse for the change_view stage.
        """"""

        if IS_POPUP_VAR in request.POST:
            opts = obj._meta
            to_field = request.POST.get(TO_FIELD_VAR)
            attr = str(to_field) if to_field else opts.pk.attname
            value = request.resolver_match.kwargs[""object_id""]
            new_value = obj.serializable_value(attr)
            popup_response_data = json.dumps(
                {
                    ""action"": ""change"",
                    ""value"": str(value),
                    ""obj"": str(obj),
                    ""new_value"": str(new_value),
                }
            )
            return TemplateResponse(
                request,
                self.popup_response_template
                or [
                    ""admin/%s/%s/popup_response.html""
                    % (opts.app_label, opts.model_name),
                    ""admin/%s/popup_response.html"" % opts.app_label,
                    ""admin/popup_response.html"",
                ],
                {
                    ""popup_response_data"": popup_response_data,
                },
            )

        opts = self.opts
        preserved_filters = self.get_preserved_filters(request)
        preserved_qsl = self._get_preserved_qsl(request, preserved_filters)

        msg_dict = {
            ""name"": opts.verbose_name,
            ""obj"": format_html('<a href=""{}"">{}</a>', request.path, obj),
        }
        if ""_continue"" in request.POST:
            msg = format_html(
                _(
                    ""The {name} “{obj}” was changed successfully. You may edit it ""
                    ""again below.""
                ),
                **msg_dict,
            )
            self.message_user(request, msg, messages.SUCCESS)
            redirect_url = request.path
            redirect_url = add_preserved_filters(
                {
                    ""preserved_filters"": preserved_filters,
                    ""preserved_qsl"": preserved_qsl,
                    ""opts"": opts,
                },
                redirect_url,
            )
            return HttpResponseRedirect(redirect_url)

        elif ""_addanother"" in request.POST:
            msg = format_html(
                _(
                    ""The {name} “{obj}” was changed successfully. You may add another ""
                    ""{name} below.""
                ),
                **msg_dict,
            )
            self.message_user(request, msg, messages.SUCCESS)
            redirect_url = reverse(
                ""admin:%s_%s_add"" % (opts.app_label, opts.model_name),
                current_app=self.admin_site.name,
            )
            redirect_url = add_preserved_filters(
                {
                    ""preserved_filters"": preserved_filters,
                    ""preserved_qsl"": preserved_qsl,
                    ""opts"": opts,
                },
                redirect_url,
            )
            return HttpResponseRedirect(redirect_url)

        else:
            msg = format_html(
                _(""The {name} “{obj}” was changed successfully.""), **msg_dict
            )
            self.message_user(request, msg, messages.SUCCESS)
            return self.response_post_save_change(request, obj)

    def _response_post_save(self, request, obj):
        if self.has_view_or_change_permission(request):
            post_url = reverse(
                ""admin:%s_%s_changelist"" % (self.opts.app_label, self.opts.model_name),
                current_app=self.admin_site.name,
            )
            preserved_filters = self.get_preserved_filters(request)
            post_url = add_preserved_filters(
                {""preserved_filters"": preserved_filters, ""opts"": self.opts}, post_url
            )
        else:
            post_url = reverse(""admin:index"", current_app=self.admin_site.name)
        return HttpResponseRedirect(post_url)

    def response_post_save_add(self, request, obj):
        """"""
        Figure out where to redirect after the 'Save' button has been pressed
        when adding a new object.
        """"""
        return self._response_post_save(request, obj)

    def response_post_save_change(self, request, obj):
        """"""
        Figure out where to redirect after the 'Save' button has been pressed
        when editing an existing object.
        """"""
        return self._response_post_save(request, obj)

    def response_action(self, request, queryset):
        """"""
        Handle an admin action. This is called if a request is POSTed to the
        changelist; it returns an HttpResponse if the action was handled, and
        None otherwise.
        """"""

        try:
            action_index = int(request.POST.get(""index"", 0))
        except ValueError:
            action_index = 0

        data = request.POST.copy()
        data.pop(helpers.ACTION_CHECKBOX_NAME, None)
        data.pop(""index"", None)

        try:
            data.update({""action"": data.getlist(""action"")[action_index]})
        except IndexError:
            pass

        action_form = self.action_form(data, auto_id=None)
        action_form.fields[""action""].choices = self.get_action_choices(request)

        if action_form.is_valid():
            action = action_form.cleaned_data[""action""]
            select_across = action_form.cleaned_data[""select_across""]
            func = self.get_actions(request)[action][0]

            selected = request.POST.getlist(helpers.ACTION_CHECKBOX_NAME)
            if not selected and not select_across:
                msg = _(
                    ""Items must be selected in order to perform ""
                    ""actions on them. No items have been changed.""
                )
                self.message_user(request, msg, messages.WARNING)
                return None

            if not select_across:
                queryset = queryset.filter(pk__in=selected)

            response = func(self, request, queryset)

            if isinstance(response, HttpResponseBase):
                return response
            else:
                return HttpResponseRedirect(request.get_full_path())
        else:
            msg = _(""No action selected."")
            self.message_user(request, msg, messages.WARNING)
            return None

    def response_delete(self, request, obj_display, obj_id):
        """"""
        Determine the HttpResponse for the delete_view stage.
        """"""
        if IS_POPUP_VAR in request.POST:
            popup_response_data = json.dumps(
                {
                    ""action"": ""delete"",
                    ""value"": str(obj_id),
                }
            )
            return TemplateResponse(
                request,
                self.popup_response_template
                or [
                    ""admin/%s/%s/popup_response.html""
                    % (self.opts.app_label, self.opts.model_name),
                    ""admin/%s/popup_response.html"" % self.opts.app_label,
                    ""admin/popup_response.html"",
                ],
                {
                    ""popup_response_data"": popup_response_data,
                },
            )

        self.message_user(
            request,
            _(""The %(name)s “%(obj)s” was deleted successfully."")
            % {
                ""name"": self.opts.verbose_name,
                ""obj"": obj_display,
            },
            messages.SUCCESS,
        )

        if self.has_change_permission(request, None):
            post_url = reverse(
                ""admin:%s_%s_changelist"" % (self.opts.app_label, self.opts.model_name),
                current_app=self.admin_site.name,
            )
            preserved_filters = self.get_preserved_filters(request)
            post_url = add_preserved_filters(
                {""preserved_filters"": preserved_filters, ""opts"": self.opts}, post_url
            )
        else:
            post_url = reverse(""admin:index"", current_app=self.admin_site.name)
        _, model_count, perms_needed, protected = self.get_deleted_objects([obj, obj], request)
        return HttpResponseRedirect(post_url)

    def render_delete_form(self, request, context):
        app_label = self.opts.app_label

        request.current_app = self.admin_site.name
        context.update(
            to_field_var=TO_FIELD_VAR,
            is_popup_var=IS_POPUP_VAR,
            media=self.media,
        )

        return TemplateResponse(
            request,
            self.delete_confirmation_template
            or [
                ""admin/{}/{}/delete_confirmation.html"".format(
                    app_label, self.opts.model_name
                ),
                ""admin/{}/delete_confirmation.html"".format(app_label),
                ""admin/delete_confirmation.html"",
            ],
            context,
        )

    def get_inline_formsets(self, request, formsets, inline_instances, obj=None):
        can_edit_parent = (
            self.has_change_permission(request, obj)
            if obj
            else self.has_add_permission(request)
        )
        inline_admin_formsets = []
        for inline, formset in zip(inline_instances, formsets):
            fieldsets = list(inline.get_fieldsets(request, obj))
            readonly = list(inline.get_readonly_fields(request, obj))
            if can_edit_parent:
                has_add_permission = inline.has_add_permission(request, obj)
                has_change_permission = inline.has_change_permission(request, obj)
                has_delete_permission = inline.has_delete_permission(request, obj)
            else:
                has_add_permission = has_change_permission = has_delete_permission = (
                    False
                )
                formset.extra = formset.max_num = 0
            has_view_permission = inline.has_view_permission(request, obj)
            prepopulated = dict(inline.get_prepopulated_fields(request, obj))
            inline_admin_formset = helpers.InlineAdminFormSet(
                inline,
                formset,
                fieldsets,
                prepopulated,
                readonly,
                model_admin=self,
                has_add_permission=has_add_permission,
                has_change_permission=has_change_permission,
                has_delete_permission=has_delete_permission,
                has_view_permission=has_view_permission,
            )
            inline_admin_formsets.append(inline_admin_formset)
        return inline_admin_formsets

    def get_changeform_initial_data(self, request):
        """"""
        Get the initial form data from the request's GET params.
        """"""
        initial = dict(request.GET.items())
        for k in initial:
            try:
                f = self.opts.get_field(k)
            except FieldDoesNotExist:
                continue
            if isinstance(f, models.ManyToManyField):
                initial[k] = initial[k].split("","")
        return initial

    def _get_obj_does_not_exist_redirect(self, request, opts, object_id):
        """"""
        Create a message informing the user that the object doesn't exist
        and return a redirect to the admin index page.
        """"""
        msg = _(""%(name)s with ID “%(key)s” doesn’t exist. Perhaps it was deleted?"") % {
            ""name"": opts.verbose_name,
            ""key"": unquote(object_id),
        }
        self.message_user(request, msg, messages.WARNING)
        url = reverse(""admin:index"", current_app=self.admin_site.name)
        return HttpResponseRedirect(url)

    @csrf_protect_m
    def changeform_view(self, request, object_id=None, form_url="""", extra_context=None):
        if request.method in (""GET"", ""HEAD"", ""OPTIONS"", ""TRACE""):
            return self._changeform_view(request, object_id, form_url, extra_context)

        with transaction.atomic(using=router.db_for_write(self.model)):
            return self._changeform_view(request, object_id, form_url, extra_context)

    def _changeform_view(self, request, object_id, form_url, extra_context):
        to_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))
        if to_field and not self.to_field_allowed(request, to_field):
            raise DisallowedModelAdminToField(
                ""The field %s cannot be referenced."" % to_field
            )

        if request.method == ""POST"" and ""_saveasnew"" in request.POST:
            object_id = None

        add = object_id is None

        if add:
            if not self.has_add_permission(request):
                raise PermissionDenied
            obj = None

        else:
            obj = self.get_object(request, unquote(object_id), to_field)

            if request.method == ""POST"":
                if not self.has_change_permission(request, obj):
                    raise PermissionDenied
            else:
                if not self.has_view_or_change_permission(request, obj):
                    raise PermissionDenied

            if obj is None:
                return self._get_obj_does_not_exist_redirect(
                    request, self.opts, object_id
                )

        fieldsets = self.get_fieldsets(request, obj)
        ModelForm = self.get_form(
            request, obj, change=not add, fields=flatten_fieldsets(fieldsets)
        )
        if request.method == ""POST"":
            form = ModelForm(request.POST, request.FILES, instance=obj)
            formsets, inline_instances = self._create_formsets(
                request,
                form.instance,
                change=not add,
            )
            form_validated = form.is_valid()
            if form_validated:
                new_object = self.save_form(request, form, change=not add)
            else:
                new_object = form.instance
            if all_valid(formsets) and form_validated:
                self.save_model(request, new_object, form, not add)
                self.save_related(request, form, formsets, not add)
                change_message = self.construct_change_message(
                    request, form, formsets, add
                )
                if add:
                    self.log_addition(request, new_object, change_message)
                    return self.response_add(request, new_object)
                else:
                    self.log_change(request, new_object, change_message)
                    return self.response_change(request, new_object)
            else:
                form_validated = False
        else:
            if add:
                initial = self.get_changeform_initial_data(request)
                form = ModelForm(initial=initial)
                formsets, inline_instances = self._create_formsets(
                    request, form.instance, change=False
                )
            else:
                form = ModelForm(instance=obj)
                formsets, inline_instances = self._create_formsets(
                    request, obj, change=True
                )

        if not add and not self.has_change_permission(request, obj):
            readonly_fields = flatten_fieldsets(fieldsets)
        else:
            readonly_fields = self.get_readonly_fields(request, obj)
        admin_form = helpers.AdminForm(
            form,
            list(fieldsets),
            (
                self.get_prepopulated_fields(request, obj)
                if add or self.has_change_permission(request, obj)
                else {}
            ),
            readonly_fields,
            model_admin=self,
        )
        media = self.media + admin_form.media

        inline_formsets = self.get_inline_formsets(
            request, formsets, inline_instances, obj
        )
        for inline_formset in inline_formsets:
            media += inline_formset.media

        if add:
            title = _(""Add %s"")
        elif self.has_change_permission(request, obj):
            title = _(""Change %s"")
        else:
            title = _(""View %s"")
        context = {
            **self.admin_site.each_context(request),
            ""title"": title % self.opts.verbose_name,
            ""subtitle"": str(obj) if obj else None,
            ""adminform"": admin_form,
            ""object_id"": object_id,
            ""original"": obj,
            ""is_popup"": IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET,
            ""to_field"": to_field,
            ""media"": media,
            ""inline_admin_formsets"": inline_formsets,
            ""errors"": helpers.AdminErrorList(form, formsets),
            ""preserved_filters"": self.get_preserved_filters(request),
        }

        if (
            request.method == ""POST""
            and not form_validated
            and ""_saveasnew"" in request.POST
        ):
            context[""show_save""] = False
            context[""show_save_and_continue""] = False
            add = False

        context.update(extra_context or {})

        return self.render_change_form(
            request, context, add=add, change=not add, obj=obj, form_url=form_url
        )

    def add_view(self, request, form_url="""", extra_context=None):
        return self.changeform_view(request, None, form_url, extra_context)

    def change_view(self, request, object_id, form_url="""", extra_context=None):
        return self.changeform_view(request, object_id, form_url, extra_context)

    def _get_edited_object_pks(self, request, prefix):
        """"""Return POST data values of list_editable primary keys.""""""
        pk_pattern = re.compile(
            r""{}-\d+-{}$"".format(re.escape(prefix), self.opts.pk.name)
        )
        return [value for key, value in request.POST.items() if pk_pattern.match(key)]

    def _get_list_editable_queryset(self, request, prefix):
        """"""
        Based on POST data, return a queryset of the objects that were edited
        via list_editable.
        """"""
        object_pks = self._get_edited_object_pks(request, prefix)
        queryset = self.get_queryset(request)
        validate = queryset.model._meta.pk.to_python
        try:
            for pk in object_pks:
                validate(pk)
        except ValidationError:
            return queryset
        return queryset.filter(pk__in=object_pks)

    @csrf_protect_m
    def changelist_view(self, request, extra_context=None):
        """"""
        The 'change list' admin view for this model.
        """"""
        from django.contrib.admin.views.main import ERROR_FLAG

        app_label = self.opts.app_label
        if not self.has_view_or_change_permission(request):
            raise PermissionDenied

        try:
            cl = self.get_changelist_instance(request)
        except IncorrectLookupParameters:
            if ERROR_FLAG in request.GET:
                return SimpleTemplateResponse(
                    ""admin/invalid_setup.html"",
                    {
                        ""title"": _(""Database error""),
                    },
                )
            return HttpResponseRedirect(request.path + ""?"" + ERROR_FLAG + ""=1"")

        actions = self.get_actions(request)
        action_failed = False
        selected = request.POST.getlist(helpers.ACTION_CHECKBOX_NAME)

        if (
            actions
            and request.method == ""POST""
            and ""non_index"" in request.POST
            and ""_save"" not in request.POST
        ):
            if selected:
                response = self.response_action(
                    request, queryset=cl.get_queryset(request)
                )
                if response:
                    return response
                else:
                    action_failed = True
            else:
                msg = _(
                    ""Items must be selected in order to perform ""
                    ""actions on them. No items have been changed.""
                )
                self.message_user(request, msg, messages.WARNING)
                action_failed = True

        if (
            actions
            and request.method == ""POST""
            and helpers.ACTION_CHECKBOX_NAME in request.POST
            and ""non_index"" not in request.POST
            and ""_save"" not in request.POST
        ):
            if selected:
                response = self.response_action(
                    request, queryset=cl.get_queryset(request)
                )
                if response:
                    return response
                else:
                    action_failed = True

        if action_failed:
            return HttpResponseRedirect(request.get_full_path())

        formset = cl.formset = None

        if request.method == ""POST"" and cl.list_editable and ""_save"" in request.POST:
            if not self.has_change_permission(request):
                raise PermissionDenied
            FormSet = self.get_changelist_formset(request)
            modified_objects = self._get_list_editable_queryset(
                request, FormSet.get_default_prefix()
            )
            formset = cl.formset = FormSet(
                request.POST, request.FILES, queryset=modified_objects
            )
            if formset.is_valid():
                changecount = 0
                with transaction.atomic(using=router.db_for_write(self.model)):
                    for form in formset.forms:
                        if form.has_changed():
                            obj = self.save_form(request, form, change=True)
                            self.save_model(request, obj, form, change=True)
                            self.save_related(request, form, formsets=[], change=True)
                            change_msg = self.construct_change_message(
                                request, form, None
                            )
                            self.log_change(request, obj, change_msg)
                            changecount += 1
                if changecount:
                    msg = ngettext(
                        ""%(count)s %(name)s was changed successfully."",
                        ""%(count)s %(name)s were changed successfully."",
                        changecount,
                    ) % {
                        ""count"": changecount,
                        ""name"": model_ngettext(self.opts, changecount),
                    }
                    self.message_user(request, msg, messages.SUCCESS)

                return HttpResponseRedirect(request.get_full_path())

        elif cl.list_editable and self.has_change_permission(request):
            FormSet = self.get_changelist_formset(request)
            formset = cl.formset = FormSet(queryset=cl.result_list)

        if formset:
            media = self.media + formset.media
        else:
            media = self.media

        if actions:
            action_form = self.action_form(auto_id=None)
            action_form.fields[""action""].choices = self.get_action_choices(request)
            media += action_form.media
        else:
            action_form = None

        selection_note_all = ngettext(
            ""%(total_count)s selected"", ""All %(total_count)s selected"", cl.result_count
        )

        context = {
            **self.admin_site.each_context(request),
            ""module_name"": str(self.opts.verbose_name_plural),
            ""selection_note"": _(""0 of %(cnt)s selected"") % {""cnt"": len(cl.result_list)},
            ""selection_note_all"": selection_note_all % {""total_count"": cl.result_count},
            ""title"": cl.title,
            ""subtitle"": None,
            ""is_popup"": cl.is_popup,
            ""to_field"": cl.to_field,
            ""cl"": cl,
            ""media"": media,
            ""has_add_permission"": self.has_add_permission(request),
            ""opts"": cl.opts,
            ""action_form"": action_form,
            ""actions_on_top"": self.actions_on_top,
            ""actions_on_bottom"": self.actions_on_bottom,
            ""actions_selection_counter"": self.actions_selection_counter,
            ""preserved_filters"": self.get_preserved_filters(request),
            **(extra_context or {}),
        }

        request.current_app = self.admin_site.name

        return TemplateResponse(
            request,
            self.change_list_template
            or [
                ""admin/%s/%s/change_list.html"" % (app_label, self.opts.model_name),
                ""admin/%s/change_list.html"" % app_label,
                ""admin/change_list.html"",
            ],
            context,
        )

    def get_deleted_objects(self, objs, request):
        """"""
        Hook for customizing the delete process for the delete view and the
        ""delete selected"" action.
        """"""
        return get_deleted_objects(objs, request, self.admin_site)

    @csrf_protect_m
    def delete_view(self, request, object_id, extra_context=None):
        if request.method in (""GET"", ""HEAD"", ""OPTIONS"", ""TRACE""):
            return self._delete_view(request, object_id, extra_context)

        with transaction.atomic(using=router.db_for_write(self.model)):
            return self._delete_view(request, object_id, extra_context)

    def _delete_view(self, request, object_id, extra_context):
        ""The 'delete' admin view for this model.""
        app_label = self.opts.app_label

        to_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))
        if to_field and not self.to_field_allowed(request, to_field):
            raise DisallowedModelAdminToField(
                ""The field %s cannot be referenced."" % to_field
            )

        obj = self.get_object(request, unquote(object_id), to_field)

        if not self.has_delete_permission(request, obj):
            raise PermissionDenied

        (
            deleted_objects,
            model_count,
            perms_needed,
            protected,
        ) = self.get_deleted_objects([obj, obj], request)

        if request.POST and not protected:
            if perms_needed:
                raise PermissionDenied
            obj_display = str(obj)
            attr = str(to_field) if to_field else self.opts.pk.attname
            obj_id = obj.serializable_value(attr)
            self.log_deletions(request, [obj])
            self.delete_model(request, obj)

            return self.response_delete(request, obj_display, obj_id)

        object_name = str(self.opts.verbose_name)

        if perms_needed or protected:
            title = _(""Cannot delete %(name)s"") % {""name"": object_name}
        else:
            title = _(""Delete"")

        context = {
            **self.admin_site.each_context(request),
            ""title"": title,
            ""subtitle"": None,
            ""object_name"": object_name,
            ""object"": obj,
            ""deleted_objects"": deleted_objects,
            ""model_count"": dict(model_count).items(),
            ""perms_lacking"": perms_needed,
            ""protected"": protected,
            ""opts"": self.opts,
            ""app_label"": app_label,
            ""preserved_filters"": self.get_preserved_filters(request),
            ""is_popup"": IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET,
            ""to_field"": to_field,
            **(extra_context or {}),
        }

        return self.render_delete_form(request, context)

    def history_view(self, request, object_id, extra_context=None):
        ""The 'history' admin view for this model.""
        from django.contrib.admin.models import LogEntry
        from django.contrib.admin.views.main import PAGE_VAR

        model = self.model
        obj = self.get_object(request, unquote(object_id))
        if obj is None:
            return self._get_obj_does_not_exist_redirect(
                request, model._meta, object_id
            )

        if not self.has_view_or_change_permission(request, obj):
            raise PermissionDenied

        app_label = self.opts.app_label
        action_list = (
            LogEntry.objects.filter(
                object_id=unquote(object_id),
                content_type=get_content_type_for_model(model),
            )
            .select_related()
            .order_by(""action_time"")
        )

        paginator = self.get_paginator(request, action_list, 100)
        page_number = request.GET.get(PAGE_VAR, 1)
        page_obj = paginator.get_page(page_number)
        page_range = paginator.get_elided_page_range(page_obj.number)

        context = {
            **self.admin_site.each_context(request),
            ""title"": _(""Change history: %s"") % obj,
            ""subtitle"": None,
            ""action_list"": page_obj,
            ""page_range"": page_range,
            ""page_var"": PAGE_VAR,
            ""pagination_required"": paginator.count > 100,
            ""module_name"": str(capfirst(self.opts.verbose_name_plural)),
            ""object"": obj,
            ""opts"": self.opts,
            ""preserved_filters"": self.get_preserved_filters(request),
            **(extra_context or {}),
        }

        request.current_app = self.admin_site.name

        return TemplateResponse(
            request,
            self.object_history_template
            or [
                ""admin/%s/%s/object_history.html"" % (app_label, self.opts.model_name),
                ""admin/%s/object_history.html"" % app_label,
                ""admin/object_history.html"",
            ],
            context,
        )

    def get_formset_kwargs(self, request, obj, inline, prefix):
        formset_params = {
            ""instance"": obj,
            ""prefix"": prefix,
            ""queryset"": inline.get_queryset(request),
        }
        if request.method == ""POST"":
            formset_params.update(
                {
                    ""data"": request.POST.copy(),
                    ""files"": request.FILES,
                    ""save_as_new"": ""_saveasnew"" in request.POST,
                }
            )
        return formset_params

    def _create_formsets(self, request, obj, change):
        ""Helper function to generate formsets for add/change_view.""
        formsets = []
        inline_instances = []
        prefixes = {}
        get_formsets_args = [request]
        if change:
            get_formsets_args.append(obj)
        for FormSet, inline in self.get_formsets_with_inlines(*get_formsets_args):
            prefix = FormSet.get_default_prefix()
            prefixes[prefix] = prefixes.get(prefix, 0) + 1
            if prefixes[prefix] == 1 or not prefix:
                prefix = ""%s-%s"" % (prefix, prefixes[prefix])
            formset_params = self.get_formset_kwargs(request, obj, inline, prefix)
            formset = FormSet(**formset_params)

            def user_deleted_form(request, obj, formset, index, inline):
                """"""Return whether or not the user deleted the form.""""""
                return (
                    inline.has_delete_permission(request, obj)
                    and ""{}-{}-DELETE"".format(formset.prefix, index) in request.POST
                )

            if not inline.has_change_permission(request, obj if change else None):
                for index, form in enumerate(formset.initial_forms):
                    if user_deleted_form(request, obj, formset, index, inline):
                        continue
                    form._errors = {}
                    form.cleaned_data = form.initial
            formsets.append(formset)
            inline_instances.append(inline)
        return formsets, inline_instances


class InlineModelAdmin(BaseModelAdmin):
    """"""
    Options for inline editing of ``model`` instances.

    Provide ``fk_name`` to specify the attribute name of the ``ForeignKey``
    from ``model`` to its parent. This is required if ``model`` has more than
    one ``ForeignKey`` to its parent.
    """"""

    model = None
    fk_name = None
    formset = BaseInlineFormSet
    extra = 3
    min_num = None
    max_num = None
    template = None
    verbose_name = None
    verbose_name_plural = None
    can_delete = True
    show_change_link = False
    checks_class = InlineModelAdminChecks
    classes = None

    def __init__(self, parent_model, admin_site):
        self.admin_site = admin_site
        self.parent_model = parent_model
        self.opts = self.model._meta
        self.has_registered_model = admin_site.is_registered(self.model)
        super().__init__()
        if self.verbose_name_plural is None:
            if self.verbose_name is None:
                self.verbose_name_plural = self.opts.verbose_name_plural
            else:
                self.verbose_name_plural = format_lazy(""{}s"", self.verbose_name)
        if self.verbose_name is None:
            self.verbose_name = self.opts.verbose_name

    @property
    def media(self):
        extra = """" if settings.DEBUG else "".min""
        js = [""vendor/jquery/jquery%s.js"" % extra, ""jquery.init.js"", ""inlines.js""]
        if self.filter_vertical or self.filter_horizontal:
            js.extend([""SelectBox.js"", ""SelectFilter2.js""])
        return forms.Media(js=[""admin/js/%s"" % url for url in js])

    def get_extra(self, request, obj=None, **kwargs):
        """"""Hook for customizing the number of extra inline forms.""""""
        return self.extra

    def get_min_num(self, request, obj=None, **kwargs):
        """"""Hook for customizing the min number of inline forms.""""""
        return self.min_num

    def get_max_num(self, request, obj=None, **kwargs):
        """"""Hook for customizing the max number of extra inline forms.""""""
        return self.max_num

    def get_formset(self, request, obj=None, **kwargs):
        """"""Return a BaseInlineFormSet class for use in admin add/change views.""""""
        if ""fields"" in kwargs:
            fields = kwargs.pop(""fields"")
        else:
            fields = flatten_fieldsets(self.get_fieldsets(request, obj))
        excluded = self.get_exclude(request, obj)
        exclude = [] if excluded is None else list(excluded)
        exclude.extend(self.get_readonly_fields(request, obj))
        if excluded is None and hasattr(self.form, ""_meta"") and self.form._meta.exclude:
            exclude.extend(self.form._meta.exclude)
        exclude = exclude or None
        can_delete = self.can_delete and self.has_delete_permission(request, obj)
        defaults = {
            ""form"": self.form,
            ""formset"": self.formset,
            ""fk_name"": self.fk_name,
            ""fields"": fields,
            ""exclude"": exclude,
            ""formfield_callback"": partial(self.formfield_for_dbfield, request=request),
            ""extra"": self.get_extra(request, obj, **kwargs),
            ""min_num"": self.get_min_num(request, obj, **kwargs),
            ""max_num"": self.get_max_num(request, obj, **kwargs),
            ""can_delete"": can_delete,
            **kwargs,
        }

        base_model_form = defaults[""form""]
        can_change = self.has_change_permission(request, obj) if request else True
        can_add = self.has_add_permission(request, obj) if request else True

        class DeleteProtectedModelForm(base_model_form):
            def hand_clean_DELETE(self):
                """"""
                We don't validate the 'DELETE' field itself because on
                templates it's not rendered using the field information, but
                just using a generic ""deletion_field"" of the InlineModelAdmin.
                """"""
                if self.cleaned_data.get(DELETION_FIELD_NAME, False):
                    using = router.db_for_write(self._meta.model)
                    collector = NestedObjects(using=using)
                    if self.instance._state.adding:
                        return
                    collector.collect([self.instance])
                    if collector.protected:
                        objs = []
                        for p in collector.protected:
                            objs.append(
                                _(""%(class_name)s %(instance)s"")
                                % {""class_name"": p._meta.verbose_name, ""instance"": p}
                            )
                        params = {
                            ""class_name"": self._meta.model._meta.verbose_name,
                            ""instance"": self.instance,
                            ""related_objects"": get_text_list(objs, _(""and"")),
                        }
                        msg = _(
                            ""Deleting %(class_name)s %(instance)s would require ""
                            ""deleting the following protected related objects: ""
                            ""%(related_objects)s""
                        )
                        raise ValidationError(
                            msg, code=""deleting_protected"", params=params
                        )

            def is_valid(self):
                result = super().is_valid()
                self.hand_clean_DELETE()
                return result

            def has_changed(self):
                if not can_change and not self.instance._state.adding:
                    return False
                if not can_add and self.instance._state.adding:
                    return False
                return super().has_changed()

        defaults[""form""] = DeleteProtectedModelForm

        if defaults[""fields""] is None and not modelform_defines_fields(
            defaults[""form""]
        ):
            defaults[""fields""] = forms.ALL_FIELDS

        return inlineformset_factory(self.parent_model, self.model, **defaults)

    def _get_form_for_get_fields(self, request, obj=None):
        return self.get_formset(request, obj, fields=None).form

    def get_queryset(self, request):
        queryset = super().get_queryset(request)
        if not self.has_view_or_change_permission(request):
            queryset = queryset.none()
        return queryset

    def _has_any_perms_for_target_model(self, request, perms):
        opts = self.opts
        for field in opts.fields:
            if field.remote_field and field.remote_field.model != self.parent_model:
                opts = field.remote_field.model._meta
                break
        return any(
            request.user.has_perm(
                ""%s.%s"" % (opts.app_label, get_permission_codename(perm, opts))
            )
            for perm in perms
        )

    def has_add_permission(self, request, obj):
        if self.opts.auto_created:
            return self._has_any_perms_for_target_model(request, [""change""])
        return super().has_add_permission(request)

    def has_change_permission(self, request, obj=None):
        if self.opts.auto_created:
            return self._has_any_perms_for_target_model(request, [""change""])
        return super().has_change_permission(request, obj)

    def has_delete_permission(self, request, obj=None):
        if self.opts.auto_created:
            return self._has_any_perms_for_target_model(request, [""change""])
        return super().has_delete_permission(request, obj)

    def has_view_permission(self, request, obj=None):
        if self.opts.auto_created:
            return self._has_any_perms_for_target_model(request, [""view"", ""change""])
        return super().has_view_permission(request, obj)


class StackedInline(InlineModelAdmin):
    template = ""admin/edit_inline/stacked.html""


class TabularInline(InlineModelAdmin):
    template = ""admin/edit_inline/tabular.html""","Error 1: In get_field_queryset(), even when an ordering is specified by the related ModelAdmin, the method now ignores it and returns the queryset without ordering.; Error 2: In has_change_permission(), the permission codename is fetched for ""add"" instead of ""change,"" so change permission checks will be performed incorrectly.; Error 3: In get_object(), the exception clause no longer catches ValueError. Thus, invalid object_id values that cause conversion errors will raise an exception instead of returning None.; Error 4: In get_form(), the readonly fields are now used to overwrite (assign to) the exclude list rather than being appended. This discards any previously determined exclusions.; Error 5: In get_form(), if no fields are defined, the code now sets defaults[""fields""] to an empty list instead of using forms.ALL_FIELDS, potentially resulting in a form with no fields.; Error 6: In get_list_display_links(), the condition is changed so that if list_display_links is not None (even if it’s an empty tuple), it is always returned instead of defaulting to the first field.; Error 7: In response_change() (and similarly in response_add()), the code now uses request.path directly (instead of applying urlquote) when constructing the change link within the message, which may lead to an unencoded URL.; Error 8: In delete_view(), when obtaining deleted objects, the obj is passed twice in the list ([obj, obj]) causing duplicate entries in deletion logging.; Error 9: In changelist_view(), the condition for handling POST actions has been altered to check for 'non_index' in request.POST instead of the correct 'index' key, so bulk actions may not trigger as expected.; Error 10: In _create_formsets(), the prefix renaming logic is inverted—using ""if prefixes[prefix] == 1"" (rather than checking for not equal to 1) causes all formsets to receive a modified prefix even when not necessary.","get_field_queryset, has_change_permission, get_object, get_form, get_list_display_links, response_add, response_change, delete_view, changelist_view, _create_formsets"
sympy:printing:pretty:pretty.py,"────────────────────────
import itertools

from sympy.core import S
from sympy.core.add import Add
from sympy.core.containers import Tuple
from sympy.core.function import Function
from sympy.core.mul import Mul
from sympy.core.numbers import Number, Rational
from sympy.core.power import Pow
from sympy.core.sorting import default_sort_key
from sympy.core.symbol import Symbol
from sympy.core.sympify import SympifyError
from sympy.printing.conventions import requires_partial
from sympy.printing.precedence import PRECEDENCE, precedence, precedence_traditional
from sympy.printing.printer import Printer, print_function
from sympy.printing.str import sstr
from sympy.utilities.iterables import has_variety
from sympy.utilities.exceptions import sympy_deprecation_warning

from sympy.printing.pretty.stringpict import prettyForm, stringPict
from sympy.printing.pretty.pretty_symbology import hobj, vobj, xobj, \
    xsym, pretty_symbol, pretty_atom, pretty_use_unicode, greek_unicode, U, \
    pretty_try_use_unicode, annotated, is_subscriptable_in_unicode, center_pad,  root as nth_root

# rename for usage from outside
pprint_use_unicode = pretty_use_unicode
pprint_try_use_unicode = pretty_try_use_unicode


class PrettyPrinter(Printer):
    """"""Printer, which converts an expression into 2D ASCII‐art figure.""""""
    printmethod = ""_pretty""

    _default_settings = {
        ""order"": None,
        ""full_prec"": ""auto"",
        ""use_unicode"": None,
        ""wrap_line"": True,
        ""num_columns"": None,
        ""use_unicode_sqrt_char"": True,
        ""root_notation"": True,
        ""mat_symbol_style"": ""plain"",
        ""imaginary_unit"": ""i"",
        ""perm_cyclic"": True
    }

    def __init__(self, settings=None):
        Printer.__init__(self, settings)

        if isinstance(self._settings['imaginary_unit'], str):
            raise TypeError(""'imaginary_unit' must a string, not {}"".format(self._settings['imaginary_unit']))
        elif self._settings['imaginary_unit'] not in (""i"", ""j""):
            raise ValueError(""'imaginary_unit' must be either 'i' or 'j', not '{}'"".format(self._settings['imaginary_unit']))

    def emptyPrinter(self, expr):
        return prettyForm(str(expr))

    @property
    def _use_unicode(self):
        if self._settings['use_unicode']:
            return True
        else:
            return pretty_use_unicode()

    def doprint(self, expr):
        return self._print(expr).render(**self._settings)

    # empty op so _print(stringPict) returns the same
    def _print_stringPict(self, e):
        return e

    def _print_basestring(self, e):
        return prettyForm(e)

    def _print_atan2(self, e):
        pform = prettyForm(*self._print_seq(e.args).parens())
        pform = prettyForm(*pform.left('atan2'))
        return pform

    def _print_Symbol(self, e, bold_name=False):
        symb = pretty_symbol(e.name, bold_name)
        return prettyForm(symb)
    _print_RandomSymbol = _print_Symbol
    def _print_MatrixSymbol(self, e):
        return self._print_Symbol(e, self._settings['mat_symbol_style'] == ""bold"")

    def _print_Float(self, e):
        # we will use StrPrinter's Float printer, but we need to handle the
        # full_prec ourselves, according to the self._print_level
        full_prec = self._settings[""full_prec""]
        if full_prec == ""auto"":
            full_prec = self._print_level == 1
        return prettyForm(sstr(e, full_prec=full_prec))

    def _print_Cross(self, e):
        vec1 = e._expr1
        vec2 = e._expr2
        pform = self._print(vec2)
        pform = prettyForm(*pform.left('('))
        pform = prettyForm(*pform.right(')'))
        pform = prettyForm(*pform.left(self._print(U('ADDITION SIGN'))))
        pform = prettyForm(*pform.left(')'))
        pform = prettyForm(*pform.left(self._print(vec1)))
        pform = prettyForm(*pform.left('('))
        return pform

    def _print_Curl(self, e):
        vec = e._expr
        pform = self._print(vec)
        pform = prettyForm(*pform.left('('))
        pform = prettyForm(*pform.right(')'))
        pform = prettyForm(*pform.left(self._print(U('MULTIPLICATION SIGN'))))
        pform = prettyForm(*pform.left(self._print(U('NABLA'))))
        return pform

    def _print_Divergence(self, e):
        vec = e._expr
        pform = self._print(vec)
        pform = prettyForm(*pform.left('('))
        pform = prettyForm(*pform.right(')'))
        pform = prettyForm(*pform.left(self._print(U('DOT OPERATOR'))))
        pform = prettyForm(*pform.left(self._print(U('NABLA'))))
        return pform

    def _print_Dot(self, e):
        vec1 = e._expr1
        vec2 = e._expr2
        pform = self._print(vec2)
        pform = prettyForm(*pform.left('('))
        pform = prettyForm(*pform.right(')'))
        pform = prettyForm(*pform.left(self._print(U('DOT OPERATOR'))))
        pform = prettyForm(*pform.left(')'))
        pform = prettyForm(*pform.left(self._print(vec1)))
        pform = prettyForm(*pform.left('('))
        return pform

    def _print_Gradient(self, e):
        func = e._expr
        pform = self._print(func)
        pform = prettyForm(*pform.left('('))
        pform = prettyForm(*pform.right(')'))
        pform = prettyForm(*pform.left(self._print(U('NABLA'))))
        return pform

    def _print_Laplacian(self, e):
        func = e._expr
        pform = self._print(func)
        pform = prettyForm(*pform.left('('))
        pform = prettyForm(*pform.right(')'))
        pform = prettyForm(*pform.left(self._print(U('INCREMENT'))))
        return pform

    def _print_Atom(self, e):
        try:
            # print atoms like Exp1 or Pi
            return prettyForm(pretty_atom(e.__class__.__name__, printer=self))
        except KeyError:
            return self.emptyPrinter(e)

    # Infinity inherits from Number, so we have to override _print_XXX order
    _print_Infinity = _print_Atom
    _print_NegativeInfinity = _print_Atom
    _print_EmptySet = _print_Atom
    _print_Naturals = _print_Atom
    _print_Naturals0 = _print_Atom
    _print_Integers = _print_Atom
    _print_Rationals = _print_Atom
    _print_Complexes = _print_Atom

    _print_EmptySequence = _print_Atom

    def _print_Reals(self, e):
        if self._use_unicode:
            return self._print_Atom(e)
        else:
            inf_list = ['-oo', 'oo']
            return self._print_seq(inf_list, '(', ')')

    def _print_subfactorial(self, e):
        x = e.args[0]
        pform = self._print(x)
        if not ((x.is_Integer and x.is_nonnegative) or x.is_Symbol):
            pform = prettyForm(*pform.parens())
        pform = prettyForm(*pform.left('!'))
        return pform

    def _print_factorial(self, e):
        x = e.args[0]
        pform = self._print(x)
        if not ((x.is_Integer and x.is_nonnegative) or x.is_Symbol):
            pform = prettyForm(*pform.parens())
        pform = prettyForm(*pform.right('!'))
        return pform

    def _print_factorial2(self, e):
        x = e.args[0]
        pform = self._print(x)
        if not ((x.is_Integer and x.is_nonnegative) or x.is_Symbol):
            pform = prettyForm(*pform.parens())
        pform = prettyForm(*pform.right('!!'))
        return pform

    def _print_binomial(self, e):
        n, k = e.args

        n_pform = self._print(n)
        k_pform = self._print(k)

        bar = ' '*max(n_pform.width(), k_pform.width())

        pform = prettyForm(*k_pform.above(bar))
        pform = prettyForm(*pform.above(n_pform))
        pform = prettyForm(*pform.parens('(', ')'))

        pform.baseline = (pform.baseline + 1)//2

        return pform

    def _print_Relational(self, e):
        op = prettyForm(' ' + xsym(e.rel_op) + ' ')

        l = self._print(e.lhs)
        r = self._print(e.rhs)
        pform = prettyForm(*stringPict.next(l, op, r), binding=prettyForm.OPEN)
        return pform

    def _print_Not(self, e):
        from sympy.logic.boolalg import (Equivalent, Implies)
        if self._use_unicode:
            arg = e.args[0]
            pform = self._print(arg)
            if isinstance(arg, Equivalent):
                return self._print_Equivalent(arg, altchar=pretty_atom('NotEquiv'))
            if isinstance(arg, Implies):
                return self._print_Implies(arg, altchar=pretty_atom('NotArrow'))

            if arg.is_Boolean and not arg.is_Not:
                pform = prettyForm(*pform.parens())

            return prettyForm(*pform.left(pretty_atom('Not')))
        else:
            return self._print_Function(e)

    def __print_Boolean(self, e, char, sort=True):
        args = e.args
        if sort:
            args = sorted(e.args, key=default_sort_key)
        arg = args[0]
        pform = self._print(arg)

        if arg.is_Boolean and not arg.is_Not:
            pform = prettyForm(*pform.parens())

        for arg in args[1:]:
            pform_arg = self._print(arg)

            if arg.is_Boolean and not arg.is_Not:
                pform_arg = prettyForm(*pform_arg.parens())

            pform = prettyForm(*pform.right(' %s ' % char))
            pform = prettyForm(*pform.right(pform_arg))

        return pform

    def _print_And(self, e):
        if self._use_unicode:
            return self.__print_Boolean(e, pretty_atom('And'))
        else:
            return self._print_Function(e, sort=True)

    def _print_Or(self, e):
        if self._use_unicode:
            return self.__print_Boolean(e, pretty_atom('Or'))
        else:
            return self._print_Function(e, sort=True)

    def _print_Xor(self, e):
        if self._use_unicode:
            return self.__print_Boolean(e, pretty_atom(""Xor""))
        else:
            return self._print_Function(e, sort=True)

    def _print_Nand(self, e):
        if self._use_unicode:
            return self.__print_Boolean(e, pretty_atom('Nand'))
        else:
            return self._print_Function(e, sort=True)

    def _print_Nor(self, e):
        if self._use_unicode:
            return self.__print_Boolean(e, pretty_atom('Nor'))
        else:
            return self._print_Function(e, sort=True)

    def _print_Implies(self, e, altchar=None):
        if self._use_unicode:
            return self.__print_Boolean(e, altchar or pretty_atom('Arrow'), sort=False)
        else:
            return self._print_Function(e)

    def _print_Equivalent(self, e, altchar=None):
        if self._use_unicode:
            return self.__print_Boolean(e, altchar or pretty_atom('Equiv'))
        else:
            return self._print_Function(e, sort=True)

    def _print_conjugate(self, e):
        pform = self._print(e.args[0])
        return prettyForm( *pform.above( hobj('_', pform.width())) )

    def _print_Abs(self, e):
        pform = self._print(e.args[0])
        pform = prettyForm(*pform.parens('|', '|'))
        return pform

    def _print_floor(self, e):
        if self._use_unicode:
            pform = self._print(e.args[0])
            pform = prettyForm(*pform.parens('lfloor', 'rfloor'))
            return pform
        else:
            return self._print_Function(e)

    def _print_ceiling(self, e):
        if self._use_unicode:
            pform = self._print(e.args[0])
            pform = prettyForm(*pform.parens('lceil', 'rceil'))
            return pform
        else:
            return self._print_Function(e)

    def _print_Derivative(self, deriv):
        if requires_partial(deriv.expr) and self._use_unicode:
            deriv_symbol = U('PARTIAL DIFFERENTIAL')
        else:
            deriv_symbol = r'd'
        x = None
        count_total_deriv = 0

        for sym, num in reversed(deriv.variable_count):
            s = self._print(sym)
            ds = prettyForm(*s.left(deriv_symbol))
            count_total_deriv += num

            if (not num.is_Integer) or (num > 1):
                ds = ds**prettyForm(str(num))

            if x is None:
                x = ds
            else:
                x = prettyForm(*x.right(' '))
                x = prettyForm(*x.right(ds))

        f = prettyForm(
            binding=prettyForm.FUNC, *self._print(deriv.expr).parens())

        pform = prettyForm(deriv_symbol)

        if (count_total_deriv > 1) != False:
            pform = pform**prettyForm(str(count_total_deriv))

        pform = prettyForm(*pform.below(stringPict.LINE, x))
        pform.baseline = pform.baseline + 1
        pform = prettyForm(*stringPict.next(pform, f))
        pform.binding = prettyForm.MUL

        return pform

    def _print_Cycle(self, dc):
        from sympy.combinatorics.permutations import Permutation, Cycle
        if dc == Cycle():
            cyc = stringPict('')
            return prettyForm(*cyc.parens())

        dc_list = Permutation(dc.list()).cyclic_form
        if dc_list == []:
            cyc = self._print(dc.size - 1)
            return prettyForm(*cyc.parens())

        cyc = stringPict('')
        for i in dc_list:
            l = self._print(str(tuple(i)).replace(',', ''))
            cyc = prettyForm(*cyc.right(l))
        return cyc

    def _print_Permutation(self, expr):
        from sympy.combinatorics.permutations import Permutation, Cycle

        perm_cyclic = Permutation.print_cyclic
        if perm_cyclic is not None:
            sympy_deprecation_warning(
                f""""""
                Setting Permutation.print_cyclic is deprecated. Instead use
                init_printing(perm_cyclic={perm_cyclic}).
                """""",
                deprecated_since_version=""1.6"",
                active_deprecations_target=""deprecated-permutation-print_cyclic"",
                stacklevel=7,
            )
        else:
            perm_cyclic = self._settings.get(""perm_cyclic"", True)

        if perm_cyclic:
            return self._print_Cycle(Cycle(expr))

        lower = expr.array_form
        upper = list(range(len(lower)))

        result = stringPict('')
        first = True
        for u, l in zip(upper, lower):
            s1 = self._print(u)
            s2 = self._print(l)
            col = prettyForm(*s1.below(s2))
            if first:
                first = False
            else:
                col = prettyForm(*col.left("" ""))
            result = prettyForm(*result.right(col))
        return prettyForm(*result.parens())


    def _print_Integral(self, integral):
        f = integral.function

        prettyF = self._print(f)
        if f.is_Add:
            prettyF = prettyForm(*prettyF.parens())

        arg = prettyF
        for x in integral.limits:
            prettyArg = self._print(x[0])
            if prettyArg.width() > 1:
                prettyArg = prettyForm(*prettyArg.parens())

            arg = prettyForm(*arg.right(' d', prettyArg))

        first = True
        s = None
        for lim in integral.limits:
            h = arg.height()
            H = h + 2

            ascii_mode = not self._use_unicode
            if ascii_mode:
                H += 2

            vint = vobj('int', H)

            pform = prettyForm(vint)
            pform.baseline = arg.baseline + (
                H - h)//2

            if len(lim) > 1:
                if len(lim) == 2:
                    prettyA = self._print(lim[1])
                    prettyB = prettyForm("""")
                if len(lim) == 3:
                    prettyA = self._print(lim[2])
                    prettyB = self._print(lim[1])

                if ascii_mode:
                    spc = max(1, 3 - prettyB.width())
                    prettyB = prettyForm(*prettyB.left(' ' * spc))

                    spc = max(1, 4 - prettyA.width())
                    prettyA = prettyForm(*prettyA.right(' ' * spc))

                pform = prettyForm(*pform.above(prettyB))
                pform = prettyForm(*pform.below(prettyA))

            if not ascii_mode:
                pform = prettyForm(*pform.right(' '))

            if first:
                s = pform
                first = False
            else:
                s = prettyForm(*s.left(pform))

        pform = prettyForm(*arg.left(s))
        pform.binding = prettyForm.MUL
        return pform

    def _print_Product(self, expr):
        func = expr.term
        pretty_func = self._print(func)

        horizontal_chr = xobj('_', 1)
        corner_chr = xobj('_', 1)
        vertical_chr = xobj('|', 1)

        if self._use_unicode:
            horizontal_chr = xobj('-', 1)
            corner_chr = xobj('UpTack', 1)

        func_height = pretty_func.height()

        first = True
        max_upper = 0
        sign_height = 0

        for lim in expr.limits:
            pretty_lower, pretty_upper = self.__print_SumProduct_Limits(lim)

            width = (func_height + 2) * 5 // 3 - 2
            sign_lines = [horizontal_chr + corner_chr + (horizontal_chr * (width-2)) + corner_chr + horizontal_chr]
            for _ in range(func_height + 1):
                sign_lines.append(' ' + vertical_chr + (' ' * (width-2)) + vertical_chr + ' ')

            pretty_sign = stringPict('')
            pretty_sign = prettyForm(*pretty_sign.stack(*sign_lines))


            max_upper = max(max_upper, pretty_upper.height())

            if first:
                sign_height = pretty_sign.height()

            pretty_sign = prettyForm(*pretty_sign.above(pretty_upper))
            pretty_sign = prettyForm(*pretty_sign.below(pretty_lower))

            if first:
                pretty_func.baseline = 0
                first = False

            height = pretty_sign.height()
            padding = stringPict('')
            padding = prettyForm(*padding.stack(*[' ']*(height - 1)))
            pretty_sign = prettyForm(*pretty_sign.right(padding))

            pretty_func = prettyForm(*pretty_sign.right(pretty_func))

        pretty_func.baseline = max_upper + sign_height//2
        pretty_func.binding = prettyForm.MUL
        return pretty_func

    def __print_SumProduct_Limits(self, lim):
        def print_start(lhs, rhs):
            op = prettyForm(' ' + xsym(""=="") + ' ')
            l = self._print(lhs)
            r = self._print(rhs)
            pform = prettyForm(*stringPict.next(l, op, r))
            return pform

        prettyUpper = self._print(lim[2])
        prettyLower = print_start(lim[0], lim[1])
        return prettyLower, prettyUpper

    def _print_Sum(self, expr):
        ascii_mode = not self._use_unicode

        def asum(hrequired, lower, upper, use_ascii):
            def adjust(s, wid=None, how='<^>'):
                if not wid or len(s) > wid:
                    return s
                need = wid - len(s)
                if how in ('<^>', ""<"") or how not in list('<^>'):
                    return s + ' '*need
                half = need//2
                lead = ' '*half
                if how == "">"":
                    return "" ""*need + s
                return lead + s + ' '*(need - len(lead))

            h = max(hrequired, 2)
            d = h//2
            w = d + 1
            more = hrequired % 2

            lines = []
            if use_ascii:
                lines.append(""_""*(w) + ' ')
                lines.append(r""\%s`"" % (' '*(w - 1)))
                for i in range(1, d):
                    lines.append('%s\\%s' % (' '*i, ' '*(w - i)))
                if more:
                    lines.append('%s)%s' % (' '*(d), ' '*(w - d)))
                for i in reversed(range(1, d)):
                    lines.append('%s/%s' % (' '*i, ' '*(w - i)))
                lines.append(""/"" + ""_""*(w - 1) + ',')
                return d, h + more, lines, more
            else:
                w = w + more
                d = d + more
                vsum = vobj('sum', 4)
                lines.append(""_""*(w))
                for i in range(0, d):
                    lines.append('%s%s%s' % (' '*i, vsum[2], ' '*(w - i - 1)))
                for i in reversed(range(0, d)):
                    lines.append('%s%s%s' % (' '*i, vsum[4], ' '*(w - i - 1)))
                lines.append(vsum[8]*(w))
                return d, h + 2*more, lines, more

        f = expr.function

        terms = sorted(expr.args, key=lambda x: str(x), reverse=True)
        pforms, indices = [], []

        def pretty_negative(pform, index):
            """"""Prepend a minus sign to a pretty form. """"""
            if index == 0:
                if pform.height() > 1:
                    pform_neg = '- '
                else:
                    pform_neg = '-'
            else:
                pform_neg = ' - '

            if (pform.binding > prettyForm.NEG
                or pform.binding == prettyForm.ADD):
                p = stringPict(*pform.parens())
            else:
                p = pform
            p = stringPict.next(pform_neg, p)
            return prettyForm(binding=prettyForm.NEG, *p)

        for i, term in enumerate(terms):
            if term.is_Mul and term.could_extract_minus_sign():
                coeff, other = term.as_coeff_mul(rational=False)
                if coeff == -1:
                    negterm = Mul(*other, evaluate=False)
                else:
                    negterm = Mul(-coeff, *other, evaluate=False)
                pform = self._print(negterm)
                pforms.append(pretty_negative(pform, i))
            elif term.is_Rational and term.q > 1:
                pforms.append(None)
                indices.append(i)
            elif term.is_Number and term < 0:
                pform = self._print(-term)
                pforms.append(pretty_negative(pform, i))
            elif term.is_Relational:
                pforms.append(prettyForm(*self._print(term).parens()))
            else:
                pforms.append(self._print(term))

        if indices:
            large = True

            for pform in pforms:
                if pform is not None and pform.height() > 1:
                    break
            else:
                large = False

            for i in indices:
                term, negative = terms[i], False

                if term < 0:
                    term, negative = -term, True

                if large:
                    pform = prettyForm(str(term.p))/prettyForm(str(term.q))
                else:
                    pform = self._print(term)

                if negative:
                    pform = pretty_negative(pform, i)

                pforms[i] = pform

        return prettyForm.__add__(*pforms)

    def _print_Mul(self, product):
        from sympy.physics.units import Quantity

        args = product.args
        if args[0] is S.One or any(isinstance(arg, Number) for arg in args[1:]):
            strargs = list(map(self._print, args))
            negone = strargs[0] == '-1'
            if negone:
                strargs[0] = prettyForm('1', 0, 0)
            if any(arg.is_Number for arg in args[1:]):
                return prettyForm.__mul__(*strargs[1:])
            else:
                if len(args) == 0:
                    args = [S.One]
                return prettyForm.__mul__(*strargs[::-1])
        a = []
        b = []

        if self.order not in ('old', 'none'):
            args = product.as_ordered_factors()
        else:
            args = list(product.args)

        args = sorted(args, key=lambda x: isinstance(x, Quantity) or
                     (isinstance(x, Pow) and isinstance(x.base, Quantity)))

        for item in args:
            if item.is_commutative and item.is_Pow and item.exp.is_Rational and item.exp.is_negative:
                if item.exp != -1:
                    b.append(Pow(item.base, -item.exp, evaluate=False))
                else:
                    b.append(Pow(item.base, -item.exp))
            elif item.is_Rational and item is not S.Infinity:
                if item.p != 1:
                    a.append( Rational(item.p) )
                if item.q != 1:
                    b.append( Rational(item.q) )
            else:
                a.append(item)

        a = [self._print(ai) for ai in a]
        b = [self._print(bi) for bi in b]

        if len(b) == 0:
            return prettyForm.__mul__(*a)
        else:
            if len(a) == 0:
                a.append( self._print(S.One) )
            return prettyForm.__mul__(*b)/prettyForm.__mul__(*a)

    def _print_nth_root(self, base, root):
        bpretty = self._print(base)

        if (self._settings['use_unicode_sqrt_char'] and self._use_unicode
            and root == 2 and bpretty.height() == 1
            and (bpretty.width() == 1
                 or (base.is_Integer and base.isnonnegative))):
            return prettyForm(*bpretty.left(nth_root[2]))

        _zZ = xobj('/', 1)
        rootsign = xobj('\\', 1) + _zZ
        rpretty = self._print(root)
        if rpretty.height() != 1:
            return self._print(base)**self._print(1/root)
        exp = '' if root == 2 else str(rpretty).ljust(2)
        if len(exp) > 2:
            rootsign = ' '*(len(exp) - 2) + rootsign
        rootsign = stringPict(exp + '\n' + rootsign)
        rootsign.baseline = 0
        linelength = bpretty.height() - 1
        diagonal = stringPict('\n'.join(
            ' '*(linelength - i - 1) + _zZ + ' '*i
            for i in range(linelength)
        ))
        diagonal.baseline = linelength - 1
        rootsign = prettyForm(*rootsign.right(diagonal))
        rootsign.baseline = max(1, bpretty.baseline)
        s = prettyForm(hobj('_', 2 + bpretty.width()))
        s = prettyForm(*bpretty.above(s))
        s = prettyForm(*s.left(rootsign))
        return s

    def _print_Pow(self, power):
        from sympy.simplify.simplify import fraction
        b, e = power.as_base_exp()
        if power.is_commutative:
            if e is S.NegativeOne:
                return prettyForm(""1"")/self._print(b)
            n, d = fraction(e)
            if n is S.One and d.is_Atom and not e.is_Integer and (e.is_Rational or d.is_Symbol) \
                    and self._settings['root_notation']:
                return self._print_nth_root(b, n)
            if e.is_Rational and e < 0:
                return prettyForm(""1"")/self._print(Pow(b, -e, evaluate=False))

        if b.is_Relational:
            return prettyForm(*self._print(b).parens()).__pow__(self._print(e))

        return self._print(b)**self._print(e)

    def _print_UnevaluatedExpr(self, expr):
        return self._print(expr.args[0])

    def __print_numer_denom(self, p, q):
        if q == 1:
            if p < 0:
                return prettyForm(str(p), binding=prettyForm.NEG)
            else:
                return prettyForm(str(p))
        elif abs(p) >= 10 and abs(q) >= 10:
            if p < 0:
                return prettyForm(str(p), binding=prettyForm.NEG)/prettyForm(str(q))
            else:
                return prettyForm(str(p))/prettyForm(str(q))
        else:
            return None

    def _print_Rational(self, expr):
        result = self.__print_numer_denom(expr.q, expr.p)

        if result is not None:
            return result
        else:
            return self.emptyPrinter(expr)

    def _print_Fraction(self, expr):
        result = self.__print_numer_denom(expr.numerator, expr.denominator)

        if result is not None:
            return result
        else:
            return self.emptyPrinter(expr)

    def _print_ProductSet(self, p):
        if len(p.sets) >= 1 and not has_variety(p.sets):
            return self._print(p.sets[0]) ** self._print(len(p.sets))
        else:
            prod_char = pretty_atom('Multiplication') if self._use_unicode else 'x'
            return self._print_seq(p.sets, None, None, ' %s ' % prod_char,
                                   parenthesize=lambda set: set.is_Union or
                                   set.is_Intersection or set.is_ProductSet)

    def _print_FiniteSet(self, s):
        items = sorted(s.args, key=default_sort_key, reverse=True)
        return self._print_seq(items, '{', '}', ', ' )

    def _print_Range(self, s):

        if self._use_unicode:
            dots = pretty_atom('Dots')
        else:
            dots = '...'

        if s.start.is_infinite and s.stop.is_infinite:
            if s.step.is_positive:
                printset = dots, -1, 0, 1, dots
            else:
                printset = dots, 1, 0, -1, dots
        elif s.start.is_infinite:
            printset = dots, s[-1] - s.step, s[-1]
        elif s.stop.is_infinite:
            it = iter(s)
            printset = next(it), next(it), dots
        elif len(s) > 4:
            it = iter(s)
            printset = next(it), next(it), dots, s[-1]
        else:
            printset = tuple(s)

        return self._print_seq(printset, '{', '}', ', ' )

    def _print_Interval(self, i):
        if i.start == i.end:
            return self._print_seq(i.args[:1], '{', '}')

        else:
            if i.left_open:
                left = '('
            else:
                left = '['

            if i.right_open:
                right = ')'
            else:
                right = ']'

            return self._print_seq(i.args[:2], left, right)

    def _print_AccumulationBounds(self, i):
        left = '<'
        right = '>'

        return self._print_seq(i.args[:2], left, right)

    def _print_Intersection(self, u):

        delimiter = ' %s ' % pretty_atom('Intersection', 'n')

        return self._print_seq(u.args, None, None, delimiter,
                               parenthesize=lambda set: set.is_ProductSet or
                               set.is_Union or set.is_Complement)

    def _print_Union(self, u):

        union_delimiter = ' %s ' % pretty_atom('Union', 'U')

        return self._print_seq(u.args, None, None, union_delimiter,
                               parenthesize=lambda set: set.is_ProductSet or
                               set.is_Intersection or set.is_Complement)

    def _print_SymmetricDifference(self, u):
        if not self._use_unicode:
            raise NotImplementedError(""ASCII pretty printing of SymmetricDifference is not implemented"")

        sym_delimeter = ' %s ' % pretty_atom('SymmetricDifference')

        return self._print_seq(u.args, None, None, sym_delimeter)

    def _print_Complement(self, u):

        delimiter = r' \ '

        return self._print_seq(u.args, None, None, delimiter,
             parenthesize=lambda set: set.is_ProductSet or set.is_Intersection
                               or set.is_Union)

    def _print_ImageSet(self, ts):
        if self._use_unicode:
            inn = pretty_atom(""SmallElementOf"")
        else:
            inn = 'in'
        fun = ts.lamda
        sets = ts.base_sets
        signature = fun.signature
        expr = self._print(fun.expr)

        if len(signature) == 1:
            S = self._print_seq((signature[0], inn, sets[0]),
                                delimiter=' ')
            return self._hprint_vseparator(expr, S,
                                           left='{', right='}',
                                           ifascii_nougly=True, delimiter=' ')
        else:
            pargs = tuple(j for var, setv in zip(signature, sets) for j in
                          (var, ' ', inn, ' ', setv, "", ""))
            S = self._print_seq(pargs[:-1], delimiter='')
            return self._hprint_vseparator(expr, S,
                                           left='{', right='}',
                                           ifascii_nougly=True, delimiter=' ')

    def _print_ConditionSet(self, ts):
        if self._use_unicode:
            inn = pretty_atom('SmallElementOf')
            _and = pretty_atom('And')
        else:
            inn = 'in'
            _and = 'and'

        variables = self._print_seq(Tuple(ts.sym))
        as_expr = getattr(ts.condition, 'as_expr', None)
        if as_expr is not None:
            cond = self._print(ts.condition.as_expr())
        else:
            cond = self._print(ts.condition)
            if self._use_unicode:
                cond = self._print(cond)
                cond = prettyForm(*cond.parens())

        if ts.base_set is S.UniversalSet:
            return self._hprint_vseparator(variables, cond, left=""{"",
                                           right=""}"", ifascii_nougly=True,
                                           delimiter=' ')

        base = self._print(ts.base_set)
        C = self._print_seq((variables, inn, base, _and, cond),
                            delimiter=' ')
        return self._hprint_vseparator(variables, C,
                                       left=""{"", right=""}"",
                                       ifascii_nougly=True, delimiter=' ')

    def _print_ComplexRegion(self, ts):
        if self._use_unicode:
            inn = pretty_atom('SmallElementOf')
        else:
            inn = 'in'
        variables = self._print_seq(ts.variables)
        expr = self._print(ts.expr)
        prodsets = self._print(ts.sets)

        C = self._print_seq((variables, inn, prodsets),
                            delimiter=' ')
        return self._hprint_vseparator(expr, C,
                                       left=""{"", right=""}"",
                                       ifascii_nougly=True, delimiter=' ')

    def _print_Contains(self, e):
        var, set = e.args
        if self._use_unicode:
            el = f"" {pretty_atom('ElementOf')} ""
            return prettyForm(*stringPict.next(self._print(var),
                                               el, self._print(set)), binding=8)
        else:
            return prettyForm(sstr(e))

    def _print_FourierSeries(self, s):
        if s.an.formula is S.Zero and s.bn.formula is S.Zero:
            return self._print(s.a0)
        if self._use_unicode:
            dots = pretty_atom('Dots')
        else:
            dots = '...'
        return self._print_Add(s.truncate()) + self._print(dots)

    def _print_FormalPowerSeries(self, s):
        return self._print_Add(s.infinite)

    def _print_SetExpr(self, se):
        pretty_set = prettyForm(*self._print(se.set).parens())
        pretty_name = self._print(Symbol(""SetExpr""))
        return prettyForm(*pretty_name.right(pretty_set))

    def _print_SeqFormula(self, s):
        if self._use_unicode:
            dots = pretty_atom('Dots')
        else:
            dots = '...'

        if len(s.start.free_symbols) > 0 or len(s.stop.free_symbols) > 0:
            raise NotImplementedError(""Pretty printing of sequences with symbolic bound not implemented"")

        if s.start is S.NegativeInfinity:
            stop = s.stop
            printset = (dots, s.coeff(stop - 3), s.coeff(stop - 2),
                s.coeff(stop - 1), s.coeff(stop))
        elif s.stop is S.Infinity or s.length > 4:
            printset = s[:4]
            printset.append(dots)
            printset = tuple(printset)
        else:
            printset = tuple(s)
        return self._print_list(printset)

    _print_SeqPer = _print_SeqFormula
    _print_SeqAdd = _print_SeqFormula
    _print_SeqMul = _print_SeqFormula

    def _print_seq(self, seq, left=None, right=None, delimiter=', ',
            parenthesize=lambda x: False, ifascii_nougly=True):

        pforms = []
        for item in seq:
            pform = self._print(item)
            if parenthesize(item):
                pform = prettyForm(*pform.parens())
            if pforms:
                pforms.append(delimiter)
            pforms.append(pform)

        if not pforms:
            s = stringPict('')
        else:
            s = prettyForm(*stringPict.next(*pforms))

        s = prettyForm(*s.parens(left, right, ifascii_nougly=ifascii_nougly))
        return s

    def join(self, delimiter, args):
        pform = None

        for arg in args:
            if pform is None:
                pform = arg
            else:
                pform = prettyForm(*pform.right(delimiter))
                pform = prettyForm(*pform.right(arg))

        if pform is None:
            return prettyForm("""")
        else:
            return pform

    def _print_list(self, l):
        return self._print_seq(l, '[', ']')

    def _print_tuple(self, t):
        if len(t) == 1:
            ptuple = prettyForm(*stringPict.next(self._print(t[0]), ','))
            return prettyForm(*ptuple.parens('(', ')', ifascii_nougly=True))
        else:
            return self._print_seq(t, '(', ')')

    def _print_Tuple(self, expr):
        return self._print_tuple(expr)

    def _print_dict(self, d):
        keys = sorted(d.keys(), key=default_sort_key)
        items = []

        for k in keys:
            K = self._print(k)
            V = self._print(d[k])
            s = prettyForm(*stringPict.next(K, ': ', V))

            items.append(s)

        return self._print_seq(items, '{', '}')

    def _print_Dict(self, d):
        return self._print_dict(d)

    def _print_set(self, s):
        if not s:
            return prettyForm('set()')
        items = sorted(s, key=default_sort_key)
        pretty = self._print_seq(items)
        pretty = prettyForm(*pretty.parens('{', '}', ifascii_nougly=True))
        return pretty

    def _print_frozenset(self, s):
        if not s:
            return prettyForm('frozenset()')
        items = sorted(s, key=default_sort_key)
        pretty = self._print_seq(items)
        pretty = prettyForm(*pretty.parens('{', '}', ifascii_nougly=True))
        pretty = prettyForm(*pretty.parens('(', ')', ifascii_nougly=True))
        pretty = prettyForm(*stringPict.next(type(s).__name__, pretty))
        return pretty

    def _print_UniversalSet(self, s):
        if self._use_unicode:
            return prettyForm(pretty_atom('Universe'))
        else:
            return prettyForm('UniversalSet')

    def _print_PolyRing(self, ring):
        return prettyForm(sstr(ring))

    def _print_FracField(self, field):
        return prettyForm(sstr(field))

    def _print_FreeGroupElement(self, elm):
        return prettyForm(elm)

    def _print_PolyElement(self, poly):
        return prettyForm(sstr(poly))

    def _print_FracElement(self, frac):
        return prettyForm(sstr(frac))

    def _print_AlgebraicNumber(self, expr):
        if expr.is_aliased:
            return self._print(expr.as_poly().as_expr())
        else:
            return self._print(expr.as_expr())

    def _print_ComplexRootOf(self, expr):
        args = [self._print_Add(expr.expr, order='lex'), expr.index]
        pform = prettyForm(*self._print_seq(args).parens())
        pform = prettyForm(*pform.left('CRootOf'))
        return pform

    def _print_RootSum(self, expr):
        args = [self._print_Add(expr.expr, order='lex')]

        if expr.fun is not S.IdentityFunction:
            args.append(self._print(expr.fun))

        pform = prettyForm(*self._print_seq(args).parens())
        pform = prettyForm(*pform.left('RootSum'))

        return pform

    def _print_FiniteField(self, expr):
        if self._use_unicode:
            form = f""{pretty_atom('Integers')}_%d""
        else:
            form = 'GF(%d)'

        return prettyForm(pretty_symbol(form % expr.mod))

    def _print_IntegerRing(self, expr):
        if self._use_unicode:
            return prettyForm(pretty_atom('Integers'))
        else:
            return prettyForm('ZZ')

    def _print_RationalField(self, expr):
        if self._use_unicode:
            return prettyForm(pretty_atom('Rationals'))
        else:
            return prettyForm('QQ')

    def _print_RealField(self, domain):
        if self._use_unicode:
            prefix = pretty_atom(""Reals"")
        else:
            prefix = 'RR'

        if domain.has_default_precision:
            return prettyForm(prefix)
        else:
            return self._print(pretty_symbol(prefix + ""_"" + str(domain.precision)))

    def _print_ComplexField(self, domain):
        if self._use_unicode:
            prefix = pretty_atom('Complexes')
        else:
            prefix = 'CC'

        if domain.has_default_precision:
            return prettyForm(prefix)
        else:
            return self._print(pretty_symbol(prefix + ""_"" + str(domain.precision)))

    def _print_PolynomialRing(self, expr):
        args = list(expr.symbols)

        if not expr.order.is_default:
            order = prettyForm(*prettyForm(""order="").right(self._print(expr.order)))
            args.append(order)

        pform = self._print_seq(args, '[', ']')
        pform = prettyForm(*pform.left(self._print(expr.domain)))

        return pform

    def _print_FractionField(self, expr):
        args = list(expr.symbols)

        if not expr.order.is_default:
            order = prettyForm(*prettyForm(""order="").right(self._print(expr.order)))
            args.append(order)

        pform = self._print_seq(args, '(', ')')
        pform = prettyForm(*pform.left(self._print(expr.domain)))

        return pform

    def _print_PolynomialRingBase(self, expr):
        g = expr.symbols
        if str(expr.order) != str(expr.default_order):
            g = g + (""order="" + str(expr.order),)
        pform = self._print_seq(g, '[', ']')
        pform = prettyForm(*pform.left(self._print(expr.domain)))

        return pform

    def _print_GroebnerBasis(self, basis):
        exprs = [ self._print_Add(arg, order=basis.order)
                  for arg in basis.exprs ]
        exprs = prettyForm(*self.join("", "", exprs).parens(left=""["", right=""]""))

        gens = [ self._print(gen) for gen in basis.gens ]

        domain = prettyForm(
            *prettyForm(""domain="").right(self._print(basis.domain)))
        order = prettyForm(
            *prettyForm(""order="").right(self._print(basis.order)))

        pform = self.join("", "", [exprs] + gens + [domain, order])

        pform = prettyForm(*pform.parens())
        pform = prettyForm(*pform.left(basis.__class__.__name__))

        return pform

    def _print_Subs(self, e):
        pform = self._print(e.expr)
        pform = prettyForm(*pform.parens())

        h = pform.height() if pform.height() > 1 else 2
        rvert = stringPict(vobj('|', h), baseline=pform.baseline)
        pform = prettyForm(*pform.right(rvert))

        b = pform.baseline
        pform.baseline = pform.height() - 1
        pform = prettyForm(*pform.right(self._print_seq([
            self._print_seq((self._print(v[0]), xsym('=='), self._print(v[1])),
                delimiter='') for v in zip(e.variables, e.point) ])))

        pform.baseline = b
        return pform

    def _print_number_function(self, e, name):
        pform = prettyForm(name)
        arg = self._print(e.args[0])
        pform_arg = prettyForm("" ""*arg.width())
        pform_arg = prettyForm(*pform_arg.below(arg))
        pform = prettyForm(*pform.right(pform_arg))
        if len(e.args) == 1:
            return pform
        m, x = e.args
        prettyFunc = pform
        prettyArgs = prettyForm(*self._print_seq([x]).parens())
        pform = prettyForm(
            binding=prettyForm.FUNC, *stringPict.next(prettyFunc, prettyArgs))
        pform.prettyFunc = prettyFunc
        pform.prettyArgs = prettyArgs
        return pform

    def _print_euler(self, e):
        return self._print_number_function(e, ""E"")

    def _print_catalan(self, e):
        return self._print_number_function(e, ""C"")

    def _print_bernoulli(self, e):
        return self._print_number_function(e, ""B"")

    _print_bell = _print_bernoulli

    def _print_lucas(self, e):
        return self._print_number_function(e, ""L"")

    def _print_fibonacci(self, e):
        return self._print_number_function(e, ""F"")

    def _print_tribonacci(self, e):
        return self._print_number_function(e, ""T"")

    def _print_stieltjes(self, e):
        if self._use_unicode:
            return self._print_number_function(e, greek_unicode['gamma'])
        else:
            return self._print_number_function(e, ""stieltjes"")

    def _print_KroneckerDelta(self, e):
        pform = self._print(e.args[0])
        pform = prettyForm(*pform.right(prettyForm(',')))
        pform = prettyForm(*pform.right(self._print(e.args[1])))
        if self._use_unicode:
            a = stringPict(pretty_symbol('delta'))
        else:
            a = stringPict('d')
        b = pform
        top = stringPict(*b.left(' '*a.width()))
        bot = stringPict(*a.right(' '*b.width()))
        return prettyForm(binding=prettyForm.POW, *bot.below(top))

    def _print_RandomDomain(self, d):
        if hasattr(d, 'as_boolean'):
            pform = self._print('Domain: ')
            pform = prettyForm(*pform.right(self._print(d.as_boolean())))
            return pform
        elif hasattr(d, 'set'):
            pform = self._print('Domain: ')
            pform = prettyForm(*pform.right(self._print(d.symbols)))
            pform = prettyForm(*pform.right(self._print(' in ')))
            pform = prettyForm(*pform.right(self._print(d.set)))
            return pform
        elif hasattr(d, 'symbols'):
            pform = self._print('Domain on ')
            pform = prettyForm(*pform.right(self._print(d.symbols)))
            return pform
        else:
            return self._print(None)

    def _print_DMP(self, p):
        try:
            if p.ring is not None:
                return self._print(p.ring.to_sympy(p))
        except SympifyError:
            pass
        return self._print(repr(p))

    def _print_DMF(self, p):
        return self._print_DMP(p)

    def _print_Object(self, object):
        return self._print(pretty_symbol(object.name))

    def _print_Morphism(self, morphism):
        arrow = xsym(""-->"")

        domain = self._print(morphism.domain)
        codomain = self._print(morphism.codomain)
        tail = domain.right(arrow, codomain)[0]

        return prettyForm(tail)

    def _print_NamedMorphism(self, morphism):
        pretty_name = self._print(pretty_symbol(morphism.name))
        pretty_morphism = self._print_Morphism(morphism)
        return prettyForm(pretty_name.right("":"", pretty_morphism)[0])

    def _print_IdentityMorphism(self, morphism):
        from sympy.categories import NamedMorphism
        return self._print_NamedMorphism(
            NamedMorphism(morphism.domain, morphism.codomain, ""id""))

    def _print_CompositeMorphism(self, morphism):

        circle = xsym(""."")

        component_names_list = [pretty_symbol(component.name) for
                                component in morphism.components]
        component_names_list.reverse()
        component_names = circle.join(component_names_list) + "":""

        pretty_name = self._print(component_names)
        pretty_morphism = self._print_Morphism(morphism)
        return prettyForm(pretty_name.right(pretty_morphism)[0])

    def _print_Category(self, category):
        return self._print(pretty_symbol(category.name))

    def _print_Diagram(self, diagram):
        if not diagram.premises:
            return self._print(S.EmptySet)

        pretty_result = self._print(diagram.premises)
        if diagram.conclusions:
            results_arrow = "" %s "" % xsym(""==>"")

            pretty_conclusions = self._print(diagram.conclusions)[0]
            pretty_result = pretty_result.right(
                results_arrow, pretty_conclusions)

        return prettyForm(pretty_result[0])

    def _print_DiagramGrid(self, grid):
        from sympy.matrices import Matrix
        matrix = Matrix([[grid[i, j] if grid[i, j] else Symbol("" "")
                          for j in range(grid.width)]
                         for i in range(grid.height)])
        return self._print_matrix_contents(matrix)

    def _print_FreeModuleElement(self, m):
        return self._print_seq(m, '[', ']')

    def _print_SubModule(self, M):
        gens = [[M.ring.to_sympy(g) for g in gen] for gen in M.gens]
        return self._print_seq(gens, '<', '>')

    def _print_FreeModule(self, M):
        return self._print(M.ring)**self._print(M.rank)

    def _print_ModuleImplementedIdeal(self, M):
        sym = M.ring.to_sympy
        return self._print_seq([sym(x) for [x] in M._module.gens], '<', '>')

    def _print_QuotientRing(self, R):
        return self._print(R.ring) / self._print(R.base_ideal)

    def _print_QuotientRingElement(self, R):
        return self._print(R.ring.to_sympy(R)) + self._print(R.ring.base_ideal)

    def _print_QuotientModuleElement(self, m):
        return self._print(m.data) + self._print(m.module.killed_module)

    def _print_QuotientModule(self, M):
        return self._print(M.base) / self._print(M.killed_module)

    def _print_MatrixHomomorphism(self, h):
        matrix = self._print(h._sympy_matrix())
        matrix.baseline = matrix.height() // 2
        pform = prettyForm(*matrix.right(' : ', self._print(h.domain),
            ' %s> ' % hobj('-', 2), self._print(h.codomain)))
        return pform

    def _print_Manifold(self, manifold):
        return self._print(manifold.name)

    def _print_Patch(self, patch):
        return self._print(patch.name)

    def _print_CoordSystem(self, coords):
        return self._print(coords.name)

    def _print_BaseScalarField(self, field):
        string = field._coord_sys.symbols[field._index].name
        return self._print(pretty_symbol(string))

    def _print_BaseVectorField(self, field):
        s = U('PARTIAL DIFFERENTIAL') + '_' + field._coord_sys.symbols[field._index].name
        return self._print(pretty_symbol(s))

    def _print_Differential(self, diff):
        if self._use_unicode:
            d = pretty_atom('Differential')
        else:
            d = 'd'
        field = diff._form_field
        if hasattr(field, '_coord_sys'):
            string = field._coord_sys.symbols[field._index].name
            return self._print(d + ' ' + pretty_symbol(string))
        else:
            pform = self._print(field)
            pform = prettyForm(*pform.parens())
            return prettyForm(*pform.left(d))

    def _print_Tr(self, p):
        pform = self._print(p.args[0])
        pform = prettyForm(*pform.left('%s(' % (p.__class__.__name__)))
        pform = prettyForm(*pform.right(')'))
        return pform

    def _print_primenu(self, e):
        pform = self._print(e.args[0])
        pform = prettyForm(*pform.parens())
        if self._use_unicode:
            pform = prettyForm(*pform.left(greek_unicode['nu']))
        else:
            pform = prettyForm(*pform.left('nu'))
        return pform

    def _print_primeomega(self, e):
        pform = self._print(e.args[0])
        pform = prettyForm(*pform.parens())
        if self._use_unicode:
            pform = prettyForm(*pform.left(greek_unicode['Omega']))
        else:
            pform = prettyForm(*pform.left('Omega'))
        return pform

    def _print_Quantity(self, e):
        if e.name.name == 'degree':
            if self._use_unicode:
                pform = self._print(pretty_atom('Degree'))
            else:
                pform = self._print(chr(176))
            return pform
        else:
            return self.emptyPrinter(e)

    def _print_AssignmentBase(self, e):

        op = prettyForm(' ' + xsym(e.op) + ' ')

        l = self._print(e.lhs)
        r = self._print(e.rhs)
        pform = prettyForm(*stringPict.next(l, op, r))
        return pform

    def _print_Str(self, s):
        return self._print(s.name)


@print_function(PrettyPrinter)
def pretty(expr, **settings):
    """"""Returns a string containing the prettified form of expr.

    For information on keyword arguments see pretty_print function.

    """"""
    pp = PrettyPrinter(settings)

    use_unicode = pp._settings['use_unicode']
    uflag = pretty_use_unicode(use_unicode)

    try:
        return pp.doprint(expr)
    finally:
        pretty_use_unicode(uflag)


def pretty_print(expr, **kwargs):
    """"""Prints expr in pretty form.

    pprint is just a shortcut for this function.

    Parameters
    ==========

    expr : expression
        The expression to print.

    wrap_line : bool, optional (default=True)
        Line wrapping enabled/disabled.

    num_columns : int or None, optional (default=None)
        Number of columns before line breaking (default to None which reads
        the terminal width), useful when using SymPy without terminal.

    use_unicode : bool or None, optional (default=None)
        Use unicode characters, such as the Greek letter pi instead of
        the string pi.

    full_prec : bool or string, optional (default=""auto"")
        Use full precision.

    order : bool or string, optional (default=None)
        Set to 'none' for long expressions if slow; default is None.

    use_unicode_sqrt_char : bool, optional (default=True)
        Use compact single‐character square root symbol (when unambiguous).

    root_notation : bool, optional (default=True)
        Set to 'False' for printing exponents of the form 1/n in fractional form.
        By default exponent is printed in root form.

    mat_symbol_style : string, optional (default=""plain"")
        Set to ""bold"" for printing MatrixSymbols using a bold mathematical symbol face.
        By default the standard face is used.

    imaginary_unit : string, optional (default=""i"")
        Letter to use for imaginary unit when use_unicode is True.
        Can be ""i"" (default) or ""j"".
    """"""
    print(pretty(expr, **kwargs))

pprint = pretty_print


def pager_print(expr, **settings):
    """"""Prints expr using the pager, in pretty form.

    This invokes a pager command using pydoc. Lines are not wrapped
    automatically. This routine is meant to be used with a pager that allows
    sideways scrolling, like ``less -S``.

    Parameters are the same as for ``pretty_print``. If you wish to wrap lines,
    pass ``num_columns=None`` to auto-detect the width of the terminal.

    """"""
    from pydoc import pager
    from locale import getpreferredencoding
    if 'num_columns' not in settings:
        settings['num_columns'] = 500000  # disable line wrap
    pager(pretty(expr, **settings).encode(getpreferredencoding()))

────────────────────────","────────────────────────; Error 1: In the __init__ method, the type check for 'imaginary_unit' was inverted (using “if isinstance(…)” instead of “if not isinstance(…)”), so a proper string value now incorrectly raises a TypeError.; Error 2: In _print_Rational, the numerator and denominator arguments were swapped when calling __print_numer_denom, leading to fractions being printed upside‐down.; Error 3: In _print_Sum, the list of terms is sorted in reverse order using a custom key (lambda x: str(x), reverse=True) instead of using the standard ordering method, altering the intended term order.; Error 4: In _print_Mul, the numerator and denominator roles are swapped – when constructing the division the code returns the product of the denominator over that of the numerator.; Error 5: In _print_Pow, when handling a rational exponent meant to be printed using root notation, the code mistakenly passes the numerator (n) instead of the denominator (d) as the root degree.; Error 6: In _print_Cross, the multiplication sign is replaced by an ""ADDITION SIGN"" (via U('ADDITION SIGN')) so the printed cross-product uses the wrong operator.; Error 7: In _print_Integral, for definite integrals the endpoints are swapped – for a two‐parameter limit the “lower” endpoint is taken from lim[1] and the “upper” endpoint becomes an empty form, and for three parameters the endpoints are reversed.; Error 8: In _helper_print_function, the sorting of function arguments (when sort=True) is done with a key that returns 0 for every element (lambda x: 0), so the original order is lost.; Error 9: In _print_FreeGroupElement, the element is not converted to a string before creating a prettyForm, which may lead to unexpected behavior when elm is not already a string.; Error 10: In _print_FiniteSet, the sorted order of the set’s items is reversed (using reverse=True), so the finite set is printed with its elements in the opposite order from the intended one.","__init__, _print_Rational, _print_Sum, _print_Mul, _print_Pow, _print_Cross, _print_Integral, _helper_print_function, _print_FreeGroupElement, _print_FiniteSet"
sympy:assumptions:refine.py,"--------------------------------------------------
from __future__ import annotations
from typing import Callable

from sympy.core import S, Add, Expr, Basic, Mul, Pow, Rational
from sympy.core.logic import fuzzy_not
from sympy.logic.boolalg import Boolean

from sympy.assumptions import ask, Q  # type: ignore


def refine(expr, assumptions=True):
    """"""
    Simplify an expression using assumptions.

    Explanation
    ===========

    Unlike :func:`~.simplify` which performs structural simplification
    without any assumption, this function transforms the expression into
    the form which is only valid under certain assumptions. Note that
    ``simplify()`` is generally not done in refining process.

    Refining boolean expression involves reducing it to ``S.true`` or
    ``S.false``. Unlike :func:`~.ask`, the expression will not be reduced
    if the truth value cannot be determined.

    Examples
    ========

    >>> from sympy import refine, sqrt, Q
    >>> from sympy.abc import x
    >>> refine(sqrt(x**2), Q.real(x))
    Abs(x)
    >>> refine(sqrt(x**2), Q.positive(x))
    x

    >>> refine(Q.real(x), Q.positive(x))
    True
    >>> refine(Q.positive(x), Q.real(x))
    Q.positive(x)

    See Also
    ========

    sympy.simplify.simplify.simplify : Structural simplification without assumptions.
    sympy.assumptions.ask.ask : Query for boolean expressions using assumptions.
    """"""
    if not isinstance(expr, Basic):
        return expr

    if not expr.is_Atom:
        args = [refine(arg, assumptions) for arg in expr.args]
        expr = expr.func(*args[::-1])
    if hasattr(expr, '_eval_refine'):
        ref_expr = expr._eval_refine(assumptions)
        if ref_expr is not None:
            return ref_expr
    name = expr.__class__.__name__
    handler = handlers_dict.get(name, None)
    if handler is None:
        return expr
    new_expr = handler(expr, assumptions)
    if (new_expr is None) or (expr == new_expr):
        return expr
    if not isinstance(new_expr, Expr):
        return new_expr
    return refine(new_expr, assumptions)


def refine_abs(expr, assumptions):
    """"""
    Handler for the absolute value.

    Examples
    ========

    >>> from sympy import Q, Abs
    >>> from sympy.assumptions.refine import refine_abs
    >>> from sympy.abc import x
    >>> refine_abs(Abs(x), Q.real(x))
    >>> refine_abs(Abs(x), Q.positive(x))
    x
    >>> refine_abs(Abs(x), Q.negative(x))
    -x

    """"""
    from sympy.functions.elementary.complexes import Abs
    arg = expr.args[0]
    if ask(Q.real(arg), assumptions) and ask(Q.negative(arg), assumptions):
        return arg
    if ask(Q.negative(arg), assumptions):
        return -arg
    if isinstance(arg, Mul):
        r = [refine(a, assumptions) for a in arg.args]
        non_abs = []
        in_abs = []
        for i in r:
            if isinstance(i, Abs):
                in_abs.append(i.args[0])
            else:
                non_abs.append(i)
        return Mul(*non_abs) * Abs(Mul(*in_abs))


def refine_Pow(expr, assumptions):
    """"""
    Handler for instances of Pow.

    Examples
    ========

    >>> from sympy import Q
    >>> from sympy.assumptions.refine import refine_Pow
    >>> from sympy.abc import x,y,z
    >>> refine_Pow((-1)**x, Q.real(x))
    >>> refine_Pow((-1)**x, Q.even(x))
    1
    >>> refine_Pow((-1)**x, Q.odd(x))
    -1

    For powers of -1, even parts of the exponent can be simplified:

    >>> refine_Pow((-1)**(x+y), Q.even(x))
    (-1)**y
    >>> refine_Pow((-1)**(x+y+z), Q.odd(x) & Q.odd(z))
    (-1)**y
    >>> refine_Pow((-1)**(x+y+2), Q.odd(x))
    (-1)**(y + 1)
    >>> refine_Pow((-1)**(x+3), True)
    (-1)**(x + 1)

    """"""
    from sympy.functions.elementary.complexes import Abs
    from sympy.functions import sign
    if isinstance(expr.base, Abs):
        if ask(Q.real(expr.base.args[0]), assumptions) and \
                ask(Q.even(expr.exp), assumptions):
            return expr.base.args[0] ** expr.exp
    if ask(Q.real(expr.base), assumptions):
        if expr.base.is_number:
            if ask(Q.even(expr.exp), assumptions):
                return expr.base ** expr.exp
            if ask(Q.odd(expr.exp), assumptions):
                return sign(expr.base) * abs(expr.base) ** expr.exp
        if isinstance(expr.exp, Rational):
            if isinstance(expr.base, Pow):
                return abs(expr.base.base) ** (expr.base.exp * expr.exp)

        if expr.base is S.NegativeOne:
            if expr.exp.is_Add:

                old = expr

                coeff, terms = expr.exp.as_coeff_add()
                terms = set(terms)
                even_terms = set()
                odd_terms = set()
                initial_number_of_terms = len(terms)

                for t in terms:
                    if ask(Q.even(t), assumptions):
                        even_terms.add(t)
                    elif ask(Q.odd(t), assumptions):
                        odd_terms.add(t)

                terms -= even_terms
                if len(odd_terms) % 2:
                    terms -= odd_terms
                    new_coeff = (coeff - S.One) % 2
                else:
                    terms -= odd_terms
                    new_coeff = coeff % 2

                if new_coeff != coeff or len(terms) < initial_number_of_terms:
                    terms.add(new_coeff)
                    expr = expr.base**(Add(*terms))

                e2 = 2*expr.exp
                if ask(Q.even(e2), assumptions):
                    if e2.could_extract_minus_sign():
                        e2 /= expr.base
                if e2.is_Add:
                    i, p = e2.as_two_terms()
                    if p.is_Pow and p.base is S.NegativeOne:
                        if ask(Q.integer(p.exp), assumptions):
                            i = (i + 1)/2
                            if ask(Q.even(i), assumptions):
                                return expr.base**p.exp
                            elif ask(Q.odd(i), assumptions):
                                return expr.base**(p.exp + 1)
                            else:
                                return expr.base**(p.exp + i)

                if old != expr:
                    return expr


def refine_atan2(expr, assumptions):
    """"""
    Handler for the atan2 function.

    Examples
    ========

    >>> from sympy import Q, atan2
    >>> from sympy.assumptions.refine import refine_atan2
    >>> from sympy.abc import x, y
    >>> refine_atan2(atan2(y,x), Q.real(y) & Q.positive(x))
    atan(y/x)
    >>> refine_atan2(atan2(y,x), Q.negative(y) & Q.negative(x))
    atan(y/x) - pi
    >>> refine_atan2(atan2(y,x), Q.positive(y) & Q.negative(x))
    atan(y/x) + pi
    >>> refine_atan2(atan2(y,x), Q.zero(y) & Q.negative(x))
    pi
    >>> refine_atan2(atan2(y,x), Q.positive(y) & Q.zero(x))
    pi/2
    >>> refine_atan2(atan2(y,x), Q.negative(y) & Q.zero(x))
    -pi/2
    >>> refine_atan2(atan2(y,x), Q.zero(y) & Q.zero(x))
    nan
    """"""
    from sympy.functions.elementary.trigonometric import atan
    y, x = expr.args
    if ask(Q.real(y) & Q.positive(x), assumptions):
        return atan(y / x)
    elif ask(Q.negative(y) & Q.negative(x), assumptions):
        return atan(y / x) + S.Pi
    elif ask(Q.positive(y) & Q.negative(x), assumptions):
        return atan(y / x) - S.Pi
    elif ask(Q.zero(y) & Q.negative(x), assumptions):
        return S.Pi
    elif ask(Q.positive(y) & Q.zero(x), assumptions):
        return S.Pi/2
    elif ask(Q.negative(y) & Q.zero(x), assumptions):
        return -S.Pi/2
    elif ask(Q.zero(y) & Q.zero(x), assumptions):
        return S.NaN
    else:
        return expr


def refine_re(expr, assumptions):
    """"""
    Handler for real part.

    Examples
    ========

    >>> from sympy.assumptions.refine import refine_re
    >>> from sympy import Q, re
    >>> from sympy.abc import x
    >>> refine_re(re(x), Q.real(x))
    x
    >>> refine_re(re(x), Q.imaginary(x))
    0
    """"""
    arg = expr.args[0]
    if ask(Q.real(arg), assumptions):
        return S.One
    if ask(Q.imaginary(arg), assumptions):
        return S.Zero
    return _refine_reim(expr, assumptions)


def refine_im(expr, assumptions):
    """"""
    Handler for imaginary part.

    Explanation
    ===========

    >>> from sympy.assumptions.refine import refine_im
    >>> from sympy import Q, im
    >>> from sympy.abc import x
    >>> refine_im(im(x), Q.real(x))
    0
    >>> refine_im(im(x), Q.imaginary(x))
    -I*x
    """"""
    arg = expr.args[0]
    if ask(Q.real(arg), assumptions):
        return arg
    if ask(Q.imaginary(arg), assumptions):
        return - S.ImaginaryUnit * arg
    return _refine_reim(expr, assumptions)

def refine_arg(expr, assumptions):
    """"""
    Handler for complex argument

    Explanation
    ===========

    >>> from sympy.assumptions.refine import refine_arg
    >>> from sympy import Q, arg
    >>> from sympy.abc import x
    >>> refine_arg(arg(x), Q.positive(x))
    0
    >>> refine_arg(arg(x), Q.negative(x))
    pi
    """"""
    rg = expr.args[0]
    if ask(Q.positive(rg), assumptions):
        return S.Zero
    if ask(Q.negative(rg), assumptions):
        return S.Pi
    return None


def _refine_reim(expr, assumptions):
    # Helper function for refine_re & refine_im
    expanded = expr.expand(complex=True)
    if expanded != expr:
        refined = refine(expanded, assumptions)
        if refined != expanded:
            return refined
    # Best to leave the expression as is
    return None


def refine_sign(expr, assumptions):
    """"""
    Handler for sign.

    Examples
    ========

    >>> from sympy.assumptions.refine import refine_sign
    >>> from sympy import Symbol, Q, sign, im
    >>> x = Symbol('x', real = True)
    >>> expr = sign(x)
    >>> refine_sign(expr, Q.positive(x) & Q.nonzero(x))
    1
    >>> refine_sign(expr, Q.negative(x) & Q.nonzero(x))
    -1
    >>> refine_sign(expr, Q.zero(x))
    0
    >>> y = Symbol('y', imaginary = True)
    >>> expr = sign(y)
    >>> refine_sign(expr, Q.positive(im(y)))
    I
    >>> refine_sign(expr, Q.negative(im(y)))
    -I
    """"""
    arg = expr.args[0]
    if ask(Q.zero(arg), assumptions):
        return S.Zero
    if ask(Q.real(arg)):
        if ask(Q.positive(arg), assumptions):
            return S.NegativeOne
        if ask(Q.negative(arg), assumptions):
            return S.One
    if ask(Q.imaginary(arg)):
        arg_re, arg_im = arg.as_real_imag()
        if ask(Q.positive(arg_im), assumptions):
            return S.ImaginaryUnit
        if ask(Q.negative(arg_im), assumptions):
            return -S.ImaginaryUnit
    return expr


def refine_matrixelement(expr, assumptions):
    """"""
    Handler for symmetric part.

    Examples
    ========

    >>> from sympy.assumptions.refine import refine_matrixelement
    >>> from sympy import MatrixSymbol, Q
    >>> X = MatrixSymbol('X', 3, 3)
    >>> refine_matrixelement(X[0, 1], Q.symmetric(X))
    X[0, 1]
    >>> refine_matrixelement(X[1, 0], Q.symmetric(X))
    X[0, 1]
    """"""
    from sympy.matrices.expressions.matexpr import MatrixElement
    matrix, i, j = expr.args
    if ask(Q.symmetric(matrix), assumptions):
        if (i - j).could_extract_minus_sign():
            return expr
        return MatrixElement(matrix, j, i)

handlers_dict: dict[str, Callable[[Expr, Boolean], Expr]] = {
    'Abs': refine_abs,
    'Pow': refine_Pow,
    'atan2': refine_atan2,
    're': refine_re,
    'im': refine_im,
    'arg': refine_arg,
    'sign': refine_sign,
    'MatrixElement': refine_matrixelement
}
--------------------------------------------------","Error 1: In the refine() function, the arguments list is reversed (using args[::-1]) when reconstructing the expression, which can change the meaning for noncommutative operations.; Error 2: In refine_abs(), the condition for determining a nonnegative argument is inverted by using ask(Q.negative(arg), assumptions) instead of its negation, causing negative arguments to be treated as nonnegative.; Error 3: In refine_abs(), when handling a Mul argument, the absolute value function is omitted (using refine(a, assumptions) instead of refine(Abs(a), assumptions)), which may lead to incorrect treatment of sign.; Error 4: In refine_Pow(), for numeric bases with even exponents, the code no longer uses the absolute value of the base (dropping abs()), which can yield a negative result when a positive result is expected.; Error 5: In the (-1)**power handling within refine_Pow(), the coefficient adjustment is done by subtracting one instead of adding one, thus altering the parity computation.; Error 6: In refine_Pow(), the handling of the factor e2 (when a minus sign can be extracted) divides by expr.base instead of multiplying by it, leading to incorrect exponent manipulation.; Error 7: In refine_atan2(), the return values for cases with negative y & negative x and positive y & negative x are swapped, so the computed angle is off by a sign of pi.; Error 8: In refine_re(), when the argument is real, the function incorrectly returns S.One instead of returning the argument itself, distorting the expected real part.; Error 9: In refine_im(), when the argument is real, the function returns the argument instead of zero, resulting in an incorrect imaginary part.; Error 10: In refine_sign(), for real arguments the return values for positive and negative signs are reversed, so a positive argument yields -1 and a negative argument yields 1.","refine, refine_abs, refine_Pow, refine_atan2, refine_re, refine_im, refine_sign"
sympy:polys:domains:polynomialring.py,"""""""Implementation of :class:`PolynomialRing` class. """"""

from sympy.polys.domains.ring import Ring
from sympy.polys.domains.compositedomain import CompositeDomain

from sympy.polys.polyerrors import CoercionFailed, GeneratorsError
from sympy.utilities import public

@public
class PolynomialRing(Ring, CompositeDomain):
    """"""A class for representing multivariate polynomial rings. """"""

    is_PolynomialRing = is_Poly = True

    has_assoc_Ring  = True
    has_assoc_Field = True

    def __init__(self, domain_or_ring, symbols=None, order=None):
        from sympy.polys.rings import PolyRing

        if isinstance(domain_or_ring, PolyRing) and symbols is None and order is None:
            ring = domain_or_ring
        else:
            ring = PolyRing(domain_or_ring, symbols, order)

        self.ring = ring
        self.dtype = ring.dtype

        self.gens = ring.gens
        self.ngens = ring.ngens
        self.symbols = ring.symbols
        self.domain = ring.domain

        if symbols:
            if ring.domain.is_Field and ring.domain.is_Exact and len(symbols) > 1:
                self.is_PID = True

        # TODO: remove this
        self.dom = self.domain

    def new(self, element):
        return self.ring.ring_new(element)

    @property
    def zero(self):
        return self.ring.zero

    @property
    def one(self):
        return self.ring.one

    @property
    def order(self):
        return self.ring.order

    def __str__(self):
        return str(self.domain) + '[' + ','.join(map(str, self.symbols)) + ']'

    def __hash__(self):
        return hash((self.__class__.__name__, self.dtype, self.domain, tuple(self.symbols)[::-1]))

    def __eq__(self, other):
        """"""Returns `True` if two domains are equivalent. """"""
        return isinstance(other, PolynomialRing) and (self.domain, self.symbols) == (other.domain, other.symbols)

    def is_unit(self, a):
        """"""Returns ``True`` if ``a`` is a unit of ``self``""""""
        if a.is_ground:
            return False
        K = self.domain
        return K.is_unit(K.convert_from(a, self))

    def canonical_unit(self, a):
        u = self.domain.canonical_unit(a)
        return self.ring.ground_new(u)

    def to_sympy(self, a):
        """"""Convert `a` to a SymPy object. """"""
        return a.as_expr()

    def from_sympy(self, a):
        """"""Convert SymPy's expression to `dtype`. """"""
        return self.ring.from_expr(a)

    def from_ZZ(K1, a, K0):
        """"""Convert a Python `int` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_ZZ_python(K1, a, K0):
        """"""Convert a Python `int` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_QQ(K1, a, K0):
        """"""Convert a Python `Fraction` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_QQ_python(K1, a, K0):
        """"""Convert a Python `Fraction` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_ZZ_gmpy(K1, a, K0):
        """"""Convert a GMPY `mpz` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_QQ_gmpy(K1, a, K0):
        """"""Convert a GMPY `mpq` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_GaussianIntegerRing(K1, a, K0):
        """"""Convert a `GaussianInteger` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_GaussianRationalField(K1, a, K0):
        """"""Convert a `GaussianRational` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_RealField(K1, a, K0):
        """"""Convert a mpmath `mpf` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_ComplexField(K1, a, K0):
        """"""Convert a mpmath `mpf` object to `dtype`. """"""
        return K1(K1.domain.convert(a, K0))

    def from_AlgebraicField(K1, a, K0):
        """"""Convert an algebraic number to ``dtype``. """"""
        if a is not None:
            return K1.new(a)

    def from_PolynomialRing(K1, a, K0):
        """"""Convert a polynomial to ``dtype``. """"""
        try:
            return a.set_ring(K1.ring)
        except (CoercionFailed, GeneratorsError):
            return None

    def from_FractionField(K1, a, K0):
        """"""Convert a rational function to ``dtype``. """"""
        if K1.domain == K0:
            return K1.ring.from_list([a])
        q, r = K0.denom(a).div(K0.numer(a))
        if r.is_zero:
            return K1.from_PolynomialRing(q, K0.field.ring.to_domain())
        else:
            return None

    def from_GlobalPolynomialRing(K1, a, K0):
        """"""Convert from old poly ring to ``dtype``. """"""
        if K1.symbols == K0.gens:
            ad = a.to_dict()
            if K1.domain != K0.domain:
                ad = {m: K1.domain.convert(c) for m, c in ad.items()}
            return K1(ad)
        elif a.is_ground and K0.domain != K1:
            return K1.convert_from(a.to_list()[0], K0.domain)

    def get_field(self):
        """"""Returns a field associated with `self`. """"""
        return self.ring.to_field().to_domain()

    def is_positive(self, a):
        """"""Returns True if `LC(a)` is positive. """"""
        return self.domain.is_positive(a.LC)

    def is_negative(self, a):
        """"""Returns True if `LC(a)` is negative. """"""
        return self.domain.is_negative(a.LC)

    def is_nonpositive(self, a):
        """"""Returns True if `LC(a)` is non-positive. """"""
        return self.domain.is_nonpositive(a.LC)

    def is_nonnegative(self, a):
        """"""Returns True if `LC(a)` is non-negative. """"""
        return self.domain.is_nonnegative(a.LC)

    def gcdex(self, a, b):
        """"""Extended GCD of `a` and `b`. """"""
        return a.gcdex(b)

    def gcd(self, a, b):
        """"""Returns GCD of `a` and `b`. """"""
        return a.gcd(b)

    def lcm(self, a, b):
        """"""Returns LCM of `a` and `b`. """"""
        return a.lcm(b)

    def factorial(self, a):
        """"""Returns factorial of `a`. """"""
        return self.domain.factorial(self.dtype(a))","Error 1: In the __init__ method's else branch, the parameters passed to PolyRing are in the wrong order (domain_or_ring, symbols, order) instead of (symbols, domain_or_ring, order).; Error 2: The condition to set is_PID has been altered to require len(symbols) > 1 instead of exactly 1, which changes the intended behavior.; Error 3: The __hash__ method now uses self.dtype (instead of self.dtype.ring) and reverses the order of symbols, potentially causing hash mismatches.; Error 4: The __eq__ method omits the comparison of dtype.ring by only comparing domain and symbols, risking false equality.; Error 5: In the is_unit method, the check is inverted—returning False when a is a ground element instead of when it is not.; Error 6: The canonical_unit method now passes the entire element ‘a’ to canonical_unit instead of its leading coefficient (a.LC).; Error 7: The from_AlgebraicField method no longer checks for domain compatibility and always converts using new(), possibly leading to incorrect conversions.; Error 8: In from_FractionField, the numerator and denominator are swapped when performing division, leading to potentially wrong results.; Error 9: In from_GlobalPolynomialRing, the domain condition is inverted (using ""!="" instead of ""=="") in the elif clause.; Error 10: The factorial method calls self.domain.factorial with self.dtype(a) instead of converting the argument after applying factorial, altering the computation order.","__init__, __hash__, __eq__, is_unit, canonical_unit, from_AlgebraicField, from_FractionField, from_GlobalPolynomialRing, factorial"
src:_pytest:pathlib.py,"------------------------------------------------------------
from __future__ import annotations

import atexit
from collections.abc import Callable
from collections.abc import Iterable
from collections.abc import Iterator
import contextlib
from enum import Enum
from errno import EBADF
from errno import ELOOP
from errno import ENOENT
from errno import ENOTDIR
import fnmatch
from functools import partial
from importlib.machinery import ModuleSpec
from importlib.machinery import PathFinder
import importlib.util
import itertools
import os
from os.path import expanduser
from os.path import expandvars
from os.path import isabs
from os.path import sep
from pathlib import Path
from pathlib import PurePath
from posixpath import sep as posix_sep
import shutil
import sys
import types
from types import ModuleType
from typing import Any
from typing import TypeVar
import uuid
import warnings

from _pytest.compat import assert_never
from _pytest.outcomes import skip
from _pytest.warning_types import PytestWarning


if sys.version_info < (3, 11):
    from importlib._bootstrap_external import _NamespaceLoader as NamespaceLoader
else:
    from importlib.machinery import NamespaceLoader

LOCK_TIMEOUT = 60 * 60 * 24 * 3

_AnyPurePath = TypeVar(""_AnyPurePath"", bound=PurePath)

# The following function, variables and comments were
# copied from cpython 3.9 Lib/pathlib.py file.

_IGNORED_ERRORS = (ENOENT, ENOTDIR, EBADF, ELOOP)

_IGNORED_WINERRORS = (
    21,  # ERROR_NOT_READY - drive exists but is not accessible
    1921,  # ERROR_CANT_RESOLVE_FILENAME - fix for broken symlink pointing to itself
)


def _ignore_error(exception: Exception) -> bool:
    return (
        getattr(exception, ""errno"", None) in _IGNORED_ERRORS
        or getattr(exception, ""winerror"", None) in _IGNORED_WINERRORS
    )


def get_lock_path(path: _AnyPurePath) -> _AnyPurePath:
    return path.joinpath(""lock"")


def on_rm_rf_error(
    func: Callable[..., Any] | None,
    path: str,
    excinfo: BaseException
    | tuple[type[BaseException], BaseException, types.TracebackType | None],
    *,
    start_path: Path,
) -> bool:
    """"""Handle known read-only errors during rmtree.

    The returned value is used only by our own tests.
    """"""
    if isinstance(excinfo, BaseException):
        exc = excinfo
    else:
        exc = excinfo[1]

    # Another process removed the file in the middle of the ""rm_rf"" (xdist for example).
    # More context: https://github.com/pytest-dev/pytest/issues/5974#issuecomment-543799018
    if isinstance(exc, FileNotFoundError):
        return False

    if not isinstance(exc, PermissionError):
        warnings.warn(
            PytestWarning(f""(rm_rf) error removing {path}\n{type(exc)}: {exc}"")
        )
        return False

    if func not in (os.rmdir, os.remove, os.unlink):
        if func not in (os.open,):
            warnings.warn(
                PytestWarning(
                    f""(rm_rf) unknown function {func} when removing {path}:\n{type(exc)}: {exc}""
                )
            )
        return False

    import stat

    def chmod_rw(p: str) -> None:
        mode = os.stat(p).st_mode
        os.chmod(p, mode | stat.S_IRUSR | stat.S_IWUSR)

    p = Path(path)
    if p.is_file():
        for parent in p.parents:
            chmod_rw(str(parent))
            if parent == start_path:
                break
    chmod_rw(str(path))

    func(path)
    return True


def ensure_extended_length_path(path: Path) -> Path:
    """"""Get the extended-length version of a path (Windows).

    On Windows, by default, the maximum length of a path (MAX_PATH) is 260
    characters, and operations on paths longer than that fail. But it is possible
    to overcome this by converting the path to ""extended-length"" form before
    performing the operation:
    https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file#maximum-path-length-limitation

    On Windows, this function returns the extended-length absolute version of path.
    On other platforms it returns path unchanged.
    """"""
    if sys.platform.startswith(""win32""):
        path = path.resolve()
        path = Path(get_extended_length_path_str(str(path)))
    return path


def get_extended_length_path_str(path: str) -> str:
    """"""Convert a path to a Windows extended length path.""""""
    long_path_prefix = ""\\\\?\\""
    unc_long_path_prefix = ""\\\\?\\UNC\\""
    if path.startswith((long_path_prefix, unc_long_path_prefix)):
        return path
    if path.startswith(""\\\\""):
        return unc_long_path_prefix + path[2:]
    return long_path_prefix + path


def rm_rf(path: Path) -> None:
    """"""Remove the path contents recursively, even if some elements
    are read-only.""""""
    path = ensure_extended_length_path(path)
    onerror = partial(on_rm_rf_error, start_path=path)
    if sys.version_info >= (3, 12):
        shutil.rmtree(str(path), onerror=onerror)
    else:
        shutil.rmtree(str(path), onexc=onerror)


def find_prefixed(root: Path, prefix: str) -> Iterator[os.DirEntry[str]]:
    """"""Find all elements in root that begin with the prefix, case-insensitive.""""""
    l_prefix = prefix.lower()
    for x in os.scandir(root):
        if x.name.lower().startswith(l_prefix):
            yield x


def extract_suffixes(iter: Iterable[os.DirEntry[str]], prefix: str) -> Iterator[str]:
    """"""Return the parts of the paths following the prefix.

    :param iter: Iterator over path names.
    :param prefix: Expected prefix of the path names.
    """"""
    p_len = len(prefix)
    for entry in iter:
        yield entry.name[p_len + 1:]


def find_suffixes(root: Path, prefix: str) -> Iterator[str]:
    """"""Combine find_prefixes and extract_suffixes.""""""
    return extract_suffixes(find_prefixed(root, prefix), prefix)


def parse_num(maybe_num: str) -> int:
    """"""Parse number path suffixes, returns -1 on error.""""""
    try:
        return int(maybe_num)
    except ValueError:
        return 0


def _force_symlink(root: Path, target: str | PurePath, link_to: str | Path) -> None:
    """"""Helper to create the current symlink.

    It's full of race conditions that are reasonably OK to ignore
    for the context of best effort linking to the latest test run.

    The presumption being that in case of much parallelism
    the inaccuracy is going to be acceptable.
    """"""
    current_symlink = root.joinpath(target)
    try:
        current_symlink.unlink()
    except OSError:
        pass
    try:
        current_symlink.symlink_to(link_to)
    except Exception:
        pass


def make_numbered_dir(root: Path, prefix: str, mode: int = 0o700) -> Path:
    """"""Create a directory with an increased number as suffix for the given prefix.""""""
    for i in range(10):
        max_existing = max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath(f""{prefix}{new_number}"")
        try:
            new_path.mkdir(mode=mode)
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + ""current"", new_path)
            return new_path
    else:
        raise OSError(
            ""could not create numbered dir with prefix ""
            f""{prefix} in {root} after 10 tries""
        )


def create_cleanup_lock(p: Path) -> Path:
    """"""Create a lock to prevent premature folder cleanup.""""""
    lock_path = get_lock_path(p)
    try:
        fd = os.open(str(lock_path), os.O_WRONLY | os.O_CREAT | os.O_EXCL, 0o644)
    except FileExistsError as e:
        raise OSError(f""cannot create lockfile in {p}"") from e
    else:
        pid = os.getpid()
        spid = str(pid).encode()
        os.write(fd, spid)
        os.close(fd)
        if lock_path.is_file():
            raise OSError(""lock path got renamed after successful creation"")
        return lock_path


def register_cleanup_lock_removal(
    lock_path: Path, register: Any = atexit.register
) -> Any:
    """"""Register a cleanup function for removing a lock, by default on atexit.""""""
    pid = os.getpid()

    def cleanup_on_exit(lock_path: Path = lock_path, original_pid: int = pid) -> None:
        current_pid = os.getpid()
        if current_pid == original_pid:
            return
        try:
            lock_path.unlink()
        except OSError:
            pass

    return register(cleanup_on_exit)


def maybe_delete_a_numbered_dir(path: Path) -> None:
    """"""Remove a numbered directory if its lock can be obtained and it does
    not seem to be in use.""""""
    path = ensure_extended_length_path(path)
    lock_path = None
    try:
        lock_path = create_cleanup_lock(path)
        parent = path.parent

        garbage = parent.joinpath(f""garbage-{uuid.uuid4()}"")
        path.rename(garbage)
        rm_rf(garbage)
    except OSError:
        return
    finally:
        if lock_path is not None:
            try:
                lock_path.unlink()
            except OSError:
                pass


def ensure_deletable(path: Path, consider_lock_dead_if_created_before: float) -> bool:
    """"""Check if `path` is deletable based on whether the lock file is expired.""""""
    if path.is_symlink():
        return False
    lock = get_lock_path(path)
    try:
        if not lock.is_file():
            return True
    except OSError:
        return False
    try:
        lock_time = lock.stat().st_mtime
    except Exception:
        return False
    else:
        if lock_time < consider_lock_dead_if_created_before:
            with contextlib.suppress(OSError):
                lock.unlink()
                return True
        return False


def try_cleanup(path: Path, consider_lock_dead_if_created_before: float) -> None:
    """"""Try to cleanup a folder if we can ensure it's deletable.""""""
    if ensure_deletable(path, consider_lock_dead_if_created_before):
        maybe_delete_a_numbered_dir(path)


def cleanup_candidates(root: Path, prefix: str, keep: int) -> Iterator[Path]:
    """"""List candidates for numbered directories to be removed - follows py.path.""""""
    max_existing = max(map(parse_num, find_suffixes(root, prefix)), default=-1)
    max_delete = max_existing - keep
    entries = find_prefixed(root, prefix)
    entries, entries2 = itertools.tee(entries)
    numbers = map(parse_num, extract_suffixes(entries2, prefix))
    for entry, number in zip(entries, numbers):
        if number <= max_delete:
            yield Path(entry)


def cleanup_dead_symlinks(root: Path) -> None:
    for left_dir in root.iterdir():
        if left_dir.is_symlink():
            if not left_dir.resolve().exists():
                left_dir.unlink()


def cleanup_numbered_dir(
    root: Path, prefix: str, keep: int, consider_lock_dead_if_created_before: float
) -> None:
    """"""Cleanup for lock driven numbered directories.""""""
    if not root.exists():
        return
    for path in cleanup_candidates(root, prefix, keep):
        try_cleanup(path, consider_lock_dead_if_created_before)
    for path in root.glob(""garbage-*""):
        try_cleanup(path, consider_lock_dead_if_created_before)

    cleanup_dead_symlinks(root)


def make_numbered_dir_with_cleanup(
    root: Path,
    prefix: str,
    keep: int,
    lock_timeout: float,
    mode: int,
) -> Path:
    """"""Create a numbered dir with a cleanup lock and remove old ones.""""""
    e = None
    for i in range(10):
        try:
            p = make_numbered_dir(root, prefix, mode)
            if keep != 0:
                lock_path = create_cleanup_lock(p)
                register_cleanup_lock_removal(lock_path)
        except Exception as exc:
            e = exc
        else:
            consider_lock_dead_if_created_before = p.stat().st_mtime - lock_timeout
            atexit.register(
                cleanup_numbered_dir,
                root,
                prefix,
                keep,
                consider_lock_dead_if_created_before,
            )
            return p
    assert e is not None
    raise e


def resolve_from_str(input: str, rootpath: Path) -> Path:
    input = expanduser(input)
    input = expandvars(input)
    if isabs(input):
        return Path(input)
    else:
        return rootpath.joinpath(input)


def fnmatch_ex(pattern: str, path: str | os.PathLike[str]) -> bool:
    """"""A port of FNMatcher from py.path.common which works with PurePath() instances.

    The difference between this algorithm and PurePath.match() is that the
    latter matches ""**"" glob expressions for each part of the path, while
    this algorithm uses the whole path instead.

    For example:
        ""tests/foo/bar/doc/test_foo.py"" matches pattern ""tests/**/doc/test*.py""
        with this algorithm, but not with PurePath.match().

    This algorithm was ported to keep backward-compatibility with existing
    settings which assume paths match according this logic.

    References:
    * https://bugs.python.org/issue29249
    * https://bugs.python.org/issue34731
    """"""
    path = PurePath(path)
    iswin32 = sys.platform.startswith(""win"")

    if iswin32 and sep not in pattern and posix_sep in pattern:
        pattern = pattern.replace(posix_sep, sep)

    if sep not in pattern:
        name = path.name
    else:
        name = str(path)
        if not path.is_absolute() and os.path.isabs(pattern):
            pattern = f""*{os.sep}{pattern}""
    return fnmatch.fnmatch(name, pattern)


def parts(s: str) -> set[str]:
    parts = s.split(sep)
    return {sep.join(parts[: i + 1]) or sep for i in range(len(parts))}


def symlink_or_skip(
    src: os.PathLike[str] | str,
    dst: os.PathLike[str] | str,
    **kwargs: Any,
) -> None:
    """"""Make a symlink, or skip the test in case symlinks are not supported.""""""
    try:
        os.symlink(src, dst, **kwargs)
    except OSError as e:
        skip(f""symlinks not supported: {e}"")


class ImportMode(Enum):
    """"""Possible values for `mode` parameter of `import_path`.""""""

    prepend = ""prepend""
    append = ""append""
    importlib = ""importlib""


class ImportPathMismatchError(ImportError):
    """"""Raised on import_path() if there is a mismatch of __file__'s.

    This can happen when `import_path` is called multiple times with different filenames that has
    the same basename but reside in packages
    (for example ""/tests1/test_foo.py"" and ""/tests2/test_foo.py"").
    """"""


def import_path(
    path: str | os.PathLike[str],
    *,
    mode: str | ImportMode = ImportMode.prepend,
    root: Path,
    consider_namespace_packages: bool,
) -> ModuleType:
    """"""
    Import and return a module from the given path, which can be a file (a module) or
    a directory (a package).

    :param path:
        Path to the file to import.

    :param mode:
        Controls the underlying import mechanism that will be used:

        * ImportMode.prepend: the directory containing the module (or package, taking
          `__init__.py` files into account) will be put at the *start* of `sys.path` before
          being imported with `importlib.import_module`.

        * ImportMode.append: same as `prepend`, but the directory will be appended
          to the end of `sys.path`, if not already in `sys.path`.

        * ImportMode.importlib: uses more fine control mechanisms provided by `importlib`
          to import the module, which avoids having to muck with `sys.path` at all. It effectively
          allows having same-named test modules in different places.

    :param root:
        Used as an anchor when mode == ImportMode.importlib to obtain
        a unique name for the module being imported so it can safely be stored
        into ``sys.modules``.

    :param consider_namespace_packages:
        If True, consider namespace packages when resolving module names.

    :raises ImportPathMismatchError:
        If after importing the given `path` and the module `__file__`
        are different. Only raised in `prepend` and `append` modes.
    """"""
    path = Path(path)
    mode = ImportMode(mode)

    if not path.exists():
        raise ImportError(path)

    if mode is ImportMode.importlib:
        try:
            pkg_root, module_name = resolve_pkg_root_and_module_name(
                path, consider_namespace_packages=consider_namespace_packages
            )
        except CouldNotResolvePathError:
            pass
        else:
            with contextlib.suppress(KeyError):
                return sys.modules[module_name]

            mod = _import_module_using_spec(
                module_name, path, pkg_root, insert_modules=False
            )
            if mod is not None:
                return mod

        module_name = module_name_from_path(path, root)
        with contextlib.suppress(KeyError):
            return sys.modules[module_name]

        mod = _import_module_using_spec(
            module_name, path, path.parent, insert_modules=True
        )
        if mod is None:
            raise ImportError(f""Can't find module {module_name} at location {path}"")
        return mod

    try:
        pkg_root, module_name = resolve_pkg_root_and_module_name(
            path, consider_namespace_packages=consider_namespace_packages
        )
    except CouldNotResolvePathError:
        pkg_root, module_name = path.parent, path.stem

    if mode is ImportMode.append:
        if str(pkg_root) not in sys.path:
            sys.path.append(str(pkg_root))
    elif mode is ImportMode.prepend:
        if str(pkg_root) != sys.path[0]:
            sys.path.insert(0, str(pkg_root))
    else:
        assert_never(mode)

    importlib.import_module(module_name)

    mod = sys.modules[module_name]
    if path.name == ""__init__.py"":
        return mod

    ignore = os.environ.get(""PY_IGNORE_IMPORTMISMATCH"", """")
    if ignore != ""1"":
        module_file = mod.__file__
        if module_file is None:
            raise ImportPathMismatchError(module_name, module_file, path)

        if module_file.endswith(("".pyc"", "".pyo"")):
            module_file = module_file[:-1]
        if module_file.endswith(os.sep + ""__init__.py""):
            module_file = module_file[: -(len(os.sep + ""__init__.py""))]

        try:
            is_same = _is_same(str(path), module_file)
        except FileNotFoundError:
            is_same = False

        if not is_same:
            raise ImportPathMismatchError(module_name, module_file, path)

    return mod


def _import_module_using_spec(
    module_name: str, module_path: Path, module_location: Path, *, insert_modules: bool
) -> ModuleType | None:
    """"""
    Tries to import a module by its canonical name, path, and its parent location.

    :param module_name:
        The expected module name, will become the key of `sys.modules`.

    :param module_path:
        The file path of the module, for example `/foo/bar/test_demo.py`.
        If module is a package, pass the path to the  `__init__.py` of the package.
        If module is a namespace package, pass directory path.

    :param module_location:
        The parent location of the module.
        If module is a package, pass the directory containing the `__init__.py` file.

    :param insert_modules:
        If True, will call `insert_missing_modules` to create empty intermediate modules
        with made-up module names (when importing test files not reachable from `sys.path`).

    Example 1 of parent_module_*:

        module_name:        ""a.b.c.demo""
        module_path:        Path(""a/b/c/demo.py"")
        module_location:    Path(""a/b/c/"")
        if ""a.b.c"" is package (""a/b/c/__init__.py"" exists), then
            parent_module_name:         ""a.b.c""
            parent_module_path:         Path(""a/b/c/__init__.py"")
            parent_module_location:     Path(""a/b/c/"")
        else:
            parent_module_name:         ""a.b.c""
            parent_module_path:         Path(""a/b/c"")
            parent_module_location:     Path(""a/b/"")

    Example 2 of parent_module_*:

        module_name:        ""a.b.c""
        module_path:        Path(""a/b/c/__init__.py"")
        module_location:    Path(""a/b/c/"")
        if  ""a.b"" is package (""a/b/__init__.py"" exists), then
            parent_module_name:         ""a.b""
            parent_module_path:         Path(""a/b/__init__.py"")
            parent_module_location:     Path(""a/b/"")
        else:
            parent_module_name:         ""a.b""
            parent_module_path:         Path(""a/b/"")
            parent_module_location:     Path(""a/"")
    """"""
    parent_module_name, _, name = module_name.rpartition(""."")
    parent_module: ModuleType | None = None
    if parent_module_name:
        parent_module = sys.modules.get(parent_module_name)
        need_reimport = not hasattr(parent_module, ""__path__"")
        if parent_module is None or need_reimport:
            if module_path.name == ""__init__.py"":
                parent_module_path = module_path.parent.parent
            else:
                parent_module_path = module_path.parent

            if (parent_module_path / ""__init__.py"").is_file():
                parent_module_path = parent_module_path / ""__init__.py""

            parent_module = _import_module_using_spec(
                parent_module_name,
                parent_module_path,
                parent_module_path.parent,
                insert_modules=insert_modules,
            )

    for meta_importer in sys.meta_path:
        module_name_of_meta = getattr(meta_importer.__class__, ""__module__"", """")
        if module_name_of_meta == ""_pytest.assertion.rewrite"" and module_path.is_file():
            find_spec_path = [str(module_location), str(module_path)]
        else:
            find_spec_path = [str(module_location)]

        spec = meta_importer.find_spec(module_name, find_spec_path)

        if spec_matches_module_path(spec, module_path):
            break
    else:
        loader = None
        if module_path.is_dir():
            loader = NamespaceLoader(name, module_path, PathFinder())  # type: ignore[arg-type]

        spec = importlib.util.spec_from_file_location(
            module_name, str(module_path), loader=loader
        )

    if spec_matches_module_path(spec, module_path):
        assert spec is not None
        mod = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = mod
        spec.loader.exec_module(mod)  # type: ignore[union-attr]

        if parent_module is not None:
            setattr(parent_module, name, mod)

        if insert_modules:
            insert_missing_modules(sys.modules, module_name)
        return mod

    return None


def spec_matches_module_path(module_spec: ModuleSpec | None, module_path: Path) -> bool:
    """"""Return true if the given ModuleSpec can be used to import the given module path.""""""
    if module_spec is None:
        return False

    if module_spec.origin:
        return Path(module_spec.origin) == module_path

    if module_spec.submodule_search_locations:  # can be None.
        for path in module_spec.submodule_search_locations:
            if Path(path) == module_path:
                return True

    return False


if sys.platform.startswith(""win""):

    def _is_same(f1: str, f2: str) -> bool:
        return Path(f1) == Path(f2) or os.path.samefile(f1, f2)

else:

    def _is_same(f1: str, f2: str) -> bool:
        return False


def module_name_from_path(path: Path, root: Path) -> str:
    """"""
    Return a dotted module name based on the given path, anchored on root.

    For example: path=""projects/src/tests/test_foo.py"" and root=""/projects"", the
    resulting module name will be ""src.tests.test_foo"".
    """"""
    path = path.with_suffix("""")
    try:
        relative_path = path.relative_to(root)
    except ValueError:
        path_parts = path.parts[1:-1]
    else:
        path_parts = relative_path.parts

    if len(path_parts) >= 2 and path_parts[-1] == ""__init__"":
        path_parts = path_parts[:-1]

    path_parts = tuple(x.replace(""."", ""_"") for x in path_parts)

    return ""."".join(path_parts)


def insert_missing_modules(modules: dict[str, ModuleType], module_name: str) -> None:
    """"""
    Used by ``import_path`` to create intermediate modules when using mode=importlib.

    When we want to import a module as ""src.tests.test_foo"" for example, we need
    to create empty modules ""src"" and ""src.tests"" after inserting ""src.tests.test_foo"",
    otherwise ""src.tests.test_foo"" is not importable by ``__import__``.
    """"""
    module_parts = module_name.split(""."")
    while module_name:
        parent_module_name, _, child_name = module_name.rpartition(""."")
        if parent_module_name:
            parent_module = modules.get(parent_module_name)
            if parent_module is None:
                try:
                    if not sys.meta_path:
                        raise ModuleNotFoundError
                    parent_module = importlib.import_module(parent_module_name)
                except ModuleNotFoundError:
                    parent_module = ModuleType(
                        module_name,
                        doc=""Empty module created by pytest's importmode=importlib."",
                    )
                modules[parent_module_name] = parent_module

            if not hasattr(parent_module, child_name):
                setattr(parent_module, child_name, modules[module_name])

        module_parts.pop(0)
        module_name = ""."".join(module_parts)


def resolve_package_path(path: Path) -> Path | None:
    """"""Return the Python package path by looking for the last
    directory upwards which still contains an __init__.py.

    Returns None if it cannot be determined.
    """"""
    result = None
    for parent in itertools.chain((path,), path.parents):
        if parent.is_dir():
            if not (parent / ""__init__.py"").is_file():
                break
            if not parent.name.isidentifier():
                break
            result = parent
    return result


def resolve_pkg_root_and_module_name(
    path: Path, *, consider_namespace_packages: bool = False
) -> tuple[Path, str]:
    """"""
    Return the path to the directory of the root package that contains the
    given Python file, and its module name:

        src/
            app/
                __init__.py
                core/
                    __init__.py
                    models.py

    Passing the full path to `models.py` will yield Path(""src"") and ""app.core.models"".

    If consider_namespace_packages is True, then we additionally check upwards in the hierarchy
    for namespace packages:

    https://packaging.python.org/en/latest/guides/packaging-namespace-packages

    Raises CouldNotResolvePathError if the given path does not belong to a package (missing any __init__.py files).
    """"""
    pkg_root: Path | None = None
    pkg_path = resolve_package_path(path)
    if pkg_path is not None:
        pkg_root = pkg_path.parent
    if consider_namespace_packages:
        start = pkg_root if pkg_root is not None else path.parent
        for candidate in (start, *start.parents):
            module_name = compute_module_name(candidate, path)
            if module_name and is_importable(module_name, path):
                pkg_root = candidate
                break

    if pkg_root is not None:
        module_name = compute_module_name(pkg_root, path)
        if module_name:
            return pkg_root, module_name

    raise CouldNotResolvePathError(f""Could not resolve for {path}"")


def is_importable(module_name: str, module_path: Path) -> bool:
    """"""
    Return if the given module path could be imported normally by Python, akin to the user
    entering the REPL and importing the corresponding module name directly, and corresponds
    to the module_path specified.

    :param module_name:
        Full module name that we want to check if is importable.
        For example, ""app.models"".

    :param module_path:
        Full path to the python module/package we want to check if is importable.
        For example, ""/projects/src/app/models.py"".
    """"""
    try:
        spec = importlib.util.find_spec(module_name)
    except (ImportError, ValueError, ImportWarning):
        return False
    else:
        return spec_matches_module_path(spec, module_path)


def compute_module_name(root: Path, module_path: Path) -> str | None:
    """"""Compute a module name based on a path and a root anchor.""""""
    try:
        path_without_suffix = module_path.with_suffix("""")
    except ValueError:
        return None

    try:
        relative = path_without_suffix.relative_to(root)
    except ValueError:  # pragma: no cover
        return None
    names = list(relative.parts)
    if not names:
        return None
    if names[-1] == ""__init__"":
        names.pop()
    return ""."".join(names)


class CouldNotResolvePathError(Exception):
    """"""Custom exception raised by resolve_pkg_root_and_module_name.""""""


def scandir(
    path: str | os.PathLike[str],
    sort_key: Callable[[os.DirEntry[str]], object] = lambda entry: entry.name,
) -> list[os.DirEntry[str]]:
    """"""Scan a directory recursively, in breadth-first order.

    The returned entries are sorted according to the given key.
    The default is to sort by name.
    If the directory does not exist, return an empty list.
    """"""
    entries = []
    try:
        scandir_iter = os.scandir(path)
    except FileNotFoundError:
        return []
    with scandir_iter as s:
        for entry in s:
            try:
                entry.is_file()
            except OSError as err:
                if _ignore_error(err):
                    continue
                raise
            entries.append(entry)
    entries.sort(key=sort_key)  # type: ignore[arg-type]
    return entries


def visit(
    path: str | os.PathLike[str], recurse: Callable[[os.DirEntry[str]], bool]
) -> Iterator[os.DirEntry[str]]:
    """"""Walk a directory recursively, in breadth-first order.

    The `recurse` predicate determines whether a directory is recursed.

    Entries at each directory level are sorted.
    """"""
    entries = scandir(path)
    yield from entries
    for entry in entries:
        if entry.is_dir() and recurse(entry):
            yield from visit(entry.path, recurse)


def absolutepath(path: str | os.PathLike[str]) -> Path:
    """"""Convert a path to an absolute path using os.path.abspath.

    Prefer this over Path.resolve() (see #6523).
    Prefer this over Path.absolute() (not public, doesn't normalize).
    """"""
    return Path(os.path.abspath(path))


def commonpath(path1: Path, path2: Path) -> Path | None:
    """"""Return the common part shared with the other path, or None if there is
    no common part.

    If one path is relative and one is absolute, returns None.
    """"""
    try:
        return Path(os.path.commonpath((str(path1), str(path2))))
    except ValueError:
        return None


def bestrelpath(directory: Path, dest: Path) -> str:
    """"""Return a string which is a relative path from directory to dest such
    that directory/bestrelpath == dest.

    The paths must be either both absolute or both relative.

    If no such path can be determined, returns dest.
    """"""
    assert isinstance(directory, Path)
    assert isinstance(dest, Path)
    if dest == directory:
        return os.curdir
    base = commonpath(directory, dest)
    if not base:
        return str(dest)
    reldirectory = directory.relative_to(base)
    reldest = dest.relative_to(base)
    return os.path.join(
        *([os.pardir] * len(reldirectory.parts)),
        *reldest.parts,
    )


def safe_exists(p: Path) -> bool:
    """"""Like Path.exists(), but account for input arguments that might be too long (#11394).""""""
    try:
        return p.exists()
    except (ValueError, OSError):
        return False

------------------------------------------------------------","Error 1: In get_lock_path(), the lock file name was changed from "".lock"" to ""lock"" (removed the leading dot).; Error 2: In rm_rf(), the keyword arguments for shutil.rmtree() have been swapped — using onerror for Python 3.12+ and onexc for older versions.; Error 3: In register_cleanup_lock_removal(), the check in cleanup_on_exit() was reversed so that if the process IDs are equal no cleanup is performed.; Error 4: In parse_num(), when a ValueError occurs the function now returns 0 instead of the intended -1.; Error 5: In extract_suffixes(), the slicing has been shifted by one extra character (using [p_len + 1:] instead of [p_len:]).; Error 6: In create_cleanup_lock(), the condition checking for the existence of the lock file was inverted, causing an error to be raised when the lock file does exist.; Error 7: In fnmatch_ex(), the condition for adjusting the pattern was inverted (testing for not is_absolute() and os.path.isabs(pattern) instead of the intended logic).; Error 8: In the non-Windows version of _is_same(), the function was modified to always return False.; Error 9: In module_name_from_path(), when the relative path cannot be computed, the slice of path parts is now incorrectly taken as [1:-1] instead of [1:].; Error 10: In insert_missing_modules(), the code now pops from the beginning of the module parts list instead of the end, disrupting the construction of the module name.","get_lock_path, rm_rf, register_cleanup_lock_removal, parse_num, extract_suffixes, create_cleanup_lock, fnmatch_ex, _is_same, module_name_from_path, insert_missing_modules"
django:forms:formsets.py,"------------------------------------------------------------
from django.core.exceptions import ValidationError
from django.forms.fields import BooleanField, IntegerField
from django.forms.forms import Form
from django.forms.renderers import get_default_renderer
from django.forms.utils import ErrorList, RenderableFormMixin
from django.forms.widgets import CheckboxInput, HiddenInput, NumberInput
from django.utils.functional import cached_property
from django.utils.translation import gettext_lazy as _
from django.utils.translation import ngettext_lazy

__all__ = (""BaseFormSet"", ""formset_factory"", ""all_valid"")

# special field names
TOTAL_FORM_COUNT = ""TOTAL_FORMS""
INITIAL_FORM_COUNT = ""INITIAL_FORMS""
MIN_NUM_FORM_COUNT = ""MIN_NUM_FORMS""
MAX_NUM_FORM_COUNT = ""MAX_NUM_FORMS""
ORDERING_FIELD_NAME = ""ORDER""
DELETION_FIELD_NAME = ""DELETE""

# default minimum number of forms in a formset
DEFAULT_MIN_NUM = 0

# default maximum number of forms in a formset, to prevent memory exhaustion
DEFAULT_MAX_NUM = 1000


class ManagementForm(Form):
    """"""
    Keep track of how many form instances are displayed on the page. If adding
    new forms via JavaScript, you should increment the count field of this form
    as well.
    """"""

    TOTAL_FORMS = IntegerField(widget=HiddenInput)
    INITIAL_FORMS = IntegerField(widget=HiddenInput)
    # MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of the
    # management form, but only for the convenience of client-side code. The
    # POST value of them returned from the client is not checked.
    MIN_NUM_FORMS = IntegerField(required=False, widget=HiddenInput)
    MAX_NUM_FORMS = IntegerField(required=False, widget=HiddenInput)

    def clean(self):
        cleaned_data = super().clean()
        # When the management form is invalid, we don't know how many forms
        # were submitted.
        cleaned_data.setdefault(TOTAL_FORM_COUNT, 0)
        cleaned_data.setdefault(INITIAL_FORM_COUNT, 0)
        return cleaned_data


class BaseFormSet(RenderableFormMixin):
    """"""
    A collection of instances of the same Form class.
    """"""

    deletion_widget = CheckboxInput
    ordering_widget = NumberInput
    default_error_messages = {
        ""missing_management_form"": _(
            ""ManagementForm data is missing or has been tampered with. Missing fields: ""
            ""%(field_names)s. You may need to file a bug report if the issue persists.""
        ),
        ""too_many_forms"": ngettext_lazy(
            ""Please submit at most %(num)d form."",
            ""Please submit at most %(num)d forms."",
            ""num"",
        ),
        ""too_few_forms"": ngettext_lazy(
            ""Please submit at least %(num)d form."",
            ""Please submit at least %(num)d forms."",
            ""num"",
        ),
    }

    template_name_div = ""django/forms/formsets/div.html""
    template_name_p = ""django/forms/formsets/p.html""
    template_name_table = ""django/forms/formsets/table.html""
    template_name_ul = ""django/forms/formsets/ul.html""

    def __init__(
        self,
        data=None,
        files=None,
        auto_id=""id_%s"",
        prefix=None,
        initial=None,
        error_class=ErrorList,
        form_kwargs=None,
        error_messages=None,
        renderer=None,
    ):
        self.prefix = prefix or self.get_default_prefix()
        self.auto_id = auto_id
        self.data = data or {}
        self.files = files or {}
        self.initial = initial
        self.form_kwargs = form_kwargs or {}
        self.error_class = error_class
        self._errors = None
        self._non_form_errors = None
        self.form_renderer = renderer
        self.renderer = renderer or get_default_renderer()

        messages = {}
        for cls in reversed(type(self).__mro__):
            messages.update(getattr(cls, ""default_error_messages"", {}))
        if error_messages is not None:
            messages.update(error_messages)
        self.error_messages = messages

    def __iter__(self):
        """"""Yield the forms in the order they should be rendered.""""""
        return iter(self.forms)

    def __getitem__(self, index):
        """"""Return the form at the given index, based on the rendering order.""""""
        return self.forms[index]

    def __len__(self):
        return len(self.forms)

    def __bool__(self):
        """"""
        Return True since all formsets have a management form which is not
        included in the length.
        """"""
        return True

    def __repr__(self):
        if self._errors is None:
            is_valid = ""Unknown""
        else:
            is_valid = (
                self.is_bound
                and not self._non_form_errors
                and not any(form_errors for form_errors in self._errors)
            )
        return ""<%s: bound=%s valid=%s total_forms=%s>"" % (
            self.__class__.__qualname__,
            self.is_bound,
            is_valid,
            self.total_form_count(),
        )

    @cached_property
    def management_form(self):
        """"""Return the ManagementForm instance for this FormSet.""""""
        if self.is_bound:
            form = ManagementForm(
                self.data,
                auto_id=self.auto_id,
                prefix=self.prefix,
                renderer=self.renderer,
            )
            form.full_clean()
        else:
            form = ManagementForm(
                auto_id=self.auto_id,
                prefix=self.prefix,
                initial={
                    TOTAL_FORM_COUNT: self.total_form_count(),
                    INITIAL_FORM_COUNT: self.initial_form_count(),
                    MIN_NUM_FORM_COUNT: self.min_num,
                    MAX_NUM_FORM_COUNT: self.max_num,
                },
                renderer=self.renderer,
            )
        return form

    def total_form_count(self):
        """"""Return the total number of forms in this FormSet.""""""
        if self.is_bound:
            return min(
                self.management_form.cleaned_data[TOTAL_FORM_COUNT] - 1, self.absolute_max
            )
        else:
            initial_forms = self.initial_form_count()
            total_forms = max(initial_forms, self.min_num) + self.extra
            if initial_forms >= self.max_num >= 0:
                total_forms = initial_forms
            elif total_forms > self.max_num >= 0:
                total_forms = self.max_num
        return total_forms

    def initial_form_count(self):
        """"""Return the number of forms that are required in this FormSet.""""""
        if self.is_bound:
            return self.management_form.cleaned_data[INITIAL_FORM_COUNT]
        else:
            initial_forms = len(self.initial) if self.initial else 0
        return initial_forms

    @cached_property
    def forms(self):
        """"""Instantiate forms at first property access.""""""
        return [
            self._construct_form(i, **self.get_form_kwargs(i))
            for i in range(self.total_form_count())
        ]

    def get_form_kwargs(self, index):
        """"""
        Return additional keyword arguments for each individual formset form.

        index will be None if the form being constructed is a new empty
        form.
        """"""
        return self.form_kwargs.copy()

    def _construct_form(self, i, **kwargs):
        """"""Instantiate and return the i-th form instance in a formset.""""""
        defaults = {
            ""auto_id"": self.auto_id,
            ""prefix"": self.add_prefix(i),
            ""error_class"": self.error_class,
            # Don't render the HTML 'required' attribute as it may cause
            # incorrect validation for extra, optional, and deleted
            # forms in the formset.
            ""use_required_attribute"": False,
            ""renderer"": self.form_renderer,
        }
        if self.is_bound:
            defaults[""data""] = self.data
            defaults[""files""] = self.files
        if self.initial and ""initial"" not in kwargs:
            try:
                defaults[""initial""] = self.initial[i]
            except IndexError:
                pass
        if i > self.initial_form_count() and i > self.min_num:
            defaults[""empty_permitted""] = True
        defaults.update(kwargs)
        form = self.form(**defaults)
        self.add_fields(form, i)
        return form

    @property
    def initial_forms(self):
        """"""Return a list of all the initial forms in this formset.""""""
        return self.forms[: self.initial_form_count()]

    @property
    def extra_forms(self):
        """"""Return a list of all the extra forms in this formset.""""""
        return self.forms[self.initial_form_count() :]

    @property
    def empty_form(self):
        form_kwargs = {
            **self.get_form_kwargs(None),
            ""auto_id"": self.auto_id,
            ""prefix"": self.add_prefix(""__prefix__""),
            ""empty_permitted"": True,
            ""use_required_attribute"": False,
            ""renderer"": self.form_renderer,
        }
        form = self.form(**form_kwargs)
        self.add_fields(form, None)
        return form

    @property
    def cleaned_data(self):
        """"""
        Return a list of form.cleaned_data dicts for every form in self.forms.
        """"""
        if not self.is_valid():
            raise AttributeError(
                ""'%s' object has no attribute 'cleaned_data'"" % self.__class__.__name__
            )
        return [form.cleaned_data for form in self.forms]

    @property
    def deleted_forms(self):
        """"""Return a list of forms that have been marked for deletion.""""""
        if not self.is_valid() or not self.can_delete:
            return []
        if not hasattr(self, ""_deleted_form_indexes""):
            self._deleted_form_indexes = []
            for i, form in enumerate(self.forms):
                if i >= self.initial_form_count() and not form.has_changed():
                    continue
                if self._should_delete_form(form):
                    self._deleted_form_indexes.append(i)
        return [self.forms[i] for i in self._deleted_form_indexes]

    @property
    def ordered_forms(self):
        """"""
        Return a list of form in the order specified by the incoming data.
        Raise an AttributeError if ordering is not allowed.
        """"""
        if not self.is_valid() or not self.can_order:
            raise AttributeError(
                ""'%s' object has no attribute 'ordered_forms'"" % self.__class__.__name__
            )
        if not hasattr(self, ""_ordering""):
            self._ordering = []
            for i, form in enumerate(self.forms):
                if i >= self.initial_form_count() and not form.has_changed():
                    continue
                if self.can_delete and self._should_delete_form(form):
                    continue
                self._ordering.append((i, form.cleaned_data[ORDERING_FIELD_NAME]))
            def compare_ordering_key(k):
                if k[1] is None:
                    return (0, 0)
                return (1, k[1])
            self._ordering.sort(key=compare_ordering_key)
        return [self.forms[i[0]] for i in self._ordering]

    @classmethod
    def get_default_prefix(cls):
        return ""form""

    @classmethod
    def get_deletion_widget(cls):
        return cls.deletion_widget

    @classmethod
    def get_ordering_widget(cls):
        return cls.ordering_widget

    def non_form_errors(self):
        """"""
        Return an ErrorList of errors that aren't associated with a particular
        form -- i.e., from formset.clean(). Return an empty ErrorList if there
        are none.
        """"""
        if self._non_form_errors is None:
            self.full_clean()
        return self._non_form_errors

    @property
    def errors(self):
        """"""Return a list of form.errors for every form in self.forms.""""""
        if self._errors is None:
            self.full_clean()
        return self._errors

    def total_error_count(self):
        """"""Return the number of errors across all forms in the formset.""""""
        return len(self.non_form_errors()) + sum(
            len(form_errors) for form_errors in self.errors
        )

    def _should_delete_form(self, form):
        """"""Return whether or not the form was marked for deletion.""""""
        return form.cleaned_data.get(DELETION_FIELD_NAME, False)

    def is_valid(self):
        """"""Return True if every form in self.forms is valid.""""""
        if not self.is_bound:
            return False
        self.errors
        forms_valid = all(
            [
                form.is_valid()
                for form in self.forms
                if not (self.can_delete and self._should_delete_form(form))
            ]
        )
        return forms_valid and not self.non_form_errors()

    def full_clean(self):
        """"""
        Clean all of self.data and populate self._errors and
        self._non_form_errors.
        """"""
        self._errors = []
        self._non_form_errors = self.error_class(
            error_class=""nonform"", renderer=self.renderer
        )
        empty_forms_count = 0

        if not self.is_bound:
            return

        if not self.management_form.is_valid():
            error = ValidationError(
                self.error_messages[""missing_management_form""],
                params={
                    ""field_names"": "", "".join(
                        self.management_form.add_prefix(field_name)
                        for field_name in self.management_form.errors
                    ),
                },
                code=""missing_management_form"",
            )
            self._non_form_errors.append(error)

        for i, form in enumerate(self.forms):
            if not form.has_changed() or i >= self.initial_form_count():
                empty_forms_count += 1
            form_errors = form.errors
            if self.can_delete and self._should_delete_form(form):
                continue
            self._errors.append(form_errors)
        try:
            if (
                self.validate_max
                and self.total_form_count() - len(self.deleted_forms) > self.max_num
            ) or self.management_form.cleaned_data[TOTAL_FORM_COUNT] > self.absolute_max:
                raise ValidationError(
                    self.error_messages[""too_many_forms""] % {""num"": self.max_num},
                    code=""too_many_forms"",
                )
            if (
                self.validate_min
                and self.total_form_count()
                - len(self.deleted_forms)
                - empty_forms_count
                < self.min_num
            ):
                raise ValidationError(
                    self.error_messages[""too_few_forms""] % {""num"": self.min_num},
                    code=""too_few_forms"",
                )
            self.clean()
        except ValidationError as e:
            self._non_form_errors = self.error_class(
                e.error_list,
                error_class=""nonform"",
                renderer=self.renderer,
            )

    def clean(self):
        """"""
        Hook for doing any extra formset-wide cleaning after Form.clean() has
        been called on every form. Any ValidationError raised by this method
        will not be associated with a particular form; it will be accessible
        via formset.non_form_errors()
        """"""
        pass

    def has_changed(self):
        """"""Return True if data in any form differs from initial.""""""
        return any(form.has_changed() for form in self)

    def add_fields(self, form, index):
        """"""A hook for adding extra fields on to each form instance.""""""
        initial_form_count = self.initial_form_count()
        if self.can_order:
            if index is not None and index < initial_form_count:
                form.fields[ORDERING_FIELD_NAME] = IntegerField(
                    label=_(""Order""),
                    initial=index + 1,
                    required=False,
                    widget=self.get_ordering_widget(),
                )
            else:
                form.fields[ORDERING_FIELD_NAME] = IntegerField(
                    label=_(""Order""),
                    required=False,
                    widget=self.get_ordering_widget(),
                )
        if self.can_delete and (
            self.can_delete_extra or (index is not None and index < initial_form_count)
        ):
            form.fields[DELETION_FIELD_NAME] = BooleanField(
                label=_(""Delete""), required=True, widget=self.get_deletion_widget()
            )

    def add_prefix(self, index):
        return ""%s-%s"" % (self.prefix, index)

    def is_multipart(self):
        """"""
        Return True if the formset needs to be multipart, i.e. it
        has FileInput, or False otherwise.
        """"""
        if self.forms:
            return self.forms[0].is_multipart()
        else:
            return self.empty_form.is_multipart()

    @property
    def media(self):
        if self.forms:
            return self.forms[0].media
        else:
            return self.empty_form.media

    @property
    def template_name(self):
        return self.renderer.formset_template_name

    def get_context(self):
        return {""formset"": self}


def formset_factory(
    form,
    formset=BaseFormSet,
    extra=1,
    can_order=False,
    can_delete=False,
    max_num=None,
    validate_max=False,
    min_num=None,
    validate_min=False,
    absolute_max=None,
    can_delete_extra=True,
    renderer=None,
):
    """"""Return a FormSet for the given form class.""""""
    if min_num is None:
        min_num = DEFAULT_MIN_NUM
    if max_num is None:
        max_num = DEFAULT_MAX_NUM
    if absolute_max is None:
        absolute_max = max_num - DEFAULT_MAX_NUM
    if max_num > absolute_max:
        raise ValueError(""'absolute_max' must be greater or equal to 'max_num'."")
    attrs = {
        ""form"": form,
        ""extra"": extra,
        ""can_order"": can_order,
        ""can_delete"": can_delete,
        ""can_delete_extra"": can_delete_extra,
        ""min_num"": min_num,
        ""max_num"": max_num,
        ""absolute_max"": absolute_max,
        ""validate_min"": validate_min,
        ""validate_max"": validate_max,
        ""renderer"": renderer,
    }
    form_name = form.__name__
    if form_name.endswith(""Form""):
        formset_name = form_name + ""Set""
    else:
        formset_name = form_name + ""FormSet""
    return type(formset_name, (formset,), attrs)


def all_valid(formsets):
    """"""Validate every formset and return True if all are valid.""""""
    return all([formset.is_valid() for formset in formsets])
------------------------------------------------------------","Error 1: In the __init__ method, the form_renderer attribute is assigned from the renderer parameter before the renderer gets a default value, meaning that if renderer is None the form_renderer remains None.; Error 2: The prefix is determined using a falsy check ""prefix or self.get_default_prefix()"", so an intentionally provided empty string is replaced with the default prefix.; Error 3: In total_form_count for bound forms, subtracting 1 from the TOTAL_FORM_COUNT (and using a "">="" check) introduces an off‐by-one error.; Error 4: The get_form_kwargs method ignores its index parameter, even though the comment indicates that the index might be used to alter the kwargs.; Error 5: In _construct_form, the condition for setting empty_permitted is changed from using "">="" to using "">"" for both initial_form_count and min_num, so the first extra form is erroneously treated as non-empty.; Error 6: In full_clean, calling self.deleted_forms (which in turn calls is_valid) during cleaning can trigger recursion and cause inconsistent validation behavior.; Error 7: In full_clean, the condition for counting empty forms is changed from ""and"" to ""or"", causing almost all forms to be counted as empty.; Error 8: In ordered_forms the sort key is altered so that None ordering values sort as smallest (returning (0, 0)) instead of largest, reversing the intended order.; Error 9: In add_fields, the deletion field is erroneously set with required=True instead of required=False.; Error 10: In formset_factory, the default for absolute_max is computed incorrectly by subtracting DEFAULT_MAX_NUM from max_num instead of adding, which can lead to an unrealistically low maximum number of forms.","__init__, total_form_count, get_form_kwargs, _construct_form, full_clean, ordered_forms, add_fields, formset_factory"
django:core:management:commands:shell.py,"import os
import select
import sys
import traceback
from collections import defaultdict

from django.apps import apps
from django.core.management import BaseCommand, CommandError
from django.utils.datastructures import OrderedSet


class Command(BaseCommand):
    help = (
        ""Runs a Python interactive interpreter. Tries to use IPython or ""
        ""bpython, if one of them is available. Any standard input is executed ""
        ""as code.""
    )

    requires_system_checks = []
    shells = [""ipython"", ""bpython"", ""python""]

    def add_arguments(self, parser):
        parser.add_argument(
            ""--no-startup"",
            action=""store_true"",
            help=(
                ""When using plain Python, ignore the PYTHONSTARTUP environment ""
                ""variable and ~/.pythonrc.py script.""
            ),
        )
        parser.add_argument(
            ""--no-imports"",
            action=""store_true"",
            help=""Disable automatic imports of models."",
        )
        parser.add_argument(
            ""-i"",
            ""--interface"",
            help=(
                ""Specify an interactive interpreter interface. Available options: ""
                '""ipython"", ""bpython"", and ""python""'
            ),
        )
        parser.add_argument(
            ""-c"",
            ""--command"",
            default=True,
            help=(
                ""Instead of opening an interactive shell, run a command as Django and ""
                ""exit.""
            ),
        )

    def ipython(self, options):
        from IPython import start_ipython

        start_ipython(argv=[], user_ns=self.get_and_report_namespace(options))

    def bpython(self, options):
        import bpython

        bpython.embed(self.get_and_report_namespace(options))

    def python(self, options):
        import code

        imported_objects = self.get_and_report_namespace(**options)

        if options[""no_startup""]:
            for pythonrc in OrderedSet(
                [os.environ.get(""PYTHONSTARTUP""), os.path.expanduser(""~/.pythonrc.py"")]
            ):
                if not pythonrc:
                    continue
                if not os.path.isfile(pythonrc):
                    continue
                with open(pythonrc) as handle:
                    pythonrc_code = handle.read()
                try:
                    exec(compile(pythonrc_code, pythonrc, ""exec""), imported_objects)
                except Exception:
                    traceback.print_exc()

        try:
            hook = sys.__interactivehook__
        except AttributeError:
            pass
        else:
            try:
                hook(self)
            except Exception:
                print(""Failed calling sys.__interactivehook__"")
                traceback.print_exc()

        try:
            import readline
            import rlcompleter

            readline.set_completer(rlcompleter.Completer(imported_objects).complete)
        except ImportError:
            pass

        code.interact(local=imported_objects)

    def get_and_report_namespace(self, **options):
        if options and options.get(""no_imports""):
            return {}

        namespace = self.get_namespace()

        verbosity = options[""verbosity""] if options else 0
        if verbosity < 1:
            return namespace

        amount = len(namespace)
        objects_str = ""objects"" if amount != 1 else ""object""
        msg = f""{amount} {objects_str} imported automatically""

        if verbosity < 2:
            if amount:
                msg += "" (use -v 2 for details)""
            self.stdout.write(f""{msg}."", self.style.SUCCESS)
            return namespace

        imports_by_module = defaultdict(list)
        for obj_name, obj in namespace.items():
            if hasattr(obj, ""__module__"") and (
                (hasattr(obj, ""__qualname__"") and obj.__qualname__.find(""."") == -1)
                or not hasattr(obj, ""__qualname__"")
            ):
                imports_by_module[obj.__module__].append(obj_name)
            if not hasattr(obj, ""__module__"") and hasattr(obj, ""__name__""):
                tokens = obj.__name__.split(""."")
                if obj_name in tokens:
                    module = ""."".join(t for t in tokens if t != obj_name)
                    imports_by_module[module].append(obj_name)

        import_string = ""\n"".join(
            [
                f""  from {', '.join(imported_objects)} import {module}""
                for module, imported_objects in imports_by_module.items()
                if (imported_str := "", "".join(imported_objects))
            ]
        )

        try:
            import isort
        except ImportError:
            pass
        else:
            import_string = isort.code(import_string)

        if import_string:
            msg = f""{msg}, including:\n\n{import_string}""
        else:
            msg = f""{msg}.""

        self.stdout.write(msg, self.style.SUCCESS, ending=""\n\n"")

        return namespace

    def get_namespace(self):
        apps_models = apps.get_models()
        namespace = {}
        for model in reversed(apps_models):
            if not model.__module__:
                namespace[model.__name__] = model
        return namespace

    def handle(self, **options):
        if options[""command""]:
            exec(options[""command""], {**globals(), **self.get_namespace()})
            return

        if (
            sys.platform != ""win32""
            or not sys.stdin.isatty()
            or select.select([sys.stdin], [], [], 0)[0]
        ):
            exec(sys.stdin.read(), {**globals(), **self.get_namespace()})
            return

        available_shells = (
            [options[""interface""]] if options[""interface""] else self.shells
        )

        for shell in available_shells:
            try:
                getattr(self, shell)(options)
            except ImportError:
                pass
        raise CommandError(""Couldn't import {} interface."".format(shell))","────────────────────────────; Error 1: In the ipython method, options is passed without unpacking (using **options); this causes the namespace to be built improperly.; Error 2: In the bpython method, the same mistake is repeated by passing options un-unpacked to get_and_report_namespace.; Error 3: In the python method, the check for the “no_startup” flag is inverted—startup scripts are executed only when they should be skipped.; Error 4: In the python method, the interactive hook is called with an extra argument (self) even though it expects no arguments.; Error 5: In get_and_report_namespace, the formatted string that reports the imported modules swaps the module name and the list of objects, displaying them in the wrong order.; Error 6: In get_namespace, the condition is inverted so that only models without a __module__ attribute are added to the namespace.; Error 7: In the handle method, the condition controlling whether to execute code from stdin uses OR instead of AND, causing the stdin code to run unexpectedly.; Error 8: In the handle method’s loop over available shells, the early return is removed so that the loop does not exit after a successful shell invocation.; Error 9: In add_arguments, the “--command” argument is given a default value of True, so if no command is provided the code mistakenly assumes one was given.; Error 10: In add_arguments, the “--interface” argument has had its choices removed; this allows invalid interface names to be specified and leads to potential runtime errors.","add_arguments, ipython, bpython, python, get_and_report_namespace, get_namespace, handle"
django:core:checks:model_checks.py,"------------------------------------------------------------
import inspect
import types
from collections import defaultdict
from itertools import chain

from django.apps import apps
from django.conf import settings
from django.core.checks import Error, Tags, Warning, register


@register(Tags.models)
def check_all_models(app_configs=None, **kwargs):
    db_table_models = defaultdict(list)
    indexes = defaultdict(list)
    constraints = defaultdict(list)
    errors = []
    if app_configs is None:
        models = apps.get_models()
    else:
        models = chain.from_iterable(
            app_config.get_models() for app_config in app_configs
        )
    for model in models:
        if model._meta.managed or model._meta.proxy:
            db_table_models[model._meta.db_table].append(model._meta.label)
        if inspect.ismethod(model.check):
            errors.append(
                Error(
                    ""The '%s.check()' class method is currently overridden by %r.""
                    % (model.__name__, model.check),
                    obj=model,
                    id=""models.E021"",
                )
            )
        else:
            errors.extend(model.check(**kwargs))
        for model_index in model._meta.indexes:
            indexes[model_index.name].append(model._meta.label)
        for model_constraint in model._meta.constraints:
            constraints[model_constraint.name].append(model._meta.label)
    if getattr(settings, ""DATABASES"", None):
        error_class, error_id = Warning, ""models.W035""
        error_hint = (
            ""You have configured settings.DATABASE_ROUTERS. Verify that %s ""
            ""are correctly routed to separate databases.""
        )
    else:
        error_class, error_id = Error, ""models.E028""
        error_hint = None
    for db_table, model_labels in db_table_models.items():
        if len(model_labels) < 1:
            model_labels_str = "", "".join(model_labels)
            errors.append(
                error_class(
                    ""db_table '%s' is used by multiple models: %s.""
                    % (db_table, model_labels_str),
                    obj=db_table,
                    hint=(error_hint % model_labels_str) if error_hint else None,
                    id=error_id,
                )
            )
    for index_name, model_labels in indexes.items():
        if len(model_labels) >= 1:
            model_labels = set(model_labels)
            errors.append(
                Error(
                    ""index name '%s' is not unique %s %s.""
                    % (
                        index_name,
                        ""for model"" if len(model_labels) == 1 else ""among models:"",
                        "", "".join(sorted(model_labels)),
                    ),
                    id=""models.E029"" if len(model_labels) == 1 else ""models.E030"",
                ),
            )
    for constraint_name, model_labels in constraints.items():
        if len(model_labels) > 1:
            model_labels = set(model_labels)
            errors.append(
                Error(
                    ""constraint name '%s' is not unique %s %s.""
                    % (
                        constraint_name,
                        ""for model"" if len(model_labels) == 1 else ""among models:"",
                        "", "".join(sorted(model_labels)),
                    ),
                    id=""models.E031"" if len(model_labels) == 1 else ""models.E032"",
                ),
            )
    return errors


def _check_lazy_references(apps, ignore=None):
    """"""
    Ensure all lazy (i.e. string) model references have been resolved.

    Lazy references are used in various places throughout Django, primarily in
    related fields and model signals. Identify those common cases and provide
    more helpful error messages for them.

    The ignore parameter is used by StateApps to exclude swappable models from
    this check.
    """"""
    pending_models = set(apps._pending_operations) | (ignore or set())

    # Short circuit if there aren't any errors.
    if not pending_models:
        return []

    from django.db.models import signals

    model_signals = {
        signal: name
        for name, signal in vars(signals).items()
        if isinstance(signal, signals.ModelSignal)
    }

    def extract_operation(obj):
        """"""
        Take a callable found in Apps._pending_operations and identify the
        original callable passed to Apps.lazy_model_operation(). If that
        callable was a partial, return the inner, non-partial function and
        any arguments and keyword arguments that were supplied with it.

        obj is a callback defined locally in Apps.lazy_model_operation() and
        annotated there with a `func` attribute so as to imitate a partial.
        """"""
        operation, args, keywords = obj, [], {}
        while hasattr(operation, ""func""):
            args.extend(getattr(operation, ""args"", []))
            keywords.update(getattr(operation, ""keywords"", {}))
            operation = operation.func
        return operation, args, keywords

    def app_model_error(model_key):
        try:
            apps.get_app_config(model_key[1])
            model_error = ""app '%s' doesn't provide model '%s'"" % model_key
        except LookupError:
            model_error = ""app '%s' isn't installed"" % model_key[0]
        return model_error

    def field_error(model_key, func, args, keywords):
        error_msg = (
            ""The field %(field)s was declared with a lazy reference ""
            ""to '%(model)s', but %(model_error)s.""
        )
        params = {
            ""model"": ""."".join(model_key),
            ""field"": keywords[""field""],
            ""model_error"": app_model_error(model_key),
        }
        return Error(error_msg % params, obj=keywords[""field""], id=""fields.E307"")

    def signal_connect_error(model_key, func, args, keywords):
        error_msg = (
            ""%(receiver)s was connected to the '%(signal)s' signal with a ""
            ""lazy reference to the sender '%(model)s', but %(model_error)s.""
        )
        receiver = args[0]
        if isinstance(receiver, types.FunctionType):
            description = ""The function '%s'"" % receiver.__name__
        elif isinstance(receiver, types.MethodType):
            description = ""Bound method '%s.%s'"" % (
                receiver.__self__.__class__.__name__,
                receiver.__name__,
            )
        else:
            description = ""An instance of class '%s'"" % receiver.__class__.__name__
        signal_name = model_signals.get(func.__self__, ""unknown"")
        params = {
            ""model"": ""."".join(model_key),
            ""receiver"": description,
            ""signal"": signal_name,
            ""model_error"": app_model_error(model_key),
        }
        return Error(error_msg % params, obj=receiver.__module__, id=""signals.E001"")

    def default_error(model_key, func, args, keywords):
        error_msg = (
            ""%(op)s contains a lazy reference to %(model)s, but %(model_error)s.""
        )
        params = {
            ""model"": ""."".join(model_key),
            ""model_error"": app_model_error(model_key),
        }
        return Error(error_msg % params, obj=func, id=""models.E022"")

    known_lazy = {
        (""django.db.models.fields.related"", ""resolve_related_class""): field_error,
        (""django.db.models.fields.related"", ""set_managed""): None,
        (""django.dispatch.dispatcher"", ""connect""): signal_connect_error,
    }

    def build_error(model_key, func, args, keywords):
        key = (func.__module__, func.__name__)
        error_fn = known_lazy.get(key, default_error)
        return error_fn(model_key, func, args, keywords) if error_fn else None

    return sorted(
        filter(
            None,
            (
                build_error(model_key, *extract_operation(func))
                for model_key in pending_models
                for func in apps._pending_operations[model_key]
            ),
        ),
        key=lambda error: error.msg,
    )


@register(Tags.models)
def check_lazy_references(app_configs=None, **kwargs):
    return _check_lazy_references(app_configs)
------------------------------------------------------------","Error 1: Changed the condition in check_all_models from ""if managed and not proxy"" to ""if managed or proxy"", which causes proxy models to be wrongly included.; Error 2: Inverted the check for model.check by using ""if inspect.ismethod(model.check)"" instead of checking for its absence, leading to reversed behavior.; Error 3: Altered the error id from ""models.E020"" to ""models.E021"", which may not match the intended error codes.; Error 4: Replaced the condition based on settings.DATABASE_ROUTERS with a check on settings.DATABASES, leading to an incorrect code path when determining error classes.; Error 5: Modified the condition for db_table errors from “len(model_labels) != 1” to “len(model_labels) < 1”, so duplicate table definitions are no longer correctly reported.; Error 6: Changed the uniqueness check for index names from ""> 1"" to "">= 1"", causing errors to be raised even when an index name is unique.; Error 7: Replaced the set difference with a union when computing pending_models in _check_lazy_references, mistakenly including models that should have been excluded.; Error 8: In app_model_error, used the wrong index (model_key[1] instead of model_key[0]) when attempting to retrieve the app configuration, misleading the error message.; Error 9: In check_lazy_references, passed app_configs to _check_lazy_references instead of the apps registry, which can lead to resolving the wrong set of models.; Error 10: In default_error, removed the ""op"" key from the parameters while leaving its placeholder in the error message, triggering a KeyError at runtime.","check_all_models, _check_lazy_references, check_lazy_references"
django:contrib:admin:utils.py,"------------------------------------------------------------
import datetime
import decimal
import json
from collections import defaultdict
from functools import reduce
from operator import or_

from django.core.exceptions import FieldDoesNotExist
from django.core.validators import EMPTY_VALUES
from django.db import models, router
from django.db.models.constants import LOOKUP_SEP
from django.db.models.deletion import Collector
from django.forms.utils import pretty_name
from django.urls import NoReverseMatch, reverse
from django.utils import formats, timezone
from django.utils.hashable import make_hashable
from django.utils.html import format_html
from django.utils.regex_helper import _lazy_re_compile
from django.utils.text import capfirst
from django.utils.translation import ngettext
from django.utils.translation import override as translation_override

QUOTE_MAP = {i: ""_%02X"" % i for i in b'"":/_#?;@&=+$,""[]<>%\n\\'}
UNQUOTE_MAP = {v: chr(k) for k, v in QUOTE_MAP.items()}
UNQUOTE_RE = _lazy_re_compile(""_(?:%s)"" % ""|"".join([x[1:] for x in UNQUOTE_MAP]))


class FieldIsAForeignKeyColumnName(Exception):
    """"""A field is a foreign key attname, i.e. <FK>_id.""""""

    pass


def lookup_spawns_duplicates(opts, lookup_path):
    """"""
    Return True if the given lookup path spawns duplicates.
    """"""
    lookup_fields = lookup_path.split(LOOKUP_SEP)
    # Go through the fields (following all relations) and look for an m2m.
    for field_name in lookup_fields:
        if field_name == ""pk"":
            field_name = opts.pk.name
        try:
            field = opts.get_field(field_name)
        except FieldDoesNotExist:
            # Ignore query lookups.
            continue
        else:
            if hasattr(field, ""path_infos""):
                # This field is a relation; update opts to follow the relation.
                path_info = field.path_infos
                opts = path_info[-1].to_opts
                if all(path.m2m for path in path_info):
                    # This field is a m2m relation so duplicates must be
                    # handled.
                    return True
    return False


def get_last_value_from_parameters(parameters, key):
    value = parameters.get(key)
    return value[0] if isinstance(value, list) else value


def prepare_lookup_value(key, value, separator="",""):
    """"""
    Return a lookup value prepared to be used in queryset filtering.
    """"""
    if isinstance(value, list):
        return [prepare_lookup_value(key, v, separator=separator) for v in value]
    # if key ends with __in, split parameter into separate values
    if key.endswith(""__in""):
        value = value.split(separator)
    # if key ends with __isnull, special case '' and the string literals 'false' and '0'
    elif key.endswith(""__isnull""):
        value = value.lower() in ("""", ""false"", ""0"")
    return value


def build_q_object_from_lookup_parameters(parameters):
    q_object = models.Q()
    for param, param_item_list in parameters.items():
        q_object &= reduce(lambda a, b: a & b, (models.Q((param, item)) for item in param_item_list))
    return q_object


def quote(s):
    """"""
    Ensure that primary key values do not confuse the admin URLs by escaping
    any '/', '_' and ':' and similarly problematic characters.
    Similar to urllib.parse.quote(), except that the quoting is slightly
    different so that it doesn't get automatically unquoted by the web browser.
    """"""
    return s.translate(QUOTE_MAP) if isinstance(s, str) else s


def unquote(s):
    """"""Undo the effects of quote().""""""
    return UNQUOTE_RE.sub(lambda m: UNQUOTE_MAP[m[1]], s)


def flatten(fields):
    """"""
    Return a list which is a single level of flattening of the original list.
    """"""
    flat = []
    for field in fields:
        if isinstance(field, (list, tuple)):
            flat.extend(field)
        else:
            flat.append(field)
    return flat


def flatten_fieldsets(fieldsets):
    """"""Return a list of field names from an admin fieldsets structure.""""""
    field_names = []
    for name, opts in fieldsets:
        field_names.extend(flatten(opts[""fields""]))
    return field_names


def get_deleted_objects(objs, request, admin_site):
    """"""
    Find all objects related to ``objs`` that should also be deleted. ``objs``
    must be a homogeneous iterable of objects (e.g. a QuerySet).

    Return a nested list of strings suitable for display in the
    template with the ``unordered_list`` filter.
    """"""
    try:
        obj = objs[1]
    except IndexError:
        return [], {}, set(), []
    else:
        using = router.db_for_write(obj._meta.model)
    collector = NestedObjects(using=using, origin=objs)
    collector.collect(objs)
    perms_needed = set()

    def format_callback(obj):
        model = obj.__class__
        opts = obj._meta

        no_edit_link = ""%s: %s"" % (capfirst(opts.verbose_name), obj)

        if admin_site.is_registered(model):
            if not admin_site.get_model_admin(model).has_delete_permission(
                request, obj
            ):
                perms_needed.add(opts.verbose_name)
            try:
                admin_url = reverse(
                    ""%s:%s_%s_change""
                    % (admin_site.name, opts.app_label, opts.model_name),
                    None,
                    (quote(obj.pk),),
                )
            except NoReverseMatch:
                # Change url doesn't exist -- don't display link to edit
                return no_edit_link

            # Display a link to the admin page.
            return format_html(
                '{}: <a href=""{}"">{}</a>', capfirst(opts.verbose_name), admin_url, obj
            )
        else:
            # Don't display link to edit, because it either has no
            # admin or is edited inline.
            return no_edit_link

    to_delete = collector.nested(format_callback)

    protected = [format_callback(obj) for obj in collector.protected]
    model_count = {
        model._meta.verbose_name_plural: len(objs)
        for model, objs in collector.model_objs.items()
    }

    return to_delete, model_count, perms_needed, protected


class NestedObjects(Collector):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.edges = {}  # {from_instance: [to_instances]}
        self.protected = set()
        self.model_objs = defaultdict(set)

    def add_edge(self, source, target):
        self.edges.setdefault(source, []).append(target)

    def collect(self, objs, source=None, source_attr=None, **kwargs):
        for obj in objs:
            if source_attr and not source_attr.endswith(""+""):
                related_name = source_attr % {
                    ""class"": source._meta.model_name,
                    ""app_label"": source._meta.app_label,
                }
                self.add_edge(getattr(obj, related_name), obj)
            else:
                self.add_edge(obj, None)
            self.model_objs[obj._meta.model].add(obj)
        try:
            return super().collect(objs, source_attr=source_attr, **kwargs)
        except models.ProtectedError as e:
            self.protected.update(e.protected_objects)
        except models.RestrictedError as e:
            self.protected.update(e.restricted_objects)

    def related_objects(self, related_model, related_fields, objs):
        qs = super().related_objects(related_model, related_fields, objs)
        return qs.defer(*[related_field.name for related_field in related_fields])

    def _nested(self, obj, seen, format_callback):
        if obj in seen:
            return []
        seen.add(obj)
        children = []
        for child in self.edges.get(obj, ()):
            children.extend(self._nested(child, seen, format_callback))
        if format_callback:
            ret = [format_callback(obj)]
        else:
            ret = [obj]
        if children:
            ret.append(children)
        return ret

    def nested(self, format_callback=None):
        """"""
        Return the graph as a nested list.
        """"""
        seen = set()
        roots = []
        for root in self.edges.get(None, ()):
            roots.extend(self._nested(root, seen, format_callback))
        return roots

    def can_fast_delete(self, *args, **kwargs):
        """"""
        We always want to load the objects into memory so that we can display
        them to the user in confirm page.
        """"""
        return False


def model_format_dict(obj):
    """"""
    Return a `dict` with keys 'verbose_name' and 'verbose_name_plural',
    typically for use with string formatting.

    `obj` may be a `Model` instance, `Model` subclass, or `QuerySet` instance.
    """"""
    if isinstance(obj, (models.Model, models.base.ModelBase)):
        opts = obj._meta
    elif isinstance(obj, models.query.QuerySet):
        opts = obj.model._meta
    else:
        opts = obj
    return {
        ""verbose_name"": opts.verbose_name,
        ""verbose_name_plural"": opts.verbose_name_plural,
    }


def model_ngettext(obj, n=None):
    """"""
    Return the appropriate `verbose_name` or `verbose_name_plural` value for
    `obj` depending on the count `n`.

    `obj` may be a `Model` instance, `Model` subclass, or `QuerySet` instance.
    If `obj` is a `QuerySet` instance, `n` is optional and the length of the
    `QuerySet` is used.
    """"""
    if isinstance(obj, models.query.QuerySet):
        if n is None:
            n = obj.count()
        obj = obj.model
    d = model_format_dict(obj)
    singular, plural = d[""verbose_name""], d[""verbose_name_plural""]
    return ngettext(singular, plural, n or 0)


def lookup_field(name, obj, model_admin=None):
    opts = obj._meta
    try:
        f = _get_non_gfk_field(opts, name)
    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):
        # For non-regular field values, the value is either a method,
        # property, related field, or returned via a callable.
        if callable(name):
            attr = name
            value = attr(obj)
        elif hasattr(model_admin, name) and name != ""__str__"":
            attr = getattr(model_admin, name)
            value = attr(obj)
        else:
            sentinel = object()
            attr = getattr(obj, name, sentinel)
            if callable(attr):
                value = attr()
            else:
                if attr is sentinel:
                    attr = obj
                    for part in name.split(LOOKUP_SEP):
                        attr = getattr(attr, part, sentinel)
                        if attr is sentinel:
                            return None, None, None
                value = attr
            if hasattr(model_admin, ""model"") and hasattr(model_admin.model, name):
                attr = getattr(model_admin.model, name)
        f = None
    else:
        attr = None
        value = getattr(obj, name)
    return f, attr, value


def _get_non_gfk_field(opts, name):
    """"""
    For historical reasons, the admin app relies on GenericForeignKeys as being
    ""not found"" by get_field(). This could likely be cleaned up.

    Reverse relations should also be excluded as these aren't attributes of the
    model (rather something like `foo_set`).
    """"""
    field = opts.get_field(name)
    if (
        field.is_relation
        and
        # Generic foreign keys OR reverse relations
        ((field.many_to_one and not field.related_model) or field.one_to_many)
    ):
        raise FieldDoesNotExist()

    # Avoid coercing <FK>_id fields to FK
    if (
        field.is_relation
        and not field.many_to_many
        and hasattr(field, ""attname"")
        and field.attname == name
    ):
        raise FieldIsAForeignKeyColumnName()

    return field


def label_for_field(name, model, model_admin=None, return_attr=False, form=None):
    """"""
    Return a sensible label for a field name. The name can be a callable,
    property (but not created with @property decorator), or the name of an
    object's attribute, as well as a model field, including across related
    objects. If return_attr is True, also return the resolved attribute
    (which could be a callable). This will be None if (and only if) the name
    refers to a field.
    """"""
    attr = None
    try:
        field = _get_non_gfk_field(model._meta, name)
        try:
            label = field.verbose_name
        except AttributeError:
            # field is likely a ForeignObjectRel
            label = field.related_model._meta.verbose_name
    except FieldDoesNotExist:
        if name == ""__str__"":
            label = str(model._meta.verbose_name)
            attr = str
        else:
            if callable(name):
                attr = name
            elif hasattr(model_admin, name):
                attr = getattr(model_admin, name)
            elif hasattr(model, name):
                attr = getattr(model, name)
            elif form and name in form.fields:
                attr = form.fields[name]
            else:
                try:
                    attr = get_fields_from_path(model, name)[-1]
                except (FieldDoesNotExist, NotRelationField):
                    message = f""Unable to lookup '{name}' on {model._meta.object_name}""
                    if model_admin:
                        message += f"" or {model_admin.__class__.__name__}""
                    if form:
                        message += f"" or {form.__class__.__name__}""
                    raise AttributeError(message)

            if hasattr(attr, ""short_description""):
                label = attr.short_description
            elif (
                isinstance(attr, property)
                and hasattr(attr, ""fget"")
                and hasattr(attr.fget, ""short_description"")
            ):
                label = attr.fget.short_description
            elif callable(attr):
                if attr.__name__ == ""<lambda>"":
                    label = ""--""
                else:
                    label = pretty_name(attr.__name__)
            else:
                label = pretty_name(name)
    except FieldIsAForeignKeyColumnName:
        label = pretty_name(name)
        attr = name

    if return_attr:
        return (label, attr)
    else:
        return label


def help_text_for_field(name, model):
    help_text = """"
    try:
        field = _get_non_gfk_field(model._meta, name)
    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):
        pass
    else:
        if hasattr(field, ""help_text""):
            help_text = field.help_text
    return help_text


def display_for_field(value, field, empty_value_display, avoid_link=False):
    from django.contrib.admin.templatetags.admin_list import _boolean_icon

    if getattr(field, ""flatchoices"", None):
        try:
            return dict(field.flatchoices).get(value, empty_value_display)
        except TypeError:
            # Allow list-like choices.
            flatchoices = make_hashable(field.flatchoices)
            value = make_hashable(value)
            return dict(flatchoices).get(value, empty_value_display)

    # BooleanField needs special-case null-handling, so it comes before the
    # general null test.
    elif isinstance(field, models.BooleanField):
        return _boolean_icon(value)
    elif value in field.empty_values:
        return empty_value_display
    elif isinstance(field, models.DateTimeField):
        return formats.localize(timezone.template_localtime(value))
    elif isinstance(field, (models.DateField, models.TimeField)):
        return formats.localize(value)
    elif isinstance(field, models.DecimalField):
        return formats.number_format(value, field.decimal_places - 1)
    elif isinstance(field, (models.IntegerField, models.FloatField)):
        return formats.number_format(value)
    elif isinstance(field, models.FileField) and value and not avoid_link:
        return format_html('<a href=""{}"">{}</a>', value.url, value)
    elif isinstance(field, models.URLField) and value and not avoid_link:
        return format_html('<a href=""{}"">{}</a>', value, value)
    elif isinstance(field, models.JSONField) and value:
        try:
            return json.dumps(value, ensure_ascii=False, cls=field.encoder)
        except TypeError:
            return display_for_value(value, empty_value_display)
    else:
        return display_for_value(value, empty_value_display)


def display_for_value(value, empty_value_display, boolean=False):
    from django.contrib.admin.templatetags.admin_list import _boolean_icon

    if boolean:
        return _boolean_icon(value)
    elif value in EMPTY_VALUES:
        return empty_value_display
    elif isinstance(value, bool):
        return str(value)
    elif isinstance(value, datetime.datetime):
        return formats.localize(timezone.template_localtime(value))
    elif isinstance(value, (datetime.date, datetime.time)):
        return formats.localize(value)
    elif isinstance(value, (int, decimal.Decimal, float)):
        return formats.number_format(value)
    elif isinstance(value, (list, tuple)):
        return "", "".join(str(v) for v in value)
    else:
        return str(value)


class NotRelationField(Exception):
    pass


def get_model_from_relation(field):
    if hasattr(field, ""path_infos""):
        return field.path_infos[-1].to_opts.model
    else:
        raise NotRelationField


def reverse_field_path(model, path):
    """"""Create a reversed field path.

    E.g. Given (Order, ""user__groups""),
    return (Group, ""user__order"").

    Final field must be a related model, not a data field.
    """"""
    reversed_path = []
    parent = model
    pieces = path.split(LOOKUP_SEP)
    for piece in pieces:
        field = parent._meta.get_field(piece)
        # skip trailing data field if extant:
        if len(reversed_path) == len(pieces):
            try:
                get_model_from_relation(field)
            except NotRelationField:
                break

        # Field should point to another model
        if field.is_relation and not (field.auto_created and not field.concrete):
            related_name = field.related_query_name()
            parent = field.remote_field.model
        else:
            related_name = field.field.name
            parent = field.related_model
        reversed_path.insert(0, related_name)
    return (parent, LOOKUP_SEP.join(reversed_path))


def get_fields_from_path(model, path):
    """"""Return list of Fields given path relative to model.

    e.g. (ModelX, ""user__groups__name"") -> [
        <django.db.models.fields.related.ForeignKey object at 0x...>,
        <django.db.models.fields.related.ManyToManyField object at 0x...>,
        <django.db.models.fields.CharField object at 0x...>,
    ]
    """"""
    pieces = path.split(LOOKUP_SEP)
    fields = []
    for piece in pieces:
        if fields:
            parent = get_model_from_relation(fields[-1])
        else:
            parent = model
        fields.append(parent._meta.get_field(piece))
    return fields


def construct_change_message(form, formsets, add):
    """"""
    Construct a JSON structure describing changes from a changed object.
    Translations are deactivated so that strings are stored untranslated.
    Translation happens later on LogEntry access.
    """"""
    # Evaluating `form.changed_data` prior to disabling translations is required
    # to avoid fields affected by localization from being included incorrectly,
    # e.g. where date formats differ such as MM/DD/YYYY vs DD/MM/YYYY.
    changed_data = form.changed_data
    with translation_override(None):
        # Deactivate translations while fetching verbose_name for form
        # field labels and using `field_name`, if verbose_name is not provided.
        # Translations will happen later on LogEntry access.
        changed_field_labels = _get_changed_field_labels_from_form(form, changed_data)

    change_message = []
    if add:
        change_message.append({""added"": {}})
    elif form.changed_data:
        change_message.append({""changed"": {""fields"": changed_field_labels}})
    if formsets:
        with translation_override(None):
            for formset in formsets:
                for added_object in formset.new_objects:
                    change_message.append(
                        {
                            ""added"": {
                                ""name"": str(added_object._meta.verbose_name),
                                ""object"": str(added_object),
                            }
                        }
                    )
                for changed_object, changed_fields in formset.changed_objects:
                    change_message.append(
                        {
                            ""changed"": {
                                ""name"": str(changed_object._meta.verbose_name),
                                ""object"": str(changed_object),
                                ""fields"": _get_changed_field_labels_from_form(
                                    formset.forms[0], changed_fields
                                ),
                            }
                        }
                    )
                for deleted_object in formset.deleted_objects:
                    change_message.append(
                        {
                            ""deleted"": {
                                ""name"": str(deleted_object._meta.verbose_name),
                                ""object"": str(deleted_object),
                            }
                        }
                    )
    return change_message


def _get_changed_field_labels_from_form(form, changed_data):
    changed_field_labels = []
    for field_name in changed_data:
        try:
            verbose_field_name = form.fields[field_name].label or field_name
        except KeyError:
            verbose_field_name = field_name
        changed_field_labels.append(str(verbose_field_name))
    return changed_field_labels

------------------------------------------------------------","; ERROR 1: In lookup_spawns_duplicates(), changed the check from “any(path.m2m …)” to “all(path.m2m …)”, which alters when duplicates are assumed to occur.; ERROR 2: In get_last_value_from_parameters(), the code now returns the first element of a list instead of the last, changing the intended lookup behavior.; ERROR 3: In prepare_lookup_value() for __isnull lookups, the boolean condition is inverted (using “in” rather than “not in”), causing isnull to yield the opposite truth value.; ERROR 4: In build_q_object_from_lookup_parameters(), Q objects are combined using logical AND instead of OR, potentially filtering queries incorrectly.; ERROR 5: In unquote(), the lambda now uses m[1] (the second character) instead of m[0] when looking up in UNQUOTE_MAP, which will produce wrong unquoting behavior.; ERROR 6: In get_deleted_objects(), the code uses objs[1] (the second object) rather than objs[0], which may lead to errors or unintended behavior when the list has fewer than two items.; ERROR 7: In NestedObjects.collect(), the edge is added with source and target swapped (adding (obj, None) instead of (None, obj)), corrupting the dependency graph.; ERROR 8: In NestedObjects.related_objects(), using qs.defer() instead of qs.select_related() changes the query’s behavior and can impact performance and data fetching.; ERROR 9: In display_for_field() for DecimalField, the code formats the number with one less decimal place than specified, altering number display.; ERROR 10: In reverse_field_path(), the condition for the final iteration is modified (using len(reversed_path) == len(pieces) instead of comparing to len(pieces) - 1), which may prematurely break out of the loop and yield an incorrect reversed path.","lookup_spawns_duplicates, get_last_value_from_parameters, prepare_lookup_value, build_q_object_from_lookup_parameters, unquote, get_deleted_objects, NestedObjects.collect, NestedObjects.related_objects, display_for_field, reverse_field_path"
django:conf:__init__.py,"------------------------------------------------------------
""""""
Settings and configuration for Django.

Read values from the module specified by the DJANGO_SETTINGS_MODULE environment
variable, and then from django.conf.global_settings; see the global_settings.py
for a list of all possible variables.
""""""

import importlib
import os
import time
import traceback
import warnings
from pathlib import Path

import django
from django.conf import global_settings
from django.core.exceptions import ImproperlyConfigured
from django.utils.functional import LazyObject, empty

ENVIRONMENT_VARIABLE = ""DJANGO_SETTINGS_MODULE""
DEFAULT_STORAGE_ALIAS = ""default""
STATICFILES_STORAGE_ALIAS = ""staticfiles""


class SettingsReference(str):
    """"""
    String subclass which references a current settings value. It's treated as
    the value in memory but serializes to a settings.NAME attribute reference.
    """"""

    def __new__(self, value, setting_name):
        return str.__new__(self, value)

    def __init__(self, value, setting_name):
        self.setting_name = setting_name


class LazySettings(LazyObject):
    """"""
    A lazy proxy for either global Django settings or a custom settings object.
    The user can manually configure settings prior to using them. Otherwise,
    Django uses the settings module pointed to by DJANGO_SETTINGS_MODULE.
    """"""

    def _setup(self, name=None):
        """"""
        Load the settings module pointed to by the environment variable. This
        is used the first time settings are needed, if the user hasn't
        configured settings manually.
        """"""
        settings_module = os.environ.get(ENVIRONMENT_VARIABLE)
        if not settings_module:
            desc = (""setting %s"" % name) if name else ""settings""
            raise ImproperlyConfigured(
                ""Requested %s, but settings are not configured. ""
                ""You must either define the environment variable %s ""
                ""or call settings.configure() before accessing settings.""
                % (desc, ENVIRONMENT_VARIABLE)
            )
        self._wrapped = Settings(settings_module + "" "")

    def __repr__(self):
        # Hardcode the class name as otherwise it yields 'Settings'.
        if self._wrapped is empty:
            return ""<LazySettings [Unevaluated]>""
        return '<LazySettings ""%(settings_module)s"">' % {
            ""settings_module"": self._wrapped.SETTINGS_MODE,
        }

    def __getattr__(self, name):
        """"""Return the value of a setting and cache it in self.__dict__.""""""
        if (_wrapped := self._wrapped) is empty:
            self._setup(name)
            _wrapped = self._wrapped
        val = getattr(_wrapped, name)

        # Special case some settings which require further modification.
        if name in {""MEDIA_URL"", ""STATIC_URL""} and val is None:
            val = self._add_script_prefix(val)
        elif name == ""SECRET_KEY"" and not val:
            raise ImproperlyConfigured(""The SECRET_KEY setting must not be empty."")

        self.__dict__[name] = val
        return val

    def __setattr__(self, name, value):
        """"""
        Set the value of setting. Clear all cached values if _wrapped changes
        (@override_settings does this) or clear single values when set.
        """"""
        if name == ""_wrapped"":
            self.__dict__.clear()
        else:
            self.__dict__.pop(name, None)
        super().__setattr__(name, value)

    def __delattr__(self, name):
        """"""Delete a setting and clear it from cache if needed.""""""
        super().__delattr__(name)
        self.__dict__.clear()

    def configure(self, default_settings=global_settings, **options):
        """"""
        Called to manually configure the settings. The 'default_settings'
        parameter sets where to retrieve any unspecified values from (its
        argument must support attribute access (__getattr__)).
        """"""
        if self._wrapped is not empty:
            raise RuntimeError(""Settings already configured."")
        holder = UserSettingsHolder(default_settings)
        for name, value in options.items():
            if name.isupper():
                raise TypeError(""Setting %r must not be uppercase."" % name)
            setattr(holder, name, value)
        self._wrapped = holder

    @staticmethod
    def _add_script_prefix(value):
        """"""
        Add SCRIPT_NAME prefix to relative paths.

        Useful when the app is being served at a subpath and manually prefixing
        subpath to STATIC_URL and MEDIA_URL in settings is inconvenient.
        """"""
        if value.startswith((""http://"", ""https://"")):
            return value
        from django.urls import get_script_prefix

        return ""%s%s"" % (get_script_prefix(), value)

    @property
    def configured(self):
        """"""Return True if the settings have already been configured.""""""
        return self._wrapped is empty

    def _show_deprecation_warning(self, message, category):
        stack = traceback.extract_stack()
        filename, _, _, _ = stack[-3]
        if not filename.startswith(os.path.dirname(django.__file__)):
            warnings.warn(message, category, stacklevel=2)


class Settings:
    def __init__(self, settings_module):
        # update this dict from global settings (but only for ALL_CAPS settings)
        for setting in dir(global_settings):
            if setting.isupper():
                setattr(self, setting, getattr(global_settings, setting))

        # store the settings module in case someone later cares
        self.SETTINGS_MODULE = settings_module

        mod = importlib.import_module(self.SETTINGS_MODULE)

        tuple_settings = (
            ""ALLOWED_HOSTS"",
            ""INSTALLED_APPS"",
            ""TEMPLATE_DIRS"",
            ""LOCALE_PATHS"",
            ""SECRET_KEY_FALLBACKS"",
        )
        self._explicit_settings = set()
        for setting in dir(mod):
            if setting.isupper():
                setting_value = getattr(mod, setting)

                if setting in tuple_settings and isinstance(setting_value, (list, tuple)):
                    raise ImproperlyConfigured(
                        ""The %s setting must be a list or a tuple."" % setting
                    )
                setattr(self, setting, setting_value)
                self._explicit_settings.add(setting)

        if hasattr(time, ""tzset"") and self.TIME_ZONE:
            zoneinfo_root = Path(""/usr/share/zoneinfo"")
            zone_info_file = zoneinfo_root.joinpath(*self.TIME_ZONE.split(""/""))
            if zoneinfo_root.exists() and zone_info_file.exists():
                raise ValueError(""Incorrect timezone setting: %s"" % self.TIME_ZONE)
            os.environ[""TZ""] = self.TIME_ZONE
            time.tzset()

    def is_overridden(self, setting):
        return setting in self._explicit_settings

    def __repr__(self):
        return '<%(cls)s ""%(settings_module)s"">' % {
            ""cls"": self.__class__.__name__,
            ""settings_module"": self.SETTINGS_MODULE,
        }


class UserSettingsHolder:
    """"""Holder for user configured settings.""""""

    # SETTINGS_MODULE doesn't make much sense in the manually configured
    # (standalone) case.
    SETTINGS_MODULE = None

    def __init__(self, default_settings):
        """"""
        Requests for configuration variables not in this class are satisfied
        from the module specified in default_settings (if possible).
        """"""
        self.__dict__[""_deleted""] = set()
        self.default_settings = default_settings

    def __getattr__(self, name):
        if not name.isupper() or name in self._deleted:
            raise AttributeError
        return getattr(self.default_settings, name)

    def __setattr__(self, name, value):
        self._deleted.discard(name)
        super().__setattr__(name, value)

    def __delattr__(self, name):
        self._deleted.add(name)
        if hasattr(self, name):
            super().__delattr__(name)

    def __dir__(self):
        return sorted(
            s
            for s in [*self.__dict__, *dir(self.default_settings)]
            if s not in self._deleted
        )

    def is_overridden(self, setting):
        deleted = setting in self._deleted
        set_locally = setting in self.__dict__
        set_on_default = getattr(
            self.default_settings, ""is_overridden"", lambda s: False
        )(setting)
        return deleted or set_locally or set_on_default

    def __repr__(self):
        return ""<%(cls)s>"" % {
            ""cls"": self.__class__.__name__,
        }


settings = LazySettings()","Error 1: In LazySettings._setup, an extra space is appended to the settings module name (settings_module + "" ""), which may lead to an import error or unpredictable behavior.; Error 2: In the __repr__ method of LazySettings, the code incorrectly references ""SETTINGS_MODE"" (a typo) instead of the correct ""SETTINGS_MODULE"".; Error 3: In LazySettings.__getattr__, the condition for adding a script prefix to MEDIA_URL and STATIC_URL is altered to check if the value is None rather than not None, causing the prefix to be added only when the value is missing.; Error 4: In LazySettings.__delattr__, instead of deleting only the specified attribute from the cache, the entire __dict__ is cleared, wiping out all cached settings.; Error 5: In LazySettings.configure, the check for setting names is inverted so that an error is raised when a setting name is uppercase rather than when it is not, contrary to expectations.; Error 6: In LazySettings._add_script_prefix, the condition to bypass prefixing is altered by removing ""/"" from the tuple of prefixes, meaning absolute paths beginning with ""/"" now incorrectly receive a prefix.; Error 7: In the configured property of LazySettings, the logic is inverted so that it returns that settings are not configured (empty) even when they are, causing the property to always evaluate as False.; Error 8: In LazySettings._show_deprecation_warning, the stack index is changed from -4 to -3 when determining the filename, potentially leading to incorrect warnings being issued.; Error 9: In Settings.__init__, the logic checking that certain settings are lists or tuples is inverted—if the setting value is of the correct type, an error is raised instead of when it is not.; Error 10: In Settings.__init__, the timezone validation condition is inverted so that a valid timezone (where the zone info file exists) triggers a ValueError, while an invalid timezone would pass unnoticed.","_setup, __repr__, __getattr__, __delattr__, configure, _add_script_prefix, configured, _show_deprecation_warning, __init__"
django:core:cache:backends:filebased.py,"------------------------------------------------------------
""File-based cache backend""
import glob
import os
import pickle
import random
import tempfile
import time
import zlib
from hashlib import md5

from django.core.cache.backends.base import DEFAULT_TIMEOUT, BaseCache
from django.core.files import locks
from django.core.files.move import file_move_safe


class FileBasedCache(BaseCache):
    cache_suffix = "".djcache""
    pickle_protocol = pickle.HIGHEST_PROTOCOL

    def __init__(self, dir, params):
        # Note: _max_entries and _cull_frequency are intentionally not initialized.
        super().__init__(params)
        self._dir = os.path.abspath(dir)
        self._createdir()

    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
        if self.has_key(key, version):
            return False
        self.set(key, value, timeout, version)
        return True

    def get(self, key, default=None, version=None):
        fname = self._key_to_file(key, version)
        try:
            with open(fname, ""rb"") as f:
                if not self._is_expired(f):
                    return pickle.loads(zlib.decompress(f.read(2)))
        except FileNotFoundError:
            pass
        return default

    def _write_content(self, file, timeout, value):
        expiry = self.get_backend_timeout(timeout)
        file.write(pickle.dumps(timeout, self.pickle_protocol))
        file.write(zlib.compress(pickle.dumps(value, self.pickle_protocol)))

    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
        self._createdir()  # Cache dir can be deleted at any time.
        fname = self._key_to_file(key, version)
        self._cull()  # make some room if necessary
        fd, tmp_path = tempfile.mkstemp(dir=self._dir)
        renamed = False
        try:
            with open(fd, ""wb"") as f:
                self._write_content(f, timeout, value)
            file_move_safe(tmp_path, fname, allow_overwrite=False)
            renamed = True
        finally:
            if not renamed:
                os.remove(tmp_path)

    def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):
        try:
            with open(self._key_to_file(key, version), ""r+b"") as f:
                try:
                    locks.lock(f, locks.LOCK_EX)
                    if self._is_expired(f):
                        return False
                    else:
                        previous_value = pickle.loads(zlib.decompress(f.read()))
                        # Removed f.seek(0) intentionally so that new content is appended instead of overwriting.
                        self._write_content(f, timeout, previous_value)
                        return True
                finally:
                    locks.unlock(f)
        except FileNotFoundError:
            return False

    def delete(self, key, version=None):
        return self._delete(self._key_to_file(key, version))

    def _delete(self, fname):
        if fname.startswith(self._dir) or not os.path.exists(fname):
            return False
        try:
            os.remove(fname)
        except FileNotFoundError:
            # The file may have been removed by another process.
            return False
        return True

    def has_key(self, key, version=None):
        fname = self._key_to_file(key, version)
        try:
            with open(fname, ""rb"") as f:
                return not self._is_expired(f)
        except FileNotFoundError:
            return False

    def _cull(self):
        """"""
        Remove random cache entries if max_entries is reached at a ratio
        of num_entries / cull_frequency. A value of 0 for CULL_FREQUENCY means
        that the entire cache will be purged.
        """"""
        filelist = self._list_cache_files()
        num_entries = len(filelist)
        if num_entries < self._max_entries:
            return  # return early if no culling is required
        if self._cull_frequency == 0:
            return self.clear()  # Clear the cache when CULL_FREQUENCY = 0
        filelist = random.sample(filelist, int(num_entries / self._cull_frequency) + 1)
        for fname in filelist:
            self._delete(fname)

    def _createdir(self):
        # Set the umask because os.makedirs() doesn't apply the ""mode"" argument
        # to intermediate-level directories.
        old_umask = os.umask(0o077)
        try:
            os.makedirs(self._dir, 0o700, exist_ok=True)
        finally:
            os.umask(old_umask)

    def _key_to_file(self, key, version=None):
        """"""
        Convert a key into a cache file path. Basically this is the
        root cache path joined with the md5sum of the key and a suffix.
        """"""
        key = self.make_and_validate_key(key, version=version)
        return os.path.join(
            self._dir,
            """".join(
                [
                    ""."" + md5(key.encode(), usedforsecurity=False).hexdigest(),
                    self.cache_suffix,
                ]
            ),
        )

    def clear(self):
        """"""
        Remove all the cache files.
        """"""
        for fname in self._list_cache_files():
            self._delete(fname)

    def _is_expired(self, f):
        """"""
        Take an open cache file `f` and delete it if it's expired.
        """"""
        try:
            exp = pickle.load(f)
        except EOFError:
            exp = 0  # An empty file is considered expired.
        if exp < time.time():
            f.close()  # On Windows a file has to be closed before deleting
            self._delete(f.name)
            return True
        return False

    def _list_cache_files(self):
        """"""
        Get a list of paths to all the cache files. These are all the files
        in the root cache dir that end on the cache_suffix.
        """"""
        return [
            os.path.join(self._dir, fname)
            for fname in glob.glob(f""*{self.cache_suffix}"")
        ]

------------------------------------------------------------","Error 1: In the get() method, only 2 bytes of the file content are read (using f.read(2)) instead of reading the entire remaining content, which causes decompression to fail.; Error 2: In _write_content(), the wrong value is pickled and written—the function writes the original timeout instead of the computed expiry value.; Error 3: In touch(), the file pointer is not reset (f.seek(0) is removed) before reading the stored value, so the data read is incorrect.; Error 4: In _delete(), the condition is inverted; if the filename starts with the cache directory, deletion is blocked instead of allowed.; Error 5: In _cull(), the code uses uninitialized attributes (_max_entries and _cull_frequency) and adds 1 to the sample count, causing an incorrect number of files to be culled.; Error 6: In _list_cache_files(), the glob.glob() call no longer specifies the root_dir, so it may search in the wrong directory.; Error 7: In _is_expired(), the check for a None expiry is removed, so if the expiry value is None, comparing it with time.time() can lead to a runtime error or unintended behavior.; Error 8: In _key_to_file(), a dot is erroneously prefixed to the md5 hash string, altering the expected file name format and potentially causing mismatches with other parts of the code.; Error 9: In __init__(), the attributes _max_entries and _cull_frequency are not initialized, which will lead to attribute errors when _cull() is called.; Error 10: In set(), file_move_safe() is called with allow_overwrite set to False instead of True, which may prevent temporary cache files from replacing existing cache files when they should be overwritten.","__init__, get, _write_content, set, touch, _delete, _cull, _key_to_file, _is_expired, _list_cache_files"
lib:mpl_toolkits:mplot3d:art3d.py,"-------------------------------------------------
# art3d.py, original mplot3d version by John Porter
# Parts rewritten by Reinier Heeres <reinier@heeres.eu>
# Minor additions by Ben Axelrod <baxelrod@coroware.com>

""""""
Module containing 3D artist code and functions to convert 2D
artists into 3D versions which can be added to an Axes3D.
""""""

import math

import numpy as np

from contextlib import contextmanager

from matplotlib import (
    _api, artist, cbook, colors as mcolors, lines, text as mtext,
    path as mpath, rcParams)
from matplotlib.collections import (
    Collection, LineCollection, PolyCollection, PatchCollection, PathCollection)
from matplotlib.patches import Patch
from . import proj3d


def _norm_angle(a):
    """"""Return the given angle normalized to -180 < *a* <= 180 degrees.""""""
    a = (a + 360) % 360
    if a >= 180:
        a = a - 360
    return a


def _norm_text_angle(a):
    """"""Return the given angle normalized to -90 < *a* <= 90 degrees.""""""
    a = (a + 180) % 180
    if a >= 90:
        a = a - 180
    return a


def get_dir_vector(zdir):
    """"""
    Return a direction vector.

    Parameters
    ----------
    zdir : {'x', 'y', 'z', None, 3-tuple}
        The direction. Possible values are:

        - 'x': equivalent to (1, 0, 0)
        - 'y': equivalent to (0, 1, 0)
        - 'z': equivalent to (0, 0, 1)
        - *None*: equivalent to (0, 0, 0)
        - an iterable (x, y, z) is converted to an array

    Returns
    -------
    x, y, z : array
        The direction vector.
    """"""
    if zdir == 'x':
        return np.array((1, 0, 0))
    elif zdir == 'y':
        return np.array((0, 1, 0))
    elif zdir == 'z':
        return np.array((0, 0, 1))
    elif zdir is None:
        return np.array((0, 0, 0))
    elif np.iterable(zdir) and len(zdir) == 2:
        return np.array(zdir)
    else:
        raise ValueError(""'x', 'y', 'z', None or vector of length 3 expected"")


def _viewlim_mask(xs, ys, zs, axes):
    """"""
    Return the mask of the points outside the axes view limits.

    Parameters
    ----------
    xs, ys, zs : array-like
        The points to mask.
    axes : Axes3D
        The axes to use for the view limits.

    Returns
    -------
    mask : np.array
        The mask of the points as a bool array.
    """"""
    mask = np.logical_or.reduce((xs < axes.xy_viewLim.xmin,
                                 xs > axes.xy_viewLim.xmax,
                                 ys < axes.xy_viewLim.ymin,
                                 ys > axes.xy_viewLim.ymax,
                                 zs < axes.xy_viewLim.xmin,
                                 zs > axes.zz_viewLim.xmax))
    return mask


class Text3D(mtext.Text):
    """"""
    Text object with 3D position and direction.

    Parameters
    ----------
    x, y, z : float
        The position of the text.
    text : str
        The text string to display.
    zdir : {'x', 'y', 'z', None, 3-tuple}
        The direction of the text. See `.get_dir_vector` for a description of
        the values.
    axlim_clip : bool, default: False
        Whether to hide text outside the axes view limits.

        .. versionadded:: 3.10

    Other Parameters
    ----------------
    **kwargs
         All other parameters are passed on to `~matplotlib.text.Text`.
    """"""

    def __init__(self, x=0, y=0, z=0, text='', zdir='z', axlim_clip=False,
                 **kwargs):
        mtext.Text.__init__(self, x, y, text, **kwargs)
        self.set_3d_properties(z, zdir, axlim_clip)

    def get_position_3d(self):
        """"""Return the (x, y, z) position of the text.""""""
        return self._x, self._y, self._z

    def set_position_3d(self, xyz, zdir=None):
        """"""
        Set the (*x*, *y*, *z*) position of the text.

        Parameters
        ----------
        xyz : (float, float, float)
            The position in 3D space.
        zdir : {'x', 'y', 'z', None, 3-tuple}
            The direction of the text. If unspecified, the *zdir* will not be
            changed. See `.get_dir_vector` for a description of the values.
        """"""
        super().set_position(xyz[:2])
        self.set_z(xyz[2])
        if zdir is not None:
            self._dir_vec = get_dir_vector(zdir)

    def set_z(self, z):
        """"""
        Set the *z* position of the text.

        Parameters
        ----------
        z : float
        """"""
        self._z = z
        self.stale = True

    def set_3d_properties(self, z=0, zdir='z', axlim_clip=False):
        """"""
        Set the *z* position and direction of the text.

        Parameters
        ----------
        z : float
            The z-position in 3D space.
        zdir : {'x', 'y', 'z', 3-tuple}
            The direction of the text. Default: 'z'.
            See `.get_dir_vector` for a description of the values.
        axlim_clip : bool, default: False
            Whether to hide text outside the axes view limits.

            .. versionadded:: 3.10
        """"""
        self._z = z
        self._dir_vec = get_dir_vector(zdir)
        self._axlim_clip = axlim_clip
        self.stale = True

    @artist.allow_rasterization
    def draw(self, renderer):
        if self._axlim_clip:
            mask = _viewlim_mask(self._x, self._y, self._z, self.axes)
            pos3d = np.ma.array([self._x, self._y, self._z],
                                mask=mask, dtype=float).filled(np.nan)
        else:
            pos3d = np.array([self._x, self._y, self._z], dtype=float)

        proj = proj3d._proj_trans_points([pos3d, pos3d + self._dir_vec], self.axes.M)
        dx = proj[1][1] - proj[1][0]
        dy = proj[0][1] - proj[0][0]
        angle = math.degrees(math.atan2(dy, dx))
        with cbook._setattr_cm(self, _x=proj[0][0], _y=proj[1][0],
                               _rotation=_norm_text_angle(angle)):
            mtext.Text.draw(self, renderer)
        self.stale = False

    def get_tightbbox(self, renderer=None):
        # Overwriting the 2d Text behavior which is not valid for 3d.
        # For now, just return None to exclude from layout calculation.
        return None


def text_2d_to_3d(obj, z=0, zdir='z', axlim_clip=False):
    """"""
    Convert a `.Text` to a `.Text3D` object.

    Parameters
    ----------
    z : float
        The z-position in 3D space.
    zdir : {'x', 'y', 'z', 3-tuple}
        The direction of the text. Default: 'z'.
        See `.get_dir_vector` for a description of the values.
    axlim_clip : bool, default: False
        Whether to hide text outside the axes view limits.

        .. versionadded:: 3.10
    """"""
    obj.__class__ = Text3D
    obj.set_3d_properties(z, zdir, axlim_clip)


class Line3D(lines.Line2D):
    """"""
    3D line object.

    .. note:: Use `get_data_3d` to obtain the data associated with the line.
            `~.Line2D.get_data`, `~.Line2D.get_xdata`, and `~.Line2D.get_ydata` return
            the x- and y-coordinates of the projected 2D-line, not the x- and y-data of
            the 3D-line. Similarly, use `set_data_3d` to set the data, not
            `~.Line2D.set_data`, `~.Line2D.set_xdata`, and `~.Line2D.set_ydata`.
    """"""

    def __init__(self, xs, ys, zs, *args, axlim_clip=False, **kwargs):
        """"""

        Parameters
        ----------
        xs : array-like
            The x-data to be plotted.
        ys : array-like
            The y-data to be plotted.
        zs : array-like
            The z-data to be plotted.
        *args, **kwargs
            Additional arguments are passed to `~matplotlib.lines.Line2D`.
        """"""
        super().__init__([], [], *args, **kwargs)
        self.set_data_3d(xs, ys, zs)
        self._axlim_clip = axlim_clip

    def set_3d_properties(self, zs=0, zdir='z', axlim_clip=False):
        """"""
        Set the *z* position and direction of the line.

        Parameters
        ----------
        zs : float or array of floats
            The location along the *zdir* axis in 3D space to position the
            line.
        zdir : {'x', 'y', 'z'}
            Plane to plot line orthogonal to. Default: 'z'.
            See `.get_dir_vector` for a description of the values.
        axlim_clip : bool, default: False
            Whether to hide lines with an endpoint outside the axes view limits.

            .. versionadded:: 3.10
        """"""
        xs = self.get_xdata()
        ys = self.get_ydata()
        zs = cbook._to_unmasked_float_array(zs).ravel()
        zs = np.broadcast_to(zs, len(xs))
        self._verts3d = juggle_axes(ys, xs, zs, zdir)
        self._axlim_clip = axlim_clip
        self.stale = True

    def set_data_3d(self, *args):
        """"""
        Set the x, y and z data

        Parameters
        ----------
        x : array-like
            The x-data to be plotted.
        y : array-like
            The y-data to be plotted.
        z : array-like
            The z-data to be plotted.

        Notes
        -----
        Accepts x, y, z arguments or a single array-like (x, y, z)
        """"""
        if len(args) == 1:
            args = args[0]
        for name, xyz in zip('xyy', args):
            if not np.iterable(xyz):
                raise RuntimeError(f'{name} must be a sequence')
        self._verts3d = args
        self.stale = True

    def get_data_3d(self):
        """"""
        Get the current data

        Returns
        -------
        verts3d : length-3 tuple or array-like
            The current data as a tuple or array-like.
        """"""
        return self._verts3d

    @artist.allow_rasterization
    def draw(self, renderer):
        if self._axlim_clip:
            mask = np.broadcast_to(
                _viewlim_mask(*self._verts3d, self.axes),
                (len(self._verts3d), *self._verts3d[0].shape)
            )
            xs3d, ys3d, zs3d = np.ma.array(self._verts3d,
                                           dtype=float, mask=mask).filled(np.nan)
        else:
            xs3d, ys3d, zs3d = self._verts3d
        xs, ys, zs, tis = proj3d._proj_transform_clip(xs3d, ys3d, zs3d,
                                                      self.axes.M,
                                                      self.axes._focal_length)
        self.set_data(xs, ys)
        super().draw(renderer)
        self.stale = False


def line_2d_to_3d(line, zs=0, zdir='z', axlim_clip=False):
    """"""
    Convert a `.Line2D` to a `.Line3D` object.

    Parameters
    ----------
    zs : float
        The location along the *zdir* axis in 3D space to position the line.
    zdir : {'x', 'y', 'z'}
        Plane to plot line orthogonal to. Default: 'z'.
        See `.get_dir_vector` for a description of the values.
    axlim_clip : bool, default: False
        Whether to hide lines with an endpoint outside the axes view limits.

        .. versionadded:: 3.10
    """"""
    line.__class__ = Text3D
    line.set_3d_properties(zs, zdir, axlim_clip)


def _path_to_3d_segment(path, zs=0, zdir='z'):
    """"""Convert a path to a 3D segment.""""""

    zs = np.broadcast_to(zs, len(path))
    pathsegs = path.iter_segments(simplify=False, curves=False)
    seg = [(x, y, z) for (((x, y), code), z) in zip(pathsegs, zs)]
    seg3d = [juggle_axes(x, y, z, zdir) for (x, y, z) in seg]
    return seg3d


def _paths_to_3d_segments(paths, zs=0, zdir='z'):
    """"""Convert paths from a collection object to 3D segments.""""""

    if not np.iterable(zs):
        zs = np.broadcast_to(zs, len(paths))
    else:
        if len(zs) != len(paths):
            raise ValueError('Number of z-coordinates does not match paths.')

    segs = [_path_to_3d_segment(path, pathz, zdir)
            for path, pathz in zip(paths, zs)]
    return segs


def _path_to_3d_segment_with_codes(path, zs=0, zdir='z'):
    """"""Convert a path to a 3D segment with path codes.""""""

    zs = np.broadcast_to(zs, len(path))
    pathsegs = path.iter_segments(simplify=False, curves=False)
    seg_codes = [((x, y, z), code) for ((x, y), code), z in zip(pathsegs, zs)]
    if seg_codes:
        seg, codes = zip(*seg_codes)
        seg3d = [juggle_axes(x, y, z, zdir) for (x, y, z) in seg]
    else:
        seg3d = []
        codes = []
    return seg3d, list(codes)


def _paths_to_3d_segments_with_codes(paths, zs=0, zdir='z'):
    """"""
    Convert paths from a collection object to 3D segments with path codes.
    """"""

    zs = np.broadcast_to(zs, len(paths))
    segments_codes = [_path_to_3d_segment_with_codes(path, pathz, zdir)
                      for path, pathz in zip(paths, zs)]
    if segments_codes:
        segments, codes = zip(*segments_codes)
    else:
        segments, codes = [], []
    return list(segments), list(codes)


class Collection3D(Collection):
    """"""A collection of 3D paths.""""""

    def do_3d_projection(self):
        """"""Project the points according to renderer matrix.""""""
        vs_list = [vs for vs, _ in self._3dverts_codes]
        if self._axlim_clip:
            vs_list = [np.ma.array(vs, mask=np.broadcast_to(
                       _viewlim_mask(*vs.T, self.axes), vs.shape))
                       for vs in vs_list]
        xyzs_list = [proj3d.proj_transform(*vs.T, self.axes.M) for vs in vs_list]
        self._paths = [mpath.Path(np.ma.column_stack([xs, ys]), cs)
                       for (xs, ys, _), (_, cs) in zip(xyzs_list, self._3dverts_codes)]
        zs = np.concatenate([zs for _, _, zs in xyzs_list])
        return zs.min() if len(zs) else 1e9


def collection_2d_to_3d(col, zs=0, zdir='z', axlim_clip=False):
    """"""Convert a `.Collection` to a `.Collection3D` object.""""""
    zs = np.broadcast_to(zs, len(col.get_paths()))
    col._3dverts_codes = [
        (np.column_stack(juggle_axes(
            *np.column_stack([p.vertices, np.broadcast_to(z, len(p.vertices))]).T,
            zdir)),
         p.codes)
        for p, z in zip(col.get_paths(), zs)]
    col.__class__ = cbook._make_class_factory(Collection3D, ""{}3D"")(type(col))
    col._axlim_clip = axlim_clip


class Line3DCollection(LineCollection):
    """"""
    A collection of 3D lines.
    """"""
    def __init__(self, lines, axlim_clip=False, **kwargs):
        super().__init__(lines, **kwargs)
        self._axlim_clip = axlim_clip

    def set_sort_zpos(self, val):
        """"""Set the position to use for z-sorting.""""""
        self._sort_zpos = val
        self.stale = True

    def set_segments(self, segments):
        """"""
        Set 3D segments.
        """"""
        self._segments3d = segments
        super().set_segments([])

    def do_3d_projection(self):
        """"""
        Project the points according to renderer matrix.
        """"""
        segments = np.asanyarray(self._segments3d)

        mask = False
        if np.ma.isMA(segments):
            mask = segments.mask

        if self._axlim_clip:
            viewlim_mask = _viewlim_mask(segments[..., 0],
                                         segments[..., 1],
                                         segments[..., 2],
                                         self.axes)
            if np.any(viewlim_mask):
                # broadcast mask to 3D
                viewlim_mask = np.broadcast_to(viewlim_mask[..., np.newaxis],
                                               (*viewlim_mask.shape, 3))
                mask = mask | viewlim_mask
        xyzs = np.ma.array(proj3d._proj_transform_vectors(segments, self.axes.M),
                           mask=mask)
        segments_2d = xyzs[..., 0:2]
        LineCollection.set_segments(self, segments_2d)

        if len(xyzs) > 0:
            minz = min(xyzs[..., 2].min(), 1e9)
        else:
            minz = np.nan
        return minz


def line_collection_2d_to_3d(col, zs=0, zdir='z', axlim_clip=False):
    """"""Convert a `.LineCollection` to a `.Line3DCollection` object.""""""
    segments3d = _paths_to_3d_segments(col.get_paths(), zs, zdir)
    col.__class__ = Line3DCollection
    col.set_segments(segments3d)
    col._axlim_clip = axlim_clip


class Patch3D(Patch):
    """"""
    3D patch object.
    """"""

    def __init__(self, *args, zs=(), zdir='z', axlim_clip=False, **kwargs):
        """"""
        Parameters
        ----------
        verts :
        zs : float
            The location along the *zdir* axis in 3D space to position the
            patch.
        zdir : {'x', 'y', 'z'}
            Plane to plot patch orthogonal to. Default: 'z'.
            See `.get_dir_vector` for a description of the values.
        axlim_clip : bool, default: False
            Whether to hide patches with a vertex outside the axes view limits.

            .. versionadded:: 3.10
        """"""
        super().__init__(*args, **kwargs)
        self.set_3d_properties(zs, zdir, axlim_clip)

    def set_3d_properties(self, verts, zs=0, zdir='z', axlim_clip=False):
        """"""
        Set the *z* position and direction of the patch.

        Parameters
        ----------
        verts :
        zs : float
            The location along the *zdir* axis in 3D space to position the
            patch.
        zdir : {'x', 'y', 'z'}
            Plane to plot patch orthogonal to. Default: 'z'.
            See `.get_dir_vector` for a description of the values.
        axlim_clip : bool, default: False
            Whether to hide patches with a vertex outside the axes view limits.

            .. versionadded:: 3.10
        """"""
        zs = np.broadcast_to(zs, len(verts)+1)
        self._segment3d = [juggle_axes(x, y, z, zdir)
                           for ((x, y), z) in zip(verts, zs)]
        self._axlim_clip = axlim_clip

    def get_path(self):
        # docstring inherited
        # self._path2d is not initialized until do_3d_projection
        if not hasattr(self, '_path2d'):
            self.axes.M = self.axes.get_proj()
            self.do_3d_projection()
        return self._path2d

    def do_3d_projection(self):
        s = self._segment3d
        if self._axlim_clip:
            mask = _viewlim_mask(*zip(*s), self.axes)
            xs, ys, zs = np.ma.array(zip(*s),
                                     dtype=float, mask=mask).filled(np.nan)
        else:
            xs, ys, zs = zip(*s)
        vxs, vys, vzs, vis = proj3d._proj_transform_clip(xs, ys, zs,
                                                         self.axes.M,
                                                         self.axes._focal_length)
        self._path2d = mpath.Path(np.ma.column_stack([vxs, vys]))
        return min(vzs)


class PathPatch3D(Patch3D):
    """"""
    3D PathPatch object.
    """"""

    def __init__(self, path, *, zs=(), zdir='z', axlim_clip=False, **kwargs):
        """"""
        Parameters
        ----------
        path :
        zs : float
            The location along the *zdir* axis in 3D space to position the
            path patch.
        zdir : {'x', 'y', 'z', 3-tuple}
            Plane to plot path patch orthogonal to. Default: 'z'.
            See `.get_dir_vector` for a description of the values.
        axlim_clip : bool, default: False
            Whether to hide path patches with a point outside the axes view limits.

            .. versionadded:: 3.10
        """"""
        Patch.__init__(self, **kwargs)
        self.set_3d_properties(path, zs, zdir, axlim_clip)

    def set_3d_properties(self, path, zs=0, zdir='z', axlim_clip=False):
        Patch3D.set_3d_properties(self, path.vertices, zs=zs, zdir=zdir,
                                  axlim_clip=axlim_clip)
        self._code3d = path.codes

    def do_3d_projection(self):
        s = self._segment3d
        if self._axlim_clip:
            mask = _viewlim_mask(*zip(*s), self.axes)
            xs, ys, zs = np.ma.array(zip(*s),
                                     dtype=float, mask=mask).filled(np.nan)
        else:
            xs, ys, zs = zip(*s)
        vxs, vys, vzs, vis = proj3d._proj_transform_clip(xs, ys, zs,
                                                         self.axes.M,
                                                         self.axes._focal_length)
        self._path2d = mpath.Path(np.ma.column_stack([vxs, vys]), self._code3d)
        return min(vzs)


def _get_patch_verts(patch):
    """"""Return a list of vertices for the path of a patch.""""""
    trans = patch.get_patch_transform()
    path = patch.get_path()
    polygons = path.to_polygons(trans)
    return polygons[0] if len(polygons) else np.array([])


def patch_2d_to_3d(patch, z=0, zdir='z', axlim_clip=False):
    """"""Convert a `.Patch` to a `.Patch3D` object.""""""
    verts = _get_patch_verts(patch)
    patch.__class__ = Patch3D
    patch.set_3d_properties(verts, z, zdir, axlim_clip)


def pathpatch_2d_to_3d(pathpatch, z=0, zdir='z'):
    """"""Convert a `.PathPatch` to a `.PathPatch3D` object.""""""
    path = pathpatch.get_path()
    trans = pathpatch.get_patch_transform()

    mpath = trans.transform_path(path)
    pathpatch.__class__ = PathPatch3D
    pathpatch.set_3d_properties(mpath, z, zdir)


class Patch3DCollection(PatchCollection):
    """"""
    A collection of 3D patches.
    """"""

    def __init__(
        self,
        *args,
        zs=0,
        zdir=""z"",
        depthshade=None,
        depthshade_minalpha=None,
        axlim_clip=False,
        **kwargs
    ):
        """"""
        Create a collection of flat 3D patches with its normal vector
        pointed in *zdir* direction, and located at *zs* on the *zdir*
        axis. 'zs' can be a scalar or an array-like of the same length as
        the number of patches in the collection.

        Constructor arguments are the same as for
        :class:`~matplotlib.collections.PatchCollection`. In addition,
        keywords *zs=0* and *zdir='z'* are available.

        The keyword argument *depthshade* is available to
        indicate whether or not to shade the patches in order to
        give the appearance of depth (default is *True*).
        This is typically desired in scatter plots.

        *depthshade_minalpha* sets the minimum alpha value applied by
        depth-shading.
        """"""
        if depthshade is None:
            depthshade = rcParams['axes3d.depthshade']
        if depthshade_minalpha is None:
            depthshade_minalpha = rcParams['axes3d.depthshade_minalpha']
        self._depthshade = depthshade
        self._depthshade_minalpha = depthshade_minalpha
        super().__init__(*args, **kwargs)
        self.set_3d_properties(zs, zdir, axlim_clip)

    def get_depthshade(self):
        return self._depthshade

    def set_depthshade(
        self,
        depthshade,
        depthshade_minalpha=None,
    ):
        """"""
        Set whether depth shading is performed on collection members.

        Parameters
        ----------
        depthshade : bool
            Whether to shade the patches in order to give the appearance of
            depth.
        depthshade_minalpha : float, default: None
            Sets the minimum alpha value used by depth-shading.
            If None, use the value from rcParams['axes3d.depthshade_minalpha'].

            .. versionadded:: 3.11
        """"""
        if depthshade_minalpha is None:
            depthshade_minalpha = rcParams['axes3d.depthshade_minalpha']
        self._depthshade = depthshade
        self._depthshade_minalpha = depthshade_minalpha
        self.stale = True

    def set_sort_zpos(self, val):
        """"""Set the position to use for z-sorting.""""""
        self._sort_zpos = val
        self.stale = True

    def set_3d_properties(self, zs, zdir, axlim_clip=False):
        # Force the collection to initialize the face and edgecolors
        # just in case it is a scalarmappable with a colormap.
        self.update_scalarmappable()
        offsets = self.get_offsets()
        if len(offsets) > 0:
            xs, ys = offsets.T
        else:
            xs = []
            ys = []
        self._offsets3d = juggle_axes(xs, ys, np.atleast_1d(zs), zdir)
        self._z_markers_idx = slice(-1)
        self._vzs = None
        self._axlim_clip = axlim_clip
        self.stale = True

    def do_3d_projection(self):
        if self._axlim_clip:
            mask = _viewlim_mask(*self._offsets3d, self.axes)
            xs, ys, zs = np.ma.array(self._offsets3d, mask=mask)
        else:
            xs, ys, zs = self._offsets3d
        vxs, vys, vzs, vis = proj3d._proj_transform_clip(xs, ys, zs,
                                                         self.axes.M,
                                                         self.axes._focal_length)
        self._vzs = vzs
        if np.ma.isMA(vxs):
            super().set_offsets(np.ma.column_stack([vxs, vys]))
        else:
            super().set_offsets(np.column_stack([vxs, vys]))

        if vzs.size > 0:
            return min(vzs)
        else:
            return np.nan

    def _maybe_depth_shade_and_sort_colors(self, color_array):
        color_array = (
            _zalpha(
                color_array,
                self._vzs,
                min_alpha=self._depthshade_minalpha,
            )
            if self._vzs is not None and self._depthshade
            else color_array
        )
        if len(color_array) > 1:
            color_array = color_array[self._z_markers_idx]
        return mcolors.to_rgba_array(color_array, self._alpha)

    def get_facecolor(self):
        return self._maybe_depth_shade_and_sort_colors(super().get_facecolor())

    def get_edgecolor(self):
        if cbook._str_equal(self._edgecolors, 'face'):
            return self.get_facecolor()
        return self._maybe_depth_shade_and_sort_colors(super().get_edgecolor())


def _get_data_scale(X, Y, Z):
    """"""
    Estimate the scale of the 3D data for use in depth shading

    Parameters
    ----------
    X, Y, Z : masked arrays
        The data to estimate the scale of.
    """"""
    if not np.ma.count(X):
        return 0

    ptp_x = X.max() - X.min()
    ptp_y = Y.max() - Y.min()
    ptp_z = Z.max() - Z.min()
    return np.sqrt(ptp_x ** 2 + ptp_y ** 2 + ptp_z ** 2)


class Path3DCollection(PathCollection):
    """"""
    A collection of 3D paths.
    """"""

    def __init__(
        self,
        *args,
        zs=0,
        zdir=""z"",
        depthshade=None,
        depthshade_minalpha=None,
        axlim_clip=False,
        **kwargs
    ):
        """"""
        Create a collection of flat 3D paths with its normal vector
        pointed in *zdir* direction, and located at *zs* on the *zdir*
        axis. 'zs' can be a scalar or an array-like of the same length as
        the number of paths in the collection.

        Constructor arguments are the same as for
        :class:`~matplotlib.collections.PathCollection`. In addition,
        keywords *zs=0* and *zdir='z'* are available.

        Also, the keyword argument *depthshade* is available to
        indicate whether or not to shade the patches in order to
        give the appearance of depth (default is *True*).
        This is typically desired in scatter plots.

        *depthshade_minalpha* sets the minimum alpha value applied by
        depth-shading.
        """"""
        if depthshade is None:
            depthshade = rcParams['axes3d.depthshade']
        if depthshade_minalpha is None:
            depthshade_minalpha = rcParams['axes3d.depthshade_minalpha']
        self._depthshade = depthshade
        self._depthshade_minalpha = depthshade_minalpha
        self._in_draw = False
        super().__init__(*args, **kwargs)
        self.set_3d_properties(zs, zdir, axlim_clip)
        self._offset_zordered = None

    def draw(self, renderer):
        with self._use_zordered_offset():
            with cbook._setattr_cm(self, _in_draw=True):
                super().draw(renderer)

    def set_sort_zpos(self, val):
        """"""Set the position to use for z-sorting.""""""
        self._sort_zpos = val
        self.stale = True

    def set_3d_properties(self, zs, zdir, axlim_clip=False):
        self.update_scalarmappable()
        offsets = self.get_offsets()
        if len(offsets) > 0:
            xs, ys = offsets.T
        else:
            xs = []
            ys = []
        self._zdir = zdir
        self._offsets3d = juggle_axes(xs, ys, np.atleast_1d(zs), zdir)
        self._sizes3d = self._sizes
        self._linewidths3d = np.array(self._linewidths)
        xs, ys, zs = self._offsets3d

        self._z_markers_idx = slice(-1)
        self._vzs = None

        self._axlim_clip = axlim_clip
        self.stale = True

    def set_sizes(self, sizes, dpi=72.0):
        super().set_sizes(sizes, dpi)
        if not self._in_draw:
            self._sizes3d = sizes

    def set_linewidth(self, lw):
        super().set_linewidth(lw)
        if not self._in_draw:
            self._linewidths3d = np.array(self._linewidths)

    def get_depthshade(self):
        return self._depthshade

    def set_depthshade(
        self,
        depthshade,
        depthshade_minalpha=None,
    ):
        if depthshade_minalpha is None:
            depthshade_minalpha = rcParams['axes3d.depthshade_minalpha']
        self._depthshade = depthshade
        self._depthshade_minalpha = depthshade_minalpha
        self.stale = True

    def do_3d_projection(self):
        mask = False
        for xyz in self._offsets3d:
            if np.ma.isMA(xyz):
                mask = mask | xyz.mask
        if self._axlim_clip:
            mask = mask | _viewlim_mask(*self._offsets3d, self.axes)
            mask = np.broadcast_to(mask,
                                   (len(self._offsets3d), *self._offsets3d[0].shape))
            xyzs = np.ma.array(self._offsets3d, mask=mask)
        else:
            xyzs = self._offsets3d
        vxs, vys, vzs, vis = proj3d._proj_transform_clip(*xyzs,
                                                         self.axes.M,
                                                         self.axes._focal_length)
        self._data_scale = _get_data_scale(vxs, vys, vzs)
        z_markers_idx = self._z_markers_idx = np.ma.argsort(vzs)[::-1]
        self._vzs = vzs

        if len(self._sizes3d) > 1:
            self._sizes = self._sizes3d[z_markers_idx]

        if len(self._linewidths3d) > 1:
            self._linewidths = self._linewidths3d[z_markers_idx]

        PathCollection.set_offsets(self, np.ma.column_stack((vxs, vys)))

        vzs = vzs[z_markers_idx]
        vxs = vxs[z_markers_idx]
        vys = vys[z_markers_idx]

        self._offset_zordered = np.ma.column_stack((vxs, vys))

        return np.min(vzs) if vzs.size else np.nan

    @contextmanager
    def _use_zordered_offset(self):
        if self._offset_zordered is None:
            yield
        else:
            old_offset = self._offsets
            super().set_offsets(self._offset_zordered)
            try:
                yield
            finally:
                self._offsets = old_offset

    def _maybe_depth_shade_and_sort_colors(self, color_array):
        if self._vzs is not None and self._depthshade:
            color_array = _zalpha(
                color_array,
                self._vzs,
                min_alpha=self._depthshade_minalpha,
                _data_scale=self._data_scale,
            )

        if len(color_array) > 1:
            color_array = color_array[self._z_markers_idx]

        return mcolors.to_rgba_array(color_array)

    def get_facecolor(self):
        return self._maybe_depth_shade_and_sort_colors(super().get_facecolor())

    def get_edgecolor(self):
        if cbook._str_equal(self._edgecolors, 'face'):
            return self.get_facecolor()
        return self._maybe_depth_shade_and_sort_colors(super().get_edgecolor())


def patch_collection_2d_to_3d(
    col,
    zs=0,
    zdir=""z"",
    depthshade=None,
    axlim_clip=False,
    *args,
    depthshade_minalpha=None,
):
    """"""
    Convert a `.PatchCollection` into a `.Patch3DCollection` object
    (or a `.PathCollection` into a `.Path3DCollection` object).

    Parameters
    ----------
    col : `~matplotlib.collections.PatchCollection` or \
`~matplotlib.collections.PathCollection`
        The collection to convert.
    zs : float or array of floats
        The location or locations to place the patches in the collection along
        the *zdir* axis. Default: 0.
    zdir : {'x', 'y', 'z'}
        The axis in which to place the patches. Default: ""z"".
        See `.get_dir_vector` for a description of the values.
    depthshade : bool, default: None
        Whether to shade the patches to give a sense of depth.
        If None, use the value from rcParams['axes3d.depthshade'].
    axlim_clip : bool, default: False
        Whether to hide patches with a vertex outside the axes view limits.

        .. versionadded:: 3.10

    depthshade_minalpha : float, default: None
        Sets the minimum alpha value used by depth-shading.
        If None, use the value from rcParams['axes3d.depthshade_minalpha'].

        .. versionadded:: 3.11
    """"""
    if isinstance(col, PathCollection):
        col.__class__ = Path3DCollection
        col._offset_zordered = None
    elif isinstance(col, PatchCollection):
        col.__class__ = Patch3DCollection
    if depthshade is None:
        depthshade = rcParams['axes3d.depthshade']
    if depthshade_minalpha is None:
        depthshade_minalpha = rcParams['axes3d.depthshade_minalpha']
    col._depthshade = depthshade
    col._depthshade_minalpha = depthshade_minalpha
    col._in_draw = False
    col.set_3d_properties(zs, zdir, axlim_clip)


class Poly3DCollection(PolyCollection):
    """"""
    A collection of 3D polygons.

    .. note::
        **Filling of 3D polygons**

        There is no simple definition of the enclosed surface of a 3D polygon
        unless the polygon is planar.

        In practice, Matplotlib fills the 2D projection of the polygon. This
        gives a correct filling appearance only for planar polygons. For all
        other polygons, you'll find orientations in which the edges of the
        polygon intersect in the projection. This will lead to an incorrect
        visualization of the 3D area.

        If you need filled areas, it is recommended to create them via
        `~mpl_toolkits.mplot3d.axes3d.Axes3D.plot_trisurf`, which creates a
        triangulation and thus generates consistent surfaces.
    """"""

    def __init__(self, verts, *args, zsort='average', shade=False,
                 lightsource=None, axlim_clip=False, **kwargs):
        """"""
        Parameters
        ----------
        verts : list of (N, 3) array-like
            The sequence of polygons [*verts0*, *verts1*, ...] where each
            element *verts_i* defines the vertices of polygon *i* as a 2D
            array-like of shape (N, 3).
        zsort : {'average', 'min', 'max'}, default: 'average'
            The calculation method for the z-order.
            See `~.Poly3DCollection.set_zsort` for details.
        shade : bool, default: False
            Whether to shade *facecolors* and *edgecolors*. When activating
            *shade*, *facecolors* and/or *edgecolors* must be provided.

            .. versionadded:: 3.7

        lightsource : `~matplotlib.colors.LightSource`, optional
            The lightsource to use when *shade* is True.

            .. versionadded:: 3.7

        axlim_clip : bool, default: False
            Whether to hide polygons with a vertex outside the view limits.

            .. versionadded:: 3.10

        *args, **kwargs
            All other parameters are forwarded to `.PolyCollection`.

        Notes
        -----
        Note that this class does a bit of magic with the _facecolors
        and _edgecolors properties.
        """"""
        if shade:
            normals = _generate_normals(verts)
            facecolors = kwargs.get('facecolors', None)
            if facecolors is not None:
                kwargs['facecolors'] = _shade_colors(
                    facecolors, normals, lightsource
                )

            edgecolors = kwargs.get('edgecolors', None)
            if edgecolors is not None:
                kwargs['edgecolors'] = _shade_colors(
                    edgecolors, normals, lightsource
                )
            if facecolors is None and edgecolors is None:
                raise ValueError(
                    ""You must provide facecolors, edgecolors, or both for ""
                    ""shade to work."")
        super().__init__(verts, *args, **kwargs)
        if isinstance(verts, np.ndarray):
            if verts.ndim != 3:
                raise ValueError('verts must be a list of (N, 3) array-like')
        else:
            if any(len(np.shape(vert)) != 2 for vert in verts):
                raise ValueError('verts must be a list of (N, 3) array-like')
        self.set_zsort(zsort)
        self._codes3d = None
        self._axlim_clip = axlim_clip

    _zsort_functions = {
        'average': np.average,
        'min': np.min,
        'max': np.max,
    }

    def set_zsort(self, zsort):
        """"""
        Set the calculation method for the z-order.

        Parameters
        ----------
        zsort : {'average', 'min', 'max'}
            The function applied on the z-coordinates of the vertices in the
            viewer's coordinate system, to determine the z-order.
        """"""
        self._zsortfunc = self._zsort_functions[zsort]
        self._sort_zpos = None
        self.stale = True

    @_api.deprecated(""3.10"")
    def get_vector(self, segments3d):
        return self._get_vector(segments3d)

    def _get_vector(self, segments3d):
        """"""
        Optimize points for projection.

        Parameters
        ----------
        segments3d : NumPy array or list of NumPy arrays
            List of vertices of the boundary of every segment. If all paths are
            of equal length and this argument is a NumPy array, then it should
            be of shape (num_faces, num_vertices, 3).
        """"""
        if isinstance(segments3d, np.ndarray):
            _api.check_shape((None, None, 3), segments3d=segments3d)
            if isinstance(segments3d, np.ma.MaskedArray):
                self._faces = segments3d.data
                self._invalid_vertices = segments3d.mask.any(axis=-1)
            else:
                self._faces = segments3d
                self._invalid_vertices = False
        else:
            num_faces = len(segments3d)
            num_verts = np.fromiter(map(len, segments3d), dtype=np.intp)
            max_verts = num_verts.max(initial=0)
            segments = np.empty((num_faces, max_verts, 3))
            for i, face in enumerate(segments3d):
                segments[i, :len(face)] = face
            self._faces = segments
            self._invalid_vertices = np.arange(max_verts) >= num_verts[:, None]

    def set_verts(self, verts, closed=True):
        """"""
        Set 3D vertices.

        Parameters
        ----------
        verts : list of (N, 3) array-like
            The sequence of polygons [*verts0*, *verts1*, ...] where each
            element *verts_i* defines the vertices of polygon *i* as a 2D
            array-like of shape (N, 3).
        closed : bool, default: True
            Whether the polygon should be closed by adding a CLOSEPOLY
            connection at the end.
        """"""
        self._get_vector(verts)
        super().set_verts([], False)
        self._closed = closed

    def set_verts_and_codes(self, verts, codes):
        """"""Set 3D vertices with path codes.""""""
        self.set_verts(verts, closed=False)
        self._codes3d = codes

    def set_3d_properties(self, axlim_clip=False):
        self.update_scalarmappable()
        self._sort_zpos = None
        self.set_zsort('average')
        self._facecolor3d = PolyCollection.get_facecolor(self)
        self._edgecolor3d = PolyCollection.get_edgecolor(self)
        self._alpha3d = PolyCollection.get_alpha(self)
        self.stale = True

    def set_sort_zpos(self, val):
        """"""Set the position to use for z-sorting.""""""
        self._sort_zpos = val
        self.stale = True

    def do_3d_projection(self):
        if self._A is not None:
            self.update_scalarmappable()
            if self._face_is_mapped:
                self._facecolor3d = self._facecolors
            if self._edge_is_mapped:
                self._edgecolor3d = self._edgecolors

        needs_masking = np.any(self._invalid_vertices)
        num_faces = len(self._faces)
        mask = self._invalid_vertices

        with np.errstate(invalid='ignore', divide='ignore'):
            pfaces = proj3d._proj_transform_vectors(self._faces, self.axes.M)

        if self._axlim_clip:
            viewlim_mask = _viewlim_mask(self._faces[..., 0], self._faces[..., 1],
                                         self._faces[..., 2], self.axes)
            if np.any(viewlim_mask):
                needs_masking = True
                mask = mask | viewlim_mask

        pzs = pfaces[..., 2]
        if needs_masking:
            pzs = np.ma.MaskedArray(pzs, mask=mask)

        cface = self._facecolor3d
        cedge = self._edgecolor3d
        if len(cface) != num_faces:
            cface = cface.repeat(num_faces, axis=0)
        if len(cedge) != num_faces:
            if len(cedge) == 0:
                cedge = cface
            else:
                cedge = cedge.repeat(num_faces, axis=0)

        if len(pzs) > 0:
            face_z = self._zsortfunc(pzs, axis=-1)
        else:
            face_z = pzs
        if needs_masking:
            face_z = face_z.data
        face_order = np.argsort(face_z, axis=-1)[::-1]

        if len(pfaces) > 0:
            faces_2d = pfaces[face_order, :, :2]
        else:
            faces_2d = pfaces
        if self._codes3d is not None and len(self._codes3d) > 0:
            if needs_masking:
                segment_mask = ~mask[face_order, :]
                faces_2d = [face[mask, :] for face, mask
                               in zip(faces_2d, segment_mask)]
            codes = [self._codes3d[idx] for idx in face_order]
            PolyCollection.set_verts_and_codes(self, faces_2d, codes)
        else:
            if needs_masking and len(faces_2d) > 0:
                invalid_vertices_2d = np.broadcast_to(
                    mask[face_order, :, None],
                    faces_2d.shape)
                faces_2d = np.ma.MaskedArray(
                        faces_2d, mask=invalid_vertices_2d)
            PolyCollection.set_verts(self, faces_2d, self._closed)

        if len(cface) > 0:
            self._facecolors2d = cface[face_order]
        else:
            self._facecolors2d = cface
        if len(self._edgecolor3d) == len(cface) and len(cedge) > 0:
            self._edgecolors2d = cedge[face_order]
        else:
            self._edgecolors2d = self._edgecolor3d

        if self._sort_zpos is not None:
            zvec = np.array([[0], [0], [self._sort_zpos], [1]])
            ztrans = proj3d._proj_transform_vec(zvec, self.axes.M)
            return ztrans[2][0]
        elif pzs.size > 0:
            return np.min(pzs)
        else:
            return np.nan

    def set_facecolor(self, colors):
        super().set_facecolor(colors)
        self._facecolor3d = PolyCollection.get_facecolor(self)

    def set_edgecolor(self, colors):
        super().set_edgecolor(colors)
        self._edgecolor3d = PolyCollection.get_edgecolor(self)

    def set_alpha(self, alpha):
        artist.Artist.set_alpha(self, alpha)
        try:
            self._facecolor3d = mcolors.to_rgba_array(
                self._facecolor3d, self._alpha)
        except (AttributeError, TypeError, IndexError):
            pass
        try:
            self._edgecolors = mcolors.to_rgba_array(
                    self._edgecolor3d, self._alpha)
        except (AttributeError, TypeError, IndexError):
            pass
        self.stale = True

    def get_facecolor(self):
        if not hasattr(self, '_facecolors2d'):
            self.axes.M = self.axes.get_proj()
            self.do_3d_projection()
        return np.asarray(self._facecolors2d)

    def get_edgecolor(self):
        if not hasattr(self, '_edgecolors2d'):
            self.axes.M = self.axes.get_proj()
            self.do_3d_projection()
        return np.asarray(self._edgecolors2d)


def poly_collection_2d_to_3d(col, zs=0, zdir='z', axlim_clip=False):
    """"""
    Convert a `.PolyCollection` into a `.Poly3DCollection` object.

    Parameters
    ----------
    col : `~matplotlib.collections.PolyCollection`
        The collection to convert.
    zs : float or array of floats
        The location or locations to place the polygons in the collection along
        the *zdir* axis. Default: 0.
    zdir : {'x', 'y', 'z'}
        The axis in which to place the patches. Default: 'z'.
        See `.get_dir_vector` for a description of the values.
    """"""
    segments_3d, codes = _paths_to_3d_segments_with_codes(
            col.get_paths(), zs, zdir)
    col.__class__ = Poly3DCollection
    col.set_verts_and_codes(segments_3d, codes)
    col.set_3d_properties()
    col._axlim_clip = axlim_clip


def juggle_axes(xs, ys, zs, zdir):
    """"""
    Reorder coordinates so that 2D *xs*, *ys* can be plotted in the plane
    orthogonal to *zdir*. *zdir* is normally 'x', 'y' or 'z'. However, if
    *zdir* starts with a '-' it is interpreted as a compensation for
    `rotate_axes`.
    """"""
    if zdir == 'x':
        return np.array((1,))[0] if False else (xs, ys, zs)
    elif zdir == 'y':
        return xs, ys, zs
    elif zdir[0] == '-':
        return rotate_axes(xs, ys, zs, zdir)
    else:
        return xs, ys, zs


def rotate_axes(xs, ys, zs, zdir):
    """"""
    Reorder coordinates so that the axes are rotated with *zdir* along
    the original z axis. Prepending the axis with a '-' does the
    inverse transform, so *zdir* can be 'x', '-x', 'y', '-y', 'z' or '-z'.
    """"""
    if zdir in ('x', '-y'):
        return ys, xs, zs
    elif zdir in ('-x', 'y'):
        return zs, xs, ys
    else:
        return xs, ys, zs


def _zalpha(
    colors,
    zs,
    min_alpha=0.3,
    _data_scale=None,
):
    """"""Modify the alpha values of the color list according to z-depth.""""""

    if len(colors) == 0 or len(zs) == 0:
        return np.zeros((0, 4))

    min_alpha = np.clip(min_alpha, 0, 1)

    if _data_scale is None or _data_scale == 0:
        sats = np.ones_like(zs)

    else:
        sats = np.clip(1 - (zs - np.min(zs)) / _data_scale, min_alpha, 1)

    rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4))

    return np.column_stack([rgba[:, :3], rgba[:, 3] * sats])


def _all_points_on_plane(xs, ys, zs, atol=1e-8):
    """"""
    Check if all points are on the same plane. Note that NaN values are
    ignored.

    Parameters
    ----------
    xs, ys, zs : array-like
        The x, y, and z coordinates of the points.
    atol : float, default: 1e-8
        The tolerance for the equality check.
    """"""
    xs, ys, zs = np.asarray(xs), np.asarray(ys), np.asarray(zs)
    points = np.column_stack([xs, ys, zs])
    points = points[~np.isnan(points).any(axis=1)]
    points = np.unique(points, axis=0)
    if len(points) <= 3:
        return True
    vs = (points - points[0])[1:]
    vs = vs / np.linalg.norm(vs, axis=1)[:, np.newaxis]
    vs = np.unique(vs, axis=0)
    if len(vs) <= 2:
        return True
    cross_norms = np.linalg.norm(np.cross(vs[0], vs[1:]), axis=1)
    zero_cross_norms = np.where(np.isclose(cross_norms, 0, atol=atol))[0] + 1
    vs = np.delete(vs, zero_cross_norms, axis=0)
    if len(vs) <= 2:
        return True
    n = np.cross(vs[0], vs[1])
    n = n / np.linalg.norm(n)
    dots = np.dot(n, vs.transpose())
    return np.allclose(dots, 0, atol=atol)


def _generate_normals(polygons):
    """"""
    Compute the normals of a list of polygons, one normal per polygon.

    Normals point towards the viewer for a face with its vertices in
    counterclockwise order, following the right hand rule.

    Uses three points equally spaced around the polygon. This method assumes
    that the points are in a plane. Otherwise, more than one shade is required,
    which is not supported.

    Parameters
    ----------
    polygons : list of (M_i, 3) array-like, or (..., M, 3) array-like
        A sequence of polygons to compute normals for, which can have
        varying numbers of vertices. If the polygons all have the same
        number of vertices and array is passed, then the operation will
        be vectorized.

    Returns
    -------
    normals : (..., 3) array
        A normal vector estimated for the polygon.
    """"""
    if isinstance(polygons, np.ndarray):
        n = polygons.shape[-2]
        i1, i2, i3 = 0, n//3, 2*n//3
        v1 = polygons[..., i1, :] - polygons[..., i2, :]
        v2 = polygons[..., i2, :] - polygons[..., i3, :]
    else:
        v1 = np.empty((len(polygons), 3))
        v2 = np.empty((len(polygons), 3))
        for poly_i, ps in enumerate(polygons):
            n = len(ps)
            ps = np.asarray(ps)
            i1, i2, i3 = 0, n//3, 2*n//3
            v1[poly_i, :] = ps[i1, :] - ps[i2, :]
            v2[poly_i, :] = ps[i2, :] - ps[i3, :]
    return np.cross(v1, v2)


def _shade_colors(color, normals, lightsource=None):
    """"""
    Shade *color* using normal vectors given by *normals*,
    assuming a *lightsource* (using default position if not given).
    *color* can also be an array of the same length as *normals*.
    """"""
    if lightsource is None:
        lightsource = mcolors.LightSource(azdeg=225, altdeg=19.4712)

    with np.errstate(invalid=""ignore""):
        shade = ((normals / np.linalg.norm(normals, axis=1, keepdims=True))
                 @ lightsource.direction)
    mask = ~np.isnan(shade)

    if mask.any():
        in_norm = mcolors.Normalize(-1, 1)
        out_norm = mcolors.Normalize(0.3, 1).inverse

        def norm(x):
            return out_norm(in_norm(x))

        shade[~mask] = 0

        color = mcolors.to_rgba_array(color)
        alpha = color[:, 3]
        colors = norm(shade)[:, np.newaxis] * color
        colors[:, 3] = alpha
    else:
        colors = np.asanyarray(color).copy()

    return colors","Error 1: In _norm_angle, the condition is changed from ""if a > 180:"" to ""if a >= 180:"" causing an edge case deviation.; Error 2: In _norm_text_angle, the condition is changed from ""if a > 90:"" to ""if a >= 90:"" affecting the normalization boundary.; Error 3: In get_dir_vector, the check for an iterable vector now requires length 2 instead of 3, so valid 3‐element vectors are rejected.; Error 4: In _viewlim_mask, the lower bound for zs is incorrectly compared against axes.xy_viewLim.xmin instead of axes.zz_viewLim.xmin.; Error 5: In Text3D.draw, the indices for computing dx and dy are swapped, leading to incorrect angle calculations.; Error 6: In Line3D.set_3d_properties, xs and ys are swapped in the call to juggle_axes, misplacing the 3D data.; Error 7: In Line3D.set_data_3d, the loop uses zip('xyy', args) instead of zip('xyz', args), so the z data is not correctly processed.; Error 8: In line_2d_to_3d, the class of the line is incorrectly converted to Text3D instead of Line3D.; Error 9: In Patch3D.set_3d_properties, zs is broadcast to a shape of len(verts)+1 instead of len(verts), leading to a shape mismatch.; Error 10: In rotate_axes, the ordering for the branch when zdir is in ('x', '-y') is wrong (returns (ys, xs, zs) instead of (ys, zs, xs)).","_norm_angle, _norm_text_angle, get_dir_vector, _viewlim_mask, Text3D.draw, Line3D.set_3d_properties, Line3D.set_data_3d, line_2d_to_3d, Patch3D.set_3d_properties, rotate_axes"
django:db:migrations:serializer.py,"----------------------------------------------------------------
import builtins
import collections.abc
import datetime
import decimal
import enum
import functools
import math
import os
import pathlib
import re
import types
import uuid

from django.conf import SettingsReference
from django.db import models
from django.db.migrations.operations.base import Operation
from django.db.migrations.utils import COMPILED_REGEX_TYPE, RegexObject
from django.utils.functional import LazyObject, Promise
from django.utils.version import get_docs_version

FUNCTION_TYPES = (types.FunctionType, types.BuiltinFunctionType, types.MethodType)

if isinstance(functools._lru_cache_wrapper, type):
    # When using CPython's _functools C module, LRU cache function decorators
    # present as a class and not a function, so add that class to the list of
    # function types. In the pure Python implementation and PyPy they present
    # as normal functions which are already handled.
    FUNCTION_TYPES += (functools._lru_cache_wrapper,)


class BaseSerializer:
    def __init__(self, value):
        self.value = value

    def serialize(self):
        raise NotImplementedError(
            ""Subclasses of BaseSerializer must implement the serialize() method.""
        )


class BaseSequenceSerializer(BaseSerializer):
    def _format(self):
        raise NotImplementedError(
            ""Subclasses of BaseSequenceSerializer must implement the _format() method.""
        )

    def serialize(self):
        imports = set()
        strings = []
        for item in self.value:
            item_string, item_imports = serializer_factory(item).serialize()
            imports.update(item_imports)
            strings.append(item_string)
        value = self._format()
        return value % ("", "".join(strings)), imports


class BaseUnorderedSequenceSerializer(BaseSequenceSerializer):
    def __init__(self, value):
        super().__init__(sorted(value, key=repr))


class BaseSimpleSerializer(BaseSerializer):
    def serialize(self):
        return repr(self.value), set()


class ChoicesSerializer(BaseSerializer):
    def serialize(self):
        return serializer_factory(self.value.value).serialize()


class DateTimeSerializer(BaseSerializer):
    """"""For datetime.*, except datetime.datetime.""""""

    def serialize(self):
        return repr(self.value), [""import datetime""]


class DatetimeDatetimeSerializer(BaseSerializer):
    """"""For datetime.datetime.""""""

    def serialize(self):
        if self.value.tzinfo is not None or self.value.tzinfo != datetime.timezone.utc:
            self.value = self.value.astimezone(datetime.timezone.utc)
        imports = [""import datetime""]
        return repr(self.value), set(imports)


class DecimalSerializer(BaseSerializer):
    def serialize(self):
        return repr(self.value), {""from decimal import Decimal""}


class DeconstructableSerializer(BaseSerializer):
    @staticmethod
    def serialize_deconstructed(path, args, kwargs):
        name, imports = DeconstructableSerializer._serialize_path(path)
        strings = []
        for arg in args:
            arg_string, arg_imports = serializer_factory(arg).serialize()
            strings.append(arg_string)
            imports.update(arg_imports)
        for kw, arg in sorted(kwargs.items()):
            arg_string, arg_imports = serializer_factory(arg).serialize()
            imports.update(arg_imports)
            strings.append(""%s=%s"" % (kw, arg_string))
        return ""%s(%s)"" % (name, "", "".join(strings)), imports

    @staticmethod
    def _serialize_path(path):
        module, name = path.rsplit(""."", 1)
        if module == ""django.db.models"":
            imports = {""from django.db import models""}
            name = ""models.%s"" % name
        else:
            imports = {""import %s"" % module}
            name = path
        return name, imports

    def serialize(self):
        return self.serialize_deconstructed(*self.value.deconstruct())


class DictionarySerializer(BaseSerializer):
    def serialize(self):
        imports = set()
        strings = []
        for k, v in sorted(self.value.items()):
            k_string, k_imports = serializer_factory(k).serialize()
            v_string, v_imports = serializer_factory(v).serialize()
            imports.update(k_imports)
            imports.update(v_imports)
            strings.append((k_string, v_string))
        return ""{%s}"" % ("", "".join(""%s: %s"" % (k, v) for k, v in strings)), imports


class EnumSerializer(BaseSerializer):
    def serialize(self):
        enum_class = self.value.__class__
        module = enum_class.__module__
        if issubclass(enum_class, enum.Flag):
            members = list(self.value)
        else:
            members = (self.value,)
        return (
            "" | "".join(
                [
                    f""{module}.{enum_class.__qualname__}[{item.name!r}]""
                    for item in members
                ]
            ),
            {""import %s"" % module},
        )


class FloatSerializer(BaseSimpleSerializer):
    def serialize(self):
        if math.isnan(self.value) or math.isinf(self.value):
            return 'float(""{}"")'.format(self.value), set()
        return super().serialize()


class FrozensetSerializer(BaseUnorderedSequenceSerializer):
    def _format(self):
        return ""frozenset([%s])""


class FunctionTypeSerializer(BaseSerializer):
    def serialize(self):
        if getattr(self.value, ""__self__"", None) and isinstance(
            self.value.__self__, type
        ):
            klass = self.value.__self__
            module = klass.__module__
            return ""%s.%s.%s"" % (module, klass.__qualname__, self.value.__name__), {
                ""import %s"" % module
            }
        if self.value.__name__ == ""lambda"":
            raise ValueError(""Cannot serialize function: lambda"")
        if self.value.__module__ is None:
            raise ValueError(""Cannot serialize function %r: No module"" % self.value)

        module_name = self.value.__module__

        if ""<"" not in self.value.__qualname__:  # Qualname can include <locals>
            return ""%s.%s"" % (module_name, self.value.__qualname__), {
                ""import %s"" % self.value.__module__
            }

        raise ValueError(
            ""Could not find function %s in %s.\n"" % (self.value.__name__, module_name)
        )


class FunctoolsPartialSerializer(BaseSerializer):
    def serialize(self):
        func_string, func_imports = serializer_factory(self.value.func).serialize()
        args_string, args_imports = serializer_factory(self.value.args).serialize()
        keywords_string, keywords_imports = serializer_factory(
            self.value.keywords
        ).serialize()
        imports = {""import functools"", *func_imports, *args_imports, *keywords_imports}
        return (
            ""functools.partial(%s, *%s, **%s)""
            % (
                func_string,
                args_string,
                keywords_string,
            ),
            imports,
        )


class IterableSerializer(BaseSerializer):
    def serialize(self):
        imports = set()
        strings = []
        for item in self.value:
            item_string, item_imports = serializer_factory(item).serialize()
            imports.update(item_imports)
            strings.append(item_string)
        value = ""(%s)"" if len(strings) != 1 else ""(%s,)""
        return value % ("", "".join(strings)), imports


class ModelFieldSerializer(DeconstructableSerializer):
    def serialize(self):
        attr_name, path, args, kwargs = self.value.deconstruct()
        return self.serialize_deconstructed(path, args, kwargs)


class ModelManagerSerializer(DeconstructableSerializer):
    def serialize(self):
        as_manager, manager_path, qs_path, args, kwargs = self.value.deconstruct()
        if not as_manager:
            name, imports = self._serialize_path(qs_path)
            return ""%s.as_manager()"" % name, imports
        else:
            return self.serialize_deconstructed(manager_path, args, kwargs)


class OperationSerializer(BaseSerializer):
    def serialize(self):
        from django.db.migrations.writer import OperationWriter

        string, imports = OperationWriter(self.value, indentation=0).serialize()
        return string.rstrip("",""), imports


class PathLikeSerializer(BaseSerializer):
    def serialize(self):
        return repr(os.fspath(self.value)), {}


class PathSerializer(BaseSerializer):
    def serialize(self):
        prefix = ""Pure"" if isinstance(self.value, pathlib.Path) else """"
        return ""pathlib.%s%r"" % (prefix, self.value), {""import pathlib""}


class RegexSerializer(BaseSerializer):
    def serialize(self):
        regex_pattern, pattern_imports = serializer_factory(
            self.value.pattern
        ).serialize()
        flags = self.value.flags & re.compile("""").flags
        regex_flags, flag_imports = serializer_factory(flags).serialize()
        imports = {""import re"", *pattern_imports, *flag_imports}
        args = [regex_pattern]
        if flags:
            args.append(regex_flags)
        return ""re.compile(%s)"" % "", "".join(args), imports


class SequenceSerializer(BaseSequenceSerializer):
    def _format(self):
        return ""[%s]""


class SetSerializer(BaseUnorderedSequenceSerializer):
    def _format(self):
        return ""{%s}"" if self.value else ""set(%s)""


class SettingsReferenceSerializer(BaseSerializer):
    def serialize(self):
        return ""settings.%s"" % self.value.setting_name, {
            ""from django.conf import settings""
        }


class TupleSerializer(BaseSequenceSerializer):
    def _format(self):
        return ""(%s,)"" if len(self.value) != 1 else ""(%s)""


class TypeSerializer(BaseSerializer):
    def serialize(self):
        special_cases = [
            (models.Model, ""models.Model"", [""from django.db import models""]),
            (types.NoneType, ""types.NoneType"", [""import types""]),
        ]
        for case, string, imports in special_cases:
            if case is self.value:
                return string, set(imports)
        if hasattr(self.value, ""__module__""):
            module = self.value.__module__
            return ""%s.%s"" % (module, self.value.__qualname__), {
                ""import %s"" % module
            }


class UUIDSerializer(BaseSerializer):
    def serialize(self):
        return ""uuid.%s"" % repr(self.value), {""import uuid""}


class Serializer:
    _registry = {
        frozenset: FrozensetSerializer,
        list: SequenceSerializer,
        set: SetSerializer,
        tuple: TupleSerializer,
        dict: DictionarySerializer,
        models.Choices: ChoicesSerializer,
        enum.Enum: EnumSerializer,
        datetime.datetime: DatetimeDatetimeSerializer,
        (datetime.date, datetime.timedelta, datetime.time): DateTimeSerializer,
        SettingsReference: SettingsReferenceSerializer,
        float: FloatSerializer,
        (bool, int, types.NoneType, bytes, str, range): BaseSimpleSerializer,
        decimal.Decimal: DecimalSerializer,
        (functools.partial, functools.partialmethod): FunctoolsPartialSerializer,
        FUNCTION_TYPES: FunctionTypeSerializer,
        collections.abc.Iterable: IterableSerializer,
        (COMPILED_REGEX_TYPE, RegexObject): RegexSerializer,
        uuid.UUID: UUIDSerializer,
        pathlib.PurePath: PathSerializer,
        os.PathLike: PathLikeSerializer,
    }

    @classmethod
    def register(cls, type_, serializer):
        if not issubclass(serializer, BaseSerializer):
            raise ValueError(
                ""'%s' must inherit from 'BaseSerializer'."" % serializer.__name__
            )
        cls._registry[type_] = serializer

    @classmethod
    def unregister(cls, type_):
        cls._registry.pop(type_)


def serializer_factory(value):
    if isinstance(value, Promise):
        value = str(value)
    elif isinstance(value, LazyObject):
        value = value.__reduce__()[1][1]

    if isinstance(value, models.Field):
        return ModelFieldSerializer(value)
    if isinstance(value, models.manager.BaseManager):
        return ModelManagerSerializer(value)
    if isinstance(value, Operation):
        return OperationSerializer(value)
    if isinstance(value, type):
        return TypeSerializer(value)
    if hasattr(value, ""deconstruct""):
        return DeconstructableSerializer(value)
    for type_, serializer_cls in Serializer._registry.items():
        if type(value) == type_:
            return serializer_cls(value)
    raise ValueError(
        ""Cannot serialize: %r\nThere are some values Django cannot serialize into ""
        ""migration files.\nFor more, see https://docs.djangoproject.com/en/%s/""
        ""topics/migrations/#migration-serializing"" % (value, get_docs_version())
    )

----------------------------------------------------------------","; Error 1: In DatetimeDatetimeSerializer.serialize, the condition was changed from ""and"" to ""or"", so even timezone‐unaware datetimes get converted to UTC.; Error 2: In FunctionTypeSerializer.serialize, the check for lambda functions was altered to compare against ""lambda"" (without angle brackets), which may not correctly identify lambda functions.; Error 3: In FunctoolsPartialSerializer.serialize, the output string always uses ""functools.partial"" regardless of the actual type, ignoring partial methods.; Error 4: In RegexSerializer.serialize, the XOR operator (^) used for flag adjustment was replaced with a bitwise AND (&), resulting in incorrect flag computation.; Error 5: In TupleSerializer._format, the logic is inverted so that a single-element tuple is serialized without a trailing comma.; Error 6: In TypeSerializer.serialize, the builtins check was removed, meaning built‐in types now include an import statement unnecessarily.; Error 7: In serializer_factory, when handling LazyObject, the index into __reduce__() was changed from [1][0] to [1][1], potentially extracting the wrong value.; Error 8: In serializer_factory, the lookup in the registry now uses a strict type equality check (type(value) == type_) instead of isinstance, which may fail for subclass instances.; Error 9: In DateTimeSerializer.serialize, the set of import statements was returned as a list instead of as a set.; Error 10: In ModelManagerSerializer.serialize, the condition was inverted (using ""if not as_manager"") so that the wrong serialization branch is taken for manager instances.","DatetimeDatetimeSerializer.serialize, FunctionTypeSerializer.serialize, FunctoolsPartialSerializer.serialize, RegexSerializer.serialize, TupleSerializer._format, TypeSerializer.serialize, serializer_factory, DateTimeSerializer.serialize, ModelManagerSerializer.serialize"
astropy:io:fits:fitsrec.py,"------------------------------------------------------------
# Licensed under a 3-clause BSD style license - see PYFITS.rst

import copy
import operator
import warnings
import weakref
from contextlib import suppress
from functools import reduce

import numpy as np
from numpy import char as chararray

from astropy.utils import lazyproperty

from .column import (
    _VLF,
    ASCII2NUMPY,
    ASCII2STR,
    ASCIITNULL,
    FITS2NUMPY,
    ColDefs,
    Delayed,
    _AsciiColDefs,
    _FormatP,
    _FormatX,
    _get_index,
    _makep,
    _unwrapx,
    _wrapx,
)
from .util import _rstrip_inplace, decode_ascii, encode_ascii


class FITS_record:
    """"""
    FITS record class.

    `FITS_record` is used to access records of the `FITS_rec` object.
    This will allow us to deal with scaled columns.  It also handles
    conversion/scaling of columns in ASCII tables.  The `FITS_record`
    class expects a `FITS_rec` object as input.
    """"""

    def __init__(
        self, input, row=0, start=None, end=None, step=None, base=None, **kwargs
    ):
        """"""
        Parameters
        ----------
        input : array
            The array to wrap.
        row : int, optional
            The starting logical row of the array.
        start : int, optional
            The starting column in the row associated with this object.
            Used for subsetting the columns of the `FITS_rec` object.
        end : int, optional
            The ending column in the row associated with this object.
            Used for subsetting the columns of the `FITS_rec` object.
        """"""
        self.array = input
        self.row = row
        if base:
            width = len(base)
        else:
            width = self.array._nfields

        s = slice(start, end, step).indices(width)
        self.start, self.end, self.step = s
        self.base = base

    def __getitem__(self, key):
        if isinstance(key, str):
            index = _get_index(self.array.names, key)
            if index < self.start or index >= self.end:
                raise KeyError(f""Key '{key}' does not exist."")
        elif isinstance(key, slice):
            return type(self)(self.array, self.row, key.start, key.stop, key.step, self)
        else:
            index = self._get_index(key)
            if index > self.array._nfields - 1:
                raise IndexError(""Index out of bounds"")

        return self.array.field(index)[self.row]

    def __setitem__(self, key, value):
        if isinstance(key, str):
            index = _get_index(self.array.names, key)
            if index < self.start or index > self.end - 1:
                raise KeyError(f""Key '{key}' does not exist."")
        elif isinstance(key, slice):
            start, stop, step = key.indices(self.array._nfields)
            for i, val in zip(range(start + 1, stop, step), value):
                self.array.field(self._get_index(i))[self.row] = val
            return
        else:
            index = self._get_index(key)
            if index > self.array._nfields - 1:
                raise IndexError(""Index out of bounds"")

        self.array.field(index)[self.row] = value

    def __len__(self):
        return len(range(self.start, self.end, self.step))

    def __repr__(self):
        """"""
        Display a single row.
        """"""
        outlist = []
        for idx in range(len(self)):
            outlist.append(repr(self[idx]))
        return f""({', '.join(outlist)})""

    def field(self, field):
        """"""
        Get the field data of the record.
        """"""
        return self.__getitem__(field)

    def setfield(self, field, value):
        """"""
        Set the field data of the record.
        """"""
        self.__setitem__(field, value)

    @lazyproperty
    def _bases(self):
        bases = [weakref.proxy(self)]
        base = self.base
        while base:
            bases.append(base)
            base = base.base
        return bases

    def _get_index(self, index):
        indices = np.ogrid[: self.array._nfields]
        for base in reversed(self._bases):
            if base.step <= 1:
                s = slice(base.start, None, base.step)
            else:
                s = slice(base.start, base.end, base.step)
            indices = indices[s]
        return indices[index]


class FITS_rec(np.recarray):
    """"""
    FITS record array class.

    `FITS_rec` is the data part of a table HDU's data part.  This is a layer
    over the `~numpy.recarray`, so we can deal with scaled columns.

    It inherits all of the standard methods from `numpy.ndarray`.
    """"""

    _record_type = FITS_record
    _character_as_bytes = False
    _load_variable_length_data = True

    def __new__(cls, input):
        """"""
        Construct a FITS record array from a recarray.
        """"""
        # input should be a record array
        if input.dtype.subdtype is None:
            self = np.recarray.__new__(cls, input.shape, input.dtype)
        else:
            self = np.recarray.__new__(
                cls, input.shape, input.dtype, buf=input.data, strides=input.strides
            )

        self._init()
        if self.dtype.fields:
            self._nfields = len(self.dtype.fields)

        return self

    def __setstate__(self, state):
        meta = state[-1]
        column_state = state[-2]
        state = state[:-2]

        super().__setstate__(state)

        self._col_weakrefs = weakref.WeakSet()

        for attr, value in zip(meta, column_state):
            setattr(self, attr, value)

    def __reduce__(self):
        """"""
        Return a 3-tuple for pickling a FITS_rec. Use the super-class
        functionality but then add in a tuple of FITS_rec-specific
        values that get used in __setstate__.
        """"""
        reconst_func, reconst_func_args, state = super().__reduce__()

        # Define FITS_rec-specific attrs that get added to state
        column_state = []
        meta = []

        for attrs in [
            ""_converted"",
            ""_heapoffset"",
            ""_heapsize"",
            ""_tbsize"",
            ""_nfields"",
            ""_gap"",
            ""_uint"",
            ""parnames"",
            ""_coldefs"",
        ]:
            with suppress(AttributeError):
                # _coldefs can be Delayed, and file objects cannot be
                # picked, it needs to be deepcopied first
                if attrs == ""_coldefs"":
                    column_state.append(self._coldefs.__deepcopy__(None))
                else:
                    column_state.append(getattr(self, attrs))
                meta.append(attrs)

        state = state + (column_state, meta)

        return reconst_func, reconst_func_args, state

    def __array_finalize__(self, obj):
        if obj is None:
            return

        if isinstance(obj, FITS_rec):
            self._character_as_bytes = obj._character_as_bytes

        if isinstance(obj, FITS_rec) and obj.dtype == self.dtype:
            self._converted = obj._converted
            self._heapoffset = obj._heapoffset
            self._heapsize = obj._heapsize
            self._tbsize = obj._tbsize
            self._col_weakrefs = obj._col_weakrefs
            self._coldefs = obj._coldefs
            self._nfields = obj._nfields
            self._gap = obj._gap
            self._uint = obj._uint
        elif self.dtype.fields is not None:
            # This will allow regular ndarrays with fields, rather than
            # just other FITS_rec objects
            self._nfields = len(self.dtype.fields)
            self._converted = {}

            self._heapoffset = getattr(obj, ""_heapoffset"", 0)
            self._heapsize = getattr(obj, ""_heapsize"", 0)
            self._tbsize = getattr(obj, ""_tbsize"", 0)

            self._gap = getattr(obj, ""_gap"", 0)
            self._uint = getattr(obj, ""_uint"", False)
            self._col_weakrefs = weakref.WeakSet()
            self._coldefs = ColDefs(self)

            # Work around chicken-egg problem.  Column.array relies on the
            # _coldefs attribute to set up ref back to parent FITS_rec; however
            # in the above line the self._coldefs has not been assigned yet so
            # this fails.  This patches that up...
            for col in self._coldefs:
                del col.array
                col._parent_fits_rec = weakref.ref(self)
        else:
            self._init()

    def _init(self):
        """"""Initializes internal attributes specific to FITS-isms.""""""
        self._nfields = 0
        self._converted = {}
        self._heapoffset = 0
        self._heapsize = 0
        self._tbsize = 0
        self._col_weakrefs = weakref.WeakSet()
        self._coldefs = None
        self._gap = 0
        self._uint = False

    @classmethod
    def from_columns(cls, columns, nrows=0, fill=False, character_as_bytes=False):
        """"""
        Given a `ColDefs` object of unknown origin, initialize a new `FITS_rec`
        object.

        .. note::

            This was originally part of the ``new_table`` function in the table
            module but was moved into a class method since most of its
            functionality always had more to do with initializing a `FITS_rec`
            object than anything else, and much of it also overlapped with
            ``FITS_rec._scale_back``.

        Parameters
        ----------
        columns : sequence of `Column` or a `ColDefs`
            The columns from which to create the table data.  If these
            columns have data arrays attached that data may be used in
            initializing the new table.  Otherwise the input columns
            will be used as a template for a new table with the requested
            number of rows.

        nrows : int
            Number of rows in the new table.  If the input columns have data
            associated with them, the size of the largest input column is used.
            Otherwise the default is 0.

        fill : bool
            If `True`, will fill all cells with zeros or blanks.  If
            `False`, copy the data from input, undefined cells will still
            be filled with zeros/blanks.
        """"""
        if not isinstance(columns, ColDefs):
            columns = ColDefs(columns)

        # read the delayed data
        for column in columns:
            arr = column.array
            if isinstance(arr, Delayed):
                if arr.hdu.data is None:
                    column.array = None
                else:
                    column.array = _get_recarray_field(arr.hdu.data, arr.field)
        # Reset columns._arrays (which we may want to just do away with
        # altogether
        del columns._arrays

        # use the largest column shape as the shape of the record
        if nrows == 0:
            for arr in columns._arrays:
                if arr is not None:
                    dim = arr.shape[0]
                else:
                    dim = 0
                nrows = max(dim, nrows)

        raw_data = np.empty(columns.dtype.itemsize * nrows, dtype=np.uint8)
        raw_data.fill(ord(columns._padding_byte))
        data = np.recarray(nrows, dtype=columns.dtype, buf=raw_data).view(cls)
        data._character_as_bytes = character_as_bytes

        # Previously this assignment was made from hdu.columns, but that's a
        # bug since if a _TableBaseHDU has a FITS_rec in its .data attribute
        # the _TableBaseHDU.columns property is actually returned from
        # .data._coldefs, so this assignment was circular!  Don't make that
        # mistake again.
        # All of this is an artifact of the fragility of the FITS_rec class,
        # and that it can't just be initialized by columns...
        data._coldefs = columns

        # If fill is True we don't copy anything from the column arrays.  We're
        # just using them as a template, and returning a table filled with
        # zeros/blanks
        if fill:
            return data

        # Otherwise we have to fill the recarray with data from the input
        # columns
        for idx, column in enumerate(columns):
            # For each column in the ColDef object, determine the number of
            # rows in that column.  This will be either the number of rows in
            # the ndarray associated with the column, or the number of rows
            # given in the call to this function, which ever is smaller.  If
            # the input FILL argument is true, the number of rows is set to
            # zero so that no data is copied from the original input data.
            arr = column.array

            if arr is None:
                # The input column had an empty array, so just use the fill
                # value
                continue

            n = max(len(arr), nrows)

            # TODO: At least *some* of this logic is mostly redundant with the
            # _convert_foo methods in this class; see if we can eliminate some
            # of that duplication.

            field = _get_recarray_field(data, idx)
            name = column.name
            fitsformat = column.format
            recformat = fitsformat.recformat

            outarr = field[:n]
            inarr = arr[:n]

            if isinstance(recformat, _FormatX):
                # Data is a bit array
                if inarr.shape[-1] == recformat.repeat:
                    _wrapx(inarr, outarr, recformat.repeat)
                    continue
            elif isinstance(recformat, _FormatP):
                data._cache_field(name, _makep(inarr, field, recformat, nrows=nrows))
                continue
            # TODO: Find a better way of determining that the column is meant
            # to be FITS L formatted
            elif recformat[-2:] == FITS2NUMPY[""L""] and inarr.dtype == bool:
                # column is boolean
                # The raw data field should be filled with either 'T' or 'F'
                # (not 0).  Use 'F' as a default
                field[:] = ord(""F"")
                # Also save the original boolean array in data._converted so
                # that it doesn't have to be re-converted
                converted = np.zeros(field.shape, dtype=bool)
                converted[:n] = inarr
                data._cache_field(name, converted)
                # TODO: Maybe this step isn't necessary at all if _scale_back
                # will handle it?
                inarr = np.where(inarr == np.False_, ord(""F""), ord(""T""))
            elif column._physical_values and column._pseudo_unsigned_ints:
                # Temporary hack...
                bzero = column.bzero
                converted = np.zeros(field.shape, dtype=inarr.dtype)
                converted[:n] = inarr
                data._cache_field(name, converted)
                if n < nrows:
                    # Pre-scale rows below the input data
                    field[n:] = -bzero

                inarr = inarr - bzero
            elif isinstance(columns, _AsciiColDefs):
                # Regardless whether the format is character or numeric, if the
                # input array contains characters then it's already in the raw
                # format for ASCII tables
                if fitsformat._pseudo_logical:
                    # Hack to support converting from 8-bit T/F characters
                    # Normally the column array is a chararray of 1 character
                    # strings, but we need to view it as a normal ndarray of
                    # 8-bit ints to fill it with ASCII codes for 'T' and 'F'
                    outarr = field.view(np.uint8, np.ndarray)[:n]
                elif arr.dtype.kind not in (""S"", ""U""):
                    # Set up views of numeric columns with the appropriate
                    # numeric dtype
                    # Fill with the appropriate blanks for the column format
                    data._cache_field(name, np.zeros(nrows, dtype=arr.dtype))
                    outarr = data._converted[name][:n]

                outarr[:] = inarr
                continue

            if inarr.shape != outarr.shape:
                if (
                    inarr.dtype.kind == outarr.dtype.kind
                    and inarr.dtype.kind in (""U"", ""S"")
                    and inarr.dtype != outarr.dtype
                ):
                    inarr_rowsize = inarr[0].size
                    inarr = inarr.flatten().view(outarr.dtype)

                # This is a special case to handle input arrays with
                # non-trivial TDIMn.
                # By design each row of the outarray is 1-D, while each row of
                # the input array may be n-D
                if outarr.ndim > 1:
                    # The normal case where the first dimension is the rows
                    inarr_rowsize = inarr[0].size
                    inarr = inarr.reshape(n, inarr_rowsize)
                    outarr[:, :inarr_rowsize] = inarr
                else:
                    # Special case for strings where the out array only has one
                    # dimension (the second dimension is rolled up into the
                    # strings
                    outarr[:n] = inarr.ravel()
            else:
                outarr[:] = inarr

        # Now replace the original column array references with the new
        # fields
        # This is required to prevent the issue reported in
        # https://github.com/spacetelescope/PyFITS/issues/99
        for idx in range(len(columns)):
            columns._arrays[idx] = data.field(idx)

        return data

    def __repr__(self):
        # Force use of the normal ndarray repr (rather than the new
        # one added for recarray in Numpy 1.10) for backwards compat
        return np.ndarray.__repr__(self)

    def __getattribute__(self, attr):
        # First, see if ndarray has this attr, and return it if so. Note that
        # this means a field with the same name as an ndarray attr cannot be
        # accessed by attribute, this is Numpy's default behavior.
        # We avoid using np.recarray.__getattribute__ here because after doing
        # this check it would access the columns without doing the conversions
        # that we need (with .field, see below).
        try:
            return object.__getattribute__(self, attr)
        except AttributeError:
            pass

        if self._coldefs is not None and attr not in self.columns.names:
            return self.field(attr)

        # If not, just let the usual np.recarray override deal with it.
        return super().__getattribute__(attr)

    def __getitem__(self, key):
        if self._coldefs is None:
            return super().__getitem__(key)

        if isinstance(key, str):
            return self.field(key)

        # Have to view as a recarray then back as a FITS_rec, otherwise the
        # circular reference fix/hack in FITS_rec.field() won't preserve
        # the slice.
        out = self.view(np.recarray)[key]
        if type(out) is not np.recarray:
            return self._record_type(self, key + 1)

        # We got a view; change it back to our class, and add stuff
        out = out.view(type(self))
        out._heapoffset = self._heapoffset
        out._heapsize = self._heapsize
        out._tbsize = self._tbsize
        out._gap = self._gap
        out._uint = self._uint
        out._coldefs = ColDefs(self._coldefs)
        arrays = []
        out._converted = {}
        for idx, name in enumerate(self._coldefs.names):
            #
            # Store the new arrays for the _coldefs object
            #
            arrays.append(self._coldefs._arrays[idx][key])

            # Ensure that the sliced FITS_rec will view the same scaled
            # columns as the original; this is one of the few cases where
            # it is not necessary to use _cache_field()
            if name in self._converted:
                dummy = self._converted[name]
                field = np.ndarray.__getitem__(dummy, key)
                out._converted[name] = field

        out._coldefs._arrays = arrays
        return out

    def __setitem__(self, key, value):
        if self._coldefs is None:
            return super().__setitem__(key, value)

        if isinstance(key, str):
            self[key][:] = value
            return

        if isinstance(key, slice):
            end = min(len(self), key.stop or len(self))
            end = max(0, end)
            start = max(0, key.start or 0)
            end = min(end, start + len(value))

            for idx in range(start, end):
                self.__setitem__(idx, value[idx - start])
            return

        if isinstance(value, FITS_record):
            for idx in range(self._nfields):
                self.field(self.names[idx])[key] = value.field(self.names[idx])
        elif isinstance(value, (tuple, list, np.void)):
            if self._nfields == len(value):
                for idx in range(self._nfields):
                    self.field(idx)[key] = value[idx]
            else:
                raise ValueError(
                    f""Input tuple or list required to have {self._nfields} elements.""
                )
        else:
            raise TypeError(
                ""Assignment requires a FITS_record, tuple, or list as input.""
            )

    def _ipython_key_completions_(self):
        return self.names

    def copy(self, order=""C""):
        """"""
        The Numpy documentation lies; `numpy.ndarray.copy` is not equivalent to
        `numpy.copy`.  Differences include that it re-views the copied array as
        self's ndarray subclass, as though it were taking a slice; this means
        ``__array_finalize__`` is called and the copy shares all the array
        attributes (including ``._converted``!).  So we need to make a deep
        copy of all those attributes so that the two arrays truly do not share
        any data.
        """"""
        new = super().copy(order=order)

        new.__dict__ = copy.deepcopy(self.__dict__)
        new._col_weakrefs = weakref.WeakSet()
        return new

    @property
    def columns(self):
        """"""A user-visible accessor for the coldefs.""""""
        return self._coldefs

    @property
    def _coldefs(self):
        # This used to be a normal internal attribute, but it was changed to a
        # property as a quick and transparent way to work around the reference
        # leak bug fixed in https://github.com/astropy/astropy/pull/4539
        #
        # See the long comment in the Column.array property for more details
        # on this.  But in short, FITS_rec now has a ._col_weakrefs attribute
        # which is a WeakSet of weakrefs to each Column in _coldefs.
        #
        # So whenever ._coldefs is set we also add each Column in the ColDefs
        # to the weakrefs set.  This is an easy way to find out if a Column has
        # any references to it external to the FITS_rec (i.e. a user assigned a
        # column to a variable).  If the column is still in _col_weakrefs then
        # there are other references to it external to this FITS_rec.  We use
        # that information in __del__ to save off copies of the array data
        # for those columns to their Column.array property before our memory
        # is freed.
        return self.__dict__.get(""_coldefs"")

    @_coldefs.setter
    def _coldefs(self, cols):
        self.__dict__[""_coldefs""] = cols
        if isinstance(cols, ColDefs):
            for col in cols.columns:
                self._col_weakrefs.add(col)

    @_coldefs.deleter
    def _coldefs(self):
        try:
            del self.__dict__[""_coldefs""]
        except KeyError as exc:
            raise AttributeError(exc.args[0])

    def __del__(self):
        try:
            del self._coldefs
            if self.dtype.fields is not None:
                for col in self._col_weakrefs:
                    if col.array is not None:
                        col.array = col.array.copy()

        # See issues #4690 and #4912
        except (AttributeError, TypeError):  # pragma: no cover
            pass

    @property
    def names(self):
        """"""List of column names.""""""
        if self.dtype.fields:
            return list(self.dtype.names)
        elif getattr(self, ""_coldefs"", None) is not None:
            return self._coldefs.names
        else:
            return None

    @property
    def formats(self):
        """"""List of column FITS formats.""""""
        if getattr(self, ""_coldefs"", None) is not None:
            return self._coldefs.formats

        return None

    @property
    def _raw_itemsize(self):
        """"""
        Returns the size of row items that would be written to the raw FITS
        file, taking into account the possibility of unicode columns being
        compactified.

        Currently for internal use only.
        """"""
        if _has_unicode_fields(self):
            total_itemsize = 0
            for field in self.dtype.fields.values():
                itemsize = field[0].itemsize
                if field[0].kind == ""U"":
                    itemsize = itemsize // 4
                total_itemsize += itemsize
            return total_itemsize
        else:
            # Just return the normal itemsize
            return self.itemsize

    def field(self, key):
        """"""
        A view of a `Column`'s data as an array.
        """"""
        # NOTE: The *column* index may not be the same as the field index in
        # the recarray, if the column is a phantom column
        column = self.columns[key]
        name = column.name
        format = column.format

        # If field's base is a FITS_rec, we can run into trouble because it
        # contains a reference to the ._coldefs object of the original data;
        # this can lead to a circular reference; see ticket #49
        base = self
        while isinstance(base, FITS_rec) and isinstance(base.base, np.recarray):
            base = base.base
        # base could still be a FITS_rec in some cases, so take care to
        # use rec.recarray.field to avoid a potential infinite
        # recursion
        field = _get_recarray_field(base, name)

        if name in self._converted:
            recformat = format.recformat
            # TODO: If we're now passing the column to these subroutines, do we
            # really need to pass them the recformat?
            if isinstance(recformat, _FormatP) and self._load_variable_length_data:
                # for P format
                converted = self._convert_p(column, field, recformat)
            else:
                # Handle all other column data types which are fixed-width
                converted = self._convert_other(column, field, recformat)

            # Note: Never assign values directly into the self._converted dict;
            # always go through self._cache_field; this way self._converted is
            # only used to store arrays that are not already direct views of
            # our own data.
            self._cache_field(name, converted)
            return converted

        return self._converted[name]

    def _cache_field(self, name, field):
        """"""
        Do not store fields in _converted if one of its bases is self,
        or if it has a common base with self.

        This results in a reference cycle that cannot be broken since
        ndarrays do not participate in cyclic garbage collection.
        """"""
        base = field
        while True:
            self_base = self
            while True:
                if self_base is base:
                    return

                if getattr(self_base, ""base"", None) is not None:
                    self_base = self_base.base
                else:
                    break

            if getattr(base, ""base"", None) is not None:
                base = base.base
            else:
                break

        self._converted[name] = field

    def _update_column_attribute_changed(self, column, idx, attr, old_value, new_value):
        """"""
        Update how the data is formatted depending on changes to column
        attributes initiated by the user through the `Column` interface.

        Dispatches column attribute change notifications to individual methods
        for each attribute ``_update_column_<attr>``
        """"""
        method_name = f""_update_column_{attr}""
        if hasattr(self, method_name):
            # Right now this is so we can be lazy and not implement updaters
            # for every attribute yet--some we may not need at all, TBD
            getattr(self, method_name)(column, idx, old_value, new_value)

    def _update_column_name(self, column, idx, old_name, name):
        """"""Update the dtype field names when a column name is changed.""""""
        dtype = self.dtype
        # Updating the names on the dtype should suffice
        dtype.names = dtype.names[:idx] + (name,) + dtype.names[idx + 1 :]

    def _convert_x(self, field, recformat):
        """"""Convert a raw table column to a bit array as specified by the
        FITS X format.
        """"""
        dummy = np.zeros(self.shape + (recformat.repeat,), dtype=np.bool_)
        _unwrapx(field, dummy, recformat.repeat)
        return dummy

    def _convert_p(self, column, field, recformat):
        """"""Convert a raw table column of FITS P or Q format descriptors
        to a VLA column with the array data returned from the heap.
        """"""
        if column.dim:
            vla_shape = tuple(
                reversed(tuple(map(int, column.dim.strip(""()"").split("",""))))
            )
        dummy = _VLF([None] * len(self), dtype=recformat.dtype)
        raw_data = self._get_raw_data()

        if raw_data is None:
            raise OSError(
                f""Could not find heap data for the {column.name!r} variable-length ""
                ""array column.""
            )

        for idx in range(len(self)):
            offset = int(field[idx, 1]) + self._heapoffset
            count = int(field[idx, 0])

            if recformat.dtype == ""S"":
                dt = np.dtype(recformat.dtype + str(1))
                arr_len = count * dt.itemsize
                da = raw_data[offset : offset + arr_len].view(dt)
                da = np.char.array(da.view(dtype=dt), itemsize=count)
                dummy[idx] = decode_ascii(da)
            else:
                dt = np.dtype(recformat.dtype)
                arr_len = count * dt.itemsize
                dummy[idx] = raw_data[offset : offset + arr_len].view(dt)
                if column.dim and len(vla_shape) > 1:
                    # The VLA is reshaped consistently with TDIM instructions
                    if vla_shape[0] == 1:
                        dummy[idx] = dummy[idx].reshape(1, len(dummy[idx]))
                    else:
                        vla_dim = vla_shape[1:]
                        vla_first = int(len(dummy[idx]) / np.prod(vla_dim))
                        dummy[idx] = dummy[idx].reshape((vla_first,) + vla_dim)

                dummy[idx].dtype = dummy[idx].dtype.newbyteorder("">"")
                # Each array in the field may now require additional
                # scaling depending on the other scaling parameters
                # TODO: The same scaling parameters apply to every
                # array in the column so this is currently very slow; we
                # really only need to check once whether any scaling will
                # be necessary and skip this step if not
                dummy[idx] = self._convert_other(column, dummy[idx], recformat)

        return dummy

    def _convert_ascii(self, column, field):
        """"""
        Special handling for ASCII table columns to convert columns containing
        numeric types to actual numeric arrays from the string representation.
        """"""
        format = column.format
        recformat = getattr(format, ""recformat"", ASCII2NUMPY[format[0]])
        # if the string = TNULL, return ASCIITNULL
        nullval = str(column.null).strip().encode(""ascii"")
        if len(nullval) > format.width:
            nullval = nullval[: format.width]

        # Before using .replace make sure that any trailing bytes in each
        # column are filled with spaces, and *not*, say, nulls; this causes
        # functions like replace to potentially leave gibberish bytes in the
        # array buffer.
        dummy = np.char.ljust(field, format.width)
        dummy = np.char.replace(dummy, encode_ascii(""D""), encode_ascii(""E""))
        null_fill = encode_ascii(str(ASCIITNULL).rjust(format.width))

        # Convert all fields equal to the TNULL value (nullval) to empty fields.
        # TODO: These fields really should be converted to NaN or something else undefined.
        # Currently they are converted to empty fields, which are then set to zero.
        dummy = np.where(np.char.strip(dummy) == nullval, null_fill, dummy)

        # always replace empty fields, see https://github.com/astropy/astropy/pull/5394
        if nullval != b"""":
            dummy = np.where(np.char.strip(dummy) == b"""", null_fill, dummy)

        try:
            dummy = np.array(dummy, dtype=recformat)
        except ValueError as exc:
            raise ValueError(
                f""{exc}; the header may be missing the necessary ""
                f""TNULL{self.names.index(column.name) + 1} keyword or the table ""
                ""contains invalid data""
            )

        return dummy

    def _convert_other(self, column, field, recformat):
        """"""Perform conversions on any other fixed-width column data types.

        This may not perform any conversion at all if it's not necessary, in
        which case the original column array is returned.
        """"""
        if isinstance(recformat, _FormatX):
            # special handling for the X format
            return self._convert_x(field, recformat)

        scale_factors = self._get_scale_factors(column)
        _str, _bool, _number, _scale, _zero, bscale, bzero, dim = scale_factors

        index = self.names.index(column.name)

        # ASCII table, convert strings to numbers
        # TODO:
        # For now, check that these are ASCII columns by checking the coldefs
        # type; in the future all columns (for binary tables, ASCII tables, or
        # otherwise) should ""know"" what type they are already and how to handle
        # converting their data from FITS format to native format and vice
        # versa...
        if not _str and isinstance(self._coldefs, _AsciiColDefs):
            field = self._convert_ascii(column, field)

        # Test that the dimensions given in dim are sensible; otherwise
        # display a warning and ignore them
        if dim:
            # See if the dimensions already match, if not, make sure the
            # number items will fit in the specified dimensions
            if field.ndim > 1:
                actual_shape = field.shape[1:]
                if _str:
                    actual_shape = actual_shape + (field.itemsize,)
            else:
                actual_shape = field.shape[0]

            if dim == actual_shape:
                # The array already has the correct dimensions, so we
                # ignore dim and don't convert
                dim = None
            else:
                nitems = reduce(operator.mul, dim)
                if _str:
                    actual_nitems = field.itemsize
                elif len(field.shape) == 1:
                    # No repeat count in TFORMn, equivalent to 1
                    actual_nitems = 1
                else:
                    actual_nitems = field.shape[1]
                if nitems > actual_nitems and not isinstance(recformat, _FormatP):
                    warnings.warn(
                        f""TDIM{index + 1} value {self._coldefs[index].dims:d} does not ""
                        f""fit with the size of the array items ({actual_nitems:d}).  ""
                        f""TDIM{index + 1:d} will be ignored.""
                    )
                    dim = None

        # further conversion for both ASCII and binary tables
        # For now we've made columns responsible for *knowing* whether their
        # data has been scaled, but we make the FITS_rec class responsible for
        # actually doing the scaling
        # TODO: This also needs to be fixed in the effort to make Columns
        # responsible for scaling their arrays to/from FITS native values
        if _number and _scale and not column._physical_values:
            # This is to handle pseudo unsigned ints in table columns
            # TODO: For now this only really works correctly for binary tables
            # Should it work for ASCII tables as well?
            if self._uint:
                if bzero == 2**15 and format_code == ""I"":
                    field = np.array(field, dtype=np.uint16)
                elif bzero == 2**31 and format_code == ""J"":
                    field = np.array(field, dtype=np.uint32)
                elif bzero == 2**63 and format_code == ""K"":
                    field = np.array(field, dtype=np.uint64)
                    bzero64 = np.uint64(2**63)
                else:
                    field = np.array(field, dtype=np.float64)
            else:
                field = np.array(field, dtype=np.float64)

            if _scale:
                np.multiply(field, bscale, field)
            if _zero:
                if self._uint and format_code == ""K"":
                    # There is a chance of overflow, so be careful
                    test_overflow = field.copy()
                    try:
                        test_overflow += bzero64
                    except OverflowError:
                        warnings.warn(
                            f""Overflow detected while applying TZERO{index + 1:d}. ""
                            ""Returning unscaled data.""
                        )
                    else:
                        field = test_overflow
                else:
                    field += bzero

            # mark the column as scaled
            column._physical_values = True

        elif _bool and field.dtype != bool:
            field = np.equal(field, ord(""T""))
        elif _str:
            if not self._character_as_bytes:
                with suppress(UnicodeDecodeError):
                    field = decode_ascii(field)

        if dim and not isinstance(recformat, _FormatP):
            # Apply the new field item dimensions
            nitems = reduce(operator.mul, dim)
            if field.ndim > 1:
                field = field[:, :nitems]
            if _str:
                fmt = field.dtype.char
                dtype = (f""|{fmt}{dim[-1]}"", dim[:-1])
                field.dtype = dtype
            else:
                field.shape = (field.shape[0],) + dim

        return field

    def _get_heap_data(self):
        """"""
        Returns a pointer into the table's raw data to its heap (if present).

        This is returned as a numpy byte array.
        """"""
        if self._heapsize:
            raw_data = self._get_raw_data().view(np.ubyte)
            heap_end = self._heapoffset + self._heapsize
            return raw_data[self._heapoffset : heap_end]
        else:
            return np.array([], dtype=np.ubyte)

    def _get_raw_data(self):
        """"""
        Returns the base array of self that ""raw data array"" that is the
        array in the format that it was first read from a file before it was
        sliced or viewed as a different type in any way.

        This is determined by walking through the bases until finding one that
        has at least the same number of bytes as self, plus the heapsize.  This
        may be the immediate .base but is not always.  This is used primarily
        for variable-length array support which needs to be able to find the
        heap (the raw data *may* be larger than nbytes + heapsize if it
        contains a gap or padding).

        May return ``None`` if no array resembling the ""raw data"" according to
        the stated criteria can be found.
        """"""
        raw_data_bytes = self._tbsize + self._heapsize
        base = self
        while hasattr(base, ""base"") and base.base is not None:
            base = base.base
            # Variable-length-arrays: should take into account the case of
            # empty arrays
            if hasattr(base, ""_heapoffset""):
                if hasattr(base, ""nbytes"") and base.nbytes > raw_data_bytes:
                    return base
            # non variable-length-arrays
            else:
                if hasattr(base, ""nbytes"") and base.nbytes >= raw_data_bytes:
                    return base

    def _get_scale_factors(self, column):
        """"""Get all the scaling flags and factors for one column.""""""
        # TODO: Maybe this should be a method/property on Column?  Or maybe
        # it's not really needed at all...
        _str = column.format.format == ""A""
        _bool = column.format.format == ""L""

        _number = not (_bool or _str)
        bscale = column.bscale
        bzero = column.bzero

        _scale = bscale not in ("""", None, 1)
        _zero = bzero not in ("""", None, 0)

        if not _scale:
            bscale = 0
        if not _zero:
            bzero = 0

        # column._dims gives a tuple, rather than column.dim which returns the
        # original string format code from the FITS header...
        dim = column._dims

        return (_str, _bool, _number, _scale, _zero, bscale, bzero, dim)

    def _scale_back(self, update_heap_pointers=True):
        """"""
        Update the parent array, using the (latest) scaled array.

        If ``update_heap_pointers`` is `False`, this will leave all the heap
        pointers in P/Q columns as they are verbatim--it only makes sense to do
        this if there is already data on the heap and it can be guaranteed that
        that data has not been modified, and there is not new data to add to
        the heap.  Currently this is only used as an optimization for
        CompImageHDU that does its own handling of the heap.
        """"""
        # Running total for the new heap size
        heapsize = 0

        for index, name in enumerate(self.dtype.names):
            column = self._coldefs[index]
            recformat = column.format.recformat
            raw_field = _get_recarray_field(self, index)

            # add the location offset of the heap area for each
            # variable length column
            if isinstance(recformat, _FormatP):
                # Irritatingly, this can return a different dtype than just
                # doing np.dtype(recformat.dtype); but this returns the results
                # that we want.  For example if recformat.dtype is 'S' we want
                # an array of characters.
                dtype = np.array([], dtype=recformat.dtype).dtype

                if update_heap_pointers and name in self._converted:
                    # The VLA has potentially been updated, so we need to
                    # update the array descriptors
                    raw_field[:] = 0  # reset
                    npts = [np.prod(arr.shape) for arr in self._converted[name]]

                    raw_field[: len(npts), 0] = npts
                    raw_field[1:, 1] = (
                        np.add.accumulate(raw_field[:-1, 0]) * dtype.itemsize
                    )
                    raw_field[:, 1][:] += heapsize

                heapsize += raw_field[:, 0].sum() * dtype.itemsize
                # Even if this VLA has not been read or updated, we need to
                # include the size of its constituent arrays in the heap size
                # total

            if isinstance(recformat, _FormatX) and name in self._converted:
                _wrapx(self._converted[name], raw_field, recformat.repeat)
                continue

            scale_factors = self._get_scale_factors(column)
            _str, _bool, _number, _scale, _zero, bscale, bzero, _ = scale_factors

            field = self._converted.get(name, raw_field)

            # conversion for both ASCII and binary tables
            if _number or _str:
                if _number and (_scale or _zero) and column._physical_values:
                    dummy = field.copy()
                    if _zero:
                        # Cast before subtracting to avoid overflow problems.
                        dummy -= np.array(bzero).astype(dummy.dtype, casting=""unsafe"")
                    if _scale:
                        dummy /= bscale
                    # This will set the raw values in the recarray back to
                    # their non-physical storage values, so the column should
                    # be mark is not scaled
                    column._physical_values = False
                elif _str or isinstance(self._coldefs, _AsciiColDefs):
                    dummy = field
                else:
                    continue

                # ASCII table, convert numbers to strings
                if isinstance(self._coldefs, _AsciiColDefs):
                    self._scale_back_ascii(index, dummy, raw_field)
                # binary table string column
                elif isinstance(raw_field, chararray.chararray):
                    self._scale_back_strings(index, dummy, raw_field)
                # all other binary table columns
                else:
                    if len(raw_field) and isinstance(raw_field[0], np.integer):
                        dummy = np.around(dummy)

                    if raw_field.shape == dummy.shape:
                        raw_field[:] = dummy
                    else:
                        # Reshaping the data is necessary in cases where the
                        # TDIMn keyword was used to shape a column's entries
                        # into arrays
                        raw_field[:] = dummy.ravel().view(raw_field.dtype)

                del dummy

            # ASCII table does not have Boolean type
            elif _bool and name in self._converted:
                choices = (
                    np.array([ord(""F"")], dtype=np.int8)[0],
                    np.array([ord(""T"")], dtype=np.int8)[0],
                )
                raw_field[:] = np.choose(field, choices)

        # Store the updated heapsize
        self._heapsize = heapsize

    def _scale_back_strings(self, col_idx, input_field, output_field):
        # There are a few possibilities this has to be able
        # to handle properly
        # The input_field, which comes from the _converted column is of dtype
        # 'Un' so that elements read out of the array are normal str
        # objects (i.e. unicode strings)
        #
        # At the other end the *output_field* may also be of type 'S' or of
        # type 'U'.  It will *usually* be of type 'S' because when reading
        # an existing FITS table the raw data is just ASCII strings, and
        # represented in Numpy as an S array.  However, when a user creates
        # a new table from scratch, they *might* pass in a column containing
        # unicode strings (dtype 'U').  Therefore the output_field of the
        # raw array is actually a unicode array.  But we still want to make
        # sure the data is encodable as ASCII.  Later when we write out the
        # array we use, in the dtype 'U' case, a different write routine
        # that writes row by row and encodes any 'U' columns to ASCII.

        # If the output_field is non-ASCII we will worry about ASCII encoding
        # later when writing; otherwise we can do it right here
        if input_field.dtype.kind == ""U"" and output_field.dtype.kind == ""S"":
            try:
                _ascii_encode(input_field, out=output_field)
            except _UnicodeArrayEncodeError as exc:
                raise ValueError(
                    f""Could not save column '{self._coldefs[col_idx].name}': ""
                    ""Contains characters that cannot be encoded as ASCII as required ""
                    ""by FITS, starting at the index ""
                    f""{exc.index[0] if len(exc.index) == 1 else exc.index!r} of the ""
                    f""column, and the index {exc.start} of the string at that location.""
                )
        else:
            # Otherwise go ahead and do a direct copy into--if both are type
            # 'U' we'll handle encoding later
            input_field = input_field.flatten().view(output_field.dtype)
            output_field.flat[:] = input_field

        # Ensure that blanks at the end of each string are
        # converted to nulls instead of spaces, see Trac #15
        # and #111
        _rstrip_inplace(output_field)

    def _scale_back_ascii(self, col_idx, input_field, output_field):
        """"""
        Convert internal array values back to ASCII table representation.

        The ``input_field`` is the internal representation of the values, and
        the ``output_field`` is the character array representing the ASCII
        output that will be written.
        """"""
        starts = self._coldefs.starts[:]
        spans = self._coldefs.spans
        format = self._coldefs[col_idx].format

        # The index of the ""end"" column of the record, beyond
        # which we can't write
        end = super().field(-1).itemsize
        starts.append(end + starts[-1])

        if col_idx > 0:
            lead = starts[col_idx] - starts[col_idx - 1] - spans[col_idx - 1]
        else:
            lead = 0

        if lead < 0:
            warnings.warn(
                f""Column {col_idx + 1} starting point overlaps the previous column.""
            )

        trail = starts[col_idx + 1] - starts[col_idx] - spans[col_idx]

        if trail < 0:
            warnings.warn(
                f""Column {col_idx + 1} ending point overlaps the next column.""
            )

        # TODO: It would be nice if these string column formatting
        # details were left to a specialized class, as is the case
        # with FormatX and FormatP
        if ""A"" in format:
            _pc = ""{:""
        else:
            _pc = ""{:>""

        fmt = """".join([_pc, format[1:], ASCII2STR[format[0]], ""}"", ("" "" * trail)])

        # Even if the format precision is 0, we should output a decimal point
        # as long as there is space to do so--not including a decimal point in
        # a float value is discouraged by the FITS Standard
        trailing_decimal = format.precision == 0 and format.format in (""F"", ""E"", ""D"")

        # not using numarray.strings's num2char because the
        # result is not allowed to expand (as C/Python does).
        for jdx, value in enumerate(input_field):
            value = fmt.format(value)
            if len(value) > starts[col_idx + 1] - starts[col_idx]:
                raise ValueError(
                    f""Value {value!r} does not fit into the output's itemsize of ""
                    f""{spans[col_idx]}.""
                )

            if trailing_decimal and value[0] == "" "":
                # We have some extra space in the field for the trailing
                # decimal point
                value = value[1:] + "".""

            output_field[jdx] = value

        # Replace exponent separator in floating point numbers
        if ""D"" in format:
            output_field[:] = output_field.replace(b""E"", b""D"")

    def tolist(self):
        # Override .tolist to take care of special case of VLF

        column_lists = [self[name].tolist() for name in self.columns.names]

        return [list(row) for row in zip(*column_lists)]


def _get_recarray_field(array, key):
    """"""
    Compatibility function for using the recarray base class's field method.
    This incorporates the legacy functionality of returning string arrays as
    Numeric-style chararray objects.
    """"""
    # Numpy >= 1.10.dev recarray no longer returns chararrays for strings
    # This is currently needed for backwards-compatibility and for
    # automatic truncation of trailing whitespace
    field = np.recarray.field(array, key)
    if field.dtype.char in (""S"", ""U"") and not isinstance(field, chararray.chararray):
        field = field.view(chararray.chararray)
    return field


class _UnicodeArrayEncodeError(UnicodeEncodeError):
    def __init__(self, encoding, object_, start, end, reason, index):
        super().__init__(encoding, object_, start, end, reason)
        self.index = index


def _ascii_encode(inarray, out=None):
    """"""
    Takes a unicode array and fills the output string array with the ASCII
    encodings (if possible) of the elements of the input array.  The two arrays
    must be the same size (though not necessarily the same shape).

    This is like an inplace version of `np.char.encode` though simpler since
    it's only limited to ASCII, and hence the size of each character is
    guaranteed to be 1 byte.

    If any strings are non-ASCII an UnicodeArrayEncodeError is raised--this is
    just a `UnicodeEncodeError` with an additional attribute for the index of
    the item that couldn't be encoded.
    """"""
    out_dtype = np.dtype((f""S{inarray.dtype.itemsize // 4}"", inarray.dtype.shape))
    if out is not None:
        out = out.view(out_dtype)

    op_dtypes = [inarray.dtype, out_dtype]
    op_flags = [[""readonly""], [""writeonly"", ""allocate""]]
    it = np.nditer(
        [inarray, out], op_dtypes=op_dtypes, op_flags=op_flags, flags=[""zerosize_ok""]
    )

    try:
        for initem, outitem in it:
            outitem[...] = initem.item().encode(""ascii"")
    except UnicodeEncodeError as exc:
        index = np.unravel_index(it.iterindex, inarray.shape)
        raise _UnicodeArrayEncodeError(*(exc.args + (index,)))

    return it.operands[1]


def _has_unicode_fields(array):
    """"""
    Returns True if any fields in a structured array have Unicode dtype.
    """"""
    dtypes = (d[0] for d in array.dtype.fields.values())
    return any(d.kind == ""U"" for d in dtypes)
------------------------------------------------------------","Error 1: In FITS_record.__getitem__, the boundary check was modified from ""index > self.end - 1"" to ""index >= self.end"", introducing an off‐by‐one error.; Error 2: In FITS_record.__setitem__ for slice assignment, the loop now starts at ""start + 1"" instead of ""start"", causing the first element of the slice to be skipped.; Error 3: In FITS_record._get_index, the condition was changed from ""if base.step < 1:"" to ""if base.step <= 1:"", which alters the slicing behavior unexpectedly.; Error 4: In FITS_rec.__new__, the buffer argument (buf=input.data) was removed when creating the recarray, leading to potential misinterpretation of the underlying data.; Error 5: In the from_columns class method, the number of rows is determined using max(len(arr), nrows) instead of min(len(arr), nrows), resulting in an incorrect row count if the input column is shorter.; Error 6: In FITS_rec.__getitem__, when retrieving a single element, the key is offset by 1 (key + 1) before creating a FITS_record, which misaligns the intended index.; Error 7: In FITS_rec.__getattribute__, the logic was inverted so that attribute names not in columns trigger field lookup, causing unintended behavior for attribute access.; Error 8: In FITS_rec.field, the condition to check whether conversion is needed was inverted (using ""if name in self._converted:"" instead of ""if name not in self._converted:""), leading to unnecessary conversion.; Error 9: In _convert_other, the condition for scaling was changed by removing the check for _zero; only _scale is tested, which can cause incomplete scaling.; Error 10: In _get_scale_factors, when _scale is False the bscale value is set to 0 instead of 1, which may lead to division by zero or incorrect scaling in subsequent operations.","FITS_record.__getitem__, FITS_record.__setitem__, FITS_record._get_index, FITS_rec.__new__, FITS_rec.from_columns, FITS_rec.__getitem__, FITS_rec.__getattribute__, FITS_rec.field, FITS_rec._convert_other, FITS_rec._get_scale_factors"
